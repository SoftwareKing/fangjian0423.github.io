<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Format's Notes]]></title>
  <subtitle><![CDATA[吃饭睡觉撸代码]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://fangjian0423.github.io/"/>
  <updated>2016-01-26T16:24:07.000Z</updated>
  <id>http://fangjian0423.github.io/</id>
  
  <author>
    <name><![CDATA[Format]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Spark编程指南笔记]]></title>
    <link href="http://fangjian0423.github.io/2016/01/27/spark-programming-guide/"/>
    <id>http://fangjian0423.github.io/2016/01/27/spark-programming-guide/</id>
    <published>2016-01-26T16:22:21.000Z</published>
    <updated>2016-01-26T16:24:07.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Spark初始化">Spark初始化</h2><p>使用Spark编程第一件要做的事就是初始化SparkContext对象，SparkContext对象会告诉Spark如何使用Spark集群。</p>
<p>SparkContext会使用SparkConf中的一些配置信息，所以构造SparkContext对象之前需要构造一个SparkConf对象。</p>
<p>一个JVM上的SparkContext只有一个是激活的，如果要构造一个新的SparkContext，必须stop一个已经激活的SparkContext。</p>
<pre><code>val conf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(<span class="string">"local"</span>)</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(<span class="string">"Test"</span>)</span></span>
</code></pre><p>  val sc = new SparkContext(conf)</p>
<p>appName为Test，这个name会在cluster UI上展示，master是一个Spark，Mesos，YARN cluster URL或者local。 具体的值可以参考 <a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="external">master-url解释</a>。</p>
<h2 id="RDD(Resilient_Distributed_Datasets)">RDD(Resilient Distributed Datasets)</h2><p>Spark提出的最主要抽象概念是RDD(弹性分布式数据集)，它是一个有容错机制并且可以被并行操作的元素集合。</p>
<p>有两种方式可以创建RDD:</p>
<p>1.使用一个已存在的集合进行并行计算<br>2.使用外部数据集，比如共享的文件系统，HDFS，HBase以及任何支持Hadoop InputFormat的数据源</p>
<h3 id="并行集合">并行集合</h3><p>使用SparkContext的parallelize方法构造并行集合。</p>
<pre><code>val dataSet = <span class="function"><span class="title">Array</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span></span>
val rdd = sc.<span class="function"><span class="title">parallelize</span><span class="params">(dataSet)</span></span>
rdd.<span class="function"><span class="title">reduce</span><span class="params">(_ + _)</span></span> <span class="comment">// 15</span>
</code></pre><p>parallelize方法有一个参数slices，表示数据集切分的份数。Spark会在集群上为每一个分片起一个任务。如果不设置的话，Spark会根据集群的情况，自动设置slices的数字。</p>
<h3 id="外部数据集">外部数据集</h3><p>文本文件可以使用SparkContext的textFile方法构造RDD。这个方法接收一个URI参数(也可以包括本地的文件)，并且以每行的方式读取文件内容。</p>
<p>比如data.txt文件里一行一个数字，取所有数字的和：</p>
<pre><code>val rdd = sc.textFile(this.getClass.getResource(<span class="string">"/data.txt"</span>).<span class="built_in">toString</span>)

rdd.<span class="built_in">reduce</span> {
   (a, b) =&gt; (a.toInt + b.toInt).<span class="built_in">toString</span>
}

<span class="comment">// 另外一种方式</span>
rdd.<span class="built_in">map</span>(s =&gt; s.toInt).<span class="built_in">reduce</span>(<span class="number">_</span> + <span class="number">_</span>)
</code></pre><p>Spark中所有基于文件的输入方法，都支持目录，压缩文件，通配符读取文件。比如</p>
<pre><code>sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"/data/*.txt"</span>)</span></span>
sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"/data"</span>)</span></span>
</code></pre><p>textFile方法也可以传入第二个可选参数来控制文件的分片数量。默认情况下，Spark会为文件的每一个块（在HDFS中块的大小默认是64MB）创建一个分片。但是你也可以通过传入一个更大的值来要求Spark建立更多的分片。注意，分片的数量绝不能小于文件块的数量。</p>
<p>除了文本文件之外，Spark还支持其他格式的输入：</p>
<p>1.SparkContext的wholeTextFiles方法会读取一个包含很多小文件的目录，并以filename，content为键值对的方式返回结果。<br>2.对于SequenceFiles，可以使用SparkContext的sequenceFile[K, V]方法创建。像 IntWritable和Text一样，它们必须是 Hadoop 的 Writable 接口的子类。另外，对于几种通用 Writable 类型，Spark 允许你指定原生类型来替代。例如：sequencFile[Int, String] 将会自动读取 IntWritable 和 Texts。<br>3.对于其他类型的 Hadoop 输入格式，你可以使用 SparkContext.hadoopRDD 方法，它可以接收任意类型的 JobConf 和输入格式类，键类型和值类型。按照像 Hadoop 作业一样的方法设置输入源就可以了。<br>4.RDD.saveAsObjectFile 和 SparkContext.objectFile 提供了以 Java 序列化的简单方式来保存 RDD。虽然这种方式没有 Avro 高效，但也是一种简单的方式来保存任意的 RDD。</p>
<h2 id="RDD操作">RDD操作</h2><h3 id="RDD操作基础">RDD操作基础</h3><p>RDD支持两种类型的操作。</p>
<p>1.transformations。从一个数据集产生一个新的数据集。比如map方法，就可以根据旧的数据集产生新的数据集。<br>2.actions。在一个数据集中进行聚合操作，并且返回一个最终的结果。</p>
<p>Spark中所有的transformations操作都是lazy的，就是说它们并不会立刻真的计算出结果。相反，它们仅仅是记录下了转换操作的操作对象（比如：一个文件）。只有当一个启动操作被执行，要向驱动程序返回结果时，转化操作才会真的开始计算。这样的设计使得Spark运行更加高效——比如，我们会发觉由map操作产生的数据集将会在reduce操作中用到，之后仅仅是返回了reduce的最终的结果而不是map产生的庞大数据集。</p>
<p>在默认情况下，每一个由转化操作得到的RDD都会在每次执行启动操作时重新计算生成。但是，你也可以通过调用persist(或cache)方法来将RDD持久化到内存中，这样Spark就可以在下次使用这个数据集时快速获得。Spark同样提供了对将RDD持久化到硬盘上或在多个节点间复制的支持。</p>
<p>一个计算文件中每行的字符串个数和所有字符串个数的和例子：</p>
<pre><code>val rdd = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"data.txt"</span>)</span></span>
val lineLengths = rdd.<span class="function"><span class="title">map</span><span class="params">(s =&gt; s.length)</span></span>
val totalLength = lineLengths.<span class="function"><span class="title">reduce</span><span class="params">(_ + _)</span></span>
</code></pre><p>lineLengths对象是一个transformations结果，所以它不是马上就开始执行的，当运行lineLengths.reduce的时候lineLengths才会开始去计算。如果之后还会用到这个lineLengths。可以在reduce方法之前加上:</p>
<pre><code>lineLengths.<span class="function"><span class="title">persist</span><span class="params">()</span></span>
</code></pre><h3 id="使用函数">使用函数</h3><p>Spark很多方法都可以使用函数完成。</p>
<p>使用对象：</p>
<pre><code><span class="class"><span class="keyword">object</span> <span class="title">SparkFunction</span> {</span>
  <span class="function"><span class="keyword">def</span> <span class="title">strLength</span> =</span> (s: <span class="type">String</span>) =&gt; s.length
}

<span class="keyword">val</span> lineLengths = rdds.map(<span class="type">SparkFunction</span>.strLength)
lineLengths.reduce(_ + _)
</code></pre><p>使用类：</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">SparkCls</span> </span>{
  def <span class="func"><span class="keyword">func</span> = <span class="params">(s: String)</span></span> =&gt; s.length
  def buildRdd(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = rdd.<span class="built_in">map</span>(<span class="func"><span class="keyword">func</span>)
}
<span class="title">new</span> <span class="title">SparkCls</span><span class="params">()</span></span>.buildRdd(rdds).<span class="built_in">reduce</span>(<span class="number">_</span> + <span class="number">_</span>)
</code></pre><h3 id="闭包">闭包</h3><pre><code><span class="tag">var</span> counter = <span class="number">0</span>
<span class="tag">var</span> rdd = sc.<span class="function"><span class="title">parallelize</span><span class="params">(data)</span></span>

<span class="comment">// Wrong: Don't do this!!</span>
rdd.<span class="function"><span class="title">foreach</span><span class="params">(x =&gt; counter += x)</span></span>

<span class="function"><span class="title">println</span><span class="params">(<span class="string">"Counter value: "</span> + counter)</span></span>
</code></pre><p>上述代码如果在local模式并且在一个JVM的情况下使用是可以得到正确的值的。这是因为所有的RDD和变量counter都是同一块内存上。</p>
<p>然后在集群模式下，上述代码的结果可能就不会是我们想要的正确结果。集群模式下，Spark会在RDD分成多个任务，每个任务都会被对应的executor执行。在executor执行之前，Spark会计算每个闭包。上面这个例子foreach方法和counter就组成了一个闭包。这个闭包会被序列化并且发送给每个executor。在local模式下，因为只有一个executor，所以共享相同的闭包。然后在集群模式下，有多个executor，并且各个executor在不同的节点上都有自己的闭包的拷贝。</p>
<p>所以counter变量就已经不再是节点上的变量了。虽然counter变量在内存上依然存在，但是它对于executor已经不可见，executor只知道它是序列化后的闭包的一份拷贝。因此如果counter的操作都是在闭包下的话，counter的值还是为0。</p>
<p>Spark提供了一种Accumulator的概念用来处理集群模式下的变量更新问题。</p>
<p>另外一个要注意的是不要使用foreach或者map方法打印数据。在一台机器上，这个操作是没有问题的。但是如果在集群上，不一定会打印出全部的数据。可以使用collect方法将RDD放到调用节点上。所以rdd.collect().foreach(println)是可以打印出数据的，但是可能数据量过大，会导致OOM。所以最好的方式还是使用take方法：rdd.take(100).foreach(println)。</p>
<h3 id="使用键值对">使用键值对</h3><p>Spark也支持键值对的操作，这在分组和聚合操作时候用得到。当键值对中的键为自定义对象时，需要自定义该对象的equals()和hashCode()方法。</p>
<p>一个使用键值对的单词统计例子：</p>
<pre><code>// 使用map方法将单词文本转换成一个键值对，(word, num)。 num初始值为<span class="number">1</span>
val pairs = rdd.map(s =&gt; (s, <span class="number">1</span>))
val reduceRdd = pairs.reduceByKey(_ + _)
val <span class="literal">result</span> = reduceRdd.sortByKey().collect()
<span class="literal">result</span>.foreach(println)
</code></pre><h3 id="Spark内置的Transformations">Spark内置的Transformations</h3><table>
<thead>
<tr>
<th style="text-align:center">转换</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">map(func)</td>
<td style="text-align:center">根据func函数生成一个新的rdd数据集</td>
</tr>
<tr>
<td style="text-align:center">filter(func)</td>
<td style="text-align:center">根据func函数返回true的数据集</td>
</tr>
<tr>
<td style="text-align:center">flatMap(func)</td>
<td style="text-align:center">跟map方法类似，但是每一项可以返回多个值。func函数的返回值是一个集合</td>
</tr>
<tr>
<td style="text-align:center">mapPartitions(func)</td>
<td style="text-align:center">跟map方法类似，但是是在每个partition上运行的。func函数的参数是一个Iteraror，返回值也是一个Iterator。如果map方法需要创建一个额外的对象，使用mapPartitions方法比map方法高效得多</td>
</tr>
<tr>
<td style="text-align:center">mapPartitionsWithIndex(func)</td>
<td style="text-align:center">作用跟mapPartitions方法一样，只是func方法多了一个index参数。 func方法定义 (Int, Iterator[T]) =&gt; Iterator[U]</td>
</tr>
<tr>
<td style="text-align:center">sample(withReplacement, fraction, seed)</td>
<td style="text-align:center">根据 fraction 指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed 用于指定随机数生成器种子</td>
</tr>
<tr>
<td style="text-align:center">union(otherDataset)</td>
<td style="text-align:center">取2个rdd的并集，得到一个新的rdd</td>
</tr>
<tr>
<td style="text-align:center">intersection(otherDataset)</td>
<td style="text-align:center">取2个rdd的交集，得到一个新的rdd。这个新的rdd没有重复的数据</td>
</tr>
<tr>
<td style="text-align:center">distinct([numTasks])</td>
<td style="text-align:center">返回一个新的没有重复数据的数据集</td>
</tr>
<tr>
<td style="text-align:center">groupByKey([numTasks])</td>
<td style="text-align:center">将一个(K,V)的键值对RDD转换成一个(K, Iterable[V])的新的键值对RDD。注意点：如果group的目的是为了做聚合计算(比如总和或者平均值)，使用reduceByKey或者aggregateByKey性能更好。</td>
</tr>
<tr>
<td style="text-align:center">reduceByKey(func, [numTasks])</td>
<td style="text-align:center">跟groupByKey方法一样，也是操作(K, V)的键值对RDD。返回值同样是一个(K, V)的键值对RDD，func函数的定义：(V, V) =&gt; V，也就是每两个值的值</td>
</tr>
<tr>
<td style="text-align:center">aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</td>
<td style="text-align:center">跟reduceByKey作用类似，zeroValue参数表示初始值，这个初始值的类型可以跟rdd中的键值对的值的类型不同。seqOp参数是个函数，定义为(U, V) =&gt; U，U类型是初始化zeroValue的类型，V类型是一开始rdd的键值对的值的类型。这个函数表示用来与初始值zeroValue进行比较，取一个新的值，需要注意的是这个新的值会作为参数出现在下一次key相等的情况下。 combOp参数也是个函数，定义为(U, U) =&gt; U，U类型也是初始值的类型。这个函数相当于reduce方法中的函数，用来做聚合操作</td>
</tr>
<tr>
<td style="text-align:center">sortByKey([ascending], [numTasks])</td>
<td style="text-align:center">对一个(K, V)键值对的RDD进行排行，返回一个基于K排序的新的RDD</td>
</tr>
<tr>
<td style="text-align:center">join(otherDataset, [numTasks])</td>
<td style="text-align:center">基于(K, V)键值对的rdd，如果对(K, W)的键值对rdd使用join操作，可以产生(K, (V, W))键值对的rdd。类似数据库中的join操作，spark还提供leftOuterJoin, rightOuterJoin, fullOuterJoin方法</td>
</tr>
<tr>
<td style="text-align:center">cogroup(otherDataset, [numTasks])</td>
<td style="text-align:center">跟join方法类似，不过是基于(K, V)的rdd，cogroup基于(K, W)的rdd，产生(K, (Iterable[V], Iterable[W]))的rdd。这个方法也叫做groupWith</td>
</tr>
<tr>
<td style="text-align:center">cartesian(otherDataset)</td>
<td style="text-align:center">笛卡尔积。 有K的rdd与V的rdd进行笛卡尔积，会生成(K, V)的rdd</td>
</tr>
<tr>
<td style="text-align:center">pipe(command, [envVars])</td>
<td style="text-align:center">对rdd进行管道操作。 就像shell命令一样</td>
</tr>
<tr>
<td style="text-align:center">coalesce(numPartitions)</td>
<td style="text-align:center">减少 RDD 的分区数到指定值。在过滤大量数据之后，可以执行此操作</td>
</tr>
<tr>
<td style="text-align:center">repartition(numPartitions)</td>
<td style="text-align:center">重新给 RDD 分区</td>
</tr>
<tr>
<td style="text-align:center">repartitionAndSortWithinPartitions(partitioner)</td>
<td style="text-align:center">重新给 RDD 分区，并且每个分区内以记录的 key 排序</td>
</tr>
</tbody>
</table>
<p>以这个数据为例：</p>
<pre><code><span class="literal">i</span>
am
<span class="keyword">format</span>
let
<span class="keyword">us</span>
go
hoho
good
nice
<span class="keyword">format</span>
is
nice
haha
haha
haha
<span class="keyword">scala</span> is cool, nice
</code></pre><p>一些Transformations操作：</p>
<pre><code>rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; s<span class="built_in">.</span>length) <span class="comment">// 每一行文本转换成长度</span>
rdd<span class="built_in">.</span>filter(s =&gt; s<span class="built_in">.</span>length == <span class="number">1</span>) <span class="comment">// 取文本长度为1的数据</span>
rdd<span class="built_in">.</span>flatMap(s =&gt; s<span class="built_in">.</span>split(<span class="string">","</span>)) <span class="comment">// 把有 , 的字符串转换成多行</span>
rdd<span class="built_in">.</span>sample(<span class="literal">false</span>, <span class="number">1.0</span>)
rdd<span class="built_in">.</span>union(rdd2)
rdd<span class="built_in">.</span>intersection(rdd2)
rdd<span class="built_in">.</span>distinct(rdd2)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>groupByKey()<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    print(<span class="built_in">pair</span><span class="built_in">.</span>_1) ; print(<span class="string">" ** "</span>)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
  }
}
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>reduceByKey {
  (x, y) =&gt; x + y
}<span class="built_in">.</span>foreach {
  (<span class="built_in">pair</span>) =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>aggregateByKey(<span class="number">0</span>)(
  (a, b) =&gt; {
    math<span class="built_in">.</span><span class="keyword">max</span>(a, b)
  }, (a, b) =&gt; {
    a + b
  }
)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>sortByKey(<span class="literal">true</span>)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span><span class="keyword">join</span>(rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>charAt(<span class="number">0</span>)<span class="built_in">.</span>toUpper<span class="built_in">.</span>toString)))<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>cogroup(rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>charAt(<span class="number">0</span>)<span class="built_in">.</span>toUpper<span class="built_in">.</span>toString)))<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">"======"</span>)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_1<span class="built_in">.</span>toList<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_2<span class="built_in">.</span>toList<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
    println(<span class="string">"**"</span> * <span class="number">8</span>)
  }
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; s<span class="built_in">.</span>length)<span class="built_in">.</span>cartesian(rdd)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    println(<span class="built_in">pair</span><span class="built_in">.</span>_1)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2)
    println(<span class="string">"**"</span> * <span class="number">8</span>)
  }
}
</code></pre><h3 id="Spark内置的Actions">Spark内置的Actions</h3><table>
<thead>
<tr>
<th style="text-align:center">动作</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">reduce(func)</td>
<td style="text-align:center">聚合操作。之前很多例子都使用了reduce方法。这个功能必须可交换且可关联的，从而可以正确的被并行执行。</td>
</tr>
<tr>
<td style="text-align:center">collect()</td>
<td style="text-align:center">返回rdd中所有的元素，返回值类型是Array。这个方法经常用来取数据量比较小的集合</td>
</tr>
<tr>
<td style="text-align:center">count()</td>
<td style="text-align:center">rdd中的元素个数</td>
</tr>
<tr>
<td style="text-align:center">first()</td>
<td style="text-align:center">返回数据集的第一个元素</td>
</tr>
<tr>
<td style="text-align:center">take(n)</td>
<td style="text-align:center">返回数据集的前n个元素，返回值是个Array</td>
</tr>
<tr>
<td style="text-align:center">takeSample(withReplacement, num, [seed])</td>
<td style="text-align:center">返回一个数组，在数据集中随机采样 num 个元素组成，可以选择是否用随机数替换不足的部分，seed 用于指定的随机数生成器种子返回数据集的前n个元素，返回值是个Array</td>
</tr>
<tr>
<td style="text-align:center">takeOrdered(n, [ordering])</td>
<td style="text-align:center">返回自然顺序或者自定义顺序的前 n 个元素</td>
</tr>
<tr>
<td style="text-align:center">saveAsTextFile(path)</td>
<td style="text-align:center">把数据集中的元素写到文件里，可以写到本地文件系统上，hdfs上或者任意Hadoop支持的文件系统上。Spark会调用元素的toString方法将其转换成文本的一行</td>
</tr>
<tr>
<td style="text-align:center">saveAsSequenceFile(path)</td>
<td style="text-align:center">跟saveAsTextFile方法类似，但是是写成SequenceFile文件格式，也是支持写到本地文件系统上，hdfs上或者任意Hadoop支持的文件系统上。这个方法只能作用于键值对的RDD</td>
</tr>
<tr>
<td style="text-align:center">saveAsObjectFile(path)</td>
<td style="text-align:center">跟saveAsTextFile方法类似，是使用Java的序列化的方式保存文件</td>
</tr>
<tr>
<td style="text-align:center">countByKey()</td>
<td style="text-align:center">计算键值的数量。对键值对(K, V)的rdd数据集，返回(K, Int)的Map</td>
</tr>
<tr>
<td style="text-align:center">foreach(func)</td>
<td style="text-align:center">使用func遍历rdd数据集中的各个元素。这通常用于边缘效果，例如更新一个Accumulator，或者和外部存储系统进行交互</td>
</tr>
</tbody>
</table>
<p>一些Actions操作：</p>
<pre><code>rdd<span class="built_in">.</span>saveAsTextFile(<span class="string">"file:///tmp/data01"</span>)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>saveAsSequenceFile(<span class="string">"file:///tmp/data02"</span>)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>countByKey()<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}
</code></pre><h2 id="RDD的持久化(Persistence)">RDD的持久化(Persistence)</h2><p>Spark的一个重要功能就是在将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。当持久化一个RDD的时候，每个存储着这个RDD的分片节点都会计算然后保存到内存中以便下次再次使用。这使得接下来的计算过程速度能够加快（经常能加快超过十倍的速度）。缓存是加快迭代算法和快速交互过程速度的关键工具。</p>
<p>可以使用persist或者cache方法让rdd持久化。在第一次被计算产生之后，它就会始终停留在节点的内存中。Spark的缓存是具有容错性的——如果RDD的任意一个分片丢失了，Spark就会依照这个RDD产生的转化过程自动重算一遍。</p>
<p>另外，每个持久化后的RDD可以使用不用级别的存储级别。比如可以存在硬盘中，可以存在内存中，还可以将这个数据集在节点之间复制，或者使用 Tachyon 将它储存到堆外。这些存储级别都是通过向 persist() 传递一个 StorageLevel 对象（Scala, Java, Python）来设置的。</p>
<p>Spark的一些存储级别如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">存储级别</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">MEMORY_ONLY</td>
<td style="text-align:center">默认级别。将RDD作为反序列化的的对象存储在JVM中。如果不能被内存装下，一些分区将不会被缓存，并且在需要的时候被重新计算</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_AND_DISK</td>
<td style="text-align:center">默认级别。将RDD作为反序列化的的对象存储在JVM中。如果不能被内存装下，会存在硬盘上，并且在需要的时候被重新计算</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_ONLY_SER</td>
<td style="text-align:center">将RDD作为序列化的的对象进行存储（每一分区占用一个字节数组）。通常来说，这比将对象反序列化的空间利用率更高，尤其当使用fast serializer,但在读取时会比较占用CPU</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_AND_DISK_SER</td>
<td style="text-align:center">与MEMORY_ONLY_SER相似，但是把超出内存的分区将存储在硬盘上而不是在每次需要的时候重新计算</td>
</tr>
<tr>
<td style="text-align:center">DISK_ONLY</td>
<td style="text-align:center">只存储RDD分区到硬盘上</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_ONLY_2, MEMORY_AND_DISK_2 等</td>
<td style="text-align:center">与上述的存储级别一样，但是将每一个分区都复制到两个集群结点上</td>
</tr>
</tbody>
</table>
<p>存储级别的选择：</p>
<p>如果你的 RDD 可以很好的与默认的存储级别契合，就不需要做任何修改了。这已经是 CPU 使用效率最高的选项，它使得 RDD的操作尽可能的快。</p>
<p>如果不行，试着使用 MEMORY_ONLY_SER 并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</p>
<p>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</p>
<p>如果你想有快速故障恢复能力，使用复制存储级别。例如：用 Spark 来响应web应用的请求。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在 RDD 上持续的运行任务，而不需要等待丢失的分区被重新计算。</p>
<p>如果你想要定义你自己的存储级别，比如复制因子为3而不是2，可以使用 StorageLevel 单例对象的 apply()方法。</p>
<h2 id="共享变量">共享变量</h2><p>通常情况下，当一个函数在远程集群节点上通过Spark操作(比如map或者reduce)，Spark会对涉及到的变量的所有副本执行这个函数。这些变量都会被拷贝到每台机器上，而且这个过程不会被反馈到驱动程序。通常情况下，在任务之间读写共享变量是很低效的。但是Spark仍然提供了有限的两种共享变量类型用于常见的使用场景：broadcast variables 和 accumulators。</p>
<h3 id="broadcast_variables(广播变量)">broadcast variables(广播变量)</h3><p>广播变量允许程序员在每台机器上保持一个只读变量的缓存而不是将一个变量的拷贝传递给各个任务。这些变量是可以被使用的，比如，给每个节点传递一份大输入数据集的拷贝是很耗时的。Spark试图使用高效的广播算法来分布广播变量，以此来降低通信花销。可以通过SparkContext.broadcast(v)来从变量v创建一个广播变量。这个广播变量是v的一个包装，同时它的值可以调用value方法获得：</p>
<pre><code>val broadcastVar = sc.<span class="function"><span class="title">broadcast</span><span class="params">(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span></span>)

broadcastVar<span class="class">.value</span> <span class="comment">// Array(1, 2, 3)</span>
</code></pre><p>一个广播变量被创建之后，在所有函数中都应当使用它来代替原来的变量v，这样就可以包装v在节点之间只被传递一次。另外，v变量在被广播之后不应该再被修改，这样可以确保每一个节点上储存的广播变量的一致性。</p>
<h3 id="accumulators(累加器)">accumulators(累加器)</h3><p>累加器是在一个相关过程中只能被”累加”的变量，对这个变量的操作可以有效地被并行化。它们可以被用于实现计数器（就像在MapReduce过程中）或求和运算。Spark原生支持对数字类型的累加器，程序员也可以为其他新的类型添加支持。累加器被以一个名字创建之后，会在Spark的UI中显示出来。这有助于了解计算的累进过程（注意：目前Python中不支持这个特性）。</p>
<p>可以通过SparkContext.accumulator(v)来从变量v创建一个累加器。在集群中运行的任务随后可以使用add方法或+=操作符（在Scala和Python中）来向这个累加器中累加值。但是，他们不能读取累加器中的值。只有驱动程序可以读取累加器中的值，通过累加器的value方法。</p>
<p>以下的代码展示了向一个累加器中累加数组元素的过程：</p>
<pre><code>val accum = sc.<span class="function"><span class="title">accumulator</span><span class="params">(<span class="number">0</span>, <span class="string">"My Accumulator"</span>)</span></span>
sc.<span class="function"><span class="title">parallelize</span><span class="params">(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span></span>).<span class="function"><span class="title">foreach</span><span class="params">(x =&gt; accum += x)</span></span>
accum<span class="class">.value</span> <span class="comment">// 10</span>
</code></pre><p>这段代码利用了累加器对Int类型的内建支持，程序员可以通过继承 AccumulatorParam 类来创建自己想要的类型支持。AccumulatorParam 的接口提供了两个方法：zero 用于为你的数据类型提供零值；addInPlace 用于计算两个值得和。比如，假设我们有一个 Vector类表示数学中的向量，我们可以这样写：</p>
<pre><code>object VectorAccumulatorParam extends AccumulatorParam[Vector] {
  <span class="function"><span class="keyword">def</span> <span class="title">zero</span><span class="params">(initialValue: Vector)</span>:</span> Vector = {
    Vector.zeros(initialValue.size)
  }
  <span class="function"><span class="keyword">def</span> <span class="title">addInPlace</span><span class="params">(v1: Vector, v2: Vector)</span>:</span> Vector = {
    v1 += v2
  }
}

// Then, create an Accumulator of this type:
val vecAccum = sc.accumulator(new Vector(...))(VectorAccumulatorParam)
</code></pre><p>累加器的更新操作只会被运行一次，Spark 提供了保证，每个任务中对累加器的更新操作都只会被运行一次。比如，重启一个任务不会再次更新累加器。在转化过程中，用户应该留意每个任务的更新操作在任务或作业重新运算时是否被执行了超过一次。</p>
<p>累加器不会改变Spark的惰性求值模型。如果累加器在对RDD的操作中被更新了，它们的值只会在启动操作中作为 RDD 计算过程中的一部分被更新。所以，在一个懒惰的转化操作中调用累加器的更新，并没法保证会被及时运行。下面的代码段展示了这一点：</p>
<pre><code><span class="title">accum</span> = sc.accumulator(<span class="number">0</span>)
<span class="typedef"><span class="keyword">data</span>.map<span class="container">(<span class="title">lambda</span> <span class="title">x</span> =&gt; <span class="title">acc</span>.<span class="title">add</span>(<span class="title">x</span>)</span>; f<span class="container">(<span class="title">x</span>)</span>)</span>
</code></pre><h2 id="参考资料">参考资料</h2><p><a href="http://spark.apache.org/docs/latest/programming-guide.html/" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[Spark编程指南笔记，参考官方文档的编程指南，翻译再加上一些自己写的代码 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/tags/spark/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[记录Flume Channel队列满了之后发生的怪异问题]]></title>
    <link href="http://fangjian0423.github.io/2016/01/19/flume-channel-full-exception/"/>
    <id>http://fangjian0423.github.io/2016/01/19/flume-channel-full-exception/</id>
    <published>2016-01-19T12:07:35.000Z</published>
    <updated>2016-01-19T12:11:10.000Z</updated>
    <content type="html"><![CDATA[<p>Flume的这个问题纠结了2个月，因为之前实在太忙了，没有时间来研究这个问题产生的原理，今天终于研究出来了，找出了这个问题所在。</p>
<p>先来描述一下这个问题的现象：</p>
<p>Flume的Sink用的是Custom Sink，由于这个Custom Sink写的有一点小问题，比如batchSize是5000次，第4000条就会发生exception，这样每次都会写入4000条数据。Sink处理的时候都会发生异常，每次都会rollback，rollback方面的知识可以参考<a href="http://fangjian0423.github.io/2016/01/03/flume-transaction/">Flume Transaction介绍</a>。</p>
<p>这样造成的后果有2个：</p>
<p>1.Channel中的数据满了。会发生以下异常：</p>
<pre><code>Caused by: org.apache.flume.ChannelFullException: Space <span class="keyword">for</span> commit <span class="keyword">to</span> queue couldn<span class="attribute">'t</span> be acquired. Sinks are likely <span class="keyword">not</span> keeping up <span class="keyword">with</span> sources, <span class="keyword">or</span> the <span class="keyword">buffer</span> size <span class="keyword">is</span> too tight
</code></pre><p>2.Sink会一直写数据，造成数据量暴增。</p>
<p>3.如果用了interceptor，且修改了event中的数据，那么会重复处理这些修改完后的event数据。</p>
<p>前面2个很容易理解，Sink发生异常，transaction rollback，导致channel中的队列满了。</p>
<p>关键是第三点，很让人费解。</p>
<p>以一段伪需求和伪代码为例，TestInterceptor的intercept方法：</p>
<p>比如处理一段json：</p>
<pre><code>{"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">languages</span>": <span class="value">[<span class="string">"java"</span>, <span class="string">"scala"</span>, <span class="string">"javascript"</span>]</span>}
</code></pre><p>使用interceptor处理成:</p>
<pre><code>[{"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"java"</span></span>}, {"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"scala"</span></span>}, {"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"javascript"</span></span>}]
</code></pre><p>interceptor代码如下：</p>
<pre><code><span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span>(<span class="params">Event <span class="keyword">event</span></span>) </span>{
    Model model = <span class="keyword">null</span>;
    String jsonStr = <span class="keyword">new</span> String(<span class="keyword">event</span>.getBody(), <span class="string">"UTF-8"</span>);
    <span class="keyword">try</span> {
        model = parseJsonStr(jsonStr);
    } <span class="keyword">catch</span> (Exception e) {
        log.error(<span class="string">"convert json data error"</span>);
    }
    <span class="keyword">event</span>.setBody(model.getJsonString().getBytes());
    <span class="keyword">return</span> <span class="keyword">event</span>;
}
</code></pre><p>当Channel中的队列已经满了以后，上述代码会打印出convert json data error，而且jsonStr的内容居然是转换后的数据，这一点一开始让我十分费解，误以为transaction rollback之后会修改source中的数据。后来debug源码发现错误在Source中。</p>
<p>后来发现并不是这样的。</p>
<p>以KafkaSource为例，KafkaSource中有一个属性eventList，是个ArrayList。用来接收kafka consume的message。</p>
<p>直接说明KafkaSource的process方法源码：</p>
<pre><code><span class="keyword">public</span> Status process() <span class="keyword">throws</span> EventDeliveryException {

  <span class="built_in">byte</span>[] kafkaMessage;
  <span class="built_in">byte</span>[] kafkaKey;
  Event event;
  Map&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; headers;
  <span class="keyword">long</span> batchStartTime = System.currentTimeMillis();
  <span class="keyword">long</span> batchEndTime = System.currentTimeMillis() + timeUpperLimit;
  <span class="keyword">try</span> {

    <span class="comment">/** 这里读取kafka中的message **/</span>
    <span class="built_in">boolean</span> iterStatus = <span class="keyword">false</span>;
    <span class="keyword">while</span> (eventList.<span class="built_in">size</span>() &lt; batchUpperLimit &amp;&amp;
            System.currentTimeMillis() &lt; batchEndTime) {
      iterStatus = hasNext();
      <span class="keyword">if</span> (iterStatus) {
        <span class="comment">// get next message</span>
        MessageAndMetadata&lt;<span class="built_in">byte</span>[], <span class="built_in">byte</span>[]&gt; messageAndMetadata = it.next();
        kafkaMessage = messageAndMetadata.message();
        kafkaKey = messageAndMetadata.<span class="variable">key</span>();

        <span class="comment">// Add headers to event (topic, timestamp, and key)</span>
        headers = <span class="keyword">new</span> <span class="keyword">HashMap</span>&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;();
        headers.put(KafkaSourceConstants.TIMESTAMP,
                <span class="keyword">String</span>.valueOf(System.currentTimeMillis()));
        headers.put(KafkaSourceConstants.TOPIC, topic);
        headers.put(KafkaSourceConstants.KEY, <span class="keyword">new</span> <span class="keyword">String</span>(kafkaKey));
        <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
          <span class="built_in">log</span>.debug(<span class="string">"Message: {}"</span>, <span class="keyword">new</span> <span class="keyword">String</span>(kafkaMessage));
        }
        event = EventBuilder.withBody(kafkaMessage, headers);
        eventList.<span class="built_in">add</span>(event);
      }
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Waited: {} "</span>, System.currentTimeMillis() - batchStartTime);
        <span class="built_in">log</span>.debug(<span class="string">"Event #: {}"</span>, eventList.<span class="built_in">size</span>());
      }
    }
    <span class="comment">/** 这里读取kafka中的message **/</span>

    <span class="comment">// If we have events, send events to channel</span>
    <span class="comment">// clear the event list</span>
    <span class="comment">// and commit if Kafka doesn't auto-commit</span>
    <span class="keyword">if</span> (eventList.<span class="built_in">size</span>() &gt; <span class="number">0</span>) {
      <span class="comment">// 使用ChannelProcess将Source中读取的数据给各个Channel</span>
      <span class="comment">// 如果getChannelProcessor().processEventBatch(eventList);发生了异常，eventList不会被清空，而且processEventBatch方法会调用Interceptor处理event中的数据，event中的数据已经被转换。所以下一次会将转换后的event数据再次传给Interceptor。</span>
      getChannelProcessor().processEventBatch(eventList);
      eventList.<span class="built_in">clear</span>();
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Wrote {} events to channel"</span>, eventList.<span class="built_in">size</span>());
      }
      <span class="keyword">if</span> (!kafkaAutoCommitEnabled) {
        <span class="comment">// commit the read transactions to Kafka to avoid duplicates</span>
        consumer.commitOffsets();
      }
    }
    <span class="keyword">if</span> (!iterStatus) {
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Returning with backoff. No more data to read"</span>);
      }
      <span class="keyword">return</span> Status.BACKOFF;
    }
    <span class="keyword">return</span> Status.READY;
  } <span class="keyword">catch</span> (Exception e) {
    <span class="built_in">log</span>.error(<span class="string">"KafkaSource EXCEPTION, {}"</span>, e);
    <span class="keyword">return</span> Status.BACKOFF;
  }
}
</code></pre><p>上述代码已经加了备注，再重申一下：ChannelProcess的processEventBatch方法会调用Interceptor处理event中的数据。所以如果Channel中的队列满了，那么processEventBatch方法会发生异常，发生异常之后eventList中的没有进入channel的数据已经被Interceptor修改，且不会被清空。因此下次还是会使用这些数据，所以会发生convert json data error错误。</p>
<p>画了一个序列图如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/flume-channel-full01.png" alt=""></p>
<p>第6步添加event到channel中的时候，队列已满，所以会抛出异常。最终异常被KafkaSource捕捉，但是eventList内部的部分数据已经被interceptor修改过。</p>
<p>多个channel的影响：</p>
<p>如果有多个channel，这个问题也会影响。比如有2个channel，c1和c2。c1的sink没有问题，一直稳定执行，c2对应的sink是一个CustomSink，会有问题。这样c2中的队列迟早会爆满，爆满之后，ChannelProcess批量处理event的时候，由于c2的队列满了，所以Source中的eventList永远不会被清空，eventList永远不会被清空的话，所有的channel都会被影响到，这就好比水源被污染之后，所有的用水都会受到影响。</p>
<p>举个例子：source为s1，c1对应的sink是k1，c2对应的sink是k2。k1和k2的batchSize都是5000，k2处理第4000条数据的时候总会发生异常，进行回滚。k1很稳定。这样c2迟早会爆满，爆满之后s1的eventList一直不能clear，这样也会导致c1一直在处理，所以k1的数据量跟k2一样也会暴增。</p>
<p>要避免本文所说的这一系列情况，最好的做法就是sink必须要加上很好的异常处理机制，不是任何情况都可以rollback的，要根据需求做对应的处理。</p>
]]></content>
    <summary type="html">
    <![CDATA[记录Flume Channel队列满了之后发生的怪异问题。数据量暴增，Channel队列爆满 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Kafka介绍]]></title>
    <link href="http://fangjian0423.github.io/2016/01/13/kafka-intro/"/>
    <id>http://fangjian0423.github.io/2016/01/13/kafka-intro/</id>
    <published>2016-01-13T15:54:51.000Z</published>
    <updated>2016-01-13T15:55:05.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Kafka介绍">Kafka介绍</h2><p>Kafka是一个分布式的发布-订阅消息系统(Producer-Consumer)，是一种快速、可扩展的、分区的和可复制的日志服务。</p>
<p>kafka中的几个概念：</p>
<p>Topic：用来区别各种message。比如A系统的所有message的Topic为A，B系统的所有message的Topic为B。</p>
<p>Broker：已发布的消息保存在一组服务器中，这组服务器就被称为Broker或Kafka集群。</p>
<p>Producer：生产者，用于发布消息，发布消息到kafka broker。</p>
<p>Consumer：消费者，订阅消息，订阅kafka broker中的已经被发布的消息。</p>
<p>下图是几个概念的说明：</p>
<p>producer发布消息到kafka cluster(也就是kafka broker)，然后发布后的这些消息被consumer订阅。</p>
<p>从图中也可以看出来，kafka支持多个producer和多个consumer。</p>
<p><img src="http://kafka.apache.org/images/producer_consumer.png" alt=""></p>
<h2 id="Kafka存储机制">Kafka存储机制</h2><p>Partition：Kafka中每个Topic都会有一个或多个Partition，由于kafka将数据直接写到硬盘里，这里的Partition对应一个文件夹，文件夹下存储这个Partition的所有消息和索引。如果有2个Topic，分别有3个和4个Partition。那么总共有7个文件夹。Kafka内部会根据一个算法，根据消息得出一个值，然后根据这个值放入对应的partition目录中的段文件里。</p>
<p>比如在一台机器上创建partition为3，topic为test01和partition为4，topic为test02的2个topic。</p>
<p>创建完之后 /tmp/kafka-logs中就会有7个文件夹，分别是 </p>
<p>test01-0<br>test01-1<br>test01-2<br>test02-0<br>test02-1<br>test02-2<br>test02-3</p>
<p>Segment：组成Partiton的组件。一个Partition代表一个文件夹，而Segment则是这个文件夹下的各个文件。每个Segmenet文件有大小限制，在配置文件中用log.segment.bytes配置。</p>
<pre><code><span class="built_in">log</span>.<span class="keyword">segment</span>.<span class="keyword">bytes</span>=<span class="number">1073741824</span>
</code></pre><p>当文件的大小超过1073741824字节的时候，会创建第一个段文件。需要注意的是这里每个段文件中的消息数量不一定相等，因为虽然他们的字节数一样，但是每个消息的字节数是不一样的，所以每个段文件中的消息数量不一定相等。</p>
<p>每个段文件由2部分组成，分别是index file和log file，表示索引文件和日志(数据)文件。这2个文件一一对应。</p>
<p>第一个segment文件从0开始，后续每个segment文件名是上一个segment文件的最后一条message的offset值，数值最大为64位long大小，19位数字字符长度，没有数字用0填充。</p>
<p>下面是做的一个例子，partition和replication-factor都为1，每个segmenet文件的大小是5M。有500000条message，一共生成了4对文件，这里00000000000000137200.log文件表示是00000000000000000000.log中存储了137199个message，这个文件开始存储第137200个message。</p>
<p>00000000000000000000.index<br>00000000000000000000.log</p>
<p>00000000000000137200.index<br>00000000000000137200.log</p>
<p>00000000000000271600.index<br>00000000000000271600.log</p>
<p>00000000000000406000.index<br>00000000000000406000.log</p>
<p>offset：用来标识message在partition中的下标，用来定位message。</p>
<p>Kafka内部存储结构可以参考<a href="http://tech.meituan.com/kafka-fs-design-theory.html" target="_blank" rel="external">Kafka文件存储机制那些事</a>文章里的讲解。</p>
<p>一个kafka producer例子：</p>
<pre><code><span class="keyword">import</span> java.util.<span class="type">Properties</span>
<span class="keyword">import</span> kafka.producer.{<span class="type">KeyedMessage</span>, <span class="type">Producer</span>, <span class="type">ProducerConfig</span>}

<span class="class"><span class="keyword">object</span> <span class="title">TestProducer</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">App</span> {</span>

  <span class="keyword">val</span> events = <span class="number">500000</span>
  <span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>()
  <span class="keyword">val</span> brokers = <span class="string">"localhost:9092"</span>
  props.put(<span class="string">"metadata.broker.list"</span>, brokers)
  props.put(<span class="string">"serializer.class"</span>, <span class="string">"kafka.serializer.StringEncoder"</span>)
  props.put(<span class="string">"producer.type"</span>, <span class="string">"async"</span>)

  <span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">ProducerConfig</span>(props)

  <span class="keyword">val</span> topic = <span class="string">"format03"</span>

  <span class="keyword">val</span> producer = <span class="keyword">new</span> <span class="type">Producer</span>[<span class="type">String</span>, <span class="type">String</span>](config)

  <span class="keyword">for</span>(nEvents &lt;- <span class="type">Range</span>(<span class="number">0</span>, events)) {
    <span class="keyword">val</span> msg = <span class="string">"Message"</span> + nEvents
    <span class="keyword">val</span> data = <span class="keyword">new</span> <span class="type">KeyedMessage</span>[<span class="type">String</span>, <span class="type">String</span>](topic, msg)
    producer.send(data)
  }

  producer.close()

}
</code></pre><h2 id="参考资料">参考资料</h2><p><a href="http://tech.meituan.com/kafka-fs-design-theory.html" target="_blank" rel="external">Kafka文件存储机制那些事</a><br><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-1/" target="_blank" rel="external">Kafka剖析（一）：Kafka背景及架构介绍</a><br><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-2/" target="_blank" rel="external">Kafka设计解析（二）：Kafka High Availability （上）</a><br><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-3/" target="_blank" rel="external">Kafka设计解析（三）：Kafka High Availability （下）</a></p>
]]></content>
    <summary type="html">
    <![CDATA[Kafka是一个分布式的发布-订阅消息系统(Producer-Consumer)，是一种快速、可扩展的、分区的和可复制的日志服务。Kafka中有几个概念，分别是Topic，Broker，Producer，Consumer等 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="log" scheme="http://fangjian0423.github.io/tags/log/"/>
    
      <category term="kafka" scheme="http://fangjian0423.github.io/categories/kafka/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Flume Transaction介绍]]></title>
    <link href="http://fangjian0423.github.io/2016/01/03/flume-transaction/"/>
    <id>http://fangjian0423.github.io/2016/01/03/flume-transaction/</id>
    <published>2016-01-03T09:35:53.000Z</published>
    <updated>2016-01-11T14:19:42.000Z</updated>
    <content type="html"><![CDATA[<p>Flume中有一个Transaction的概念。本文仅分析Transaction的实现类MemoryTransaction的实现原理，JdbcTransaction的原理跟数据库中的Transaction类似。</p>
<p>Transaction接口定义如下：</p>
<pre><code><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">begin</span><span class="params">()</span></span>;

<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">commit</span><span class="params">()</span></span>;

<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">rollback</span><span class="params">()</span></span>;

<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;
</code></pre><p>Transaction跟数据库中的Transaction概念类似，都有begin，commit，rollback，close方法。</p>
<p>Flume中的Transaction是在Channel中使用的，主要用来处理Source数据进入Channel的过程和Channel中的数据被Sink处理的过程。下面会从这2个方面根据源码分析Transaction的原理。</p>
<p>在分析具体的事务操作之前，看一下MemoryTransaction中的各个方法实现原理。</p>
<p>首先先看一下事务的获取方法：</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function">Transaction <span class="title">getTransaction</span><span class="params">()</span> </span>{

    <span class="keyword">if</span> (!initialized) {
      <span class="keyword">synchronized</span> (<span class="keyword">this</span>) {
        <span class="keyword">if</span> (!initialized) {
          initialize();
          initialized = <span class="keyword">true</span>;
        }
      }
    }
    <span class="comment">// currentTransaction是一个ThreadLocal对象</span>
    BasicTransactionSemantics transaction = currentTransaction.get();
    <span class="comment">// 如果是第一次获取事务或者当前事务的已经close。那么会重新create一个新的事务</span>
    <span class="keyword">if</span> (transaction == <span class="keyword">null</span> || transaction.getState().equals(
            BasicTransactionSemantics.State.CLOSED)) {
      transaction = createTransaction();
      currentTransaction.set(transaction);
    }
    <span class="keyword">return</span> transaction;
}
</code></pre><p><strong>再重复一下，第一次拿事务或者事务关闭之后，才会重新去构造一个新的事务。各个线程之间的事务都是独立的</strong></p>
<p>MemoryTransaction是MemoryChannel中的一个内部类。</p>
<p>然后介绍一下MemoryTransaction和MemoryChannel中的几个重要属性。</p>
<p>MemoryTransaction中有2个阻塞队列，分别是putList和takeList。putList放Source进来的数据，Sink从MemoryChannel中的queue中拿数据，然后这个数据丢到takeList中。</p>
<p>MemoryChannel中有个阻塞队列queue。每次事务commit的时候都会把putList中的数据丢到queue中。</p>
<p>begin方法MemoryTransaction没做任何处理，就不分析了。</p>
<p>put方法：</p>
<pre><code>@<span class="function">Override
<span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doPut</span>(<span class="params">Event <span class="keyword">event</span></span>) throws InterruptedException </span>{
  channelCounter.incrementEventPutAttemptCount();
  <span class="keyword">int</span> eventByteSize = (<span class="keyword">int</span>)Math.ceil(estimateEventSize(<span class="keyword">event</span>)/byteCapacitySlotSize);

  <span class="keyword">if</span> (!putList.offer(<span class="keyword">event</span>)) {
    <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(
      <span class="string">"Put queue for MemoryTransaction of capacity "</span> +
        putList.size() + <span class="string">" full, consider committing more frequently, "</span> +
        <span class="string">"increasing capacity or increasing thread count"</span>);
  }
  putByteCounter += eventByteSize;
}
</code></pre><p>put方法把数据丢入putList中，这个也就是之前分析的putList这个属性的作用，putList放Source进来的数据。</p>
<p>commit方法的关键性代码：</p>
<pre><code>@Override
<span class="keyword">protected</span> <span class="keyword">void</span> doCommit() <span class="keyword">throws</span> InterruptedException {
  <span class="built_in">int</span> puts = putList.<span class="built_in">size</span>();
  <span class="built_in">int</span> takes = takeList.<span class="built_in">size</span>();
  <span class="keyword">synchronized</span>(queueLock) {
    <span class="keyword">if</span>(puts &gt; <span class="number">0</span> ) {
      <span class="comment">// 清空putList，丢到外部类MemoryChannel中的queue队列里</span>
      <span class="keyword">while</span>(!putList.isEmpty()) {
        <span class="comment">// MemoryChannel中的queue队列</span>
        <span class="keyword">if</span>(!queue.offer(putList.removeFirst())) {
          <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Queue add failed, this shouldn't be able to happen"</span>);
        }
      }
    }
    putList.<span class="built_in">clear</span>();
    takeList.<span class="built_in">clear</span>();
  }
}
</code></pre><p>rollback方法关键性代码：</p>
<pre><code>@Override
<span class="keyword">protected</span> <span class="keyword">void</span> doRollback() {
  <span class="built_in">int</span> takes = takeList.<span class="built_in">size</span>();
  <span class="keyword">synchronized</span>(queueLock) {
    Preconditions.checkState(queue.remainingCapacity() &gt;= takeList.<span class="built_in">size</span>(), <span class="string">"Not enough space in memory channel "</span> +
        <span class="string">"queue to rollback takes. This should never happen, please report"</span>);
    <span class="comment">// 把takeList中的数据放回到queue中</span>
    <span class="keyword">while</span>(!takeList.isEmpty()) {
      queue.addFirst(takeList.removeLast());
    }
    putList.<span class="built_in">clear</span>();
  }
}
</code></pre><p>发生异常后才会调用rollback方法。也就是说take方法被调用之后，由于take方法是从queue中拿数据，并且放到takeList里。所以回滚的时候需要把takeList中的数据还给queue。</p>
<p>MemoryTransaction的close方法只是把状态改成了CLOSED，其他没做什么，就不分析了。</p>
<p>MemoryTransaction的take方法：</p>
<p>take方法从queue中拉出数据，然后放到takeList中。</p>
<pre><code>@<span class="function">Override
<span class="keyword">protected</span> Event <span class="title">doTake</span>(<span class="params"></span>) throws InterruptedException </span>{
  channelCounter.incrementEventTakeAttemptCount();
  <span class="keyword">if</span>(takeList.remainingCapacity() == <span class="number">0</span>) {
    <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(<span class="string">"Take list for MemoryTransaction, capacity "</span> +
        takeList.size() + <span class="string">" full, consider committing more frequently, "</span> +
        <span class="string">"increasing capacity, or increasing thread count"</span>);
  }
  <span class="keyword">if</span>(!queueStored.tryAcquire(keepAlive, TimeUnit.SECONDS)) {
    <span class="keyword">return</span> <span class="keyword">null</span>;
  }
  Event <span class="keyword">event</span>;
  synchronized(queueLock) {
    <span class="keyword">event</span> = queue.poll();
  }
  Preconditions.checkNotNull(<span class="keyword">event</span>, <span class="string">"Queue.poll returned NULL despite semaphore "</span> +
      <span class="string">"signalling existence of entry"</span>);
  takeList.put(<span class="keyword">event</span>);

  <span class="keyword">int</span> eventByteSize = (<span class="keyword">int</span>)Math.ceil(estimateEventSize(<span class="keyword">event</span>)/byteCapacitySlotSize);
  takeByteCounter += eventByteSize;

  <span class="keyword">return</span> <span class="keyword">event</span>;
}
</code></pre><p>Source数据进入Channel过程中Transaction的处理过程：</p>
<p>ChannelProcessor处理这个过程：</p>
<pre><code><span class="keyword">for</span> (Channel reqChannel : reqChannelQueue.keySet()) {
  <span class="comment">// 获取事务</span>
  Transaction tx = reqChannel.getTransaction();
  Preconditions.checkNotNull(tx, <span class="string">"Transaction object must not be null"</span>);
  <span class="keyword">try</span> {
    <span class="comment">// 事务开始</span>
    tx.begin();
    <span class="comment">// 获取Source处理的一个批次中的所有Event</span>
    List&lt;Event&gt; batch = reqChannelQueue.get(reqChannel);

    <span class="keyword">for</span> (Event event : batch) {
      <span class="comment">// MemoryChannel的put方法会MemoryTransaction的put方法。</span>
      reqChannel.put(event);
    }
    <span class="comment">// 提交事务</span>
    tx.commit();
  } <span class="keyword">catch</span> (Throwable t) {
      <span class="comment">// 发生异常回滚事务</span>
    tx.rollback();
    <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) {
      LOG.<span class="keyword">error</span>(<span class="string">"Error while writing to required channel: "</span> +
          reqChannel, t);
      <span class="keyword">throw</span> (Error) t;
    } <span class="keyword">else</span> {
      <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(<span class="string">"Unable to put batch on required "</span> +
          <span class="string">"channel: "</span> + reqChannel, t);
    }
  } <span class="keyword">finally</span> {
    <span class="keyword">if</span> (tx != <span class="keyword">null</span>) {
      <span class="comment">// 最后结束事务</span>
      tx.close();
    }
  }
}
</code></pre><p>Channel中的数据被Sink处理的过程：</p>
<p>以hdfs sink为例讲解：</p>
<pre><code><span class="keyword">public</span> <span class="function">Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>{
    Channel channel = getChannel();
    Transaction transaction = channel.getTransaction();
    List&lt;BucketWriter&gt; writers = Lists.newArrayList();
    transaction.begin();
    <span class="keyword">try</span> {
      <span class="keyword">int</span> txnEventCount = <span class="number">0</span>;
      <span class="keyword">for</span> (txnEventCount = <span class="number">0</span>; txnEventCount &lt; batchSize; txnEventCount++) {
        Event event = channel.take();
        <span class="keyword">if</span> (event == <span class="keyword">null</span>) {
          <span class="keyword">break</span>;
        }

      ... 

      transaction.commit();

      <span class="keyword">if</span> (txnEventCount &lt; <span class="number">1</span>) {
        <span class="keyword">return</span> Status.BACKOFF;
      } <span class="keyword">else</span> {
        sinkCounter.addToEventDrainSuccessCount(txnEventCount);
        <span class="keyword">return</span> Status.READY;
      }
    } <span class="keyword">catch</span> (IOException eIO) {
      transaction.rollback();
      LOG.warn(<span class="string">"HDFS IO error"</span>, eIO);
      <span class="keyword">return</span> Status.BACKOFF;
    } <span class="keyword">catch</span> (Throwable th) {
      transaction.rollback();
      LOG.<span class="keyword">error</span>(<span class="string">"process failed"</span>, th);
      <span class="keyword">if</span> (th <span class="keyword">instanceof</span> Error) {
        <span class="keyword">throw</span> (Error) th;
      } <span class="keyword">else</span> {
        <span class="keyword">throw</span> <span class="keyword">new</span> EventDeliveryException(th);
      }
    } <span class="keyword">finally</span> {
      transaction.close();
    }
 }
</code></pre><p>也是一样的流程，begin，take，commit or rollback，close。</p>
<p>总结：</p>
<ol>
<li><p>MemoryTransaction是MemoryChannel中的一个内部类，内部有2个阻塞队列putList和takeList。MemoryChannel内部有个queue阻塞队列。</p>
</li>
<li><p>putList接收Source交给Channel的event数据，takeList保存Channel交给Sink的event数据。</p>
</li>
<li><p>如果是Source交给Channel任务完成，进行commit的时候。会把putList中的所有event放到MemoryChannel中的queue。</p>
</li>
<li><p>如果是Source交给Channel任务失败，进行rollback的时候。程序就不会继续走下去，比如KafkaSource需要commitOffsets，如果任务失败就不会commitOffsets。</p>
</li>
<li><p>如果是Sink处理完Channel带来的event，进行commit的时候。会清空takeList中的event数据，因为已经没consume。</p>
</li>
<li><p>如果是Sink处理Channel带来的event失败的话，进行rollback的时候。会把takeList中的event写回到queue中。</p>
</li>
</ol>
<p>缺点：</p>
<p>Flume的Transaction跟数据库的Transaction不一样。数据库中的事务回滚之后所有操作的数据都会进行处理。而Flume的却不能还原。比如HDFSSink写数据到HDFS的时候需要rollback，比如本来要写入10000条数据，但是写到5000条的时候rollback，那么已经写入的5000条数据不能回滚，而那10000条数据回到了阻塞队列里，下次再写入的时候还会重新写入这10000条数据。这样就多了5000条重复数据，这是flume设计上的缺陷。</p>
]]></content>
    <summary type="html">
    <![CDATA[Flume中有一个Transaction的概念。本文仅分析Transaction的实现类MemoryTransaction的实现原理，JdbcTransaction的原理跟数据库中的Transaction类似 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[2015总结]]></title>
    <link href="http://fangjian0423.github.io/2016/01/01/2015_end/"/>
    <id>http://fangjian0423.github.io/2016/01/01/2015_end/</id>
    <published>2015-12-31T16:22:35.000Z</published>
    <updated>2015-12-31T16:48:01.000Z</updated>
    <content type="html"><![CDATA[<h2 id="2015年总结">2015年总结</h2><p>又是一年年底，2015年年底了。总结，总结，总结，还是分3个部分。</p>
<h3 id="工作">工作</h3><p>今年2月份春节过完之后就换了工作，之后在这家公司工作到了现在。</p>
<p>来新公司主要的目的是为了做大数据的，但是一进来就是给别人擦屁股，改bug = =。 后来大数据的东西只做了一部分，大概3个月的时间，之后被拉过去做公众号相关的东西了。没办法，公司人手太少，自己负责的etl这部分还有很多不完善的地方，可惜没有时间继续做下去。</p>
<p>公司人少没时间做是一个原因，但是更重要的原因还是自己懒了。</p>
<h3 id="技术">技术</h3><p>2015年，对技术做以下总结：</p>
<ol>
<li>继续写博客，15年写了43篇博客。相比去年的34篇博客，今年写的更多了。但是这43篇博客的质量其实都很一般，没有去年写的springmvc源码分析好。 但是想要写出好的文章得花费很长的时间。争取之后的文章写得多又能写得好。</li>
<li>了解了大数据方面的知识。包括hadoop，hdfs，flume，spark，hbase，elasticsearch，sqoop，hive等方面的知识。由于自己在公司负责的是etl方面的内容，所以对flume了解的比较多，然后又用base和elasticsearch存储了一些东西，所以对这两块内容也比较了解。但是对这些东西，也只是停留在使用的基础上，没有深入了解到内部的结构，明年会深入了解这些内容的。</li>
<li>github贡献了几个开源项目。包括flume，一个es教程，waterfall等。flume的HBaseSink在stop的时候居然没有把serializer关闭掉。给flume提了个pull request，但是flume居然不接受在github提出的pull request，只能在apache jira上处理ticket，但是新建了一个ticket之后居然不能assign给任何人，所以也就没人处理了，有点尴尬。</li>
<li>玩了会grails。由于公司内部的后台系统是用grails搭建的，所以自然就得会grails，grails内部用groovy写的。用了之后发现grails的调试在intellij中特别的慢，而且只要进了一个闭包，调试就特别麻烦。grails项目大了之后启动也非常地慢，有时候还会莫名其妙地出现一些错误，重启一下就好了。综上原因，对grails不是非常喜欢。</li>
<li>spring-boot的使用。公司内部发现grails项目大了之后启动会非常慢，后来开始使用micro-service就行新项目的开发。spring-boot其实是各个框架的整合，包括hibernate，spring，springdata等。提供了一些封装好的方便的方法，但是发现使用了spring-boot之后有些它内部定义好的内容你不看文档是不会知道的，而且有的东西文档里也没有说，所以只能看源码。这个算是使用spring-boot的一个弊端吧。</li>
<li>scala的学习。今年把scala in action这本书看完了。这本书很一般，很多scala的东西感觉都没有讲清楚。自己也把spray-json(scala写的一个json库，很小)的源码看了一遍。其实感觉看源码学语法也是不错的一个方法。</li>
<li>可以勉强算一个全栈了。公司技术少，前端更是只有一个。所以今年做了一些前端的工作，感觉自己一个人能搞定一个公众号的前后端了。写前端的时候使用了angularjs。后来又了解了js的一些打包工具，grunt，gulp等。给公司做了《超级邀请》这个公众号的开发任务，这个公众号的前台页面和接口都是自己写的。</li>
<li>了解了java高级部分的一些知识。知道了java并发的一些内容，知道了内置锁，信号量，栅栏，闭锁等知识。java内存模型也了解了一点，知道了happens-before，java的内存通信是通过共享内存实现的。jvm的书买了，但是还没开始看，是个弱项。</li>
<li>代码质量。今年写的代码质量感觉还是很烂，比去年也就稍微好了一点，依旧很烂。希望16年能写出更好的代码，而不仅仅只是为了完成任务的代码。</li>
</ol>
<h3 id="生活">生活</h3><p>宅，宅，宅。</p>
<p>不过双12买了把ukulele。但是还没开始学 = =，尴尬。不过买了就不会浪费，之后会开始学。</p>
<p>玩了高达模型，跟同事学的。搭了2个MG，武者MK2和迅雷高达。1个bb版强袭高达，年底又买了个pg强袭。pg还没开始搭，搭完绝壁炫酷到爆炸。</p>
<p>附上自己搭的高达图片，就放一张吧。毕竟是写总结的，不是介绍高达的：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/gundam1.JPG" alt=""></p>
<h3 id="2016年计划">2016年计划</h3><p>去年定的2015年计划：</p>
<ol>
<li>今年真的没看书。 有的书看了一点点就没看了，java并发的书看了100多页就没下文了。。 要养成看书的习惯。 (☑。今年把这本书看完了，还看了scala in action。其实还远远不够)</li>
<li><p>技术方面的计划：<br>redis(×。看了elasticsearch)<br>docker(×，而是看了大数据)<br>github贡献开源框架(☑)<br>nio(×)<br>并发包(☑)<br>Mina(×)<br>Netty(×)<br>Python深入(×，深入学习了scala)<br>搜索相关的知识(×)<br>看源码的时候多想想作者的思路以及架构方面，不用特别在意细节(☑)</p>
</li>
<li><p>继续写博客(☑)</p>
</li>
<li>做让生活变得更有趣的事，比如guitar(☑。ukulele，高达)</li>
</ol>
<p>2015的计划虽然只完成了一半，但是由于工作中接触大数据。所以很多内容都没看，转而去看大数据方面的知识了。所以总体完成度还是可以的，算80%。</p>
<p>2016年计划以及展望：</p>
<ol>
<li>继续看书，要看更多的书。 15年居然只看了两本书，对不起自己…. 16年要5本+。</li>
<li><p>技术方面<br>大数据的深入学习，不仅仅局限于会使用。包括spark，es，hbase，hadoop等。<br>分布式方面的学习<br>docker<br>scala的继续深入，要开始用scala写代码<br>netty<br>github继续贡献开源项目<br>jvm</p>
</li>
<li><p>继续写博客</p>
</li>
<li>继续玩高达</li>
<li>学会ukulele</li>
<li>希望能做一些逼格高一点的东西，而不仅仅是做一些功能性的东西</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[2015年年终总结]]>
    
    </summary>
    
      <category term="杂事" scheme="http://fangjian0423.github.io/tags/%E6%9D%82%E4%BA%8B/"/>
    
      <category term="总结" scheme="http://fangjian0423.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spray-json源码分析]]></title>
    <link href="http://fangjian0423.github.io/2015/12/23/scala-spray-json/"/>
    <id>http://fangjian0423.github.io/2015/12/23/scala-spray-json/</id>
    <published>2015-12-22T17:47:35.000Z</published>
    <updated>2015-12-23T03:31:30.000Z</updated>
    <content type="html"><![CDATA[<h2 id="spray-json介绍以及内部结构">spray-json介绍以及内部结构</h2><p><a href="https://github.com/spray/spray-json" target="_blank" rel="external">spray-json</a>是scala的一个轻量的，简洁的，简单的关于JSON实现。</p>
<p>同时也是<a href="http://spray.io/" target="_blank" rel="external">spray</a>项目的json模块。</p>
<p>本文分析spray-json的源码。</p>
<p>在分析spray-json的源码之前，我们先介绍一下spray-json的使用方法以及里面的几个概念。</p>
<p>首先是spray-json的一个使用例子，里面有各种黑魔法：</p>
<pre><code><span class="keyword">val</span> str = <span class="string">"""{
    "name": "Ed",
    "age": 24
}"""</span>

<span class="comment">// 黑魔法。不是String的parseJson方法，而是使用了隐式转换，隐式转换成PimpedString类。PimpedString里有parseJson方法，转换成JsValue对象</span>
<span class="keyword">val</span> jsonVal = str.parseJson <span class="comment">// jsonVal是个JsObject对象的实例</span>

<span class="comment">// jsonVal是个JsObject对象，也是个JsValue实例。JsValue对象都有compactPrint和prettyPrint方法</span>
println(jsonVal.compactPrint) <span class="comment">// 压缩打印</span>
println(jsonVal.prettyPrint) <span class="comment">// 格式化打印</span>

<span class="comment">// 手动构建一个JsObject</span>
<span class="keyword">val</span> jsonObj = <span class="type">JsObject</span>(
    (<span class="string">"name"</span>, <span class="type">JsString</span>(<span class="string">"format"</span>)), (<span class="string">"age"</span>, <span class="type">JsNumber</span>(<span class="number">99</span>))
)

println(jsonObj.compactPrint)
println(jsonObj.prettyPrint)

<span class="comment">// 黑方法，不是List的toJson方法，而是使用了隐式转换，隐式转换成PimpedAny类，PimpedAny类里有toList方法，转换成对应的类型</span>
<span class="keyword">val</span> jsonList = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).toJson

<span class="comment">// JsValue的toString方法引用了compactPrint方法</span>
println(jsonList)
</code></pre><p>然后是spray-json里的几个概念介绍：</p>
<p>1.JsValue: 抽象类。对原生json中的各种数据类型的抽象。它的实现类JsArray对应json里的数组；JsBoolean对应json里的布尔值；JsNumber对应json里的数值 …</p>
<p>2.JsonFormat: 一个trait，是json序列化和反序列话的抽象。继承JsonReader(JsValue转换成对象的抽象)和JsonWriter(对象转换成JsValue的抽象)。BasicFormats和CollectionFormats、AdditionalFormats等这些trait里都有各种JsonFormat的隐式转换。</p>
<p>3.RootJsonFormat。json文档的抽象，跟JsonFormat一样，只不过RootJsonFormat只支持JsArray和JsObject。因为这是一个json文档对象，只有一个json对象或者一个json数组才能称得上是一个json文档对象。</p>
<p>4.JsonPrinter: 一个trait。json打印成字符串的抽象。具体的实现特质有CompactPrinter(压缩后的字符串)和PrettyPrinter(格式化后的字符串)</p>
<p>5.DefaultJsonProtocol: 继承BasicFormats，混入StandardFormats、CollectionFormats、ProductFormats、AdditionalFormats的特质。我们需要转换一些基础类型或者集合类型的时候需要import这个trait。</p>
<h2 id="json_package对象">json package对象</h2><p>json package对象里定义了一些隐式转换方法和一些实用方法。</p>
<pre><code>package object json {

  // JsField。 一个二元元组，代表json中的一个项(key为String，<span class="keyword">value</span>为任意json类型)
  <span class="keyword">type</span> JsField = (String, JsValue)

  // 反序列化异常
  def deserializationError(msg: String, cause: Throwable = null, fieldNames: List[String] = Nil) = throw new DeserializationException(msg, cause, fieldNames)
  // 序列化异常
  def serializationError(msg: String) = throw new SerializationException(msg)
  // jsonReader方法，是个泛型。使用了隐式参数，返回值是这个隐式参数的引用。也就是说只要调用了jsonReader方法，那么就会自动去找对应泛型类型的实现
  def jsonReader[T](<span class="type">implicit</span> reader: JsonReader[T]) = reader
  // 跟jsonReader方法一个道理。只要调用了jsonWriter方法，那么就会自动去找对应泛型类型的实现
  def jsonWriter[T](<span class="type">implicit</span> writer: JsonWriter[T]) = writer 

  // 隐式转换方法。上面例子的toList使用了这个隐式转换
  <span class="type">implicit</span> def pimpAny[T](<span class="built_in">any</span>: T) = new PimpedAny(<span class="built_in">any</span>)
  // 隐式方法。上面例子的parseJson使用了这个隐式转换
  <span class="type">implicit</span> def pimpString(string: String) = new PimpedString(string)
}

package json {

  // 反序列异常类的定义，上面的deserializationError方法实例化了这个类
  <span class="keyword">case</span> <span class="keyword">class</span> DeserializationException(msg: String, cause: Throwable = null, fieldNames: List[String] = Nil) <span class="keyword">extends</span> RuntimeException(msg, cause)
  // 序列异常类的定义，上面的serializationError方法实例化了这个类
  <span class="keyword">class</span> SerializationException(msg: String) <span class="keyword">extends</span> RuntimeException(msg)

  // 上面的隐式方法pimpAny实例化了这个类。黑魔法toJson方法，不是List的toJson方法，而是List隐式转换成PimpedAny，然后调用PimpedAny的toJson方法。toJson方法的参数是个隐式参数，跟上面代码里的jsonWriter方法一样，会找对应泛型类型的JsonWriter实现类，然后调用JsonWriter的<span class="built_in">write</span>方法
  <span class="keyword">private</span>[json] <span class="keyword">class</span> PimpedAny[T](<span class="built_in">any</span>: T) {
    def toJson(<span class="type">implicit</span> writer: JsonWriter[T]): JsValue = writer.<span class="built_in">write</span>(<span class="built_in">any</span>)
  }

  // 上面的隐式方法pimpString实例化了这个类。黑魔法parseJson方法，不是String的parseJson方法，而是String隐式转换成PimpedString，然后调用PimpedString的parseJson方法
  <span class="keyword">private</span>[json] <span class="keyword">class</span> PimpedString(string: String) {
    @deprecated(<span class="string">"deprecated in favor of parseJson"</span>, <span class="string">"1.2.6"</span>)
    def asJson: JsValue = parseJson
    def parseJson: JsValue = JsonParser(string)
  }
}
</code></pre><h2 id="JsValue(原生json中的各种数据类型的抽象)">JsValue(原生json中的各种数据类型的抽象)</h2><p>JsValue是原生json中各种数据类型的抽象，是个抽象类，直接看JsValue的定义:</p>
<pre><code><span class="keyword">sealed</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">JsValue</span> {</span>
     <span class="comment">// 重载的toString方法引用了compactPrint方法，会打印出json的压缩格式</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span> =</span> compactPrint
  <span class="function"><span class="keyword">def</span> <span class="title">toString</span>(</span>printer: (<span class="type">JsValue</span> =&gt; <span class="type">String</span>)) = printer(<span class="keyword">this</span>)
  <span class="comment">// 压缩打印，使用了CompactPrinter。CompactPrinter是一个JsonPrinter的子类</span>
  <span class="function"><span class="keyword">def</span> <span class="title">compactPrint</span> =</span> <span class="type">CompactPrinter</span>(<span class="keyword">this</span>)
  <span class="comment">// 格式化打印，PrettyPrinter。PrettyPrinter也是一个JsonPrinter的子类</span>
  <span class="function"><span class="keyword">def</span> <span class="title">prettyPrint</span> =</span> <span class="type">PrettyPrinter</span>(<span class="keyword">this</span>)
  <span class="comment">// 转换成对应的类。jsonReader方法在json package里定义，已经分析过。会找对应那个的JsonReader实现类。然后调用read方法</span>
  <span class="function"><span class="keyword">def</span> <span class="title">convertTo</span>[</span><span class="type">T</span> :<span class="type">JsonReader</span>]: <span class="type">T</span> = jsonReader[<span class="type">T</span>].read(<span class="keyword">this</span>)

  <span class="comment">// 转换成JsObject对象，除了JsObject对象重写了这个，返回了自身。其他类型的JsValue都会抛出DeserializationException异常</span>
  <span class="function"><span class="keyword">def</span> <span class="title">asJsObject</span>(</span>errorMsg: <span class="type">String</span> = <span class="string">"JSON object expected"</span>): <span class="type">JsObject</span> = deserializationError(errorMsg)

  <span class="function"><span class="keyword">def</span> <span class="title">asJsObject</span>:</span> <span class="type">JsObject</span> = asJsObject()

  <span class="annotation">@deprecated</span>(<span class="string">"Superceded by 'convertTo'"</span>, <span class="string">"1.1.0"</span>)
  <span class="function"><span class="keyword">def</span> <span class="title">fromJson</span>[</span><span class="type">T</span> :<span class="type">JsonReader</span>]: <span class="type">T</span> = convertTo
}
</code></pre><p>JsNumber是数值类型的抽象：</p>
<pre><code><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JsNumber</span>(</span>value: <span class="type">BigDecimal</span>) <span class="keyword">extends</span> <span class="type">JsValue</span>

<span class="comment">// JsNumber中定义了几个方便的构造方法</span>
<span class="class"><span class="keyword">object</span> <span class="title">JsNumber</span> {</span>
  <span class="keyword">val</span> zero: <span class="type">JsNumber</span> = apply(<span class="number">0</span>)
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Int</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Long</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Double</span>) = n <span class="keyword">match</span> {
    <span class="keyword">case</span> n <span class="keyword">if</span> n.isNaN      =&gt; <span class="type">JsNull</span>
    <span class="keyword">case</span> n <span class="keyword">if</span> n.isInfinity =&gt; <span class="type">JsNull</span>
    <span class="keyword">case</span> _                 =&gt; <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  }
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">BigInt</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">String</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Array</span>[<span class="type">Char</span>]) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
}
</code></pre><p>JsString是字符串类型的抽象：</p>
<pre><code><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JsString</span>(</span>value: <span class="type">String</span>) <span class="keyword">extends</span> <span class="type">JsValue</span>

<span class="comment">// JsString中定义了几个方便的方法</span>
<span class="class"><span class="keyword">object</span> <span class="title">JsString</span> {</span>
    <span class="keyword">val</span> empty = <span class="type">JsString</span>(<span class="string">""</span>)
    <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>value: <span class="type">Symbol</span>) = <span class="keyword">new</span> <span class="type">JsString</span>(value.name)
}
</code></pre><p>JsObject是对象类型的抽象：</p>
<pre><code><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JsObject</span>(</span>fields: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">JsValue</span>]) <span class="keyword">extends</span> <span class="type">JsValue</span> {
  <span class="comment">// 重写了asJsObject方法</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">asJsObject</span>(</span>errorMsg: <span class="type">String</span>) = <span class="keyword">this</span>
  <span class="comment">// 根据字段名获得对应的JsValue</span>
  <span class="function"><span class="keyword">def</span> <span class="title">getFields</span>(</span>fieldNames: <span class="type">String</span>*): immutable.<span class="type">Seq</span>[<span class="type">JsValue</span>] = fieldNames.flatMap(fields.get)(collection.breakOut)
}

<span class="class"><span class="keyword">object</span> <span class="title">JsObject</span> {</span>
  <span class="keyword">val</span> empty = <span class="type">JsObject</span>(<span class="type">Map</span>.empty[<span class="type">String</span>, <span class="type">JsValue</span>])
  <span class="comment">// 使用多个JsField构造JsObject。这里的JsField就是代表一个(String, JsValue)</span>
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>members: <span class="type">JsField</span>*) = <span class="keyword">new</span> <span class="type">JsObject</span>(<span class="type">Map</span>(members: _*))
  <span class="annotation">@deprecated</span>(<span class="string">"Use JsObject(JsValue*) instead"</span>, <span class="string">"1.3.0"</span>)
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>members: <span class="type">List</span>[<span class="type">JsField</span>]) = <span class="keyword">new</span> <span class="type">JsObject</span>(<span class="type">Map</span>(members: _*))
}
</code></pre><h2 id="JsonFormat(JsonWriter和JsonReader的子类)">JsonFormat(JsonWriter和JsonReader的子类)</h2><p>一个trait，是json序列化和反序列话的抽象。继承JsonReader(JsValue转换成对象的抽象)和JsonWriter(对象转换成JsValue的抽象)。</p>
<p>JsonReader的定义，把一个JsValue转换成对应的类型：</p>
<pre><code>trait JsonReader[T] {
  <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(json: JsValue)</span>:</span> T
}
</code></pre><p>JsonWriter的定义，把一个类型转换成JsValue：</p>
<pre><code>trait JsonWriter[T] {
  <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(obj: T)</span>:</span> JsValue
}
</code></pre><p>json package对象里的jsonReader和jsonWriter方法有个隐式参数，我们也分析过：只要调用了jsonReader(JsonWriter)方法，那么就会自动去找对应泛型类型的实现。</p>
<p>AdditionalFormats、BasicFormats、CollectionFormats、StandardFormats等都定义了各种JsonFormat。</p>
<p>比如Int类型就找IntJsonFormat，String类型就找StringJsonFormat …  这些基础类型的JsonFormat都定义在BasicFormats这个trait中。</p>
<p>我们就分析几个BasicFormats中定义的基础类型JsonFormat：</p>
<p>Int基本类型的JsonFormat：</p>
<pre><code><span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">IntJsonFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">JsonFormat</span>[</span><span class="type">Int</span>] {
    <span class="comment">// write方法继承自JsonWriter。 直接实例化一个JsNumber对象</span>
    <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>x: <span class="type">Int</span>) = <span class="type">JsNumber</span>(x)
    <span class="comment">// read方法继承自JsonReader。读取JsNumber中对应的值</span>
    <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value <span class="keyword">match</span> {
      <span class="keyword">case</span> <span class="type">JsNumber</span>(x) =&gt; x.intValue
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected Int as JsNumber, but got "</span> + x)
    }
}
</code></pre><p>String基本类型的JsonFormat：</p>
<pre><code><span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">StringJsonFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">JsonFormat</span>[</span><span class="type">String</span>] {
    <span class="comment">// write方法实例化一个JsString</span>
    <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>x: <span class="type">String</span>) = {
      require(x ne <span class="literal">null</span>)
      <span class="type">JsString</span>(x)
    }
    <span class="comment">// read方法读取JsString中对应的字符串</span>
    <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value <span class="keyword">match</span> {
      <span class="keyword">case</span> <span class="type">JsString</span>(x) =&gt; x
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected String as JsString, but got "</span> + x)
    }
}
</code></pre><p>…..</p>
<p>CollectionFormats中定义了几个集合类的JsonFormat:</p>
<p>List类型的JsonFormat：</p>
<pre><code><span class="type">implicit</span> def listFormat[T :JsonFormat] = new RootJsonFormat[List[T]] {
    // 将List转换成JsArray对象。遍历list中的各个元素，对每个元素调用toJson方法。最后JsArray里的每个元素都是JsValue
    def <span class="built_in">write</span>(list: List[T]) = JsArray(list.map(_.toJson).toVector)
    // JsArray转换成List。对JsArray里的各个JsValue调用convertTo转换成对应的类型
    def read(<span class="keyword">value</span>: JsValue): List[T] = <span class="keyword">value</span> match {
      <span class="keyword">case</span> JsArray(elements) =&gt; elements.map(_.convertTo[T])(collection.breakOut)
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected List as JsArray, but got "</span> + x)
    }
}
</code></pre><p>Map类型的JsonFormat：</p>
<pre><code><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">mapFormat</span>[</span><span class="type">K</span> :<span class="type">JsonFormat</span>, <span class="type">V</span> :<span class="type">JsonFormat</span>] = <span class="keyword">new</span> <span class="type">RootJsonFormat</span>[<span class="type">Map</span>[<span class="type">K</span>, <span class="type">V</span>]] {
    <span class="comment">// 遍历Map中的每一个二元元组。如果元组的第一项不是String，直接抛出SerializationException异常。否则构造key为元组第一项字符串，value为元组第二项的JsVaue对象。</span>
    <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>m: <span class="type">Map</span>[<span class="type">K</span>, <span class="type">V</span>]) = <span class="type">JsObject</span> {
      m.map { field =&gt;
        field._1.toJson <span class="keyword">match</span> {
          <span class="keyword">case</span> <span class="type">JsString</span>(x) =&gt; x -&gt; field._2.toJson
          <span class="keyword">case</span> x =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SerializationException</span>(<span class="string">"Map key must be formatted as JsString, not '"</span> + x + <span class="string">"'"</span>)
        }
      }
    }
    <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value <span class="keyword">match</span> {
      <span class="keyword">case</span> x: <span class="type">JsObject</span> =&gt; x.fields.map { field =&gt;
        (<span class="type">JsString</span>(field._1).convertTo[<span class="type">K</span>], field._2.convertTo[<span class="type">V</span>])
      } (collection.breakOut)
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected Map as JsObject, but got "</span> + x)
    }
}
</code></pre><p>AdditionalFormats中定义了一些helper和额外的JsonFormat：</p>
<pre><code><span class="comment">// JsValue的JsonFormat，JsValue调用convertTo或者toJson返回的就是自身</span>
<span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">JsValueFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">JsonFormat</span>[</span><span class="type">JsValue</span>] {
  <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>value: <span class="type">JsValue</span>) = value
  <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value
}


<span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">RootJsObjectFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">RootJsonFormat</span>[</span><span class="type">JsObject</span>] {
  <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>value: <span class="type">JsObject</span>) = value
  <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value.asJsObject
}
</code></pre><p>StandardFormats中定义了Option、Either的JsonFormat，1元-7元元组的JsonFormat。</p>
<pre><code><span class="type">implicit</span> def tuple1Format[A :JF] = new JF[Tuple1[A]] {
  def <span class="built_in">write</span>(t: Tuple1[A]) = t._1.toJson
  def read(<span class="keyword">value</span>: JsValue) = Tuple1(<span class="keyword">value</span>.convertTo[A])
}

<span class="type">implicit</span> def tuple2Format[A :JF, B :JF] = new RootJsonFormat[(A, B)] {
  def <span class="built_in">write</span>(t: (A, B)) = JsArray(t._1.toJson, t._2.toJson)
  def read(<span class="keyword">value</span>: JsValue) = <span class="keyword">value</span> match {
    <span class="keyword">case</span> JsArray(Seq(a, b)) =&gt; (a.convertTo[A], b.convertTo[B])
    <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected Tuple2 as JsArray, but got "</span> + x)
  }
}
</code></pre><h2 id="JsonPrinter(将JsValue打印成原生json字符串)">JsonPrinter(将JsValue打印成原生json字符串)</h2><p>JsonPrinter继承一个函数对象，这个函数的输入是个JsValue，输出是String：</p>
<pre><code><span class="class"><span class="keyword">trait</span> <span class="title">JsonPrinter</span> <span class="keyword"><span class="keyword">extends</span></span> (</span><span class="type">JsValue</span> =&gt; <span class="type">String</span>)
</code></pre><p>内部定义了一个抽象方法：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">print</span><span class="params">(<span class="symbol">x:</span> <span class="constant">JsValue</span>, <span class="symbol">sb:</span> <span class="constant">JStringBuilder</span>)</span></span>
</code></pre><p>CompactPrinter继承JsonPrinter，压缩打印：</p>
<p>实现的print方法：</p>
<pre><code>def <span class="built_in">print</span>(x: JsValue, sb: StringBuilder) {
    x match {
      <span class="keyword">case</span> JsObject(x) =&gt; <span class="built_in">print</span>Object(x, sb)
      <span class="keyword">case</span> JsArray(x)  =&gt; <span class="built_in">print</span>Array(x, sb)
      <span class="keyword">case</span> _ =&gt; <span class="built_in">print</span>Leaf(x, sb)
    }
}
</code></pre><p>PrettyPrinter也继承JsonPrinter，格式化打印：</p>
<p>实现的print方法：</p>
<pre><code><span class="keyword">def</span> print(<span class="string">x:</span> JsValue, <span class="string">sb:</span> StringBuilder) {
  print(x, sb, <span class="number">0</span>)
}

  <span class="comment">// indent参数是格式化打印的关键</span>
<span class="keyword">protected</span> <span class="keyword">def</span> print(<span class="string">x:</span> JsValue, <span class="string">sb:</span> StringBuilder, <span class="string">indent:</span> Int) {
  x match {
    <span class="keyword">case</span> JsObject(x) =&gt; printObject(x, sb, indent)
    <span class="keyword">case</span> JsArray(x)  =&gt; printArray(x, sb, indent)
    <span class="keyword">case</span> _ =&gt; printLeaf(x, sb)
  }
}
</code></pre><h2 id="DefaultJsonProtocol(整合了多个JsonFormat)">DefaultJsonProtocol(整合了多个JsonFormat)</h2><p>直接看源码：</p>
<pre><code><span class="class"><span class="keyword">trait</span> <span class="title">DefaultJsonProtocol</span>
</span>    <span class="keyword">extends</span> <span class="type">BasicFormats</span>
    <span class="keyword">with</span> <span class="type">StandardFormats</span>
    <span class="keyword">with</span> <span class="type">CollectionFormats</span>
    <span class="keyword">with</span> <span class="type">ProductFormats</span>
    <span class="keyword">with</span> <span class="type">AdditionalFormats</span>

<span class="class"><span class="keyword">object</span> <span class="title">DefaultJsonProtocol</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">DefaultJsonProtocol</span></span>
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[spray-json是scala的一个轻量的，简洁的，简单的关于JSON实现。同时也是spray项目的json模块，本文分析spray-json的源码 ...]]>
    
    </summary>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/tags/jvm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala 隐式转换和隐式参数]]></title>
    <link href="http://fangjian0423.github.io/2015/12/20/scala-implicit/"/>
    <id>http://fangjian0423.github.io/2015/12/20/scala-implicit/</id>
    <published>2015-12-20T07:38:22.000Z</published>
    <updated>2015-12-20T17:11:57.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>Scala的implicit功能很强大，可以自动地给对象”添加一个属性”。 这里打上引号的原因是Scala内部进行编译的时候会自动加上隐式转换函数。</p>
<p>很多Scala开源框架内部都大量使用了implicit。因为implicit真的很强大，写得好的implicit可以让代码更优雅。但个人感觉implicit也有一些缺点，比如使用了implicit之后，看源码或者使用一些library的时候无法下手，因为你根本不知道作者哪里写了implicit。这个也会对初学者造成一些困扰。</p>
<p>比如Scala中Option就有一个implicit可以将Option转换成Iterable：</p>
<pre><code>val <span class="built_in">list</span> = List(<span class="number">1</span>, <span class="number">2</span>)
val <span class="built_in">map</span> = Map(<span class="number">1</span> -&gt; <span class="number">11</span>, <span class="number">2</span> -&gt; <span class="number">22</span>, <span class="number">3</span> -&gt; <span class="number">33</span>)

val newList = <span class="built_in">list</span>.flatMap {
    num =&gt; <span class="built_in">map</span>.get(num) <span class="comment">// map.get方法返回的是Option，可以被隐式转换成Iterable</span>
} 
</code></pre><p>以下是implicit的一个小例子。</p>
<p>比如以下一个例子，定义一个Int类型的变量num，但是赋值给了一个Double类型的数值。这时候就会编译错误：</p>
<pre><code><span class="variable"><span class="keyword">val</span> num</span>: <span class="typename">Int</span> = <span class="number">3.5</span> <span class="comment">// Compile Error</span>
</code></pre><p>但是我们加了一个隐式转换之后，就没问题了:</p>
<pre><code><span class="type">implicit</span> def double2Int(d: <span class="type">Double</span>) = d.toInt

val num: <span class="built_in">Int</span> = <span class="number">3.5</span> // <span class="number">3</span>， 这段代码会被编译成 val num: <span class="built_in">Int</span> = double2Int(<span class="number">3.5</span>)
</code></pre><h2 id="隐式转换规则">隐式转换规则</h2><h3 id="标记规则(Marking_Rule)">标记规则(Marking Rule)</h3><p>任何变量，函数或者对象都可以用implicit这个关键字进行标记，表示可以进行隐式转换。</p>
<pre><code><span class="type">implicit</span> def intToString(x: <span class="built_in">Int</span>) = x.toString
</code></pre><p>编译器可能会将x + y 转换成 convert(x) + y 如果convert被标记成implicit。</p>
<h3 id="作用域规则(Scope_Rule)">作用域规则(Scope Rule)</h3><p>在一个作用域内，一个隐式转换必须是一个唯一的标识。</p>
<p>比如说MyUtils这个object里有很多隐式转换。x + y 不会使用MyUtils里的隐式转换。 除非import进来。 import MyUtils._</p>
<p>Scala编译器还能在companion class中去找companion object中定义的隐式转换。</p>
<pre><code><span class="keyword">object</span> Player {
    implicit def getClub(player: Player): Club = Club(player.clubName)
}

<span class="class"><span class="keyword">class</span> <span class="title">Player</span></span>(<span class="variable"><span class="keyword">val</span> name</span>: String, <span class="variable"><span class="keyword">val</span> age</span>: <span class="typename">Int</span>, <span class="variable"><span class="keyword">val</span> clubName</span>: String) {

}

<span class="variable"><span class="keyword">val</span> p</span> = new Player(<span class="string">"costa"</span>, <span class="number">27</span>, <span class="string">"Chelsea"</span>)

println(p.welcome) <span class="comment">// Chelsea welcome you here!</span>
println(p.playerNum) <span class="comment">// 21</span>
</code></pre><h3 id="一次编译只隐式转换一次(One-at-a-time_Rule)">一次编译只隐式转换一次(One-at-a-time Rule)</h3><p>Scala不会把 x + y 转换成 convert1(convert2(x)) + y</p>
<h2 id="隐式转换类型">隐式转换类型</h2><h3 id="隐式转换成正确的类型">隐式转换成正确的类型</h3><p>这种类型是Scala编译器对隐式转换的第一选择。 比如说编译器看到一个类型的X的数据，但是需要一个类型为Y的数据，那么就会去找把X类型转换成Y类型的隐式转换。</p>
<p>本文一开始的double2Int方法就是这种类型的隐式转换。</p>
<pre><code><span class="type">implicit</span> def double2Int(d: <span class="type">Double</span>) = d.toInt

val num: <span class="built_in">Int</span> = <span class="number">3.5</span> // <span class="number">3</span>
</code></pre><p>当编译器发现变量num是个Int类型，并且用Double类型给它赋值的时候，会报错。 但是在报错之前，编译器会查找Double =&gt; Int的隐式转换。然后发现了double2Int这个隐式转换函数。于是就使用了隐式转换。</p>
<h3 id="方法调用的隐式转换">方法调用的隐式转换</h3><p>比如这段代码  obj.doSomeThing。 比如obj对象没有doSomeThing这个方法，编译器会会去查找拥有doSomeThing方法的类型，并且看obj类型是否有隐式转换成有doSomeThing类型的函数。有的话就是将obj对象隐式转换成拥有doSomeThing方法的对象。</p>
<p>以下是一个例子：</p>
<pre><code><span class="keyword">case</span> <span class="keyword">class</span> Person(<span class="keyword">name</span>: String, age: <span class="built_in">Int</span>) {
    def +(num: <span class="built_in">Int</span>) = age + num
    def +(p: Person) = age + p.age
  }

val person = Person(<span class="string">"format"</span>, <span class="number">99</span>)
println(person + <span class="number">1</span>) // <span class="number">100</span>
//  println(<span class="number">1</span> + person)  报错，因为<span class="built_in">Int</span>的+方法没有有Person参数的重载方法

<span class="type">implicit</span> def personAddAge(x: <span class="built_in">Int</span>) = Person(<span class="string">"unknown"</span>, x)

println(<span class="number">1</span> + person) // <span class="number">100</span>
</code></pre><p>有了隐式转换方法之后，编译器检查 1 + person 表达式，发现Int的+方法没有有Person参数的重载方法。在放弃之前查看是否有将Int类型的对象转换成以Person为参数的+方法的隐式转换函数，于是找到了，然后就进行了隐式转换。</p>
<p>Scala的Predef中也使用了方法调用的隐式转换。</p>
<pre><code>Map(<span class="number">1</span> -&gt; <span class="number">11</span>, <span class="number">2</span> -&gt; <span class="number">22</span>)
</code></pre><p>上面这段Map中的参数是个二元元组。 Int没有 -&gt; 方法。 但是在Predef中定义了：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> ArrowAssoc[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    @inline def -&gt; [B](y: B): Tuple2[A, B] = Tuple2(self, y)
    def →[B](y: B): Tuple2[A, B] = -&gt;(y)
}
</code></pre><h3 id="隐式参数">隐式参数</h3><p>隐式参数的意义是当方法需要多个参数的时候，可以定义一些隐式参数，这些隐式参数可以被自动加到方法填充的参数里，而不必手填充。</p>
<pre><code>def implicitParamFunc(<span class="keyword">name</span>: String)(<span class="type">implicit</span> tiger: Tiger, lion: Lion): <span class="keyword">Unit</span> = {
    println(<span class="keyword">name</span> + <span class="string">" have a tiget and a lion, their names are: "</span> + tiger.<span class="keyword">name</span> + <span class="string">", "</span> + lion.<span class="keyword">name</span>)
}

object Zoo {
    <span class="type">implicit</span> val tiger = Tiger(<span class="string">"tiger1"</span>)
    <span class="type">implicit</span> val lion = Lion(<span class="string">"lion1"</span>)
}

<span class="keyword">import</span> Zoo._

implicitParamFunc(<span class="string">"format"</span>)
</code></pre><p>上面这个代码中implicitParamFunc中的第二个参数定义成了隐式参数。</p>
<p>然后在Zoo对象里定义了两个隐式变量，import进来之后，调用implicitParamFunc方法的时候这两个变量被自动填充到了参数里。</p>
<p>这里需要注意的是不仅仅方法中的参数需要被定义成隐式参数，对应的隐式参数的变量也需要被定义成隐式变量。</p>
<h2 id="其他">其他</h2><p>对象中的隐式转换可以只import自己需要的。</p>
<pre><code>object MyUtils {
    <span class="type">implicit</span> def a ...
    <span class="type">implicit</span> def b ...
}

<span class="keyword">import</span> MyUtils.a
</code></pre><p>隐式转换修饰符implicit可以修饰class，method，变量，object。</p>
<p>修饰方法和变量的隐式转换本文已经介绍过，就不继续说了。</p>
<p>修饰class的隐式转换，它的作用跟修饰method的隐式转换类似：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMarker(val start: <span class="built_in">Int</span>) {
    def --&gt;(<span class="keyword">end</span>: <span class="built_in">Int</span>) = start to <span class="keyword">end</span>
}

<span class="number">1</span> --&gt; <span class="number">10</span> // <span class="built_in">Range</span>(<span class="number">1</span>, <span class="number">10</span>)
</code></pre><p>上段代码可以改造成使用Value Class完成类的隐式转换：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMaker(start: <span class="built_in">Int</span>) <span class="keyword">extends</span> AnyVal {
    def --&gt;(<span class="keyword">end</span>: <span class="built_in">Int</span>) = start to <span class="keyword">end</span>
}
</code></pre><p>修饰object的隐式转换：</p>
<pre><code>trait Calculate[T] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: T, y: T)</span>:</span> T
}

implicit object IntCal extends Calculate[Int] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: Int, y: Int)</span>:</span> Int = x + y
}

implicit object ListCal extends Calculate[List[Int]] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: List[Int], y: List[Int])</span>:</span> List[Int] = x ::: y
}

<span class="function"><span class="keyword">def</span> <span class="title">implicitObjMethod</span>[<span class="title">T</span>]<span class="params">(x: T, y: T)</span><span class="params">(implicit cal: Calculate[T])</span>:</span> Unit = {
    println(x + <span class="string">" + "</span> + y + <span class="string">" = "</span> + cal.add(x, y))
}

implicitObjMethod(<span class="number">1</span>, <span class="number">2</span>) // <span class="number">1</span> + <span class="number">2</span> = <span class="number">3</span>
implicitObjMethod(List(<span class="number">1</span>, <span class="number">2</span>), List(<span class="number">3</span>, <span class="number">4</span>)) // List(<span class="number">1</span>, <span class="number">2</span>) + List(<span class="number">3</span>, <span class="number">4</span>) = List(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Scala的implicit功能很强大，可以自动地给对象"添加一个属性"。 这里打上引号的原因是Scala内部进行编译的时候会自动加上隐式转换函数 ...]]>
    
    </summary>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/tags/jvm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[elasticsearch查询模板]]></title>
    <link href="http://fangjian0423.github.io/2015/11/07/elasticsearch-search-template/"/>
    <id>http://fangjian0423.github.io/2015/11/07/elasticsearch-search-template/</id>
    <published>2015-11-07T08:04:25.000Z</published>
    <updated>2015-12-20T17:06:16.000Z</updated>
    <content type="html"><![CDATA[<p>最近在公司又用到了elasticsearch，也用到了查询模板，顺便写篇文章记录一下查询模板的使用。</p>
<p>以1个需求为例讲解es模板的使用：</p>
<p><strong>页面上某个按钮在一段时间内的点击次数统计，并且可以以小时，天，月为单位进行汇总，并且需要去重。</strong></p>
<p>创建索引，只定义3个字段，user_id, user_name和create_time:</p>
<pre><code>-POST /<span class="variable">$ES</span>/event_index

{
  <span class="string">"mappings"</span>: {
    <span class="string">"event"</span>: {
      <span class="string">"_ttl"</span>: {
        <span class="string">"enabled"</span>: false
      },
      <span class="string">"_timestamp"</span>: {
        <span class="string">"enabled"</span>: true,
        <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
      },
      <span class="string">"properties"</span>: {
        <span class="string">"user_id"</span>: {
          <span class="string">"type"</span>: <span class="string">"string"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>,
          <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>
        },
        <span class="string">"create_time"</span>: {
          <span class="string">"type"</span>: <span class="string">"date"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>,
          <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>,
          <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
        },
        <span class="string">"user_name"</span>: {
          <span class="string">"type"</span>: <span class="string">"string"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>
        }
      }
    }
  }
}
</code></pre><p>定义对应的查询模板，模板名字stats，使用了Cardinality和DateHistogram这两个Aggregation<br>，其中Date Histogram嵌套在Cardinality里。在定义模板的时候，{ { } } 的表示是个参数，需要调用模板的时候传递进来:</p>
<pre><code>  -POST /<span class="variable">$ES</span>/_search/template/stats
{
    <span class="string">"template"</span>: {
        <span class="string">"query"</span>: {
            <span class="string">"bool"</span>: {
                <span class="string">"must"</span>: [
                    {
                        <span class="string">"range"</span>: {
                            <span class="string">"create_time"</span>: {
                                <span class="string">"gte"</span>: <span class="string">""</span>,
                                <span class="string">"lte"</span>: <span class="string">""</span>
                            }
                        }
                    }
                ]
            }
        },
        <span class="string">"size"</span>: <span class="number">0</span>,
        <span class="string">"aggs"</span>: {
            <span class="string">"stats_data"</span>: {
                <span class="string">"date_histogram"</span>: {
                    <span class="string">"field"</span>: <span class="string">"create_time"</span>,
                    <span class="string">"interval"</span>: <span class="string">""</span>
                },
                <span class="string">"aggs"</span>: {
                    <span class="string">"time"</span>: {
                        <span class="string">"cardinality"</span>: {
                            <span class="string">"field"</span>: <span class="string">"user_id"</span>
                        }
                    }
                }
            }
        }
    }
}
</code></pre><p>Cardinality Aggregation的作用就是类似sql中的distinct，去重。</p>
<p>Date Histogram Aggregation的作用是根据时间进行统计。内部有个interval属性表面统计的范畴。</p>
<p>下面加几条数据到event_index里：</p>
<pre><code>-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 12:00:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"2"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format2"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:30:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"3"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format3"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:30:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:50:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:55:00"</span>
}
</code></pre><p>11-07 12-13点有1条数据，1个用户<br>11-07 13-14点有4条数据，3个用户</p>
<p>使用模板查询：</p>
<pre><code>curl -XGET <span class="string">"<span class="variable">$ES</span>/event_index/_search/template"</span> -d'{
  <span class="string">"template"</span>: { <span class="string">"id"</span>: <span class="string">"stats"</span> }, 
  <span class="string">"params"</span>: { <span class="string">"earliest"</span>: <span class="string">"2015-11-07 00:00:00"</span>, <span class="string">"latest"</span>: <span class="string">"2015-11-07 23:59:59"</span>, <span class="string">"interval"</span>: <span class="string">"hour"</span> }
}'    
</code></pre><p>结果：</p>
<pre><code>{
    "<span class="attribute">took</span>": <span class="value"><span class="number">3</span></span>,
    "<span class="attribute">timed_out</span>": <span class="value"><span class="literal">false</span></span>,
    "<span class="attribute">_shards</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">successful</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">failed</span>": <span class="value"><span class="number">0</span>
    </span>}</span>,
    "<span class="attribute">hits</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">max_score</span>": <span class="value"><span class="number">0</span></span>,
        "<span class="attribute">hits</span>": <span class="value">[]
    </span>}</span>,
    "<span class="attribute">aggregations</span>": <span class="value">{
        "<span class="attribute">stats_data</span>": <span class="value">{
            "<span class="attribute">buckets</span>": <span class="value">[
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 12:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446897600000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">1</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">1</span>
                    </span>}
                </span>},
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 13:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446901200000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">4</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">3</span>
                    </span>}
                </span>}
            ]
        </span>}
    </span>}
</span>}
</code></pre><p>12点-13点的只有1条数据，1个用户。13-14点的有4条数据，3个用户。</p>
<p>以天(day)统计：</p>
<pre><code>curl -XGET <span class="string">"<span class="variable">$ES</span>/event_index/_search/template"</span> -d'{
  <span class="string">"template"</span>: { <span class="string">"id"</span>: <span class="string">"stats"</span> }, 
  <span class="string">"params"</span>: { <span class="string">"earliest"</span>: <span class="string">"2015-11-07 00:00:00"</span>, <span class="string">"latest"</span>: <span class="string">"2015-11-07 23:59:59"</span>, <span class="string">"interval"</span>: <span class="string">"day"</span> }
}'    
</code></pre><p>结果：</p>
<pre><code>{
    "<span class="attribute">took</span>": <span class="value"><span class="number">4</span></span>,
    "<span class="attribute">timed_out</span>": <span class="value"><span class="literal">false</span></span>,
    "<span class="attribute">_shards</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">successful</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">failed</span>": <span class="value"><span class="number">0</span>
    </span>}</span>,
    "<span class="attribute">hits</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">max_score</span>": <span class="value"><span class="number">0</span></span>,
        "<span class="attribute">hits</span>": <span class="value">[]
    </span>}</span>,
    "<span class="attribute">aggregations</span>": <span class="value">{
        "<span class="attribute">stats_data</span>": <span class="value">{
            "<span class="attribute">buckets</span>": <span class="value">[
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 00:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446854400000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">5</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">3</span>
                    </span>}
                </span>}
            ]
        </span>}
    </span>}
</span>}
</code></pre><p>11-07这一天有5条数据，3个用户。</p>
<p>本文只是简单说明了es查询模板的使用，也简单使用了2个aggregation。更多内容可以去官网查看相关资料。</p>
]]></content>
    <summary type="html">
    <![CDATA[近在公司又用到了elasticsearch，也用到了查询模板，顺便写篇文章记录一下查询模板的使用 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="elasticsearch" scheme="http://fangjian0423.github.io/tags/elasticsearch/"/>
    
      <category term="elasticsearch" scheme="http://fangjian0423.github.io/categories/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[html页面左右滑动菜单效果的实现]]></title>
    <link href="http://fangjian0423.github.io/2015/10/29/html-left-right-menu/"/>
    <id>http://fangjian0423.github.io/2015/10/29/html-left-right-menu/</id>
    <published>2015-10-29T13:15:13.000Z</published>
    <updated>2015-12-20T17:06:35.000Z</updated>
    <content type="html"><![CDATA[<h2 id="正文">正文</h2><p>最近要实现一个微信端页面左边弹出菜单的实现，效果如下：</p>
<iframe src="//jsfiddle.net/format/t0xda6zv/embedded/result,js,html,css" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
<p>绿色部分是左菜单内容，高度填充满整个页面。且有滚动条，并且滚动条内容随着滚动条的滚动不会影响正文的滚动，正文内容的滚动不会影响左菜单的滚动。</p>
<p>下面说下自己实现这种效果的思路。</p>
<p>1.首先由于左右两边的滚动不影响双方，这就需要将菜单和内容的position设置为绝对定位，设置都需要滚动条: overflow: auto;  。</p>
<p>2.菜单的内容会覆盖正文的内容：所以菜单的z-index比正文要大。</p>
<p>3.给左边的菜单加点width动画，需要显示的时候设置width，需要隐藏的时候width设置为0即可。</p>
<p>如果不想把菜单的内容覆盖在正文内容上面，而是正文内容向右偏移菜单的宽度：</p>
<iframe src="//jsfiddle.net/format/of445qxn/embedded/result,js,html,css" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
<p>这个效果与上一个效果一样，唯一的区别就是正文的z-index比菜单大，而且正文需要配置一个背景色。最后切换菜单的时候正文内容加点向右便宜的动画即可。</p>
<p>刚换了next主题…. 发现这个主题右边的sidebar也是这样的效果实现  →_→ 。 囧 。</p>
<h2 id="基础小知识">基础小知识</h2><p>上面第二个例子中内容的z-index比菜单的z-index要大，而且正文需要配置一个背景色。为什么正文需要配置一个背景色呢？</p>
<p><strong>因为html中的元素没指定背景色的话，那说明这个元素的背景色是透明的</strong></p>
<p>比如下面这个效果，上面的内容没设置背景色，所以是透明的，虽然它的z-index比上面的块要大，但是还是显示了。</p>
<iframe src="//jsfiddle.net/format/yxseu05z/embedded/result,html" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
<p>给上面的内容设置黄色的背景色就可以隐藏下面的内容的。</p>
<iframe src="//jsfiddle.net/format/kec1up6t/embedded/result,html" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
]]></content>
    <summary type="html">
    <![CDATA[最近要实现一个微信端页面左边弹出菜单的实现，效果如下 ...]]>
    
    </summary>
    
      <category term="css" scheme="http://fangjian0423.github.io/tags/css/"/>
    
      <category term="html" scheme="http://fangjian0423.github.io/tags/html/"/>
    
      <category term="css" scheme="http://fangjian0423.github.io/categories/css/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[最近前端的一点总结]]></title>
    <link href="http://fangjian0423.github.io/2015/10/24/front-end/"/>
    <id>http://fangjian0423.github.io/2015/10/24/front-end/</id>
    <published>2015-10-24T12:15:13.000Z</published>
    <updated>2015-12-20T17:01:03.000Z</updated>
    <content type="html"><![CDATA[<p>这段时间被拉去当做前端了，做了快1个多月了，也好久没写博客了。 做了两个项目的前端，这周是第二个项目的启动，而且这周简直是灾难的一周，基本上都是23点后下班的，有两天还是凌晨2点。悲剧。</p>
<p>做前端经验不是很多，这个月还是学到了一些前端的自己没掌握的知识。做个总结吧。</p>
<p>1.box-sizing属性。</p>
<p>box-sizing是css3引入的。有两个值，分别content-box和border-box。 默认为content-box。这个属性的作用是这样的：</p>
<p>比如我们定义一个div，宽度和高度都是50px。padding 5px， border: 5px。 那么这个div实际的宽度和高度是：</p>
<pre><code><span class="number">50</span> + <span class="number">2</span> * <span class="number">5</span> + <span class="number">2</span> * <span class="number">5</span> = <span class="number">70</span>。
</code></pre><p>如果使用border-box的话，那么这个div的宽度还是50。 因为border-box会把padding和border都一起算到宽度和高度里面。所以使用border-box后div的高度和宽度为还是50。但是实际上真正显示内容的高度和宽度是 50 - 5 <em> 2 - 5 </em> 2 = 30。</p>
<p>直接来点实际的代码，使用content-box，也就是默认情况：</p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"background-color: blue; width: 100px; height: 100px;"</span>&gt;
    &lt;<span class="keyword">div</span> style=<span class="string">"background-color: yellow; width: 50px; height: 50px; padding: 5px; border: 5px solid red; box-sizing: content-box;"</span>&gt;&lt;/<span class="keyword">div</span>&gt;
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/content-box1.png" alt=""><br><img src="http://7x2wh6.com1.z0.glb.clouddn.com/content-box2.png" alt=""></p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"background-color: blue; width: 100px; height: 100px;"</span>&gt;
    &lt;<span class="keyword">div</span> style=<span class="string">"background-color: yellow; width: 50px; height: 50px; padding: 5px; border: 5px solid red; box-sizing: border-box;"</span>&gt;&lt;/<span class="keyword">div</span>&gt;
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png" alt=""><br><img src="http://7x2wh6.com1.z0.glb.clouddn.com/border-box2.png" alt=""></p>
<p>Bootstrap3中也大量使用了border-box。</p>
<p>2.white-space属性。</p>
<p>使用white-space是为了让多张图片在同一行展示，不会换行。</p>
<pre><code>&lt;<span class="tag">div</span> style=<span class="string">"width: 200px;"</span>&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
&lt;/div&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/white-space1.png" alt=""></p>
<p>将外层的div改为：</p>
<pre><code>&lt;<span class="tag">div</span> style=<span class="string">"width: 200px; white-space: nowrap; overflow: scroll;"</span>&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/white-space2.png" alt=""></p>
<p>white-space设置为nowrap后，文本内容不会换行直到遇到br标签为止。</p>
<p>3.CSS3的动画。</p>
<p>一开始没有使用CSS3的动画，使用了JQuery的animate，结果发现页面动画有点卡，后来改成了CSS3的动画。</p>
<p>因为都是一些比较简单的动画，所以只能写下简单的动画属性了。</p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"width: 50px; height: 50px; background-color: yellow; transition: width .2s;"</span>&gt;
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p>div的点击事件：</p>
<pre><code>$(<span class="string">"div"</span>).click(<span class="keyword">function</span>() {
    var <span class="variable">$div</span> = $(this);
    if(<span class="variable">$div</span>.width() == <span class="number">50</span>) {
      <span class="variable">$div</span>.width(<span class="string">"100"</span>);  
    } else {
      <span class="variable">$div</span>.width(<span class="string">"50"</span>);  
    }
});
</code></pre><p>动画还有延迟效果，这里就不举例了。</p>
<p>4.垂直居中</p>
<p>高度固定的垂直居中，设置line-height为容器高度，text-align为center即可：</p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"width: 100px; height: 100px; background-color: yellow; line-height: 100px; text-align: center;"</span>&gt;
    我居中了
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p>高度不固定的垂直居中：</p>
<pre><code>html, body {
  height: 100%;
}
body {
  display: -webkit-box;
  display: -webkit-flex;

  display: -moz-box;
  display: -moz-flex;

  display: -ms-flexbox;

  display: flex;
<span class="comment">
  /* 水平居中*/</span>
  -<span class="ruby">webkit-box-<span class="symbol">align:</span> center;
</span>  -<span class="ruby">moz-box-<span class="symbol">align:</span> center;
</span>  -<span class="ruby">ms-flex-<span class="symbol">pack:</span>center;<span class="regexp">/* IE 10 */</span>
</span>
  -<span class="ruby">webkit-justify-<span class="symbol">content:</span> center;
</span>  -<span class="ruby">moz-justify-<span class="symbol">content:</span> center;
</span>  justify-content: center;/* IE 11+,Firefox 22+,Chrome 29+,Opera 12.1*/
<span class="comment">
  /* 垂直居中 */</span>
  -<span class="ruby">webkit-box-<span class="symbol">pack:</span> center;
</span>  -<span class="ruby">moz-box-<span class="symbol">pack:</span> center;
</span>  -<span class="ruby">ms-flex-<span class="symbol">align:</span>center;<span class="regexp">/* IE 10 */</span>
</span>
  -<span class="ruby">webkit-align-<span class="symbol">items:</span> center;
</span>  -<span class="ruby">moz-align-<span class="symbol">items:</span> center;
</span>  align-items: center;
}

...
&lt;body&gt;
    垂直居中
&lt;/body&gt;
...
</code></pre><p>还有其他的一些比如绝对定位，固定定位，相对定位，overflow等问题就不一一举例了。 估计之后还是得做前端的一些工作，到时候用到了一些新内容的话还会继续更新前端相关的博客的。</p>
]]></content>
    <summary type="html">
    <![CDATA[这段时间被拉去当做前端了，做了快1个多月了，也好久没写博客了。 做了两个项目的前端，这周是第二个项目的启动 ...]]>
    
    </summary>
    
      <category term="css" scheme="http://fangjian0423.github.io/tags/css/"/>
    
      <category term="html" scheme="http://fangjian0423.github.io/tags/html/"/>
    
      <category term="css" scheme="http://fangjian0423.github.io/categories/css/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala的Predef介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/10/07/scala-predef/"/>
    <id>http://fangjian0423.github.io/2015/10/07/scala-predef/</id>
    <published>2015-10-06T16:37:20.000Z</published>
    <updated>2015-12-20T17:03:03.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>Scala每个程序都会自动import以下3个包，分别是 java.lang.<em>  ,  scala.</em> 和 Predef._ 。跟Java程序都会自动import java.lang一样。</p>
<p>Predef是一个单例对象，所以我们import进来之后，可以直接使用Predef中定义的方法。</p>
<h2 id="Predef中定义的方法和属性">Predef中定义的方法和属性</h2><h3 id="常用方法和类">常用方法和类</h3><pre><code><span class="function"><span class="keyword">def</span> <span class="title">classOf</span>[</span><span class="type">T</span>]: <span class="type">Class</span>[<span class="type">T</span>] = <span class="literal">null</span> <span class="comment">// This is a stub method. The actual implementation is filled in by the compiler.</span>

<span class="class"><span class="keyword">type</span> <span class="title">String</span>        =</span> java.lang.<span class="type">String</span>  <span class="comment">// scala中的String使用jdk中的String</span>
<span class="class"><span class="keyword">type</span> <span class="title">Class</span>[</span><span class="type">T</span>]      = java.lang.<span class="type">Class</span>[<span class="type">T</span>] <span class="comment">// scala中的Class使用jdk中的Class</span>

<span class="class"><span class="keyword">type</span> <span class="title">Function</span>[</span>-<span class="type">A</span>, +<span class="type">B</span>] = <span class="type">Function1</span>[<span class="type">A</span>, <span class="type">B</span>] <span class="comment">// Function1取别名Function</span>

<span class="class"><span class="keyword">type</span> <span class="title">Map</span>[</span><span class="type">A</span>, +<span class="type">B</span>] = immutable.<span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>] <span class="comment">// Map类型默认使用immutable包下的Map</span>
<span class="class"><span class="keyword">type</span> <span class="title">Set</span>[</span><span class="type">A</span>]     = immutable.<span class="type">Set</span>[<span class="type">A</span>] <span class="comment">// Set类型默认使用immutable包下的Set</span>
<span class="keyword">val</span> <span class="type">Map</span>         = immutable.<span class="type">Map</span> <span class="comment">// Map对象默认使用immutable包下的Map对象(下面测试用到)</span>
<span class="keyword">val</span> <span class="type">Set</span>         = immutable.<span class="type">Set</span> <span class="comment">// Set对象默认使用immutable包下的Set对象(下面测试用到)</span>
</code></pre><p>一些测试：</p>
<pre><code>// 默认的Map和<span class="operator"><span class="keyword">Set</span>都是immutable包下的，这里的<span class="keyword">Set</span>和<span class="keyword">Map</span>都是在Predef中定义的一个变量。
<span class="keyword">Map</span>(<span class="number">1</span> -&gt; <span class="number">1</span>, <span class="number">2</span> -&gt; <span class="number">2</span>) // scala.collection.immutable.<span class="keyword">Map</span>[<span class="built_in">Int</span>,<span class="built_in">Int</span>] = <span class="keyword">Map</span>(<span class="number">1</span> -&gt; <span class="number">1</span>, <span class="number">2</span> -&gt; <span class="number">2</span>)
<span class="keyword">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>) // scala.collection.immutable.<span class="keyword">Set</span>[<span class="built_in">Int</span>] = <span class="keyword">Set</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span>
</code></pre><h3 id="打印方法">打印方法</h3><pre><code>def <span class="function"><span class="title">print</span><span class="params">(x: Any)</span></span> = Console.<span class="function"><span class="title">print</span><span class="params">(x)</span></span>
def <span class="function"><span class="title">println</span><span class="params">()</span></span> = Console.<span class="function"><span class="title">println</span><span class="params">()</span></span>
def <span class="function"><span class="title">println</span><span class="params">(x: Any)</span></span> = Console.<span class="function"><span class="title">println</span><span class="params">(x)</span></span>
def <span class="function"><span class="title">printf</span><span class="params">(text: String, xs: Any*)</span></span> = Console.<span class="function"><span class="title">print</span><span class="params">(text.format(xs: _*)</span></span>)
</code></pre><p>因此，平时我们使用的println，print，printf这些打印方法都是在Predef中定义的，</p>
<h3 id="一些调试和错误方法">一些调试和错误方法</h3><pre><code><span class="comment">// 过期方法，抛出带有message消息的RuntimeException</span>
<span class="annotation">@deprecated</span>(<span class="string">"Use `sys.error(message)` instead"</span>, <span class="string">"2.9.0"</span>)
<span class="keyword">def</span> error(<span class="string">message:</span> String): Nothing = sys.error(message)

<span class="comment">// 断言。 参数是一个Boolean类型，失败抛出java.lang.AssertionError异常</span>
<span class="annotation">@elidable</span>(ASSERTION)
<span class="keyword">def</span> <span class="keyword">assert</span>(<span class="string">assertion:</span> Boolean) {
<span class="keyword">if</span> (!assertion)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assertion failed"</span>)
}

<span class="comment">// 跟上一个方法类似，多了一个message参数。抛出的异常就打印出这个message参数</span>
<span class="annotation">@elidable</span>(ASSERTION) <span class="annotation">@inline</span>
<span class="keyword">final</span> <span class="keyword">def</span> <span class="keyword">assert</span>(<span class="string">assertion:</span> Boolean, <span class="string">message:</span> =&gt; Any) {
<span class="keyword">if</span> (!assertion)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assertion failed: "</span>+ message)
}

<span class="comment">// 跟assert类似，唯一的区别是assume支持静态经验(static checker)</span>
<span class="annotation">@elidable</span>(ASSERTION)
<span class="keyword">def</span> assume(<span class="string">assumption:</span> Boolean) {
<span class="keyword">if</span> (!assumption)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assumption failed"</span>)
}

<span class="comment">// 跟assume一样，多了个message参数</span>
<span class="annotation">@elidable</span>(ASSERTION) <span class="annotation">@inline</span>
<span class="keyword">final</span> <span class="keyword">def</span> assume(<span class="string">assumption:</span> Boolean, <span class="string">message:</span> =&gt; Any) {
<span class="keyword">if</span> (!assumption)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assumption failed: "</span>+ message)
}

<span class="comment">// 跟assert类似，只不过抛出的是IllegalArgumentException异常</span>
<span class="keyword">def</span> require(<span class="string">requirement:</span> Boolean) {
<span class="keyword">if</span> (!requirement)
  <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"requirement failed"</span>)
}

<span class="comment">// 多个参数，作用如上一样</span>
<span class="annotation">@inline</span> <span class="keyword">final</span> <span class="keyword">def</span> require(<span class="string">requirement:</span> Boolean, <span class="string">message:</span> =&gt; Any) {
<span class="keyword">if</span> (!requirement)
  <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"requirement failed: "</span>+ message)
}
</code></pre><p>调试和错误方法测试：</p>
<pre><code><span class="function"><span class="title">assert</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>)</span></span> <span class="comment">// java.lang.AssertionError: assertion failed</span>
<span class="function"><span class="title">assert</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>, <span class="string">"test"</span>)</span></span> <span class="comment">// java.lang.AssertionError: assertion failed: test</span>
<span class="function"><span class="title">assume</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>)</span></span> <span class="comment">// java.lang.AssertionError: assumption failed</span>
<span class="function"><span class="title">assume</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>, <span class="string">"test"</span>)</span></span> <span class="comment">// java.lang.AssertionError: assumption failed: test</span>
<span class="function"><span class="title">require</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>)</span></span> <span class="comment">// java.lang.IllegalArgumentException: requirement failed</span>
<span class="function"><span class="title">require</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>, <span class="string">"test"</span>)</span></span> <span class="comment">// java.lang.IllegalArgumentException: requirement failed: test</span>
</code></pre><h3 id="一个特殊的属性">一个特殊的属性</h3><p>Predef中有个 ??? 属性，抛出一个NotImplementedError：</p>
<pre><code><span class="function"><span class="keyword">def</span> ??? :</span> Nothing = throw new NotImplementedError
</code></pre><p>比如定义一些方法的时候，这个方法还没有实现，这个时候可以使用 ???， 而非TODO：</p>
<pre><code><span class="keyword">def</span> todoMethod(x: <span class="keyword">Int</span>): <span class="keyword">Int</span> = ???

todoMethod(<span class="number">2</span>) <span class="comment">// scala.NotImplementedError: an implementation is missing</span>
</code></pre><h2 id="Predef还有大量的隐式转换和隐式转换类">Predef还有大量的隐式转换和隐式转换类</h2><p>再讲Predef中的隐式转换和隐式转换类之前，先介绍一下这2个概念。</p>
<h3 id="隐式转换">隐式转换</h3><p>隐式转换的意思是一个方法中有一个类型的参数，并返回另外一个类型的返回值。比如一个Double类型的方法返回一个Int类型的返回值。</p>
<pre><code>def <span class="function"><span class="title">double2Int</span><span class="params">(d: Double)</span></span> = d<span class="class">.toInt</span>

<span class="function"><span class="title">double2Int</span><span class="params">(<span class="number">2.4</span>)</span></span> <span class="comment">// 2</span>

val <span class="tag">a</span>: Int = <span class="number">2.3</span> <span class="comment">// 报错</span>
</code></pre><p>重新定义double2Int，使其支持隐式转换：</p>
<pre><code><span class="type">implicit</span> def double2Int(d: <span class="type">Double</span>) = d.toInt

val a: <span class="built_in">Int</span> = <span class="number">2.3</span> // a: <span class="built_in">Int</span> = <span class="number">2</span>
</code></pre><h3 id="隐式转换类">隐式转换类</h3><p>当需要给Int类型添加一个 –&gt; 方法的时候，需要使用到隐式转换类。因为隐式转换只支持1个参数，所以只能通过隐式转换类完成。</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMaker(left: <span class="built_in">Int</span>) {
    def --&gt;(right: <span class="built_in">Int</span>) = left to right
}

val <span class="built_in">range</span> = <span class="number">1</span> --&gt; <span class="number">10</span> // <span class="built_in">Range</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)
</code></pre><p>隐式转换类有个弊端，那就是每次都会创建一个类的实例，有时候这完全没有必要。</p>
<p>scala提供了一个将隐式转换类转换成value class的方法，只需要继承AnyVal即可，还需要注意参数需要加上val标识符。</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMaker(val left: <span class="built_in">Int</span>) <span class="keyword">extends</span> AnyVal {
    def --&gt;(right: <span class="built_in">Int</span>) = left to right
}
</code></pre><h3 id="Predef中的隐式转换和隐式转换类">Predef中的隐式转换和隐式转换类</h3><p>-&gt; 方法：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> ArrowAssoc[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    @inline def -&gt; [B](y: B): Tuple2[A, B] = Tuple2(self, y)
    def →[B](y: B): Tuple2[A, B] = -&gt;(y)
}
</code></pre><p>生成一个二元元组，Tuple2。</p>
<pre><code><span class="number">1</span> → <span class="number">2</span>  <span class="comment">// (Int, Int) = (1, 2)</span>
<span class="number">1</span> -&gt; <span class="number">2</span> <span class="comment">// (Int, Int) = (1, 2)</span>
</code></pre><p>ensuring 方法：</p>
<pre><code>implicit final <span class="class"><span class="keyword">class</span> <span class="title">Ensuring</span>[<span class="title">A</span>](<span class="title">private</span> <span class="title">val</span> <span class="title">self</span>: <span class="title">A</span>) <span class="title">extends</span> <span class="title">AnyVal</span> {</span>
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">Boolean</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond)</span>;</span> <span class="keyword">self</span> }
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">Boolean</span>, <span class="symbol">msg:</span> =&gt; <span class="constant">Any</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond, msg)</span>;</span> <span class="keyword">self</span> }
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">A</span> =&gt; <span class="constant">Boolean</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond(<span class="keyword">self</span>)</span>);</span> <span class="keyword">self</span> }
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">A</span> =&gt; <span class="constant">Boolean</span>, <span class="symbol">msg:</span> =&gt; <span class="constant">Any</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond(<span class="keyword">self</span>)</span>, <span class="title">msg</span>);</span> <span class="keyword">self</span> }
}
</code></pre><p>ensuring内部调用assert方法。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">doublePositive</span><span class="params">(n: Int)</span>:</span> Int = {
    n * <span class="number">2</span>
} ensuring(n =&gt; n &gt;= <span class="number">0</span> &amp;&amp; n % <span class="number">2</span> == <span class="number">0</span>)

doublelPositive(<span class="number">1</span>) // <span class="number">2</span>
doublelPositive(-<span class="number">1</span>) // java.lang.AssertionError: assertion failed
</code></pre><p>formatted 方法：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> StringFormat[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    @inline def <span class="keyword">formatted</span>(fmtstr: String): String = fmtstr <span class="keyword">format</span> self
}
</code></pre><p>格式化字符串，使用java.lang.String.format方法。</p>
<pre><code><span class="string">"Format"</span> formatted <span class="string">"%s, let's go"</span> <span class="comment">// Format, let's go</span>
</code></pre><p>+ 方法：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> any2stringadd[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    def +(other: String): String = String.valueOf(self) + other
}
</code></pre><p>case class使用+方法：</p>
<pre><code>case class <span class="function"><span class="title">Student</span><span class="params">(name: String, age: Int)</span></span>
<span class="function"><span class="title">Student</span><span class="params">(<span class="string">"format"</span>, <span class="number">11</span>)</span></span> + <span class="string">" 22"</span> <span class="comment">// Student(format,11) 22</span>
</code></pre><p>StringOps类：</p>
<pre><code>@inline <span class="type">implicit</span> def augmentString(x: String): StringOps = new StringOps(x)
@inline <span class="type">implicit</span> def unaugmentString(x: StringOps): String = x.repr
</code></pre><p>StringOps提供了丰富的原生String没提供的方法：</p>
<pre><code><span class="string">"format"</span>.length <span class="comment">// 6 原生String是没有提供length方法的</span>
<span class="string">"format"</span>.foreach(println(_))
<span class="string">"format"</span>.stripPrefix(<span class="string">"for"</span>) <span class="comment">// mat</span>
<span class="string">"format"</span>.slice(<span class="number">1</span>, <span class="number">3</span>) <span class="comment">// or</span>
<span class="string">"format"</span> * <span class="number">5</span> <span class="comment">// formatformatformatformatformat</span>
<span class="string">"true"</span>.toBoolean <span class="comment">// true</span>
</code></pre><p>ArrayOps类：</p>
<pre><code><span class="type">implicit</span> def genericArrayOps[T](xs: Array[T]): ArrayOps[T] = (xs match {
    <span class="keyword">case</span> x: Array[AnyRef]  =&gt; refArrayOps[AnyRef](x)
    <span class="keyword">case</span> x: Array[Boolean] =&gt; booleanArrayOps(x)
    <span class="keyword">case</span> x: Array[Byte]    =&gt; byteArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="built_in">Char</span>]    =&gt; charArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="type">Double</span>]  =&gt; doubleArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="built_in">Float</span>]   =&gt; floatArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="built_in">Int</span>]     =&gt; intArrayOps(x)
    <span class="keyword">case</span> x: Array[Long]    =&gt; longArrayOps(x)
    <span class="keyword">case</span> x: Array[Short]   =&gt; shortArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="keyword">Unit</span>]    =&gt; unitArrayOps(x)
    <span class="keyword">case</span> null              =&gt; null
}).asInstanceOf[ArrayOps[T]]


  <span class="type">implicit</span> def booleanArrayOps(xs: Array[Boolean]): ArrayOps[Boolean] = new ArrayOps.ofBoolean(xs)
  <span class="type">implicit</span> def byteArrayOps(xs: Array[Byte]): ArrayOps[Byte]          = new ArrayOps.ofByte(xs)
  <span class="type">implicit</span> def charArrayOps(xs: Array[<span class="built_in">Char</span>]): ArrayOps[<span class="built_in">Char</span>]          = new ArrayOps.ofChar(xs)
  <span class="type">implicit</span> def doubleArrayOps(xs: Array[<span class="type">Double</span>]): ArrayOps[<span class="type">Double</span>]    = new ArrayOps.ofDouble(xs)
  <span class="type">implicit</span> def floatArrayOps(xs: Array[<span class="built_in">Float</span>]): ArrayOps[<span class="built_in">Float</span>]       = new ArrayOps.ofFloat(xs)
  <span class="type">implicit</span> def intArrayOps(xs: Array[<span class="built_in">Int</span>]): ArrayOps[<span class="built_in">Int</span>]             = new ArrayOps.ofInt(xs)
  <span class="type">implicit</span> def longArrayOps(xs: Array[Long]): ArrayOps[Long]          = new ArrayOps.ofLong(xs)
  <span class="type">implicit</span> def refArrayOps[T &lt;: AnyRef](xs: Array[T]): ArrayOps[T]    = new ArrayOps.ofRef[T](xs)
  <span class="type">implicit</span> def shortArrayOps(xs: Array[Short]): ArrayOps[Short]       = new ArrayOps.ofShort(xs)
  <span class="type">implicit</span> def unitArrayOps(xs: Array[<span class="keyword">Unit</span>]): ArrayOps[<span class="keyword">Unit</span>]          = new ArrayOps.ofUnit(xs)
</code></pre><p>ArrayOps提供了丰富的原生数组没提供的方法：</p>
<pre><code>Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>) :+ <span class="number">4</span> <span class="comment">// Array(1, 2, 3, 4)</span>
</code></pre><p>Scala程序员可以较少关心装箱和拆箱操作，这也是由于Predef对象里定义了Scala值类型与java基本类型直接的隐式转换：</p>
<pre><code><span class="type">implicit</span> def byte2Byte(x: Byte)           = java.lang.Byte.valueOf(x)
<span class="type">implicit</span> def short2Short(x: Short)        = java.lang.Short.valueOf(x)
<span class="type">implicit</span> def char2Character(x: <span class="built_in">Char</span>)      = java.lang.<span class="type">Character</span>.valueOf(x)
<span class="type">implicit</span> def int2Integer(x: <span class="built_in">Int</span>)          = java.lang.<span class="type">Integer</span>.valueOf(x)
<span class="type">implicit</span> def long2Long(x: Long)           = java.lang.Long.valueOf(x)
<span class="type">implicit</span> def float2Float(x: <span class="built_in">Float</span>)        = java.lang.<span class="built_in">Float</span>.valueOf(x)
<span class="type">implicit</span> def double2Double(x: <span class="type">Double</span>)     = java.lang.<span class="type">Double</span>.valueOf(x)
<span class="type">implicit</span> def boolean2Boolean(x: Boolean)  = java.lang.Boolean.valueOf(x)

<span class="type">implicit</span> def Byte2byte(x: java.lang.Byte): Byte             = x.byteValue
<span class="type">implicit</span> def Short2short(x: java.lang.Short): Short         = x.shortValue
<span class="type">implicit</span> def Character2char(x: java.lang.<span class="type">Character</span>): <span class="built_in">Char</span>   = x.charValue
<span class="type">implicit</span> def Integer2int(x: java.lang.<span class="type">Integer</span>): <span class="built_in">Int</span>         = x.intValue
<span class="type">implicit</span> def Long2long(x: java.lang.Long): Long             = x.longValue
<span class="type">implicit</span> def Float2float(x: java.lang.<span class="built_in">Float</span>): <span class="built_in">Float</span>         = x.floatValue
<span class="type">implicit</span> def Double2double(x: java.lang.<span class="type">Double</span>): <span class="type">Double</span>     = x.doubleValue
<span class="type">implicit</span> def Boolean2boolean(x: java.lang.Boolean): Boolean = x.booleanValue
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Scala每个程序都会自动import以下3个包，分别是 java.lang._  ,  scala._ 和 Predef._ 。跟Java程序都会自动import java.lang一样 ...]]>
    
    </summary>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/tags/jvm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spring Boot 部分特性记录]]></title>
    <link href="http://fangjian0423.github.io/2015/09/21/springboot-intro/"/>
    <id>http://fangjian0423.github.io/2015/09/21/springboot-intro/</id>
    <published>2015-09-20T16:21:49.000Z</published>
    <updated>2015-12-20T17:07:24.000Z</updated>
    <content type="html"><![CDATA[<p>SpringBoot是Java的一个micro-service框架。它设计的目的是简化Spring应用的初始搭建以及开发过程。使用SpringBoot可以避免大量的xml配置文件，它内部使用很多约定的方式。</p>
<p>以一个最简单的MVC例子来说，使用SpringBoot进行开发的话定义好对应的Controller，Repository和Entity之后，加上各自的Annotation即可。</p>
<p>Repository框架可以选择Spring Data或者Hibernate，可通过自由配置。</p>
<p>视图框架也可通过配置选择freemarker或者velocity等视图框架。</p>
<p>下面，介绍一下SpringBoot的一些功能。</p>
<h2 id="SpringBoot框架的启动">SpringBoot框架的启动</h2><p>SpringBoot使用SpringApplication这个类进行项目的启动。一般都会这么写：</p>
<pre><code><span class="annotation">@SpringBootApplication</span>
<span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> {</span>
    <span class="keyword">public</span> <span class="keyword">static</span> <span class="typename">void</span> main(String[] args) {
        SpringApplication.run(Application.<span class="keyword">class</span>, args);
    }
}
</code></pre><p>使用SpringBootApplication注解相当于使用了3个注解，分别是@ComponentScan，@Configuration，@EnableAutoConfiguration。</p>
<p>这里需要注意的是Application这个类所有的package位置。</p>
<p>比如你的项目的包路径是 me.format.project1，对应的controller和repository包是 me.format.project1.controller和me.format.project1.repository。 那么这个Application需要在的包路径为 me.format.project1。 因为SpringBootApplication注解内部是使用ComponentScan注解，这个注解会扫描Application包所在的路径下的各个bean。</p>
<h2 id="Profile的使用">Profile的使用</h2><p>可以在springboot项目中加入配置文件application.yml。</p>
<p>yaml中可以定义多个profile，也可以指定激活的profile：</p>
<pre><code><span class="attribute">spring</span>:
  profiles.<span class="attribute">active</span>: dev

---
<span class="attribute">spring</span>:
  <span class="attribute">profiles</span>: dev
<span class="attribute">myconfig</span>:
  <span class="attribute">config1</span>: dev-enviroment

---
<span class="attribute">spring</span>:
  <span class="attribute">profiles</span>: test
<span class="attribute">myconfig</span>:
  <span class="attribute">config1</span>: test-enviroment

---
<span class="attribute">spring</span>:
  <span class="attribute">profiles</span>: prod
<span class="attribute">myconfig</span>:
  <span class="attribute">config1</span>: prod-envioment
</code></pre><p>也可以在运行的执行指定profile：</p>
<pre><code>java -Dspring<span class="class">.profiles</span><span class="class">.active</span>=<span class="string">"prod"</span> -jar yourjar.jar
</code></pre><p>还可以使用Profile注解，MyConfig只会在prod这个profile下才会生效，其他profile不会生效：</p>
<pre><code><span class="variable">@Profile</span>(<span class="string">"prod"</span>)
<span class="variable">@Component</span>
public class MyConfig {
    ....
}
</code></pre><h2 id="自定义的一些Conveter，Interceptor">自定义的一些Conveter，Interceptor</h2><p>如果想配置springmvc的HandlerInterceptorAdapter或者HttpMessageConverter。 只需要定义自己的interceptor或者converter，然后加上Component注解。这样SpringBoot会自动处理这些类，不用自己在配置文件里指定对应的内容。 这个也是相当方便的。</p>
<pre><code><span class="annotation">@Component</span>
public <span class="class"><span class="keyword">class</span> <span class="title">AuthInterceptor</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">HandlerInterceptorAdapter</span> {</span>
    ...
}

<span class="annotation">@Component</span>
public <span class="class"><span class="keyword">class</span> <span class="title">MyConverter</span> <span class="title">implements</span> <span class="title">HttpMessageConverter&lt;MyObj&gt;</span> {</span> 
    ...
}
</code></pre><h2 id="模板的使用">模板的使用</h2><p>比如使用freemarker的时候，加入以下依赖：</p>
<pre><code><span class="tag">&lt;<span class="title">dependency</span>&gt;</span>
    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span>
    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spring-boot-starter-freemarker<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span>
<span class="tag">&lt;/<span class="title">dependency</span>&gt;</span>
</code></pre><p>然后在resources目录下建立一个templates目录即可，视图将会从这个templates位置开始找。</p>
<h2 id="其他">其他</h2><p>关于其他的特性可以参考官方文档：</p>
<p><a href="http://docs.spring.io/spring-boot/docs/current/reference/html/" target="_blank" rel="external">http://docs.spring.io/spring-boot/docs/current/reference/html/</a></p>
<p>springboot还提供了一系列sample供参考：</p>
<p><a href="https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples" target="_blank" rel="external">https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples</a></p>
]]></content>
    <summary type="html">
    <![CDATA[SpringBoot是Java的一个micro-service框架。它设计的目的是简化Spring应用的初始搭建以及开发过程。使用SpringBoot可以避免大量的xml配置文件，它内部使用很多约定的方式 ...]]>
    
    </summary>
    
      <category term="microservice" scheme="http://fangjian0423.github.io/tags/microservice/"/>
    
      <category term="springboot" scheme="http://fangjian0423.github.io/tags/springboot/"/>
    
      <category term="springboot" scheme="http://fangjian0423.github.io/categories/springboot/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala持久层框架Slick介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/08/18/slick-intro/"/>
    <id>http://fangjian0423.github.io/2015/08/18/slick-intro/</id>
    <published>2015-08-18T15:22:33.000Z</published>
    <updated>2015-12-20T17:05:53.000Z</updated>
    <content type="html"><![CDATA[<h2 id="FRM介绍">FRM介绍</h2><p>最近看到了一个FRM的框架Slick。 FRM的意思是Functional Relational Mapping， 一种基于函数式的ORM。</p>
<p>举一个最简单的例子：</p>
<pre><code>val queryResult = db.query(queryStr)

queryResult.onSuccess { <span class="literal">result</span> =&gt;
    <span class="literal">result</span>.doSomething ...
}
</code></pre><p>数据库db查询一条sql语句。查询成功的时候使用闭包完成处理。 看到这段代码的第一反应就是js的ajax处理，代码几乎是一样的，也发现之前在学校里写nodejs的时候查询db也是这样的语法。</p>
<pre><code><span class="keyword">var</span> jqxhr = $.ajax( {
    url: 'url',
    <span class="keyword">method</span>: '<span class="type">GET</span>',
    data: [user: 'format']
});

jqxhr.success(function() {
    ...
});
</code></pre><p>FRM相比ORM最明显的优势就是FRM基于多线程的Future的数据查询，而ORM是单线程的线性执行。</p>
<p>FRM构造sql查询也是相当简单的：</p>
<pre><code><span class="comment">// 构造查询</span>
val newQuery = students.<span class="function"><span class="title">filter</span><span class="params">(_.age &gt; <span class="number">24</span>)</span></span>.<span class="function"><span class="title">sortBy</span><span class="params">(_.name)</span></span>
<span class="comment">// 执行查询</span>
db.<span class="function"><span class="title">run</span><span class="params">(newQuery)</span></span>
</code></pre><p>FRM其他的优势可以参考<a href="http://slick.typesafe.com/doc/3.0.1/introduction.html#functional-relational-mapping" target="_blank" rel="external">官方文档</a>。</p>
<h2 id="Slick实例">Slick实例</h2><p>下面以一个Students和Classrooms的实例来说明一下Slick的使用。</p>
<p>首先是创建对应的domain，学生与教室的关系是1对多。</p>
<p>Students domain(使用Option类型说明该列是可为空的)：</p>
<pre><code>class Student(tag: Tag) extends Table[<span class="link_label">(Int, String, Int, Int, Option[Date</span>])](tag, "Students") {

  def id: Rep[<span class="link_label">Int</span>] = column[<span class="link_label">Int</span>](<span class="link_url">"id", O.PrimaryKey, O.AutoInc</span>)
  def name: Rep[<span class="link_label">String</span>] = column[<span class="link_label">String</span>](<span class="link_url">"name"</span>)
  def age: Rep[<span class="link_label">Int</span>] = column[<span class="link_label">Int</span>](<span class="link_url">"age"</span>)
  def birthDate: Rep[<span class="link_label">Option[Date</span>]] = column[<span class="link_label">Option[Date</span>]]("birth_date")
  def classroomId = column[<span class="link_label">Int</span>](<span class="link_url">"classroom_id"</span>)

   def * : ProvenShape[(Int, String, Int, Int, Option[Date])] = (id, name, age, classroomId, birthDate)

  def classroom: ForeignKeyQuery[Classroom, (Int, String)] = foreignKey("FK<span class="emphasis">_CLASSROOM", classroomId, TableQuery[Classroom])(_</span>.id)
}
</code></pre><p>Classrooms domain：</p>
<pre><code>class Classroom(tag: Tag) extends Table[<span class="link_label">(Int, String)</span>](<span class="link_url">tag, "Classrooms"</span>) {
  def id = column[<span class="link_label">Int</span>](<span class="link_url">"id", O.PrimaryKey, O.AutoInc</span>)
  def name = column[<span class="link_label">String</span>](<span class="link_url">"name"</span>)

  def * = (id, name)
}
</code></pre><p>各个db操作，schema创建，sql插入，sql查询等操作如下，加了几句备注，具体的代码就不分析了：</p>
<pre><code><span class="keyword">object</span> <span class="type">SampleSlickDemo</span> extends <span class="type">App</span> {

  val db = <span class="type">Database</span>.forConfig(<span class="string">"h2mem1"</span>)

  <span class="keyword">try</span> {

    val classrooms = <span class="type">TableQuery</span>[<span class="type">Classroom</span>]
    val students = <span class="type">TableQuery</span>[<span class="type">Student</span>]


    val setupAction: <span class="type">DBIO</span>[<span class="type">Unit</span>] = <span class="type">DBIO</span>.<span class="type">seq</span>(
      // create student <span class="keyword">and</span> classroom table <span class="keyword">in</span> database
      (classrooms.schema ++ students.schema).create,
      // insert some rows <span class="keyword">in</span> classroom
      classrooms += (<span class="number">1</span>, <span class="string">"classroom1"</span>),
      classrooms += (<span class="number">2</span>, <span class="string">"classroom2"</span>),
      classrooms += (<span class="number">3</span>, <span class="string">"classroom2"</span>)
    )

    val setupFuture = db.run(setupAction)

    val f = setupFuture.flatMap { _ =&gt;

      val insertAction: <span class="type">DBIO</span>[<span class="type">Option</span>[<span class="type">Int</span>]] = students ++= <span class="type">Seq</span> (
        (<span class="number">1</span>, <span class="string">"format1"</span>, <span class="number">11</span>, <span class="number">1</span>, new <span class="type">Date</span>(<span class="type">System</span>.currentTimeMillis())),
        (<span class="number">2</span>, <span class="string">"format2"</span>, <span class="number">22</span>, <span class="number">2</span>, new <span class="type">Date</span>((<span class="type">System</span>.currentTimeMillis()))),
        (<span class="number">3</span>, <span class="string">"format3"</span>, <span class="number">33</span>, <span class="number">3</span>, new <span class="type">Date</span>((<span class="type">System</span>.currentTimeMillis())))
      )

      val insertAndPrintAction = insertAction.map { studentResult =&gt;
        studentResult.foreach { numRows =&gt;
          println(<span class="string">s"inserted $numRows students"</span>)
        }
      }

      db.run(insertAndPrintAction)
    }.flatMap { _ =&gt;

      // print <span class="type">All</span> <span class="type">Classrooms</span>
      db.run(classrooms.<span class="literal">result</span>).map { classroom =&gt;
        classroom.foreach(println);
      }

      // print <span class="type">All</span> <span class="type">Students</span>
      db.run(students.<span class="literal">result</span>).map { studnet =&gt;
        studnet.foreach(println);
      }

      // condition search
      val studentQuery = students.filter(_.age &gt; <span class="number">20</span>).sortBy(_.name)
      db.run(studentQuery.<span class="literal">result</span>).map { student =&gt;
        student.foreach(println)
      }
    }
    <span class="type">Await</span>.<span class="literal">result</span>(f, <span class="type">Duration</span>.<span class="type">Inf</span>)
  } <span class="keyword">finally</span> db.close()
}
</code></pre><h2 id="数据库配置">数据库配置</h2><p>在配置文件application.conf里配置数据库配置信息：</p>
<pre><code><span class="title">h2mem1</span> = {
  <span class="title">url</span> = <span class="string">"jdbc:h2:mem:test1"</span>
  driver = org.h2.Driver
  connectionPool = disabled
  keepAliveConnection = <span class="built_in">true</span>
}
</code></pre><p>然后就可使用Database初始化数据库，参数就是配置文件里对应的数据库name：</p>
<pre><code>val db = Database.<span class="function"><span class="title">forConfig</span><span class="params">(<span class="string">"h2mem1"</span>)</span></span>
</code></pre><h2 id="DBIOAction介绍">DBIOAction介绍</h2><p>DBIOAction就是数据库的一个操作，比如Insert，Update，Delete，Query等操作。</p>
<p>可以使用上面分析的数据库配置变量db进行操作。</p>
<p>db有个run方法使用DBIOAction作为参数，返回Future类型的返回值。</p>
<p>DBIO是一个单例对象，它的seq方法可以传入多个DBIOAction，然后返回一个新的DBIOAction。 += 方法返回的也是DBIOAction。</p>
<pre><code>val setupAction: DBIO[Unit] = DBIO.se<span class="string">q(
  (classrooms.schema ++ students.schema)</span>.create,
  classrooms += (<span class="number">1</span>, <span class="string">"classroom1"</span>),
  classrooms += (<span class="number">2</span>, <span class="string">"classroom2"</span>),
  classrooms += (<span class="number">3</span>, <span class="string">"classroom2"</span>)
)
</code></pre><p>++=方法跟+=方法一样会返回DBIOAction，只不过它的参数是个Iterable：</p>
<pre><code>val insertAction: DBIO[Option[Int]] = students ++= Seq (
    (<span class="number">1</span>, <span class="string">"format1"</span>, <span class="number">11</span>, <span class="number">1</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),
    (<span class="number">2</span>, <span class="string">"format2"</span>, <span class="number">22</span>, <span class="number">2</span>, <span class="keyword">new</span> Date((System.currentTimeMillis()))),
    (<span class="number">3</span>, <span class="string">"format3"</span>, <span class="number">33</span>, <span class="number">3</span>, <span class="keyword">new</span> Date((System.currentTimeMillis())))
  )
</code></pre><p>DBIOAction提供许多好用的方法：</p>
<p>map方法：参数是个函数，这个函数可以返回任意类型的值，返回是个DBIOAction。 所以可以使用map关联起来多个DBIOAction。</p>
<p>flatMap方法：参数是个函数，这个函数的返回值必须是个DBIOAction，返回值是个DBIOAction。作用跟map类似，只不过函数参数的返回值不一样。</p>
<p>filter方法：参数是个函数，这个函数的返回值必须是个Boolean，返回值是个DBIOAction。过滤作用。</p>
<p>andThen方法：参数是个DBIOAction，返回值是个DBIOAction。在Action完成后执行另外一个Action。</p>
<h2 id="增删改查操作">增删改查操作</h2><h3 id="查询">查询</h3><p>Slick的查询可以直接通过TableQuery操作，使用TableQuery提供的filter可以实现过滤操作，使用drop和take完成分页操作，使用sortBy完成排序操作。</p>
<pre><code>students.<span class="function"><span class="title">filter</span><span class="params">(_.classroomId === <span class="number">1</span>)</span></span>
students.<span class="function"><span class="title">drop</span><span class="params">(<span class="number">1</span>)</span></span>.<span class="function"><span class="title">take</span><span class="params">(<span class="number">2</span>)</span></span>
students.<span class="function"><span class="title">sortBy</span><span class="params">(_.age.desc)</span></span>
</code></pre><p>可以使用map方法找出需要的列。 </p>
<p>多列：</p>
<pre><code><span class="component">students.map { student =&gt;
  (student<span class="string">.name</span>, student<span class="string">.age)</span>
}</span>
</code></pre><p>一列：</p>
<pre><code>students.<span class="function"><span class="title">map</span><span class="params">(_.name)</span></span>
</code></pre><p>Join方法：</p>
<p>cross join操作：</p>
<pre><code>val crossJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students <span class="built_in">join</span> classrooms
} yield (s.<span class="built_in">name</span>, c.<span class="built_in">name</span>)
</code></pre><p>inner Join操作：</p>
<pre><code>val innerJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students join classrooms <span class="function_start"><span class="keyword">on</span></span> (_.classroomId === _.<span class="property">id</span>)
} yield (s.<span class="property">name</span>, c.<span class="property">name</span>)
</code></pre><p>另外一个inner join：</p>
<pre><code>val innerJoin = <span class="keyword">for</span> {
  s &lt;- students
  c &lt;- classrooms <span class="keyword">if</span> c.<span class="property">id</span> === s.classroomId
} yield (s.<span class="property">name</span>, c.<span class="property">name</span>)
</code></pre><p>left join操作：</p>
<pre><code>val leftJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students joinLeft classrooms <span class="function_start"><span class="keyword">on</span></span> (_.classroomId === _.<span class="property">id</span>)
} yield (s.<span class="property">name</span>, c.map(_.<span class="property">name</span>))
</code></pre><p>right join操作：</p>
<pre><code>val rightJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students joinRight classrooms <span class="function_start"><span class="keyword">on</span></span> (_.classroomId === _.<span class="property">id</span>)
} yield (s.map(_.<span class="property">name</span>), c.<span class="property">name</span>)
</code></pre><h3 id="新增">新增</h3><p>所有列都有值：</p>
<pre><code>val insertAction = DBIO.seq(
  students += (<span class="number">4</span>, <span class="string">"format4"</span>, <span class="number">44</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),
  students += (<span class="number">5</span>, <span class="string">"format5"</span>, <span class="number">55</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),

  students ++= Seq (
    (<span class="number">6</span>, <span class="string">"format6"</span>, <span class="number">66</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),
    (<span class="number">7</span>, <span class="string">"format7"</span>, <span class="number">77</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis()))
  )
)
</code></pre><p>部分列有值：</p>
<pre><code>students.<span class="built_in">map</span>(s =&gt; (s.name, s.age, s.classroomId)) += (<span class="string">"format8"</span>, <span class="number">88</span>, <span class="number">3</span>)
</code></pre><h3 id="删除">删除</h3><p>删除classroomId为3的所有数据：</p>
<pre><code><span class="variable"><span class="keyword">val</span> q</span> = students.filter(_.classroomId === <span class="number">3</span>)
<span class="variable"><span class="keyword">val</span> affectedRowsCountFuture</span> = db.run(q.delete)
affectedRowsCountFuture.map { rows =&gt;
  println(rows)
}
</code></pre><h3 id="修改">修改</h3><p>修改单列：</p>
<pre><code>val <span class="tag">q</span> = students.<span class="function"><span class="title">filter</span><span class="params">(_.id === <span class="number">2</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(_.name)</span></span>
val updateSql = <span class="tag">q</span>.<span class="function"><span class="title">update</span><span class="params">(<span class="string">"format2222"</span>)</span></span>
db.<span class="function"><span class="title">run</span><span class="params">(updateSql)</span></span>
</code></pre><p>修改多列：</p>
<pre><code>val <span class="tag">q</span> = students.<span class="function"><span class="title">filter</span><span class="params">(_.id === <span class="number">2</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(s =&gt; (s.name, s.age)</span></span>)
val updateSql = <span class="tag">q</span>.<span class="function"><span class="title">update</span><span class="params">((<span class="string">"format2222"</span>, <span class="number">222</span>)</span></span>)
db.<span class="function"><span class="title">run</span><span class="params">(updateSql)</span></span>
</code></pre><h2 id="CaseClass的使用">CaseClass的使用</h2><p>之前的例子都是使用Tuple构造domain。 还有一种更方便的方式，那就是使用CaseClass。</p>
<pre><code><span class="keyword">case</span> <span class="keyword">class</span> People(id: Long, <span class="keyword">name</span>: String, age: <span class="built_in">Int</span>)
</code></pre><p>例子：</p>
<pre><code>private class PeopleTable(tag: Tag) extends Table[<span class="link_label">People</span>](<span class="link_url">tag, "people"</span>) {
  val id = column[<span class="link_label">Long</span>](<span class="link_url">"id", O.PrimaryKey, O.AutoInc</span>)
  val name = column[<span class="link_label">String</span>](<span class="link_url">"name"</span>)
  val age = column[<span class="link_label">Int</span>](<span class="link_url">"age"</span>)

  def * = (id, name, age) <span class="xml"><span class="tag">&lt;&gt;</span></span> ((People.apply _).tupled, People.unapply)
}

val db = Database.forConfig("h2mem1")

try {

  val people = TableQuery[PeopleTable]

  val setupAction = DBIO.seq(
<span class="code">      people.schema.create,</span>
<span class="code">      people += People(1, "format1", 11)</span>
  )

  val setupFuture = db.run(setupAction);

  val f = setupFuture.flatMap { _ =&gt;
<span class="code">    db.run(people.result).map { p =&gt;</span>
<span class="code">      p.foreach(println)</span>
<span class="code">    }</span>
  }

  Await.result(f, Duration.Inf)

} finally db.close()
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[近看到了一个FRM的框架Slick。 FRM的意思是Functional Relational Mapping， 一种基于函数式的ORM ...]]>
    
    </summary>
    
      <category term="frm" scheme="http://fangjian0423.github.io/tags/frm/"/>
    
      <category term="orm" scheme="http://fangjian0423.github.io/tags/orm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala并发的Future]]></title>
    <link href="http://fangjian0423.github.io/2015/08/14/scala-future/"/>
    <id>http://fangjian0423.github.io/2015/08/14/scala-future/</id>
    <published>2015-08-14T12:22:33.000Z</published>
    <updated>2015-12-20T16:55:17.000Z</updated>
    <content type="html"><![CDATA[<p>Future这个概念其实java里也已经有了， 表示一个未来的意思。 某线程执行一项操作，这个操作有延迟的话，Future会提供一系列方法来处理这个线程过程，可取消，可操作完成后执行其他操作等等。</p>
<p>使用Future是非阻塞的，在Future中可以使用回调函数可以避免阻塞操作。 Scala在Future中提供了flatMap，foreach，filter等方法。</p>
<h2 id="Future概念">Future概念</h2><p>Future的状态：</p>
<p>1.未完成：线程操作还未结束<br>2.已完成：操作操作完成，并且有返回值或者有异常。 当一个Future完成的时候，它就变成了一个不可变对象，永远不会被重写</p>
<p>构造Future最简单的方法是使用Future这个object提供的apply方法：</p>
<pre><code><span class="tag">Future</span> {
    ...
}
</code></pre><p>来看一个最简单的Future操作， 计算和：</p>
<pre><code>import <span class="keyword">scala</span>.concurrent.ExecutionContext.Implicits.<span class="keyword">global</span>
import <span class="keyword">scala</span>.concurrent.duration.Duration
import <span class="keyword">scala</span>.concurrent.{Await, Future}

val sumFuture = Future[Int] {
  <span class="keyword">var</span> <span class="keyword">sum</span> = 0
  <span class="keyword">for</span>(i &lt;- <span class="keyword">Range</span>(1,100000)) <span class="keyword">sum</span> = <span class="keyword">sum</span> + <span class="literal">i</span>
  <span class="literal">sum</span>
}

sumFuture.onSuccess {
  case <span class="keyword">sum</span> =&gt; println(<span class="keyword">sum</span>)
}

Await.result(sumFuture, Duration.<span class="keyword">Inf</span>)
</code></pre><p>Await的作用是阻断Future等待Future的执行结果。 import scala.concurrent.ExecutionContext.Implicits.global 这个global表示一个ExecutionContext，类似线程池，所有线程的提交都得交给ExecutionContext。如果没有import这个global对象，那么执行的时候会报错。</p>
<p>文本文件中找关键字的例子，读io文件的时候如果文件很大，肯定会阻塞。使用Future完成，可以更有效率，等关键字索引找到的时候再去拿数据。这段时间完成可以去做其他事情：</p>
<pre><code><span class="variable"><span class="keyword">val</span> keywordIndex</span> = Future[<span class="typename">Int</span>] {
  <span class="variable"><span class="keyword">val</span> source</span> = scala.io.Source.fromFile(<span class="string">"intro.txx"</span>)
  source.toSeq.indexOfSlice(<span class="string">"format"</span>)
}
</code></pre><h2 id="回调">回调</h2><p>Future提供了3种Callback，分别是 onComplete，onFailure，onSuccess。</p>
<p>onComplete回调表示Future执行完毕了。需要1个Try[T] =&gt; U类型的参数，如果执行成功且没发生一次，那么匹配Success类型，否则匹配Failure类型。</p>
<p>onComplete例子：</p>
<pre><code>val calFuture = <span class="type">Future</span>[<span class="type">Int</span>] {
  val a = <span class="number">1</span> / <span class="number">1</span>
  a
}

calFuture.onComplete {
  <span class="keyword">case</span> <span class="type">Success</span>(<span class="literal">result</span>) =&gt; println(<span class="literal">result</span>)
  <span class="keyword">case</span> <span class="type">Failure</span>(e) =&gt; println(<span class="string">"error: "</span> + e.getMessage)
}
</code></pre><p>onFailure回调表示Future已经执行完成，但是出错了。</p>
<pre><code>val errorFuture = Future[Unit] {
  <span class="number">1</span> / <span class="number">0</span>
}

errorFuture.onFailure {
  <span class="keyword">case</span> e =&gt; println(e.getMessage)
}
</code></pre><p>onSuccess回调表示Future已经执行完成，而且执行成功了。</p>
<pre><code>val successFuture = Future[Int] {
  <span class="number">1000</span>
}

successFuture.onSuccess {
  <span class="keyword">case</span> <span class="built_in">num</span> =&gt; println(<span class="built_in">num</span>)
}
</code></pre><h2 id="Future的组合">Future的组合</h2><p>使用map方法可以组合Future：</p>
<pre><code>val firstValFuture = <span class="type">Future</span>[<span class="type">Int</span>] {
  <span class="number">1</span>
}

val secondFuture = firstValFuture.map { num =&gt;
  println(<span class="string">"firstFuture: "</span> + num)
  num + <span class="string">"111"</span>
}

secondFuture.onSuccess {
  <span class="keyword">case</span> <span class="literal">result</span> =&gt; println(<span class="literal">result</span>)
}
</code></pre><p>flapMap方法也可以组合Future，map方法和flatMap方法唯一的区别是flatMap内部需要返回Future，而map不是。</p>
<pre><code>val firstValFuture = <span class="type">Future</span>[<span class="type">Int</span>] {
  <span class="number">1</span>
}

val secondFuture = firstValFuture.flatMap { num =&gt;
  println(<span class="string">"firstFuture: "</span> + num)
  <span class="type">Future</span> {
    num + <span class="string">"111"</span>
  }
}

secondFuture.onSuccess {
  <span class="keyword">case</span> <span class="literal">result</span> =&gt; println(<span class="literal">result</span>)
}
</code></pre><p>其他资料参考<a href="http://docs.scala-lang.org/overviews/core/futures.html" target="_blank" rel="external">Scala的官方文档</a>即可。</p>
]]></content>
    <summary type="html">
    <![CDATA[Future这个概念其实java里也已经有了， 表示一个未来的意思。 某线程执行一项操作，这个操作有延迟的话，Future会提供一系列方法来处理这个线程过程，可取消，可操作完成...]]>
    
    </summary>
    
      <category term="concurrent" scheme="http://fangjian0423.github.io/tags/concurrent/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HBase介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/08/07/hbase-intro/"/>
    <id>http://fangjian0423.github.io/2015/08/07/hbase-intro/</id>
    <published>2015-08-06T16:22:33.000Z</published>
    <updated>2015-12-20T17:02:23.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>HBase在公司已经用过一段时间，在Flume中添加一个HBase sink将一些数据存储到HBase里。</p>
<p>当时HBase也没学，看了看几个例子，了解了它是基于列的表设计之后，就马上上手了，而且也把东西做出来了。 现在记录一下HBase的一些学习笔记。</p>
<h2 id="HBase简介">HBase简介</h2><p>HBase是什么？</p>
<p>HBase是运行在hadoop上的数据库，是一个分布式的，扩展性高的，存储大数据的数据库。</p>
<p>HBase也是开源的，非关系型数据库。基于Google的Bigtable设计。</p>
<p>什么时候需要使用HBase？</p>
<p>需要实时地读写大数据。HBase的目的就是管理亿级的数据。</p>
<h2 id="HBase基本概念">HBase基本概念</h2><p>HBase是基于列设计的，那什么是基于列呢？</p>
<p>首先看下关系型数据库的表结构，第一行是table的所有列，第二行开始就是各个列对应的值：</p>
<table>
<thead>
<tr>
<th style="text-align:center">id</th>
<th style="text-align:center">name</th>
<th style="text-align:center">age</th>
<th style="text-align:center">birth_date</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">format1</td>
<td style="text-align:center">11</td>
<td style="text-align:center">1980-01-01</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">format2</td>
<td style="text-align:center">22</td>
<td style="text-align:center">1985-01-01</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">format3</td>
<td style="text-align:center">33</td>
<td style="text-align:center">1990-01-01</td>
</tr>
</tbody>
</table>
<p>HBase的表结构是这样的：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Row Key</th>
<th style="text-align:center">Time Stamp</th>
<th style="text-align:center">ColumnFamily contents</th>
<th style="text-align:center">ColumnFamily names</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">‘me.format.hbase’</td>
<td style="text-align:center">t1</td>
<td style="text-align:center">contents:format = “format1”</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">‘me.format.hbase’</td>
<td style="text-align:center">t2</td>
<td style="text-align:center">contents:title = “title1”</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">‘me.format.hbase’</td>
<td style="text-align:center">t3</td>
<td style="text-align:center"></td>
<td style="text-align:center">names:gogogo = “data1”</td>
</tr>
</tbody>
</table>
<p>从上面这个HBase表的例子来说明HBase的存储结构。</p>
<p>Row Key：行的键值，其实就相当于这一行的标识符。上面的数据其实只有1行，因为他们的标识符是一样的。</p>
<p>TimeStamp：时间戳，创建数据的时间戳，hbase默认会自动生成</p>
<p>ColumnFamily：列的前缀，一列可以存储多条数据，具体存储什么类型的数据还需要另外一个标示符qualify，上面那个例子中，contents和names就是两个Column Family</p>
<p>ColumnFamily qualify：列前缀后的标识符，一个ColumnFamily可以有多个qualify。上面那个例子中format和title就是contents这个ColumnFamily的qualify。gogogo是names这个ColumnFamily的qualify</p>
<h2 id="HBase的启动">HBase的启动</h2><p>HBase下载完之后解压，解压后使用以下命令启动hbase：</p>
<pre><code>$ ./bin/<span class="literal">start</span>-hbase.sh
</code></pre><p>启动之前注意，机器要装好jdk，并且启动hadoop。因为hbase底层数据是存储在hdfs上的。</p>
<h2 id="HBase的基本操作">HBase的基本操作</h2><h3 id="表的创建">表的创建</h3><p>创建一个表名位tableName，ColumnFamily有contents和names的表，qualify不需要声明，每次添加数据随意指定qualify即可：</p>
<pre><code><span class="built_in">create</span> <span class="string">'tableName'</span>,[<span class="string">'contents'</span>, <span class="string">'names'</span>]
</code></pre><h3 id="表的删除">表的删除</h3><p>删除表的所有数据：</p>
<pre><code>truncate <span class="built_in">table</span> <span class="built_in">table</span>Name
</code></pre><p>删除表，删除之前需要先disable表，然后才可删除：</p>
<pre><code><span class="built_in">disable</span> <span class="string">'tableName'</span>
drop <span class="string">'tableName'</span>
</code></pre><h3 id="数据查询">数据查询</h3><p>查询tableName表数据：</p>
<pre><code><span class="built_in">scan</span> tableName
</code></pre><p>返回：</p>
<pre><code>ROW                                         COLUMN+CELL
 me.<span class="keyword">format</span>.hbase                            column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438875060466</span>, <span class="keyword">value</span>=format1
</code></pre><h3 id="数据删除">数据删除</h3><p>比如，表tableName里有如下数据：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875566613</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=names:gogogo, timestamp=<span class="number">1438875597592</span>, value=data1
</code></pre><p>进行删除操作，删除ColumnFamily qulify为contents:format的数据：</p>
<pre><code>delete <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:format'
</code></pre><h3 id="数据修改">数据修改</h3><p>HBase没有直接的update操作，只有put操作，put操作如果对应的地方有值，会覆盖：</p>
<pre><code>put <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:format', <span class="symbol">'format111'</span>
</code></pre><h3 id="添加数据">添加数据</h3><p>在tableName表里插入一个Row Key为me.format.hbase, ColumnFamily为contents，qualify为format，值的format1的数据：</p>
<pre><code>put <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:format', <span class="symbol">'format1'</span>
</code></pre><h3 id="计数器">计数器</h3><p>HBase提供了一种计数器的概念，每次可以对某个值进行incr操作：</p>
<pre><code>incr <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">1</span>
</code></pre><p>查询数据：</p>
<pre><code>me.format.hbase                            column=contents:num, timestamp=1438875837362, value=<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>01
</code></pre><p>可以使用get_counter命令获得计数器的值：</p>
<pre><code>get_counter <span class="symbol">'tableName'</span>,<span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">0</span>
</code></pre><p>返回：</p>
<pre><code>COUNTER VALUE = <span class="number">1</span>
</code></pre><p>再次修改：</p>
<pre><code>incr <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">100</span>

get_counter <span class="symbol">'tableName'</span>,<span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">0</span>

<span class="type">COUNTER</span> <span class="type">VALUE</span> = <span class="number">101</span>

incr <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', -<span class="number">102</span>

get_counter <span class="symbol">'tableName'</span>,<span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">0</span>

<span class="type">COUNTER</span> <span class="type">VALUE</span> = -<span class="number">1</span>
</code></pre><h3 id="带条件的数据查询">带条件的数据查询</h3><p>scan查询可以带几个参数。</p>
<p>COLUMNS： ColumnFamily和qualify的值<br>LIMIT：展示的个数<br>FILTER：过滤条件</p>
<p>比如有以下数据：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875707700</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:num, timestamp=<span class="number">1438876106259</span>, value=\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=names:gogogo, timestamp=<span class="number">1438875597592</span>, value=data1
 me<span class="class">.format</span><span class="class">.hbase1</span>                           column=contents:format, timestamp=<span class="number">1438877417358</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase2</span>                           column=contents:format, timestamp=<span class="number">1438877422756</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase3</span>                           column=contents:format, timestamp=<span class="number">1438877427312</span>, value=format1
</code></pre><p>查询ColumnFamily，qualify为contents:format的数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents:format"</span>, LIMIT =&gt; <span class="number">10</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me.<span class="keyword">format</span>.hbase                            column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438875707700</span>, <span class="keyword">value</span>=format1
 me.<span class="keyword">format</span>.hbase1                           column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438877417358</span>, <span class="keyword">value</span>=format1
 me.<span class="keyword">format</span>.hbase2                           column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438877422756</span>, <span class="keyword">value</span>=format1
 me.<span class="keyword">format</span>.hbase3                           column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438877427312</span>, <span class="keyword">value</span>=format1
</code></pre><p>查询ColumnFamily未contents的数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents"</span>, LIMIT =&gt; <span class="number">10</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875707700</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:num, timestamp=<span class="number">1438876106259</span>, value=\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase1</span>                           column=contents:format, timestamp=<span class="number">1438877417358</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase2</span>                           column=contents:format, timestamp=<span class="number">1438877422756</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase3</span>                           column=contents:format, timestamp=<span class="number">1438877427312</span>, value=format1
</code></pre><p>查询ColumnFamily未contents的数据，并只展示2行数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents"</span>, LIMIT =&gt; <span class="number">2</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875707700</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:num, timestamp=<span class="number">1438876106259</span>, value=\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase1</span>                           column=contents:format, timestamp=<span class="number">1438877417358</span>, value=format1
</code></pre><p>查询ColumnFamily未contents的数据，并只展示2行数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents"</span>, FILTER =&gt; <span class="string">"ValueFilter( =, 'binaryprefix:title' )"</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
</code></pre><p>scan命令具体其他的参数就不一一列举了，可查询文档解决。</p>
]]></content>
    <summary type="html">
    <![CDATA[HBase在公司已经用过一段时间，在Flume中添加一个HBase sink将一些数据存储到HBase里 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="hbase" scheme="http://fangjian0423.github.io/tags/hbase/"/>
    
      <category term="hbase" scheme="http://fangjian0423.github.io/categories/hbase/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hive介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/07/31/hive-intro/"/>
    <id>http://fangjian0423.github.io/2015/07/31/hive-intro/</id>
    <published>2015-07-31T05:32:33.000Z</published>
    <updated>2015-12-20T17:04:55.000Z</updated>
    <content type="html"><![CDATA[<p>Hive是基于Hadoop的一个数据仓库工具，使用它可以查询和管理分布式存储系统上的大数据集。</p>
<p>Hive提供了一种叫做HiveQL的类似SQL查询语言用来查询数据，HiveQL也允许熟悉MapReduce开发者开发自定义的mapper和reducer来处理内建的mapper和reducer无法完成的复杂的分析工作。</p>
<p>Hive的工作模式是提交一个任务，等到任务结束时被通知，而不是实时查询。</p>
<h2 id="Hive安装">Hive安装</h2><p>直接去<a href="https://hive.apache.org/" target="_blank" rel="external">Hive官网</a>下载最新的文件，解压。</p>
<p>运行bin目录里的hive文件，运行之前先启动hadoop，运行的时候可能会出现：</p>
<pre><code>Missing Hive <span class="keyword">CLI</span> Jar ....
</code></pre><p>将hive解压出来的lib目录里的jline<em>.jar拷贝到$HADOOP/share/hadoop/yarn/lib里，同时将$HADOOP/share/hadoop/yarn/lib里的jline</em>.jar删除，重启hadoop。</p>
<p>再次运行，可能还会出现</p>
<pre><code>The reported blocks <span class="number">2662</span> has reached <span class="operator">the</span> threshold <span class="number">0.9990</span> <span class="operator">of</span> total blocks <span class="number">2662.</span> The <span class="built_in">number</span> <span class="operator">of</span> live datanodes <span class="number">1</span> has reached <span class="operator">the</span> minimum <span class="built_in">number</span> <span class="number">0.</span> In safe mode extension. Safe mode will be turned off automatically <span class="operator">in</span> <span class="number">0</span> <span class="built_in">seconds</span>. ...
</code></pre><p>类似的问题，关闭hdfs的安全模式即可：</p>
<pre><code><span class="title">hadoop</span> dfsadmin -safemode leave
</code></pre><h2 id="基本命令">基本命令</h2><p>HiveQL就是模仿sql而创建的，以SQL的角度来介绍HiveQL。</p>
<h3 id="DDL操作">DDL操作</h3><p>创建表：</p>
<pre><code>hive&gt; create table users(age <span class="built_in">INT</span>, name <span class="built_in">STRING</span>)<span class="comment">;</span>
</code></pre><p>查看所有的表：</p>
<pre><code>hive&gt; <span class="built_in">show</span> <span class="built_in">tables</span>;
</code></pre><p>查看以CLIENT开头的表：</p>
<pre><code>hive&gt; <span class="built_in">show</span> <span class="built_in">tables</span> 'CLIENT.*';
</code></pre><p>表加列：</p>
<pre><code>hive&gt; alter table users add columns<span class="list">(<span class="keyword">gender</span> BOOLEAN)</span><span class="comment">;</span>
</code></pre><p>改表名字：</p>
<pre><code>hive&gt; alter <span class="built_in">table</span> users rename <span class="keyword">to</span> <span class="keyword">user</span>;
</code></pre><p>删除表：</p>
<pre><code>hive&gt; drop <span class="built_in">table</span> <span class="keyword">user</span>;
</code></pre><p>查看表的具体信息：</p>
<pre><code>hive&gt; descibe user<span class="comment">;</span>
hive&gt; desc user<span class="comment">;</span>
</code></pre><h3 id="DML操作">DML操作</h3><p>hive创建表的时候可以指定分隔符，由于hive操作的是hdfs，数据最终会存储在hdfs上，所以hdfs上的内容肯定是以某种分隔符分开各个列的。 hive默认的列分隔符是 <strong>^A</strong> 。 我们可以自定义自己的分隔符，在创建表的时候指定分隔符即可。</p>
<p>本地导入数据到hive：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table users;
</code></pre><p>users表的结构只有2列，name和age，而且使用默认的分隔符。</p>
<p>比如本地文件的内容是这样的：</p>
<pre><code><span class="xml">format1</span><span class="keyword">^A11</span><span class="xml">
format2</span><span class="keyword">^A22</span><span class="xml"></span>
</code></pre><p>导入之后进行查询：</p>
<pre><code>hive&gt; <span class="keyword">select</span> * <span class="keyword">from</span> users;
</code></pre><p>显示结果：</p>
<pre><code>OK
format1    <span class="number">11</span>
format2    <span class="number">22</span>
Time taken: <span class="number">0.362</span> seconds, Fetched: <span class="number">2</span> row(s)
</code></pre><p>在创建表的时候可以指定列分隔符和数组分隔符：</p>
<pre><code>hive&gt; create table users(name <span class="built_in">string</span>, age int)
 &gt; ROW FORMAT DELIMITED
 &gt; FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">'\t'</span>
 &gt; COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">','</span>;
</code></pre><p>导入数据还有几个参数：</p>
<p>local参数意味着从本地加载文件，如果没有local参数，那表示从hdfs加载文件。</p>
<p>关键字overwrite意味着当前表中已经存在的数据将会被删除掉，没有overwrite关键字，表示数据是追加，追加到原先数据集里面。</p>
<p>插入数据，插入数据后会起一个map reduce job去跑插入的数据：</p>
<pre><code>hive&gt; <span class="function">insert <span class="keyword">into</span> table users <span class="title">values</span>(<span class="params"><span class="string">'formatgogo'</span>, <span class="number">222</span></span>)</span>;
</code></pre><p>带条件的查询数据：</p>
<pre><code>hive&gt; <span class="keyword">select</span> * <span class="keyword">from</span> users <span class="keyword">where</span> age = <span class="number">11</span>;
</code></pre><p>group by查询：</p>
<pre><code>hive&gt; <span class="keyword">select</span> age, <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> users <span class="built_in">group</span> <span class="keyword">by</span> age;
</code></pre><p>partition的使用，以部门表为例，用type进行partition：</p>
<pre><code>hive&gt; create table dept<span class="list">(<span class="keyword">name</span> STRING)</span> partitioned by <span class="list">(<span class="keyword">type</span> INT)</span><span class="comment">;</span>
</code></pre><p>以2个文件为例，dept1.txt：</p>
<pre><code><span class="label">dept1</span>^<span class="literal">A1</span>
<span class="label">dept11</span>^<span class="literal">A1</span>
<span class="label">dept111</span>^<span class="literal">A1</span>
<span class="label">dept1111</span>^<span class="literal">A1</span>
<span class="label">dept11111</span>^<span class="literal">A1</span>
<span class="label">dept111111</span>^<span class="literal">A1</span>
</code></pre><p>dept2.txt</p>
<pre><code><span class="label">dept2</span>^<span class="literal">A2</span>
<span class="label">dept22</span>^<span class="literal">A2</span>
<span class="label">dept222</span>^<span class="literal">A2</span>
<span class="label">dept2222</span>^<span class="literal">A2</span>
<span class="label">dept22222</span>^<span class="literal">A2</span>
<span class="label">dept222222</span>^<span class="literal">A2</span>
</code></pre><p>使用partition之后，导入数据的时候需要指定对应的partition：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'$PATH/dept1.txt'</span> overwrite <span class="keyword">into</span> table dept partition(<span class="keyword">type</span>=<span class="number">1</span>);
hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'$PATH/dept2.txt'</span> overwrite <span class="keyword">into</span> table dept partition(<span class="keyword">type</span>=<span class="number">2</span>);

hive&gt; <span class="keyword">select</span> * from dept;
</code></pre><p>结果：</p>
<pre><code>OK
dept1    <span class="number">1</span>
dept11    <span class="number">1</span>
dept111    <span class="number">1</span>
dept1111    <span class="number">1</span>
dept11111    <span class="number">1</span>
dept111111    <span class="number">1</span>
dept2    <span class="number">2</span>
dept22    <span class="number">2</span>
dept222    <span class="number">2</span>
dept2222    <span class="number">2</span>
dept22222    <span class="number">2</span>
dept222222    <span class="number">2</span>
Time taken: <span class="number">0.114</span> seconds, Fetched: <span class="number">12</span> row(s)
</code></pre><p>insert可以将数据导出到指定目录，将users表导入到本地文件。 去掉local关键字表示导出到hdfs目录：</p>
<pre><code>hive&gt; insert overwrite <span class="keyword">local</span> directory <span class="string">'localFileName'</span> <span class="keyword">select</span> * from users<span class="comment">;</span>
</code></pre><h3 id="hive存储在hdfs的位置">hive存储在hdfs的位置</h3><p>进入hive控制台之后，可以使用：</p>
<pre><code><span class="tag">hive</span>&gt; <span class="tag">set</span> <span class="tag">hive</span><span class="class">.metastore</span><span class="class">.warehouse</span><span class="class">.dir</span>;
</code></pre><p>查看hive存储在hdfs的位置，默认是存在 /user/hive/warehouse 目录。</p>
<p>之前的users表，会存储在/user/hive/warehouse/user目录里。</p>
<p>有partition的表会存在的不同位置，比如之前的dept表的type为1和2的分别存储在 /user/hive/warehouse/dept/type=1 和 /user/hive/warehouse/dept/type=2。</p>
<h2 id="数据类型">数据类型</h2><p>hive中的数据类型分2种，简单类型和复杂类型。</p>
<p>简单类型有以下几种：TINYINT, SMALLINT, INT, BIGINT, BOOLEAN, FLOAT, DOUBLE, STRING。</p>
<p>复杂类型有以下几种：Structs(结构体，学过C都知道)，MAPS(key-value键值对)，Arrays(数组类型，数组内的元素类型都必须一致)</p>
<p>简单类型就不分析了，来看一下复杂类型的使用：</p>
<h3 id="Structs">Structs</h3><pre><code><span class="label">hive</span>&gt; create table employee(id INT, <span class="preprocessor">info</span> <span class="keyword">struct&lt;name:STRING, </span>age:INT&gt;)
      &gt; ROW FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY </span><span class="string">','</span> 
      &gt; COLLECTION <span class="keyword">ITEMS </span>TERMINATED <span class="keyword">BY </span><span class="string">':'</span><span class="comment">; </span>
</code></pre><p>要导入的数据：</p>
<pre><code><span class="number">1</span>,format:<span class="number">11</span>
<span class="number">2</span>,fj:<span class="number">22</span>
<span class="number">3</span>,formatfj:<span class="number">33</span>
</code></pre><p>导入数据：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table employee;
</code></pre><p>查询：</p>
<pre><code>select * from employee;

OK
<span class="number">1</span>    {<span class="string">"name"</span>:<span class="string">"format"</span>,<span class="string">"age"</span>:<span class="number">11</span>}
<span class="number">2</span>    {<span class="string">"name"</span>:<span class="string">"fj"</span>,<span class="string">"age"</span>:<span class="number">22</span>}
<span class="number">3</span>    {<span class="string">"name"</span>:<span class="string">"formatfj"</span>,<span class="string">"age"</span>:<span class="number">33</span>}
Time taken: <span class="number">0.042</span> seconds, Fetched: <span class="number">3</span> row(s)

hive&gt; select info.name from employee;

OK
format
fj
formatfj
Time taken: <span class="number">0.061</span> seconds, Fetched: <span class="number">3</span> row(s)
</code></pre><h3 id="Maps">Maps</h3><pre><code>hive&gt; create table lessons(id <span class="built_in">string</span>, score <span class="built_in">map</span>&lt;<span class="built_in">string</span>, int&gt;)
    &gt; ROW FORMAT DELIMITED
    &gt; FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">'\t'</span>
    &gt; COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">','</span>
    &gt; <span class="built_in">MAP</span> KEYS TERMINATED <span class="keyword">BY</span> <span class="string">':'</span>;
</code></pre><p>要导入的数据：</p>
<pre><code><span class="number">1</span>       chinese:<span class="number">80</span>,english:<span class="number">60</span>,math:<span class="number">70</span>  
<span class="number">2</span>       computer:<span class="number">60</span>,chemistry:<span class="number">80</span>   
</code></pre><p>导入数据：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table lessons;
</code></pre><p>查询：</p>
<pre><code>hive&gt; <span class="keyword">select</span> score[<span class="string">'chinese'</span>] <span class="keyword">from</span> lessions;

OK
<span class="number">80</span>
NULL
Time taken: <span class="number">0.05</span> seconds, Fetched: <span class="number">2</span> row(s)

hive&gt; <span class="keyword">select</span> * <span class="keyword">from</span> lessons;
OK
<span class="number">1</span>    {<span class="string">"chinese"</span>:<span class="number">80</span>,<span class="string">"english"</span>:<span class="number">60</span>,<span class="string">"math"</span>:<span class="number">70</span>}
<span class="number">2</span>    {<span class="string">"computer"</span>:<span class="number">60</span>,<span class="string">"chemistry"</span>:<span class="number">80</span>}
Time taken: <span class="number">0.032</span> seconds, Fetched: <span class="number">2</span> row(s)
</code></pre><h3 id="Arrays">Arrays</h3><pre><code>hive&gt; create table student(name <span class="built_in">string</span>, hobby_list <span class="built_in">array</span>&lt;<span class="built_in">STRING</span>&gt;)
    &gt; ROW FORMAT DELIMITED
    &gt; FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">','</span>
    &gt; COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">':'</span>;
</code></pre><p>要导入的数据：</p>
<pre><code><span class="tag">format</span>,<span class="tag">basketball</span><span class="pseudo">:football</span><span class="pseudo">:swimming</span>
<span class="tag">fj</span>,<span class="tag">coding</span><span class="pseudo">:running</span>
</code></pre><p>导入数据：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table student;
</code></pre><p>查询(数组下标没对应的值的话返回NULL)：</p>
<pre><code>hive&gt; select * from student;

OK
format    [<span class="string">"basketball"</span>,<span class="string">"football"</span>,<span class="string">"swimming"</span>]
fj    [<span class="string">"coding"</span>,<span class="string">"running"</span>]
Time taken: <span class="number">0.041</span> seconds, Fetched: <span class="number">2</span> row(s)

hive&gt; select hobby_list[<span class="number">2</span>] from student;

OK
swimming
<span class="literal">NULL</span>
Time taken: <span class="number">0.07</span> seconds, Fetched: <span class="number">2</span> row(s)
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Hive是基于Hadoop的一个数据仓库工具，使用它可以查询和管理分布式存储系统上的大数据集 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="hive" scheme="http://fangjian0423.github.io/tags/hive/"/>
    
      <category term="hive" scheme="http://fangjian0423.github.io/categories/hive/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[google guava类库介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/07/26/google-guava-intro/"/>
    <id>http://fangjian0423.github.io/2015/07/26/google-guava-intro/</id>
    <published>2015-07-26T08:32:33.000Z</published>
    <updated>2015-12-20T16:57:40.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Guava简介">Guava简介</h2><p><a href="https://mail.google.com/mail/u/0/" target="_blank" rel="external">Guava</a>是一个Google开发的基于java的扩展项目，提供了很多有用的工具类，可以让java代码更加优雅，更加简洁。</p>
<p>Guava包括诸多工具类，比如Collections，cache，concurrent，hash，reflect，annotations，eventbus等。</p>
<p>刚好在看flume源码的时候看到源码里面使用了很多guava提供的代码，于是记录学习一下这个类库。</p>
<h2 id="各个模块介绍">各个模块介绍</h2><p><a href="https://code.google.com/p/guava-libraries/wiki/GuavaExplained" target="_blank" rel="external">Guava的wiki</a>已经很明细地介绍了各个工具类的作用和说明。</p>
<p>简单翻译一下各个工具类的说明，有用到的需要了解详情的直接去官网看就可以了。</p>
<h3 id="Basic_utilties_基础工具类">Basic utilties 基础工具类</h3><p>基础工具类的作用是写java语言写的更轻松。它包括了5个子模块：</p>
<p>1.<a href="https://code.google.com/p/guava-libraries/wiki/UsingAndAvoidingNullExplained" target="_blank" rel="external">使用和避免null</a>，null值是有歧义的，也会引起错误。有时候它会让人很不舒服，<br>2.<a href="https://code.google.com/p/guava-libraries/wiki/PreconditionsExplained" target="_blank" rel="external">前置条件</a>,让方法中的条件检查更简单<br>3.<a href="https://code.google.com/p/guava-libraries/wiki/CommonObjectUtilitiesExplained" target="_blank" rel="external">公用的object方法</a>，简化object对象的hashCode和toString<br>4.<a href="https://code.google.com/p/guava-libraries/wiki/OrderingExplained" target="_blank" rel="external">排序</a>，Guava提供了强大的fluent Comparator<br>5.<a href="https://code.google.com/p/guava-libraries/wiki/ThrowablesExplained" target="_blank" rel="external">Throwables</a>，简化了异常和错误的传播与检查</p>
<h3 id="Collections_集合">Collections 集合</h3><p>Guava扩展了jdk提供的集合机制</p>
<p>1.<a href="https://code.google.com/p/guava-libraries/wiki/ImmutableCollectionsExplained" target="_blank" rel="external">不可变集合</a>用不变的集合进行防御性编程和性能提升<br>2.<a href="https://code.google.com/p/guava-libraries/wiki/NewCollectionTypesExplained" target="_blank" rel="external">新集合类型</a>multisets，multimaps，tables，bidirectional map等<br>3.<a href="https://code.google.com/p/guava-libraries/wiki/CollectionUtilitiesExplained" target="_blank" rel="external">强大的集合工具类</a>提供了jdk中没有的集合工具类<br>4.<a href="https://code.google.com/p/guava-libraries/wiki/CollectionHelpersExplained" target="_blank" rel="external">扩展工具类</a>让实现和扩展集合类变得更容易，比如创建Collection的装饰器，或实现迭代器</p>
<h3 id="Caches_缓存">Caches 缓存</h3><p>本地缓存实现，支持多种缓存过期策略</p>
<h3 id="函数式风格">函数式风格</h3><p>Guava的函数式支持可以显著简化代码，但请谨慎使用它</p>
<h3 id="并发">并发</h3><p>1.<a href="https://code.google.com/p/guava-libraries/wiki/ListenableFutureExplained" target="_blank" rel="external">ListenableFuture</a>：完成后触发回调的Future<br>2.<a href="https://code.google.com/p/guava-libraries/wiki/ServiceExplained" target="_blank" rel="external">Service框架</a>：抽象可开启和关闭的服务，帮助你维护服务的状态逻辑</p>
<h3 id="字符串处理">字符串处理</h3><p>非常有用的字符串工具，包括分割、连接、填充等操作</p>
<h3 id="原生类型">原生类型</h3><p>扩展 JDK 未提供的原生类型（如int、char）操作， 包括某些类型的无符号形式</p>
<h3 id="区间">区间</h3><p>可比较类型的区间API，包括连续和离散类型</p>
<h3 id="IO">IO</h3><p>简化I/O尤其是I/O流和文件的操作，针对Java5和6版本</p>
<h3 id="散列">散列</h3><p>提供比Object.hashCode()更复杂的散列实现，并提供布鲁姆过滤器的实现</p>
<h3 id="事件总线">事件总线</h3><p>发布-订阅模式的组件通信，但组件不需要显式地注册到其他组件中</p>
<h3 id="数学运算">数学运算</h3><p>优化的、充分测试的数学工具类</p>
<h3 id="反射">反射</h3><p>Guava的Java反射机制工具类</p>
]]></content>
    <summary type="html">
    <![CDATA[Guava是一个Google开发的基于java的扩展项目，提供了很多有用的工具类，可以让java代码更加优雅，更加简洁 ...]]>
    
    </summary>
    
      <category term="guava" scheme="http://fangjian0423.github.io/tags/guava/"/>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[java内置的线程池笔记]]></title>
    <link href="http://fangjian0423.github.io/2015/07/24/java-poolthread/"/>
    <id>http://fangjian0423.github.io/2015/07/24/java-poolthread/</id>
    <published>2015-07-24T12:32:33.000Z</published>
    <updated>2015-12-20T17:10:06.000Z</updated>
    <content type="html"><![CDATA[<p>目前的工作是接触大数据相关的内容，自己也缺少高并发的知识，刚好前几天看了flume的源码，里面也用到了各种线程池内容，刚好学习一下，做个笔记。</p>
<p>写这篇博客的时候又刚好想起了当时自己实习的时候遇到的一个问题。1000个爬虫任务使用了多线程的处理方式，比如开5个线程处理这1000个任务，每个线程分200个任务，然后各个线程处理那200个爬虫任务→_→，太笨了。其实更合理的方法是使用阻塞队列+线程池的方法。</p>
<h2 id="ExecutorService">ExecutorService</h2><p>ExecutorService就是线程池的概念，ExecutorService的初始化可以使用Executors类的静态方法。</p>
<p>Executors提供了很多方法用来初始化ExecutorService，可以初始化指定数目个线程的线城市或者单个线程的线程池。</p>
<p>比如构造一个10个线程的线程池，使用了guava的ThreadFactoryBuilder，guava的ThreadFactoryBuilder可以传入一个namFormat参数用户来表示线程的name，它内部会使用数字增量表示%d，比如一下的nameFormat，10个线程，名字分别是thread-call-runner-1，thread-call-runner-2 … thread-call-runner-10:</p>
<pre><code>Executors.newFixedThreadPool(<span class="number">10</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
</code></pre><p>ExecutorService线程池使用线程执行任务例子：</p>
<p>1.阻塞队列里有10个元素，初始化带有2个线程的线程池，跑2个线程分别去阻塞队列里取数据执行。</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test01</span><span class="params">()</span> throws Exception </span>{
    ExecutorService es = Executors.newFixedThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
    final LinkedBlockingDeque&lt;String&gt; <span class="built_in">deque</span> = <span class="keyword">new</span> LinkedBlockingDeque&lt;String&gt;();
    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10</span>; i ++) {
        <span class="built_in">deque</span>.add(i + <span class="string">""</span>);
    }
    es.submit(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            <span class="keyword">while</span>(!<span class="built_in">deque</span>.isEmpty()) {
                System.out.println(<span class="built_in">deque</span>.poll() + <span class="string">"-"</span> + Thread.currentThread().getName());
            }
        }
    });
    es.submit(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            <span class="keyword">while</span>(!<span class="built_in">deque</span>.isEmpty()) {
                System.out.println(<span class="built_in">deque</span>.poll() + <span class="string">"-"</span> + Thread.currentThread().getName());
            }
        }
    });
    Thread.sleep(<span class="number">10000l</span>);
}
</code></pre><p>打印，2个线程都会执行：</p>
<pre><code><span class="number">1</span>-thread-call-runner-<span class="number">0</span>
<span class="number">2</span>-thread-call-runner-<span class="number">0</span>
<span class="number">3</span>-thread-call-runner-<span class="number">1</span>
<span class="number">4</span>-thread-call-runner-<span class="number">0</span>
<span class="number">5</span>-thread-call-runner-<span class="number">0</span>
<span class="number">6</span>-thread-call-runner-<span class="number">1</span>
<span class="number">7</span>-thread-call-runner-<span class="number">0</span>
<span class="number">8</span>-thread-call-runner-<span class="number">1</span>
<span class="number">9</span>-thread-call-runner-<span class="number">0</span>
<span class="number">10</span>-thread-call-runner-<span class="number">0</span>
</code></pre><p>2.执行Callable线程，Callable线程和Runnable线程的区别就是Callable的线程会有返回值，这个返回值是Future，未来的意思，而且这Future是个接口，提供了几个实用的方法，比如cancel, idDone, isCancelled, get等方法。</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test02() throws <span class="type">Exception</span> {
    <span class="type">ExecutorService</span> es = <span class="type">Executors</span>.newFixedThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
    final <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt; deque = new <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt;();
    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">500</span>; i ++) {
        deque.add(i + <span class="string">""</span>);
    }
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = es.submit(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="keyword">while</span> (!deque.isEmpty()) {
                <span class="type">System</span>.<span class="keyword">out</span>.println(deque.poll() + <span class="string">"-"</span> + <span class="type">Thread</span>.currentThread().getName());
            }
            <span class="keyword">return</span> <span class="string">"done"</span>;
        }
    });
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.isDone());
    // get方法会阻塞
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.get());
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec next"</span>);
}
</code></pre><p>打印：</p>
<pre><code>先打印出几百个 数字-thread-call-runner-<span class="number">0</span>
然后打印出 isDone的结果， 是<span class="literal">false</span>
<span class="type">Future</span>的get是得到<span class="type">Callable</span>线程执行完毕后的结果，该方法会阻塞，直到该<span class="type">Future</span>对应的线程全部执行完才会继续执行下去。这个例子<span class="type">Callable</span>线程执行完返回done，所以get方法也是返回done

get方法还有个重载的方法，带有<span class="number">2</span>个参数，第一个参数是一个long类型的数字，第二个参数是时间单位。<span class="literal">result</span>.get(<span class="number">1</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>) 就表示<span class="number">1</span>毫秒，等待这个<span class="type">Future</span>的时间为<span class="number">1</span>毫秒，如果时间<span class="number">1</span>毫秒，那么这个get方法的调用会抛出java.util.concurrent.<span class="type">TimeoutException</span>异常，并且线程内部的执行也会停止。注意，但是如果我们catch这个<span class="type">TimeoutException</span>的话，那么线程里的代码还是会被执行完毕。

<span class="keyword">try</span> {
    // catch <span class="type">TimeoutException</span>的话线程里的代码还是会执行下去
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.get(<span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>));
} catch (<span class="type">TimeoutException</span> e) {
    e.printStackTrace();
}
</code></pre><p>3.Future的cancel方法的使用</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test03() throws <span class="type">Exception</span> {
    <span class="type">ExecutorService</span> es = <span class="type">Executors</span>.newFixedThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
    final <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt; deque = new <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt;();
    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">5000</span>; i ++) {
        deque.add(i + <span class="string">""</span>);
    }
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = es.submit(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="keyword">while</span> (!deque.isEmpty() &amp;&amp; !<span class="type">Thread</span>.currentThread().isInterrupted()) {
                <span class="type">System</span>.<span class="keyword">out</span>.println(deque.poll() + <span class="string">"-"</span> + <span class="type">Thread</span>.currentThread().getName());
            }
            <span class="keyword">return</span> <span class="string">"done"</span>;
        }
    });

    <span class="keyword">try</span> {
        <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.get(<span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>));
    } catch (<span class="type">TimeoutException</span> e) {
        <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"cancel result: "</span> + <span class="literal">result</span>.cancel(<span class="literal">true</span>));
        <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"is cancelled: "</span> + <span class="literal">result</span>.isCancelled());
    }
    <span class="type">Thread</span>.sleep(<span class="number">2000</span>l);
}
</code></pre><p>打印：</p>
<pre><code>先打印出几百个 数字-thread-call-runner-<span class="number">0</span>
然后打印出
cancel <span class="literal">result</span>: <span class="literal">true</span>
<span class="keyword">is</span> cancelled: <span class="literal">true</span>

cancel方法用来取消线程的继续执行，它有个boolean类型的返回值，表示是否cancel成功。这里我们使用了get方法，<span class="number">10</span>毫秒处理<span class="number">5000</span>条数据，报了<span class="type">TimeoutException</span>异常，catch之后对<span class="type">Future</span>进行了cancel调用。注意，我们在<span class="type">Callable</span>里执行的代码里加上了

!<span class="type">Thread</span>.currentThread().isInterrupted()

如果去掉了这个条件，那么还是会打印出<span class="number">5000</span>条处理数据。cancel方法底层会去interrupted对应的线程，所以才需要加上这个条件的判断。
</code></pre><h2 id="ScheduledExecutorService">ScheduledExecutorService</h2><p>ScheduledExecutorService接口是ExecutorService接口的子类。</p>
<p>看名字也知道，ScheduledExecutorService是基于调度的线程池。</p>
<p>1.ScheduledExecutorService的schedule方法例子：</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test04() throws <span class="type">Exception</span> {
    <span class="type">ScheduledExecutorService</span> ses = <span class="type">Executors</span>.newScheduledThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = ses.schedule(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec task"</span>);
            <span class="keyword">return</span> <span class="string">"ok"</span>;
        }
    }, <span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>);
    <span class="type">Thread</span>.sleep(<span class="number">15000</span>);
}
</code></pre><p>打印：</p>
<pre><code>执行<span class="number">10</span>秒后打印出  <span class="built_in">exec</span> task
</code></pre><p>2.cancel在schedule中的使用：</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test05() throws <span class="type">Exception</span> {
    <span class="type">ScheduledExecutorService</span> ses = <span class="type">Executors</span>.newScheduledThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = ses.schedule(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec task"</span>);
            <span class="keyword">try</span> {
                <span class="type">Thread</span>.sleep(<span class="number">5000</span>l);
            } catch (<span class="type">InterruptedException</span> e) {
                e.printStackTrace();
            }
            <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec done, take 5 seconds"</span>);
            <span class="keyword">return</span> <span class="string">"ok"</span>;
        }
    }, <span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>);
    <span class="type">Thread</span>.sleep(<span class="number">11000</span>);
    <span class="literal">result</span>.cancel(<span class="literal">true</span>);
    <span class="type">Thread</span>.sleep(<span class="number">10000</span>);
}
</code></pre><p>打印：</p>
<pre><code>先打印出exec task，然后抛出InterruptedException异常，异常被<span class="keyword">catch</span>。接着打印出exec done, take <span class="number">5</span> seconds
因为Callable线程是<span class="number">10</span>秒后执行的，线程会执行<span class="number">5</span>秒，在<span class="number">11</span>秒的时候会调用Future的cancel方法，会取消线程的时候，由于我们<span class="keyword">catch</span>了异常，所以线程会执行完毕。

注意一下，cancel方法有个boolean类型的参数mayInterruptIfRunning。上个例子中我们传入了<span class="literal">true</span>，所以会打断正在执行的线程，因此抛出了异常。如果我们传入<span class="literal">false</span>，线程正在执行，所以不会去打断它，因此会打印出exec task，然后再打印出exec done, take <span class="number">5</span> seconds，并且没有异常抛出。
</code></pre><p>3.scheduleWithFixedDelay方法，定时器，每隔多少时间执行</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test06</span><span class="params">()</span> throws Exception </span>{
    ScheduledExecutorService sec = Executors.newScheduledThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    sec.scheduleWithFixedDelay(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            System.out.println(<span class="string">"exec"</span>);
        }
    }, <span class="number">0</span>, <span class="number">5</span>, TimeUnit.SECONDS);
    Thread.sleep(<span class="number">16000l</span>);
}
</code></pre><p>打印：</p>
<pre><code>打印出<span class="number">4</span>次exec。
scheduleWithFixedDelay有<span class="number">4</span>个参数，第一个参数是个Runnable接口的实现类，第二个参数是首次执行线程的延迟时间，第三个参数是每隔多少时间再次执行线程时间，第四个参数是时间的单位。
如果Runnable中执行的代码发生了异常并且没有被<span class="keyword">catch</span>的话，那么发生异常之后，Runnable里的代码就不会再次执行。
</code></pre><p>4.scheduleAtFixedRate方法，scheduleAtFixedRate方法跟scheduleWithFixedDelay类似。唯一的区别是scheduleWithFixedDelay是在线程全部执行完毕之后开始计算时间的，而scheduleAtFixedRate是在线程开始执行的时候计算时间的，所以scheduleAtFixedRate有时会产生不是定时执行的感觉。</p>
<p>先看scheduleWithFixedDelay：</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test07</span><span class="params">()</span> throws Exception </span>{
    ScheduledExecutorService sec = Executors.newScheduledThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    sec.scheduleWithFixedDelay(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            System.out.println(<span class="string">"exec"</span>);
            <span class="keyword">try</span> {
                Thread.sleep(<span class="number">3000l</span>);
            } <span class="keyword">catch</span> (Exception e) {
                e.printStackTrace();
            }
        }
    }, <span class="number">0</span>, <span class="number">5</span>, TimeUnit.SECONDS);
    Thread.sleep(<span class="number">17000l</span>);
}
</code></pre><p>打印：</p>
<pre><code>执行<span class="number">3</span>次exec。Runnable每次执行<span class="number">3</span>秒。第一次是在<span class="number">0</span>秒执行，执行了<span class="number">3</span>秒，第二次是在<span class="number">8</span>秒，执行了<span class="number">3</span>秒。第三次是在<span class="number">16</span>秒执行
</code></pre><p>然后再看scheduleAtFixedRate：</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test09</span><span class="params">()</span> throws Exception </span>{
    ScheduledExecutorService sec = Executors.newScheduledThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    sec.scheduleAtFixedRate(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            System.out.println(<span class="string">"exec"</span>);
            <span class="keyword">try</span> {
                Thread.sleep(<span class="number">3000l</span>);
            } <span class="keyword">catch</span> (Exception e) {
                e.printStackTrace();
            }
        }
    }, <span class="number">0</span>, <span class="number">5</span>, TimeUnit.SECONDS);
    Thread.sleep(<span class="number">16000l</span>);
}
</code></pre><p>打印：</p>
<pre><code>执行了<span class="number">4</span>次exec，第一次在<span class="number">0</span>秒执行，第二次在<span class="number">5</span>秒，第三次是<span class="number">10</span>秒，第四次在<span class="number">15</span>秒执行
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[写这篇博客的时候又刚好想起了当时自己实习的时候遇到的一个问题。1000个爬虫任务使用了多线程的处理方式，比如开5个线程处理这1000个任务 ...]]>
    
    </summary>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="thread" scheme="http://fangjian0423.github.io/tags/thread/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[通过源码分析Flume HDFSSink 写hdfs文件的过程]]></title>
    <link href="http://fangjian0423.github.io/2015/07/20/flume-hdfs-sink/"/>
    <id>http://fangjian0423.github.io/2015/07/20/flume-hdfs-sink/</id>
    <published>2015-07-20T15:32:33.000Z</published>
    <updated>2015-12-20T17:09:49.000Z</updated>
    <content type="html"><![CDATA[<p>Flume有HDFS Sink，可以将Source进来的数据写入到hdfs中。</p>
<p>HDFS Sink具体的逻辑代码是在HDFSEventSink这个类中。</p>
<p>HDFS Sink跟写文件相关的配置如下：</p>
<p>hdfs.path -&gt; hdfs目录路径<br>hdfs.filePrefix -&gt; 文件前缀。默认值FlumeData<br>hdfs.fileSuffix -&gt; 文件后缀<br>hdfs.rollInterval -&gt; 多久时间后close hdfs文件。单位是秒，默认30秒。设置为0的话表示不根据时间close hdfs文件<br>hdfs.rollSize -&gt; 文件大小超过一定值后，close文件。默认值1024，单位是字节。设置为0的话表示不基于文件大小<br>hdfs.rollCount -&gt; 写入了多少个事件后close文件。默认值是10个。设置为0的话表示不基于事件个数<br>hdfs.fileType -&gt; 文件格式， 有3种格式可选择：SequenceFile, DataStream or CompressedStream<br>hdfs.batchSize -&gt; 批次数，HDFS Sink每次从Channel中拿的事件个数。默认值100<br>hdfs.minBlockReplicas -&gt; HDFS每个块最小的replicas数字，不设置的话会取hadoop中的配置<br>hdfs.maxOpenFiles -&gt; 允许最多打开的文件数，默认是5000。如果超过了这个值，越早的文件会被关闭<br>serializer -&gt; HDFS Sink写文件的时候会进行序列化操作。会调用对应的Serializer借口，可以自定义符合需求的Serializer<br>hdfs.retryInterval -&gt; 关闭HDFS文件失败后重新尝试关闭的延迟数，单位是秒<br>hdfs.callTimeout -&gt; HDFS操作允许的时间，比如hdfs文件的open，write，flush，close操作。单位是毫秒，默认值是10000</p>
<h2 id="HDFSEventSink分析">HDFSEventSink分析</h2><p>以一个hdfs.path，hdfs.filePrefix和hdfs.fileSuffix分别为/data/%Y/%m/%d/%H，flume, .txt 为例子，分析源码：</p>
<p>直接看HDFSEventSink的process方法：</p>
<pre><code><span class="keyword">public</span> Status process() <span class="keyword">throws</span> EventDeliveryException {
    <span class="comment">// 得到Channel</span>
    Channel channel = getChannel();
    Transaction transaction = channel.getTransaction();
    <span class="comment">// 构造一个BucketWriter集合，BucketWriter就是处理hdfs文件的具体逻辑实现类</span>
    List&lt;BucketWriter&gt; writers = Lists.newArrayList();
    <span class="comment">// Channel的事务启动</span>
    transaction.begin();
    <span class="keyword">try</span> {
      <span class="built_in">int</span> txnEventCount = <span class="number">0</span>;
      <span class="comment">// 每次处理batchSize个事件。这里的batchSize就是之前配置的hdfs.batchSize</span>
      <span class="keyword">for</span> (txnEventCount = <span class="number">0</span>; txnEventCount &lt; batchSize; txnEventCount++) {
        Event event = channel.take();
        <span class="keyword">if</span> (event == <span class="keyword">null</span>) {
          <span class="keyword">break</span>;
        }

        <span class="comment">// 构造hdfs文件所在的路径</span>
        <span class="keyword">String</span> realPath = BucketPath.escapeString(filePath, event.getHeaders(),
            timeZone, needRounding, roundUnit, roundValue, useLocalTime);
        <span class="comment">// 构造hdfs文件名, fileName就是之前配置的hdfs.filePrefix，即flume</span>
        <span class="keyword">String</span> realName = BucketPath.escapeString(fileName, event.getHeaders(),
          timeZone, needRounding, roundUnit, roundValue, useLocalTime);

        <span class="comment">// 构造hdfs文件路径，根据之前的path，filePrefix，fileSuffix</span>
        <span class="comment">// 得到这里的lookupPath为 /data/2015/07/20/15/flume</span>
        <span class="keyword">String</span> lookupPath = realPath + DIRECTORY_DELIMITER + realName;
        BucketWriter bucketWriter;
        HDFSWriter hdfsWriter = <span class="keyword">null</span>;
        <span class="comment">// 构造一个回调函数</span>
        WriterCallback closeCallback = <span class="keyword">new</span> WriterCallback() {
          @Override
          <span class="keyword">public</span> <span class="keyword">void</span> run(<span class="keyword">String</span> bucketPath) {
            LOG.info(<span class="string">"Writer callback called."</span>);
            <span class="keyword">synchronized</span> (sfWritersLock) {
              <span class="comment">// sfWriters是一个HashMap，最多支持maxOpenFiles个键值对。超过maxOpenFiles的话会关闭越早进来的文件</span>
              <span class="comment">// 回调函数的作用就是hdfs文件close的时候移除sfWriters中对应的那个文件。防止打开的文件数超过maxOpenFiles</span>
              <span class="comment">// sfWriters这个Map中的key是要写的hdfs路径，value是BucketWriter</span>
              sfWriters.remove(bucketPath);
            }
          }
        };
        <span class="keyword">synchronized</span> (sfWritersLock) {
          <span class="comment">// 先查看sfWriters是否已经存在key为/data/2015/07/20/15/flume的BucketWriter</span>
          bucketWriter = sfWriters.<span class="built_in">get</span>(lookupPath);
          <span class="keyword">if</span> (bucketWriter == <span class="keyword">null</span>) {
              <span class="comment">// 没有的话构造一个BucketWriter</span>
            <span class="comment">// 先根据fileType得到对应的HDFSWriter，fileType默认有3种类型，分别是SequenceFile, DataStream or CompressedStream</span>
            hdfsWriter = writerFactory.getWriter(fileType);
            <span class="comment">// 构造一个BucketWriter，会将刚刚构造的hdfsWriter当做参数传入，BucketWriter写hdfs文件的时候会使用HDFSWriter</span>
            bucketWriter = initializeBucketWriter(realPath, realName,
              lookupPath, hdfsWriter, closeCallback);
            <span class="comment">// 新构造的BucketWriter放入到sfWriters中</span>
            sfWriters.put(lookupPath, bucketWriter);
          }
        }

        <span class="comment">// 将BucketWriter放入到writers集合中</span>
        <span class="keyword">if</span> (!writers.contains(bucketWriter)) {
          writers.<span class="built_in">add</span>(bucketWriter);
        }

        <span class="comment">// 写hdfs数据</span>
        <span class="keyword">try</span> {
          bucketWriter.<span class="built_in">append</span>(event);
        } <span class="keyword">catch</span> (BucketClosedException ex) {
          LOG.info(<span class="string">"Bucket was closed while trying to append, "</span> +
            <span class="string">"reinitializing bucket and writing event."</span>);
          hdfsWriter = writerFactory.getWriter(fileType);
          bucketWriter = initializeBucketWriter(realPath, realName,
            lookupPath, hdfsWriter, closeCallback);
          <span class="keyword">synchronized</span> (sfWritersLock) {
            sfWriters.put(lookupPath, bucketWriter);
          }
          bucketWriter.<span class="built_in">append</span>(event);
        }
      }

      <span class="keyword">if</span> (txnEventCount == <span class="number">0</span>) {
        sinkCounter.incrementBatchEmptyCount();
      } <span class="keyword">else</span> <span class="keyword">if</span> (txnEventCount == batchSize) {
        sinkCounter.incrementBatchCompleteCount();
      } <span class="keyword">else</span> {
        sinkCounter.incrementBatchUnderflowCount();
      }

      <span class="comment">// 每个批次全部完成后flush所有的hdfs文件</span>
      <span class="keyword">for</span> (BucketWriter bucketWriter : writers) {
        bucketWriter.flush();
      }

      <span class="comment">// 事务提交</span>
      transaction.commit();

      <span class="keyword">if</span> (txnEventCount &lt; <span class="number">1</span>) {
        <span class="keyword">return</span> Status.BACKOFF;
      } <span class="keyword">else</span> {
        sinkCounter.addToEventDrainSuccessCount(txnEventCount);
        <span class="keyword">return</span> Status.READY;
      }
    } <span class="keyword">catch</span> (IOException eIO) {
      <span class="comment">// 发生异常事务回滚</span>
      transaction.rollback();
      LOG.warn(<span class="string">"HDFS IO error"</span>, eIO);
      <span class="keyword">return</span> Status.BACKOFF;
    } <span class="keyword">catch</span> (Throwable th) {
      <span class="comment">// 发生异常事务回滚</span>
      transaction.rollback();
      LOG.error(<span class="string">"process failed"</span>, th);
      <span class="keyword">if</span> (th <span class="keyword">instanceof</span> Error) {
        <span class="keyword">throw</span> (Error) th;
      } <span class="keyword">else</span> {
        <span class="keyword">throw</span> <span class="keyword">new</span> EventDeliveryException(th);
      }
    } <span class="keyword">finally</span> {
      <span class="comment">// 关闭事务</span>
      transaction.close();
    }
}
</code></pre><h2 id="BucketWriter分析">BucketWriter分析</h2><p>接下来我们看下BucketWriter的append和flush方法。</p>
<p>append方法：</p>
<pre><code>  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="keyword">append</span>(<span class="keyword">final</span> Event event)
      <span class="keyword">throws</span> IOException, InterruptedException {
    checkAndThrowInterruptedException();
    <span class="comment">// If idleFuture is not null, cancel it before we move forward to avoid a</span>
    <span class="comment">// close call in the middle of the append.</span>
    <span class="keyword">if</span>(idleFuture != <span class="keyword">null</span>) {
      idleFuture.cancel(<span class="keyword">false</span>);
      <span class="comment">// There is still a small race condition - if the idleFuture is already</span>
      <span class="comment">// running, interrupting it can cause HDFS close operation to throw -</span>
      <span class="comment">// so we cannot interrupt it while running. If the future could not be</span>
      <span class="comment">// cancelled, it is already running - wait for it to finish before</span>
      <span class="comment">// attempting to write.</span>
      <span class="keyword">if</span>(!idleFuture.isDone()) {
        <span class="keyword">try</span> {
          idleFuture.get(callTimeout, TimeUnit.MILLISECONDS);
        } <span class="keyword">catch</span> (TimeoutException ex) {
          LOG.warn(<span class="string">"Timeout while trying to cancel closing of idle file. Idle"</span> +
            <span class="string">" file close may have failed"</span>, ex);
        } <span class="keyword">catch</span> (Exception ex) {
          LOG.warn(<span class="string">"Error while trying to cancel closing of idle file. "</span>, ex);
        }
      }
      idleFuture = <span class="keyword">null</span>;
    }

    <span class="comment">// 如果hdfs文件没有被打开</span>
    <span class="keyword">if</span> (!isOpen) {
      <span class="comment">// hdfs已关闭的话抛出异常</span>
      <span class="keyword">if</span> (closed) {
        <span class="keyword">throw</span> <span class="keyword">new</span> BucketClosedException(<span class="string">"This bucket writer was closed and "</span> +
          <span class="string">"this handle is thus no longer valid"</span>);
      }
      <span class="comment">// 打开hdfs文件</span>
      open();
    }

    <span class="comment">// 查看是否需要创建新文件</span>
    <span class="keyword">if</span> (shouldRotate()) {
      <span class="keyword">boolean</span> doRotate = <span class="keyword">true</span>;

      <span class="keyword">if</span> (isUnderReplicated) {
        <span class="keyword">if</span> (maxConsecUnderReplRotations &gt; <span class="number">0</span> &amp;&amp;
            consecutiveUnderReplRotateCount &gt;= maxConsecUnderReplRotations) {
          doRotate = <span class="keyword">false</span>;
          <span class="keyword">if</span> (consecutiveUnderReplRotateCount == maxConsecUnderReplRotations) {
            LOG.error(<span class="string">"Hit max consecutive under-replication rotations ({}); "</span> +
                <span class="string">"will not continue rolling files under this path due to "</span> +
                <span class="string">"under-replication"</span>, maxConsecUnderReplRotations);
          }
        } <span class="keyword">else</span> {
          LOG.warn(<span class="string">"Block Under-replication detected. Rotating file."</span>);
        }
        consecutiveUnderReplRotateCount++;
      } <span class="keyword">else</span> {
        consecutiveUnderReplRotateCount = <span class="number">0</span>;
      }

      <span class="keyword">if</span> (doRotate) {
          <span class="comment">// 如果需要创建新文件的时候会关闭文件，然后再打开新的文件。这里的close方法没有参数，表示可以再次打开新的文件</span>
        close();
        open();
      }
    }

    <span class="comment">// 写event数据</span>
    <span class="keyword">try</span> {
      sinkCounter.incrementEventDrainAttemptCount();
      callWithTimeout(<span class="keyword">new</span> CallRunner&lt;<span class="keyword">Void</span>&gt;() {
        @Override
        <span class="keyword">public</span> <span class="keyword">Void</span> <span class="keyword">call</span>() <span class="keyword">throws</span> Exception {
          <span class="comment">// 真正的写数据使用HDFSWriter的append方法</span>
          writer.<span class="keyword">append</span>(event); <span class="comment">// could block</span>
          <span class="keyword">return</span> <span class="keyword">null</span>;
        }
      });
    } <span class="keyword">catch</span> (IOException e) {
      LOG.warn(<span class="string">"Caught IOException writing to HDFSWriter ({}). Closing file ("</span> +
          bucketPath + <span class="string">") and rethrowing exception."</span>,
          e.getMessage());
      <span class="keyword">try</span> {
        close(<span class="keyword">true</span>);
      } <span class="keyword">catch</span> (IOException e2) {
        LOG.warn(<span class="string">"Caught IOException while closing file ("</span> +
             bucketPath + <span class="string">"). Exception follows."</span>, e2);
      }
      <span class="keyword">throw</span> e;
    }

    <span class="comment">// 文件大小+起来</span>
    processSize += event.getBody().length;
    <span class="comment">// 事件个数+1</span>
    eventCounter++;
    <span class="comment">// 批次数+1</span>
    batchCounter++;

    <span class="comment">// 批次数达到配置的hdfs.batchSize的话调用flush方法</span>
    <span class="keyword">if</span> (batchCounter == batchSize) {
      flush();
    }
}
</code></pre><p>先看下open方法，打开hdfs文件的方法：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">void</span> open() <span class="keyword">throws</span> IOException, InterruptedException {
    <span class="comment">// hdfs文件路径或HDFSWriter没构造的话抛出异常</span>
    <span class="keyword">if</span> ((filePath == <span class="keyword">null</span>) || (writer == <span class="keyword">null</span>)) {
      <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Invalid file settings"</span>);
    }

    <span class="keyword">final</span> Configuration config = <span class="keyword">new</span> Configuration();
    <span class="comment">// disable FileSystem JVM shutdown hook</span>
    config.setBoolean(<span class="string">"fs.automatic.close"</span>, <span class="keyword">false</span>);

    <span class="comment">// Hadoop is not thread safe when doing certain RPC operations,</span>
    <span class="comment">// including getFileSystem(), when running under Kerberos.</span>
    <span class="comment">// open() must be called by one thread at a time in the JVM.</span>
    <span class="comment">// <span class="doctag">NOTE:</span> tried synchronizing on the underlying Kerberos principal previously</span>
    <span class="comment">// which caused deadlocks. See FLUME-1231.</span>
    <span class="keyword">synchronized</span> (staticLock) {
      checkAndThrowInterruptedException();

      <span class="keyword">try</span> {
          <span class="comment">// fileExtensionCounter是一个AtomicLong类型的实例，初始化为当前时间戳的数值</span>
        <span class="comment">// 由于之前分析的，可能存在先关闭文件，然后再次open新文件的情况。所以在同一个BucketWriter类中open方法得到的文件名时间戳仅仅相差1</span>
        <span class="comment">// 得到时间戳counter</span>
        <span class="keyword">long</span> counter = fileExtensionCounter.incrementAndGet();

        <span class="comment">// 最终的文件名加上时间戳，这就是为什么flume生成的文件名会带有时间戳的原因</span>
        <span class="comment">// 这里的fullFileName就是 flume.1437375933234</span>
        String fullFileName = fileName + <span class="string">"."</span> + counter;

        <span class="comment">// 加上后缀名， fullFileName就成了flume.1437375933234.txt</span>
        <span class="keyword">if</span> (fileSuffix != <span class="keyword">null</span> &amp;&amp; fileSuffix.length() &gt; <span class="number">0</span>) {
          fullFileName += fileSuffix;
        } <span class="keyword">else</span> <span class="keyword">if</span> (codeC != <span class="keyword">null</span>) {
          fullFileName += codeC.getDefaultExtension();
        }

        <span class="comment">// 由于没配置inUsePrefix和inUseSuffix。 故这两个属性的值分别为""和".tmp"</span>
        <span class="comment">// buckerPath为 /data/2015/07/20/15/flume.1437375933234.txt.tmp</span>
        bucketPath = filePath + <span class="string">"/"</span> + inUsePrefix
          + fullFileName + inUseSuffix;
        <span class="comment">// targetPath为 /data/2015/07/20/15/flume.1437375933234.txt</span>
        targetPath = filePath + <span class="string">"/"</span> + fullFileName;

        LOG.info(<span class="string">"Creating "</span> + bucketPath);
        callWithTimeout(<span class="keyword">new</span> CallRunner&lt;<span class="keyword">Void</span>&gt;() {
          @Override
          <span class="keyword">public</span> <span class="keyword">Void</span> <span class="keyword">call</span>() <span class="keyword">throws</span> Exception {
            <span class="keyword">if</span> (codeC == <span class="keyword">null</span>) {
              <span class="comment">// Need to get reference to FS using above config before underlying</span>
              <span class="comment">// writer does in order to avoid shutdown hook &amp;</span>
              <span class="comment">// IllegalStateExceptions</span>
              <span class="keyword">if</span>(!mockFsInjected) {
                fileSystem = <span class="keyword">new</span> Path(bucketPath).getFileSystem(
                  config);
              }
              <span class="comment">// 使用HDFSWriter打开文件</span>
              writer.open(bucketPath);
            } <span class="keyword">else</span> {
              <span class="comment">// need to get reference to FS before writer does to</span>
              <span class="comment">// avoid shutdown hook</span>
              <span class="keyword">if</span>(!mockFsInjected) {
                fileSystem = <span class="keyword">new</span> Path(bucketPath).getFileSystem(
                  config);
              }
              <span class="comment">// 使用HDFSWriter打开文件</span>
              writer.open(bucketPath, codeC, compType);
            }
            <span class="keyword">return</span> <span class="keyword">null</span>;
          }
        });
      } <span class="keyword">catch</span> (Exception ex) {
        sinkCounter.incrementConnectionFailedCount();
        <span class="keyword">if</span> (ex <span class="keyword">instanceof</span> IOException) {
          <span class="keyword">throw</span> (IOException) ex;
        } <span class="keyword">else</span> {
          <span class="keyword">throw</span> Throwables.propagate(ex);
        }
      }
    }
    isClosedMethod = getRefIsClosed();
    sinkCounter.incrementConnectionCreatedCount();
    <span class="comment">// 重置各个计数器</span>
    resetCounters();

    <span class="comment">// 开线程处理hdfs.rollInterval配置的参数，多长时间后调用close方法</span>
    <span class="keyword">if</span> (rollInterval &gt; <span class="number">0</span>) {
      Callable&lt;<span class="keyword">Void</span>&gt; action = <span class="keyword">new</span> Callable&lt;<span class="keyword">Void</span>&gt;() {
        <span class="keyword">public</span> <span class="keyword">Void</span> <span class="keyword">call</span>() <span class="keyword">throws</span> Exception {
          LOG.debug(<span class="string">"Rolling file ({}): Roll scheduled after {} sec elapsed."</span>,
              bucketPath, rollInterval);
          <span class="keyword">try</span> {
            <span class="comment">// Roll the file and remove reference from sfWriters map.</span>
            close(<span class="keyword">true</span>);
          } <span class="keyword">catch</span>(Throwable t) {
            LOG.error(<span class="string">"Unexpected error"</span>, t);
          }
          <span class="keyword">return</span> <span class="keyword">null</span>;
        }
      };
      <span class="comment">// 以秒为单位在这里指定。将这个线程执行的结果赋值给timedRollFuture这个属性</span>
      timedRollFuture = timedRollerPool.schedule(action, rollInterval,
          TimeUnit.SECONDS);
    }

    isOpen = <span class="keyword">true</span>;
}
</code></pre><p>flush方法，只会在close和append方法(处理的事件数等于批次数)中被调用：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> flush() <span class="keyword">throws</span> IOException, InterruptedException {
    checkAndThrowInterruptedException();
    <span class="keyword">if</span> (!isBatchComplete()) { <span class="comment">//isBatchComplete判断batchCount是否等于0。 所以这里只要batchCount不为0，那么执行下去</span>
      doFlush(); <span class="comment">// doFlush方法会调用HDFSWriter的sync方法，并且将batchCount设置为0</span>

      <span class="comment">// idleTimeout没有配置，以下代码不会执行</span>
      <span class="keyword">if</span>(idleTimeout &gt; <span class="number">0</span>) {
        <span class="comment">// if the future exists and couldn't be cancelled, that would mean it has already run</span>
        <span class="comment">// or been cancelled</span>
        <span class="keyword">if</span>(idleFuture == <span class="keyword">null</span> || idleFuture.cancel(<span class="keyword">false</span>)) {
          Callable&lt;<span class="keyword">Void</span>&gt; idleAction = <span class="keyword">new</span> Callable&lt;<span class="keyword">Void</span>&gt;() {
            <span class="keyword">public</span> <span class="keyword">Void</span> <span class="keyword">call</span>() <span class="keyword">throws</span> Exception {
              LOG.info(<span class="string">"Closing idle bucketWriter {} at {}"</span>, bucketPath,
                System.currentTimeMillis());
              <span class="keyword">if</span> (isOpen) {
                close(<span class="keyword">true</span>);
              }
              <span class="keyword">return</span> <span class="keyword">null</span>;
            }
          };
          idleFuture = timedRollerPool.schedule(idleAction, idleTimeout,
              TimeUnit.SECONDS);
        }
      }
    }
}
</code></pre><p>close方法：</p>
<pre><code>  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> close(<span class="keyword">boolean</span> callCloseCallback)
        <span class="keyword">throws</span> IOException, InterruptedException {
    checkAndThrowInterruptedException();
    <span class="keyword">try</span> {
      <span class="comment">// close的时候先执行flush方法，清空batchCount，并调用HDFSWriter的sync方法</span>
      flush();
    } <span class="keyword">catch</span> (IOException e) {
      LOG.warn(<span class="string">"pre-close flush failed"</span>, e);
    }
    <span class="keyword">boolean</span> failedToClose = <span class="keyword">false</span>;
    LOG.info(<span class="string">"Closing {}"</span>, bucketPath);
    <span class="comment">// 创建一个关闭线程，这个线程会调用HDFSWriter的close方法</span>
    CallRunner&lt;<span class="keyword">Void</span>&gt; closeCallRunner = createCloseCallRunner();
    <span class="keyword">if</span> (isOpen) { <span class="comment">// 如果文件还开着</span>
      <span class="keyword">try</span> {
          <span class="comment">// 执行HDFSWriter的close方法</span>
        callWithTimeout(closeCallRunner);
        sinkCounter.incrementConnectionClosedCount();
      } <span class="keyword">catch</span> (IOException e) {
        LOG.warn(
          <span class="string">"failed to close() HDFSWriter for file ("</span> + bucketPath +
            <span class="string">"). Exception follows."</span>, e);
        sinkCounter.incrementConnectionFailedCount();
        failedToClose = <span class="keyword">true</span>;
        <span class="comment">// 关闭文件失败的话起个线程，retryInterval秒后继续执行</span>
        <span class="keyword">final</span> Callable&lt;<span class="keyword">Void</span>&gt; scheduledClose =
          createScheduledCloseCallable(closeCallRunner);
        timedRollerPool.schedule(scheduledClose, retryInterval,
          TimeUnit.SECONDS);
      }
      isOpen = <span class="keyword">false</span>;
    } <span class="keyword">else</span> {
      LOG.info(<span class="string">"HDFSWriter is already closed: {}"</span>, bucketPath);
    }

    <span class="comment">// timedRollFuture就是根据hdfs.rollInterval配置生成的一个属性。如果hdfs.rollInterval配置为0，那么不会执行以下代码</span>
    <span class="comment">// 因为要close文件，所以如果开启了hdfs.rollInterval等待时间到了flush文件，由于文件已经关闭，再次关闭会有问题</span>
    <span class="comment">// 所以这里取消timedRollFuture线程的执行</span>
    <span class="keyword">if</span> (timedRollFuture != <span class="keyword">null</span> &amp;&amp; !timedRollFuture.isDone()) {
      timedRollFuture.cancel(<span class="keyword">false</span>); <span class="comment">// do not cancel myself if running!</span>
      timedRollFuture = <span class="keyword">null</span>;
    }

    <span class="comment">// 没有配置hdfs.idleTimeout， 不会执行</span>
    <span class="keyword">if</span> (idleFuture != <span class="keyword">null</span> &amp;&amp; !idleFuture.isDone()) {
      idleFuture.cancel(<span class="keyword">false</span>); <span class="comment">// do not cancel myself if running!</span>
      idleFuture = <span class="keyword">null</span>;
    }

    <span class="comment">// 重命名文件，如果报错了，不会重命名文件</span>
    <span class="keyword">if</span> (bucketPath != <span class="keyword">null</span> &amp;&amp; fileSystem != <span class="keyword">null</span> &amp;&amp; !failedToClose) {
      <span class="comment">// 将 /data/2015/07/20/15/flume.1437375933234.txt.tmp 重命名为 /data/2015/07/20/15/flume.1437375933234.txt</span>
      renameBucket(bucketPath, targetPath, fileSystem);
    }
    <span class="keyword">if</span> (callCloseCallback) { <span class="comment">// callCloseCallback是close方法的参数</span>

      <span class="comment">// 调用关闭文件的回调函数，也就是BucketWriter的onCloseCallback属性</span>
      <span class="comment">// 这个onCloseCallback属性就是在HDFSEventSink里的回调函数closeCallback。 用来处理sfWriters.remove(bucketPath);</span>
      <span class="comment">// 如果onCloseCallback属性为true，那么说明这个BucketWriter已经不会再次open新的文件了。生命周期已经到了。</span>
      <span class="comment">// onCloseCallback只有在append方法中调用shouldRotate方法的时候需要close文件的时候才会传入false，其他情况都是true</span>
      runCloseAction(); 

      closed = <span class="keyword">true</span>;
    }
}
</code></pre><p>再回过头来看下append方法里的shouldRotate方法，shouldRotate方法执行下去的话会关闭文件然后再次打开新的文件：</p>
<pre><code><span class="keyword">private</span> <span class="function"><span class="keyword">boolean</span> <span class="title">shouldRotate</span><span class="params">()</span> </span>{
    <span class="keyword">boolean</span> doRotate = <span class="keyword">false</span>;

    <span class="comment">// 调用HDFSWriter的isUnderReplicated方法，用来判断当前hdfs文件是否正在复制。</span>
    <span class="keyword">if</span> (writer.isUnderReplicated()) {
      <span class="keyword">this</span>.isUnderReplicated = <span class="keyword">true</span>;
      doRotate = <span class="keyword">true</span>;
    } <span class="keyword">else</span> {
      <span class="keyword">this</span>.isUnderReplicated = <span class="keyword">false</span>;
    }

    <span class="comment">// rollCount就是配置的hdfs.rollCount。 eventCounter事件数达到rollCount之后，会close文件，然后创建新的文件</span>
    <span class="keyword">if</span> ((rollCount &gt; <span class="number">0</span>) &amp;&amp; (rollCount &lt;= eventCounter)) {
      LOG.debug(<span class="string">"rolling: rollCount: {}, events: {}"</span>, rollCount, eventCounter);
      doRotate = <span class="keyword">true</span>;
    }

    <span class="comment">// rollSize就是配置的hdfs.rollSize。processSize是每个事件加起来的文件大小。当processSize超过rollSize的时候，会close文件，然后创建新的文件</span>
    <span class="keyword">if</span> ((rollSize &gt; <span class="number">0</span>) &amp;&amp; (rollSize &lt;= processSize)) {
      LOG.debug(<span class="string">"rolling: rollSize: {}, bytes: {}"</span>, rollSize, processSize);
      doRotate = <span class="keyword">true</span>;
    }

    <span class="keyword">return</span> doRotate;
}
</code></pre><h2 id="HDFSWriter分析">HDFSWriter分析</h2><p>每个BucketWriter中对应只有一个HDFSWriter。</p>
<p>HDFSWriter是一个接口，有3个具体的实现类，分别是：HDFSDataStream，HDFSSequenceFile和HDFSCompressedDataStream。分别对应fileType为DataStream，SequenceFile和CompressedStream。</p>
<p>我们以HDFSDataStream为例，分析一下在BucketWriter中用到的HDFSWriter的一些方法：</p>
<p>append方法，写hdfs文件：</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">append</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IOException </span>{
    <span class="comment">// 非常简单，直接使用serializer的write方法</span>
    <span class="comment">// serializer是org.apache.flume.serialization.EventSerializer接口的实现类</span>
    <span class="comment">// 默认的Serializer是BodyTextEventSerializer</span>
    serializer.write(e);
}
</code></pre><p>open方法：</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">open</span><span class="params">(String filePath)</span> <span class="keyword">throws</span> IOException </span>{
    Configuration conf = <span class="keyword">new</span> Configuration();
    <span class="comment">// 构造hdfs路径</span>
    Path dstPath = <span class="keyword">new</span> Path(filePath);
    FileSystem hdfs = getDfs(conf, dstPath);
    <span class="comment">// 调用doOpen方法</span>
    doOpen(conf, dstPath, hdfs);
}

<span class="keyword">protected</span> <span class="function"><span class="keyword">void</span> <span class="title">doOpen</span><span class="params">(Configuration conf,
    Path dstPath, FileSystem hdfs)</span> <span class="keyword">throws</span>
        IOException </span>{
    <span class="keyword">if</span>(useRawLocalFileSystem) {
      <span class="keyword">if</span>(hdfs <span class="keyword">instanceof</span> LocalFileSystem) {
        hdfs = ((LocalFileSystem)hdfs).getRaw();
      } <span class="keyword">else</span> {
        logger.warn(<span class="string">"useRawLocalFileSystem is set to true but file system "</span> +
            <span class="string">"is not of type LocalFileSystem: "</span> + hdfs.getClass().getName());
      }
    }

    <span class="keyword">boolean</span> appending = <span class="keyword">false</span>;
    <span class="comment">// 构造FSDataOutputStream，作为属性outStream</span>
    <span class="keyword">if</span> (conf.getBoolean(<span class="string">"hdfs.append.support"</span>, <span class="keyword">false</span>) == <span class="keyword">true</span> &amp;&amp; hdfs.isFile
            (dstPath)) {
      outStream = hdfs.append(dstPath);
      appending = <span class="keyword">true</span>;
    } <span class="keyword">else</span> {
      outStream = hdfs.create(dstPath);
    }

    <span class="comment">// 初始化Serializer</span>
    serializer = EventSerializerFactory.getInstance(
        serializerType, serializerContext, outStream);
    <span class="keyword">if</span> (appending &amp;&amp; !serializer.supportsReopen()) {
      outStream.close();
      serializer = <span class="keyword">null</span>;
      <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"serializer ("</span> + serializerType +
          <span class="string">") does not support append"</span>);
    }

    <span class="comment">// must call superclass to check for replication issues</span>
    registerCurrentStream(outStream, hdfs, dstPath);

    <span class="keyword">if</span> (appending) {
      serializer.afterReopen();
    } <span class="keyword">else</span> {
      serializer.afterCreate();
    }
}
</code></pre><p>close方法：</p>
<pre><code><span class="at_rule">@<span class="keyword">Override</span>
public void <span class="function">close</span>() throws IOException </span>{
    <span class="tag">serializer</span><span class="class">.flush</span>();
    <span class="tag">serializer</span><span class="class">.beforeClose</span>();
    <span class="tag">outStream</span><span class="class">.flush</span>();
    <span class="tag">outStream</span><span class="class">.sync</span>();
    <span class="tag">outStream</span><span class="class">.close</span>();

    <span class="tag">unregisterCurrentStream</span>();
}
</code></pre><p>sync方法：</p>
<pre><code><span class="at_rule">@<span class="keyword">Override</span>
public void <span class="function">sync</span>() throws IOException </span>{
    <span class="tag">serializer</span><span class="class">.flush</span>();
    <span class="tag">outStream</span><span class="class">.flush</span>();
    <span class="tag">outStream</span><span class="class">.sync</span>();
}
</code></pre><p>isUnderReplicated方法，在AbstractHDFSWriter中定义：</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isUnderReplicated</span><span class="params">()</span> </span>{
    <span class="keyword">try</span> {
      <span class="comment">// 得到目前文件replication后的块数</span>
      <span class="keyword">int</span> numBlocks = getNumCurrentReplicas();
      <span class="keyword">if</span> (numBlocks == -<span class="number">1</span>) {
        <span class="keyword">return</span> <span class="keyword">false</span>;
      }
      <span class="keyword">int</span> desiredBlocks;
      <span class="keyword">if</span> (configuredMinReplicas != <span class="keyword">null</span>) {
        <span class="comment">// 如果配置了hdfs.minBlockReplicas</span>
        desiredBlocks = configuredMinReplicas;
      } <span class="keyword">else</span> {
        <span class="comment">// 没配置hdfs.minBlockReplicas的话直接从hdfs配置中拿</span>
        desiredBlocks = getFsDesiredReplication();
      }
      <span class="comment">// 如果当前复制的块比期望要复制的块数字要小的话，返回true</span>
      <span class="keyword">return</span> numBlocks &lt; desiredBlocks;
    } <span class="keyword">catch</span> (IllegalAccessException e) {
      logger.<span class="keyword">error</span>(<span class="string">"Unexpected error while checking replication factor"</span>, e);
    } <span class="keyword">catch</span> (InvocationTargetException e) {
      logger.<span class="keyword">error</span>(<span class="string">"Unexpected error while checking replication factor"</span>, e);
    } <span class="keyword">catch</span> (IllegalArgumentException e) {
      logger.<span class="keyword">error</span>(<span class="string">"Unexpected error while checking replication factor"</span>, e);
    }
    <span class="keyword">return</span> <span class="keyword">false</span>;
}
</code></pre><h2 id="总结">总结</h2><p>hdfs.rollInterval，hdfs.rollSize，hdfs.rollCount，hdfs.minBlockReplicas，hdfs.batchSize这5个配置影响着hdfs文件的关闭。</p>
<p><strong>注意，这5个配置影响的是一个hdfs文件，是一个hdfs文件。当hdfs文件关闭的时候，这些配置指标会重新开始计算。因为BucketWriter中的open方法里会调用resetCounters方法，这个方法会重置计数器。而基于hdfs.rollInterval的timedRollFuture线程返回值是在close方法中被销毁的。因此，只要close文件，并且open新文件的时候，这5个属性都会重新开始计算。</strong></p>
<p>hdfs.rollInterval与时间有关，当时间达到hdfs.rollInterval配置的秒数，那么会close文件。</p>
<p>hdfs.rollSize与每个event的字节大小有关，当一个一个event的字节相加起来大于等于hdfs.rollSize的时候，那么会close文件。</p>
<p>hdfs.rollCount与事件的个数有关，当事件个数大于等于hdfs.rollCount的时候，那么会close文件。</p>
<p>hdfs.batchSize表示当事件添加到hdfs.batchSize个的时候，也就是说HDFS Sink每次会拿hdfs.batchSize个事件，而且这些所有的事件都写进了同一个hdfs文件，这才会触发本次条件，并且其他4个配置都未达成条件。然后会close文件。</p>
<p>hdfs.minBlockReplicas表示期望hdfs对文件最小的复制块数。所以有时候我们配置了hdfs.rollInterval，hdfs.rollSize，hdfs.rollCount这3个参数，并且这3个参数都没有符合条件，但是还是生成了多个文件，这就是因为这个参数导致的，而且这个参数的优先级比hdfs.rollSize，hdfs.rollCount要高。</p>
]]></content>
    <summary type="html">
    <![CDATA[分析Flume HDFSSink写hdfs文件过程]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Flume几个比较有用的功能和一些坑(用到新功能后会更新文章)]]></title>
    <link href="http://fangjian0423.github.io/2015/07/14/flume-notes/"/>
    <id>http://fangjian0423.github.io/2015/07/14/flume-notes/</id>
    <published>2015-07-14T14:23:23.000Z</published>
    <updated>2016-01-07T05:14:26.000Z</updated>
    <content type="html"><![CDATA[<p>根据项目的经验，介绍几个flume比较有用的功能。</p>
<h2 id="ChannelSelector功能">ChannelSelector功能</h2><p>flume内置的ChannelSelector有两种，分别是Replicating和Multiplexing。</p>
<p>Replicating类型的ChannelSelector会针对每一个Event，拷贝到所有的Channel中，这是默认的ChannelSelector。</p>
<p>replicating类型的ChannelSelector例子如下：</p>
<pre><code>a1.sources = r1
a1.channels = c1 c2 <span class="preprocessor"># 如果有<span class="number">100</span>个Event，那么c1和c2中都会有这<span class="number">100</span>个事件</span>

a1.channels.c1.type = memory
a1.channels.c1.capacity = <span class="number">1000</span>
a1.channels.c1.transactionCapacity = <span class="number">100</span>


a1.channels.c2.type = memory
a1.channels.c2.capacity = <span class="number">1000</span>
a1.channels.c2.transactionCapacity = <span class="number">100</span>
</code></pre><p>Multiplexing类型的ChannelSelector会根据Event中Header中的某个属性决定分发到哪个Channel。</p>
<p>multiplexing类型的ChannelSelector例子如下：</p>
<pre><code><span class="label">a1.sources</span> = <span class="literal">r1</span>

<span class="label">a1.sources.source1.selector.type</span> = <span class="keyword">multiplexing
</span><span class="label">a1.sources.source1.selector.header</span> = validation # 以header中的validation对应的值作为条件
<span class="label">a1.sources.source1.selector.mapping.SUCCESS</span> = <span class="literal">c2</span> # 如果header中validation的值为SUCCESS，使用<span class="literal">c2</span>这个channel
<span class="label">a1.sources.source1.selector.mapping.FAIL</span> = <span class="literal">c1</span> # 如果header中validation的值为FAIL，使用<span class="literal">c1</span>这个channel
<span class="label">a1.sources.source1.selector.default</span> = <span class="literal">c1</span> # 默认使用<span class="literal">c1</span>这个channel
</code></pre><h2 id="Sink的Serializer">Sink的Serializer</h2><p>HDFS Sink， HBase Sink，ElasticSearch Sink都支持Serializer功能。</p>
<p>Serializer的作用是sink写入的时候，做一些处理。</p>
<h3 id="HDFS_Sink的Serializer">HDFS Sink的Serializer</h3><p>在<a href="http://fangjian0423.github.io/2015/06/23/flume-sink/">Flume Sink组件分析中</a>一文中，分析过了HDFS写文件的时候使用BucketWriter写数据，BucketWriter内部使用HDFSWriter属性写数据。HDFSWriter是一个处理hdfs文件的接口。</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">HDFSWriter</span> <span class="keyword">extends</span> <span class="title">Configurable</span> </span>{

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(String filePath)</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(String filePath, CompressionCodec codec,
      CompressionType cType)</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sync</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isUnderReplicated</span><span class="params">()</span></span>;

}
</code></pre><p>HDFSWriter的结构如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/flume-note1.png" alt=""></p>
<p>hdfs sink的fileType配置如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/flume-note2.png" alt=""></p>
<p>HDFSDataStream对应DataStream类型，HDFSCompressedDataStream对应CompressedStream，HDFSSequenceFile对应SequenceFile。</p>
<p>以DataStream为例，HDFSDataStream的append方法如下：</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">append</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IOException </span>{
    serializer.write(e);
}
</code></pre><p>这个serializer是HDFSDataStream的属性。是EventSerializer接口类型的属性。HDFSDataStream的append很简单，直接调用serializer的writer方法。</p>
<p>HDFS Sink的Serializer都需要实现EventSerializer接口：</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">EventSerializer</span> </span>{

  <span class="keyword">public</span> <span class="keyword">static</span> String CTX_PREFIX = <span class="string">"serializer."</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterCreate</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterReopen</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Event event)</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flush</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">beforeClose</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">supportsReopen</span><span class="params">()</span></span>;

  <span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Builder</span> </span>{
    <span class="function"><span class="keyword">public</span> EventSerializer <span class="title">build</span><span class="params">(Context context, OutputStream out)</span></span>;
  }

}
</code></pre><p>HDFS Sink默认的serializer是BodyTextEventSerializer类，不配置的话也是使用这个Serializer。</p>
<p>BodyTextEventSerializer的writer方法：</p>
<pre><code>@Override
<span class="keyword">public</span> <span class="keyword">void</span> <span class="keyword">write</span>(Event e) <span class="keyword">throws</span> IOException {
  out.<span class="keyword">write</span>(e.getBody());
  <span class="keyword">if</span> (appendNewline) {
    out.<span class="keyword">write</span>(<span class="string">'\n'</span>);
  }
}
</code></pre><p>这就是为什么hdfs sink写数据的时候写完会自动换行的原因。</p>
<p>当然，我们可以定义自定义的Serializer来满足自身的要求。</p>
<h3 id="HBase_Sink的Serializer">HBase Sink的Serializer</h3><p>HBase Sink的Serializer都需要实现HbaseEventSerializer接口。</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">HbaseEventSerializer</span> <span class="keyword">extends</span> <span class="title">Configurable</span>,
            <span class="title">ConfigurableComponent</span> </span>{

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(Event event, <span class="keyword">byte</span>[] columnFamily)</span></span>;

  <span class="function"><span class="keyword">public</span> List&lt;Row&gt; <span class="title">getActions</span><span class="params">()</span></span>;

  <span class="function"><span class="keyword">public</span> List&lt;Increment&gt; <span class="title">getIncrements</span><span class="params">()</span></span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;

}
</code></pre><p>HBaseSink的process方法关键代码：</p>
<pre><code>for <span class="list">(<span class="comment">; i &lt; batchSize; i++) {</span>
    Event event = channel.take<span class="list">()</span><span class="comment">;</span>
    if <span class="list">(<span class="keyword">event</span> == null)</span> {
      if <span class="list">(<span class="keyword">i</span> == <span class="number">0</span>)</span> {
        status = Status.BACKOFF<span class="comment">;</span>
        sinkCounter.incrementBatchEmptyCount<span class="list">()</span><span class="comment">;</span>
      } else {
        sinkCounter.incrementBatchUnderflowCount<span class="list">()</span><span class="comment">;</span>
      }
      break<span class="comment">;</span>
    } else {
      serializer.initialize<span class="list">(<span class="keyword">event</span>, columnFamily)</span><span class="comment">;</span>
      actions.addAll<span class="list">(<span class="keyword">serializer</span>.getActions<span class="list">()</span>)</span><span class="comment">;</span>
      incs.addAll<span class="list">(<span class="keyword">serializer</span>.getIncrements<span class="list">()</span>)</span><span class="comment">;</span>
    }
  }</span>
</code></pre><p>actions和incs都加入了serializer里的actions和increments。之后会commit这里的actions和increments数据。</p>
<p>HBase默认的Serializer是org.apache.flume.sink.hbase.SimpleHbaseEventSerializer。</p>
<p>我们也可以根据需求定义自定义的HbaseEventSerializer，需要注意的是getActions和getIncrements方法。</p>
<p>HBase Sink会加入这2个方法的返回值，并写入到HBase。</p>
<h3 id="Elasticsearch_Sink的Serializer">Elasticsearch Sink的Serializer</h3><p>Elasticsearch Sink的Serializer都需要实现ElasticSearchEventSerializer接口。</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ElasticSearchEventSerializer</span> <span class="keyword">extends</span> <span class="title">Configurable</span>,
                <span class="title">ConfigurableComponent</span> </span>{

  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Charset charset = Charset.defaultCharset();

  <span class="function"><span class="keyword">abstract</span> BytesStream <span class="title">getContentBuilder</span><span class="params">(Event event)</span> <span class="keyword">throws</span> IOException</span>;
}
</code></pre><p>默认的Serializer是org.apache.flume.sink.elasticsearch.ElasticSearchLogStashEventSerializer。</p>
<p>同样，我们也可以根据需求定义自定义的ElasticSearchEventSerialize，就不分析了。</p>
<h2 id="SinkGroup">SinkGroup</h2><p>这个功能暂时还没用到，不过以后可能会用到。</p>
<p>Sink Group的作用是把多个Sink合并成一个。这样的话Sink处理器会根据配置的类型来决定如何使用Sink。比如可以使用load balance，failover策略，或者可以使用自定义的策略来处理。</p>
<p><a href="https://flume.apache.org/FlumeUserGuide.html#default-sink-processor" target="_blank" rel="external">官方文档Sink Group</a>已经写的很清楚了。</p>
<h2 id="其它">其它</h2><p>目前还正在用Flume开发一些功能，后续可能会使用一些新的功能，到时候回头更新这篇文章。</p>
]]></content>
    <summary type="html">
    <![CDATA[根据项目的经验，介绍几个flume比较有用的功能...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
</feed>
