<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Format's Notes]]></title>
  <subtitle><![CDATA[吃饭睡觉撸代码]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://fangjian0423.github.io/"/>
  <updated>2016-04-12T13:21:45.000Z</updated>
  <id>http://fangjian0423.github.io/</id>
  
  <author>
    <name><![CDATA[Format]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[jdk ConcurrentSkipListMap工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/04/12/jdk-concurrentskiplistmap/"/>
    <id>http://fangjian0423.github.io/2016/04/12/jdk-concurrentskiplistmap/</id>
    <published>2016-04-12T11:49:11.000Z</published>
    <updated>2016-04-12T13:21:45.000Z</updated>
    <content type="html"><![CDATA[<p>ConcurrentSkipListMap是一个内部使用跳表，并且支持排序和并发的一个Map。</p>
<p>跳表的介绍：</p>
<p>跳表是一种允许在一个有顺序的序列中进行快速查询的数据结构。</p>
<p>如果在普通的顺序链表中查询一个元素，需要从链表头部开始一个一个节点进行遍历，然后找到节点，如下图所示，要查找234元素的话需要从5元素节点开始一个一个节点进行遍历，这样的效率是非常低的。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/skiplist01.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/280/fill/IzFBMDBGRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>跳表可以解决这种查询时间过长的问题：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/skiplist02.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/280/fill/IzFBMDBGRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>从上图可以看到，跳表具有以下几种特性：</p>
<ol>
<li>由很多层组成，level越高的层节点越少，最后一层level用有所有的节点数据</li>
<li>每一层的节点数据也都是有顺序的</li>
<li>上面层的节点肯定会在下面层中出现</li>
<li>每个节点都有两个指针，分别是同一层的下一个节点指针和下一层节点的指针</li>
</ol>
<p>使用跳表查询元素的时间复杂度是O(log n)，跟红黑树一样。查询效率还是不错的，</p>
<p>但是跳表的存储容量变大了，本来一共只有7个节点的数据，使用跳表之后变成了14个节点。</p>
<p>所以跳表是一种使用”空间换时间”的概念用来提高查询效率的链表，开源软件Redis、LevelDB都使用到了跳表。跳表相比B树，红黑树，AVL树时间复杂度一样，但是耗费更多存储空间，但是跳表的优势就是它相比树，实现简单，不需要考虑树的一些rebalance问题。</p>
<a id="more"></a>
<p>下图是一个级别更高的跳表：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/skiplist03.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/280/fill/IzFBMDBGRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<h2 id="跳表的查询">跳表的查询</h2><p>比如要在下面这个跳表中查找93元素，过程如下：</p>
<ol>
<li>从head节点(最上层的第一个节点)开始找，发现5比93小，继续同一层(Level3)的下一个节点150进行比较</li>
<li>发现105比93大，往下一层(Level2)走，然后找Level2的5元素的下一个节点67，发现67比93小，继续同一层(Level2)的下一个节点150进行比较</li>
<li>发现105比93大，往下一层(Level1)走，然后找Level1的67元素的下一个节点93，找到，返回</li>
</ol>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/skiplist04.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/280/fill/IzFBMDBGRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<h2 id="跳表新增元素">跳表新增元素</h2><p>跳表中新增元素的话首先会确定Level层，在这个Level以及这个Level以下的层中都加入新的元素，具体的Level层数是通过一个通过一种随机算法获取的，比如之前这个跳表在Level2和Level1中插入666元素：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/skiplist05.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/280/fill/IzFBMDBGRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>如果Level大于目前跳表的层数，那么会添加新的一层。</p>
<h2 id="跳表删除元素">跳表删除元素</h2><p>在各个层中找到对应的元素并删除即可。</p>
<h2 id="ConcurrentSkipListMap分析">ConcurrentSkipListMap分析</h2><p>ConcurrentSkipListMap对跳表中的几个概念做了一层封装，如下：</p>
<pre><code><span class="comment">// 每个节点的封装，跟层数没有关系</span>
static <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Node&lt;K</span>,<span class="title">V&gt;</span> {</span>
    <span class="keyword">final</span> <span class="type">K</span> key; <span class="comment">// 节点的关键字</span>
    volatile <span class="type">Object</span> value; <span class="comment">// 节点的值</span>
    volatile <span class="type">Node</span>&lt;<span class="type">K</span>,<span class="type">V</span>&gt; next; <span class="comment">// 节点的next节点引用</span>
    ...
}

<span class="comment">// 每一层节点的封装，叫做索引</span>
static <span class="class"><span class="keyword">class</span> <span class="title">Index&lt;K</span>,<span class="title">V&gt;</span> {</span>
    <span class="keyword">final</span> <span class="type">Node</span>&lt;<span class="type">K</span>,<span class="type">V</span>&gt; node; <span class="comment">// 对应的节点</span>
    <span class="keyword">final</span> <span class="type">Index</span>&lt;<span class="type">K</span>,<span class="type">V</span>&gt; down; <span class="comment">// 下一层索引</span>
    volatile <span class="type">Index</span>&lt;<span class="type">K</span>,<span class="type">V</span>&gt; right; <span class="comment">// 同一层的下一个索引</span>
    ...
}

<span class="comment">// 每一层的头索引</span>
static <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HeadIndex&lt;K</span>,<span class="title">V&gt;</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">Index&lt;K</span>,<span class="title">V&gt;</span> {</span>
    <span class="keyword">final</span> int level; <span class="comment">// Level 级别</span>
    <span class="type">HeadIndex</span>(<span class="type">Node</span>&lt;<span class="type">K</span>,<span class="type">V</span>&gt; node, <span class="type">Index</span>&lt;<span class="type">K</span>,<span class="type">V</span>&gt; down, <span class="type">Index</span>&lt;<span class="type">K</span>,<span class="type">V</span>&gt; right, int level) {
        <span class="keyword">super</span>(node, down, right);
        <span class="keyword">this</span>.level = level;
    }
    ...
}
</code></pre><p>简单分析下ConcurrentSkipListMap的get方法：</p>
<pre><code><span class="keyword">private</span> V doGet(<span class="keyword">Object</span> <span class="variable">key</span>) {
    <span class="keyword">if</span> (<span class="variable">key</span> == <span class="keyword">null</span>)
        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();
    Comparator&lt;? <span class="keyword">super</span> K&gt; cmp = comparator;
    outer: <span class="keyword">for</span> (;;) {
        <span class="comment">// findPredecessor方法表示找到最接近要查找节点的节点，并且这个节点在最下面那一层，这样就保证会遍历所有节点</span>
        <span class="keyword">for</span> (Node&lt;K,V&gt; b = findPredecessor(<span class="variable">key</span>, cmp), n = b.next;;) {
            <span class="keyword">Object</span> v; <span class="built_in">int</span> c;
            <span class="keyword">if</span> (n == <span class="keyword">null</span>) <span class="comment">// 已经遍历节点到最后还是没有找到，break，返回null</span>
                <span class="keyword">break</span> outer;
            Node&lt;K,V&gt; f = n.next;
            <span class="keyword">if</span> (n != b.next) <span class="comment">// 判断比较下一个节点是否发生了变化，如果发生变化break重新开始死循环</span>
                <span class="keyword">break</span>;
            <span class="keyword">if</span> ((v = n.value) == <span class="keyword">null</span>) {    <span class="comment">// 如果下一个节点已经被删除了</span>
                n.helpDelete(b, f);
                <span class="keyword">break</span>;
            }
            <span class="keyword">if</span> (b.value == <span class="keyword">null</span> || v == n)  <span class="comment">// b is deleted</span>
                <span class="keyword">break</span>;
            <span class="keyword">if</span> ((c = cpr(cmp, <span class="variable">key</span>, n.<span class="variable">key</span>)) == <span class="number">0</span>) {  <span class="comment">// 比较并且找到了，直接返回</span>
                @SuppressWarnings(<span class="string">"unchecked"</span>) V vv = (V)v;
                <span class="keyword">return</span> vv;
            }
            <span class="keyword">if</span> (c &lt; <span class="number">0</span>) <span class="comment">// 找过头了，说明没有对应节点了，跳出循环，返回null</span>
                <span class="keyword">break</span> outer;
            b = n; <span class="comment">// 继续遍历</span>
            n = f; <span class="comment">// 继续遍历</span>
        }
    }
    <span class="keyword">return</span> <span class="keyword">null</span>;
}

<span class="keyword">private</span> Node&lt;K,V&gt; findPredecessor(<span class="keyword">Object</span> <span class="variable">key</span>, Comparator&lt;? <span class="keyword">super</span> K&gt; cmp) {
    <span class="keyword">if</span> (<span class="variable">key</span> == <span class="keyword">null</span>)
        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException(); <span class="comment">// don't postpone errors</span>
    <span class="keyword">for</span> (;;) { <span class="comment">// 一个死循环内部套着另外一个循环</span>
        <span class="keyword">for</span> (Index&lt;K,V&gt; q = head, r = q.right, d;;) { <span class="comment">// head表示最顶层的第一个索引，从这个索引开始找</span>
            <span class="keyword">if</span> (r != <span class="keyword">null</span>) { <span class="comment">// 如果索引的同一层下一个索引不为null</span>
                Node&lt;K,V&gt; n = r.node;
                K k = n.<span class="variable">key</span>;
                <span class="keyword">if</span> (n.value == <span class="keyword">null</span>) { <span class="comment">// 如果是个已删除节点</span>
                    <span class="keyword">if</span> (!q.unlink(r)) <span class="comment">// 使用cas把已删除节点从跳表上删除掉</span>
                        <span class="keyword">break</span>;           <span class="comment">// 已删除节点从跳表上删除失败，跳出重新循环</span>
                    r = q.right;         <span class="comment">// 继续遍历</span>
                    <span class="keyword">continue</span>;
                }
                <span class="keyword">if</span> (cpr(cmp, <span class="variable">key</span>, k) &gt; <span class="number">0</span>) { <span class="comment">// 使用cas比较要找的关键字和索引内节点的关键字，如果满足比较条件</span>
                    q = r; <span class="comment">// 当前所在索引变成同一层下一个索引</span>
                    r = r.right;  <span class="comment">// 当前所在索引的下一个索引变成下下个索引，继续遍历</span>
                    <span class="keyword">continue</span>;
                }
            }
            <span class="comment">// 直到找出上一层满足不了条件的那个索引</span>
            <span class="keyword">if</span> ((d = q.down) == <span class="keyword">null</span>) <span class="comment">// 找到下一层的索引</span>
                <span class="keyword">return</span> q.node; <span class="comment">// 如果下一层没有索引了，返回找到的最接近的节点</span>
            q = d; <span class="comment">// 下一层开始做相同的操作</span>
            r = d.right; <span class="comment">// 下一层开始做相同的操作</span>
        }
    }
}
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p>ConcurrentSkipListMap是一个内部使用跳表，并且支持排序和并发的一个Map。</p>
<p>跳表的介绍：</p>
<p>跳表是一种允许在一个有顺序的序列中进行快速查询的数据结构。</p>
<p>如果在普通的顺序链表中查询一个元素，需要从链表头部开始一个一个节点进行遍历，然后找到节点，如下图所示，要查找234元素的话需要从5元素节点开始一个一个节点进行遍历，这样的效率是非常低的。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/skiplist01.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/280/fill/IzFBMDBGRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>跳表可以解决这种查询时间过长的问题：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/skiplist02.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/280/fill/IzFBMDBGRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>从上图可以看到，跳表具有以下几种特性：</p>
<ol>
<li>由很多层组成，level越高的层节点越少，最后一层level用有所有的节点数据</li>
<li>每一层的节点数据也都是有顺序的</li>
<li>上面层的节点肯定会在下面层中出现</li>
<li>每个节点都有两个指针，分别是同一层的下一个节点指针和下一层节点的指针</li>
</ol>
<p>使用跳表查询元素的时间复杂度是O(log n)，跟红黑树一样。查询效率还是不错的，</p>
<p>但是跳表的存储容量变大了，本来一共只有7个节点的数据，使用跳表之后变成了14个节点。</p>
<p>所以跳表是一种使用”空间换时间”的概念用来提高查询效率的链表，开源软件Redis、LevelDB都使用到了跳表。跳表相比B树，红黑树，AVL树时间复杂度一样，但是耗费更多存储空间，但是跳表的优势就是它相比树，实现简单，不需要考虑树的一些rebalance问题。</p>]]>
    
    </summary>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="map" scheme="http://fangjian0423.github.io/tags/map/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk PriorityQueue优先队列工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/04/10/jdk_priorityqueue/"/>
    <id>http://fangjian0423.github.io/2016/04/10/jdk_priorityqueue/</id>
    <published>2016-04-10T13:36:08.000Z</published>
    <updated>2016-04-10T13:36:11.000Z</updated>
    <content type="html"><![CDATA[<p>优先队列跟普通的队列不一样，普通队列是一种遵循FIFO规则的队列，拿数据的时候按照加入队列的顺序拿取。   而优先队列每次拿数据的时候都会拿出优先级最高的数据。</p>
<p>优先队列内部维护着一个堆，每次取数据的时候都从堆顶拿数据，这就是优先队列的原理。</p>
<p>jdk的优先队列使用PriorityQueue这个类，使用者可以自己定义优先级规则。</p>
<a id="more"></a>
<h2 id="一个PriorityQueue例子">一个PriorityQueue例子</h2><p>定义一个Task类，有2个属性name和level。这个类放到PriorityQueue里，level越大优先级越高：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> Task {
    String name;
    <span class="keyword">int</span> level;

    <span class="function"><span class="keyword">public</span> <span class="title">Task</span><span class="params">(String name, <span class="keyword">int</span> level)</span> </span>{
        <span class="keyword">this</span>.name = name;
        <span class="keyword">this</span>.level = level;
    }

    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>{
        <span class="keyword">return</span> name;
    }

    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>{
        <span class="keyword">this</span>.name = name;
    }

    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getLevel</span><span class="params">()</span> </span>{
        <span class="keyword">return</span> level;
    }

    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setLevel</span><span class="params">(<span class="keyword">int</span> level)</span> </span>{
        <span class="keyword">this</span>.level = level;
    }

    @<span class="function">Override
    <span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>{
        <span class="keyword">return</span> <span class="string">"Task{"</span> +
                <span class="string">"name='"</span> + name + <span class="string">'\''</span> +
                <span class="string">", level="</span> + level +
                <span class="string">'}'</span>;
    }
}

<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>{
    PriorityQueue&lt;Task&gt; <span class="built_in">queue</span> = <span class="keyword">new</span> PriorityQueue&lt;Task&gt;(<span class="number">6</span>, <span class="keyword">new</span> Comparator&lt;Task&gt;() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Task t1, Task t2)</span> </span>{
            <span class="keyword">return</span> t2.getLevel() - t1.getLevel();
        }
    });
    <span class="built_in">queue</span>.add(<span class="keyword">new</span> Task(<span class="string">"游戏"</span>, <span class="number">20</span>));
    <span class="built_in">queue</span>.add(<span class="keyword">new</span> Task(<span class="string">"吃饭"</span>, <span class="number">100</span>));
    <span class="built_in">queue</span>.add(<span class="keyword">new</span> Task(<span class="string">"睡觉"</span>, <span class="number">90</span>));
    <span class="built_in">queue</span>.add(<span class="keyword">new</span> Task(<span class="string">"看书"</span>, <span class="number">70</span>));
    <span class="built_in">queue</span>.add(<span class="keyword">new</span> Task(<span class="string">"工作"</span>, <span class="number">80</span>));
    <span class="built_in">queue</span>.add(<span class="keyword">new</span> Task(<span class="string">"撩妹"</span>, <span class="number">10</span>));
    <span class="keyword">while</span>(!<span class="built_in">queue</span>.isEmpty()) {
        System.out.println(<span class="built_in">queue</span>.poll());
    }
}
</code></pre><p>输出结果：</p>
<pre><code><span class="keyword">Task</span>{name=<span class="string">'吃饭'</span>, level=<span class="number">100</span>}
<span class="keyword">Task</span>{name=<span class="string">'睡觉'</span>, level=<span class="number">90</span>}
<span class="keyword">Task</span>{name=<span class="string">'工作'</span>, level=<span class="number">80</span>}
<span class="keyword">Task</span>{name=<span class="string">'看书'</span>, level=<span class="number">70</span>}
<span class="keyword">Task</span>{name=<span class="string">'游戏'</span>, level=<span class="number">20</span>}
<span class="keyword">Task</span>{name=<span class="string">'撩妹'</span>, level=<span class="number">10</span>}
</code></pre><p>add过程其实就是在最大堆里添加新的元素，添加之后再进行调整：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/priorityqueue01.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>出队相当于每次都是堆顶出堆，堆顶出堆之后然后重新调整：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/priorityqueue02.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<h2 id="PriorityQueue原理分析">PriorityQueue原理分析</h2><p>首先看下PriorityQueue的属性：</p>
<pre><code><span class="keyword">transient</span> <span class="keyword">Object</span>[] queue; <span class="comment">// 堆</span>
<span class="keyword">private</span> <span class="built_in">int</span> <span class="built_in">size</span> = <span class="number">0</span>; <span class="comment">// 元素个数</span>
<span class="keyword">private</span> <span class="keyword">final</span> Comparator&lt;? <span class="keyword">super</span> E&gt; comparator; <span class="comment">// 比较器，如果是null，使用元素自身的比较器</span>
</code></pre><p>接下来是PriorityQueue的几个方法介绍。</p>
<p>add，添加元素：</p>
<pre><code><span class="function"><span class="keyword">public</span> boolean <span class="title">add</span><span class="params">(E e)</span> </span>{
    <span class="keyword">return</span> offer(e); <span class="comment">// add方法内部调用offer方法</span>
}

<span class="function"><span class="keyword">public</span> boolean <span class="title">offer</span><span class="params">(E e)</span> </span>{
    <span class="keyword">if</span> (e == null) <span class="comment">// 元素为空的话，抛出NullPointerException异常</span>
        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();
    modCount++;
    <span class="keyword">int</span> i = size;
    <span class="keyword">if</span> (i &gt;= <span class="built_in">queue</span>.length) <span class="comment">// 如果当前用堆表示的数组已经满了，调用grow方法扩容</span>
        grow(i + <span class="number">1</span>); <span class="comment">// 扩容</span>
    size = i + <span class="number">1</span>; <span class="comment">// 元素个数+1</span>
    <span class="keyword">if</span> (i == <span class="number">0</span>) <span class="comment">// 堆还没有元素的情况</span>
        <span class="built_in">queue</span>[<span class="number">0</span>] = e; <span class="comment">// 直接给堆顶赋值元素</span>
    <span class="keyword">else</span> <span class="comment">// 堆中已有元素的情况</span>
        siftUp(i, e); <span class="comment">// 重新调整堆，从下往上调整，因为新增元素是加到最后一个叶子节点</span>
    <span class="keyword">return</span> <span class="literal">true</span>;
}

<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">siftUp</span><span class="params">(<span class="keyword">int</span> k, E x)</span> </span>{
    <span class="keyword">if</span> (comparator != null)  <span class="comment">// 比较器存在的情况下</span>
        siftUpUsingComparator(k, x); <span class="comment">// 使用比较器调整</span>
    <span class="keyword">else</span> <span class="comment">// 比较器不存在的情况下</span>
        siftUpComparable(k, x); <span class="comment">// 使用元素自身的比较器调整</span>
}

<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">siftUpUsingComparator</span><span class="params">(<span class="keyword">int</span> k, E x)</span> </span>{
    <span class="keyword">while</span> (k &gt; <span class="number">0</span>) { <span class="comment">// 一直循环直到父节点还存在</span>
        <span class="keyword">int</span> parent = (k - <span class="number">1</span>) &gt;&gt;&gt; <span class="number">1</span>; <span class="comment">// 找到父节点索引</span>
        Object e = <span class="built_in">queue</span>[parent]; <span class="comment">// 赋值父节点元素</span>
        <span class="keyword">if</span> (comparator.compare(x, (E) e) &gt;= <span class="number">0</span>) <span class="comment">// 新元素与父元素进行比较，如果满足比较器结果，直接跳出，否则进行调整</span>
            <span class="keyword">break</span>;
        <span class="built_in">queue</span>[k] = e; <span class="comment">// 进行调整，新位置的元素变成了父元素</span>
        k = parent; <span class="comment">// 新位置索引变成父元素索引，进行递归操作</span>
    }
    <span class="built_in">queue</span>[k] = x; <span class="comment">// 新添加的元素添加到堆中</span>
}
</code></pre><p>siftUp方法调用过程如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/priorityqueue03.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>poll，出队方法：</p>
<pre><code><span class="function"><span class="keyword">public</span> E <span class="title">poll</span><span class="params">()</span> </span>{
    <span class="keyword">if</span> (size == <span class="number">0</span>)
        <span class="keyword">return</span> null;
    <span class="keyword">int</span> s = --size; <span class="comment">// 元素个数-1</span>
    modCount++;
    E result = (E) <span class="built_in">queue</span>[<span class="number">0</span>]; <span class="comment">// 得到堆顶元素</span>
    E x = (E) <span class="built_in">queue</span>[s]; <span class="comment">// 最后一个叶子节点</span>
    <span class="built_in">queue</span>[s] = null; <span class="comment">// 最后1个叶子节点置空</span>
    <span class="keyword">if</span> (s != <span class="number">0</span>)
        siftDown(<span class="number">0</span>, x); <span class="comment">// 从上往下调整，因为删除元素是删除堆顶的元素</span>
    <span class="keyword">return</span> result;
}

<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">siftDown</span><span class="params">(<span class="keyword">int</span> k, E x)</span> </span>{
    <span class="keyword">if</span> (comparator != null) <span class="comment">// 比较器存在的情况下</span>
        siftDownUsingComparator(k, x); <span class="comment">// 使用比较器调整</span>
    <span class="keyword">else</span> <span class="comment">// 比较器不存在的情况下</span>
        siftDownComparable(k, x); <span class="comment">// 使用元素自身的比较器调整</span>
}

<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">siftDownUsingComparator</span><span class="params">(<span class="keyword">int</span> k, E x)</span> </span>{
    <span class="keyword">int</span> half = size &gt;&gt;&gt; <span class="number">1</span>; <span class="comment">// 只需循环节点个数的一般即可</span>
    <span class="keyword">while</span> (k &lt; half) {
        <span class="keyword">int</span> child = (k &lt;&lt; <span class="number">1</span>) + <span class="number">1</span>; <span class="comment">// 得到父节点的左子节点索引</span>
        Object c = <span class="built_in">queue</span>[child]; <span class="comment">// 得到左子元素</span>
        <span class="keyword">int</span> right = child + <span class="number">1</span>; <span class="comment">// 得到父节点的右子节点索引</span>
        <span class="keyword">if</span> (right &lt; size &amp;&amp;
            comparator.compare((E) c, (E) <span class="built_in">queue</span>[right]) &gt; <span class="number">0</span>) <span class="comment">// 左子节点跟右子节点比较，取更大的值</span>
            c = <span class="built_in">queue</span>[child = right];
        <span class="keyword">if</span> (comparator.compare(x, (E) c) &lt;= <span class="number">0</span>)  <span class="comment">// 然后这个更大的值跟最后一个叶子节点比较</span>

            <span class="keyword">break</span>;
        <span class="built_in">queue</span>[k] = c; <span class="comment">// 新位置使用更大的值</span>
        k = child; <span class="comment">// 新位置索引变成子元素索引，进行递归操作</span>
    }
    <span class="built_in">queue</span>[k] = x; <span class="comment">// 最后一个叶子节点添加到合适的位置</span>
}
</code></pre><p>siftDown方法调用过程如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/priorityqueue04.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>grow扩容方法：</p>
<pre><code><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">grow</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>{
    <span class="keyword">int</span> oldCapacity = <span class="built_in">queue</span>.length;
    <span class="comment">// 新容量</span>
    <span class="comment">// 如果老容量小于64 新容量 = 老容量 + 老容量 + 2</span>
    <span class="comment">// 如果老容量大于等于64 老容量 = 老容量 + 老容量/2</span>
    <span class="keyword">int</span> newCapacity = oldCapacity + ((oldCapacity &lt; <span class="number">64</span>) ?
                                     (oldCapacity + <span class="number">2</span>) :
                                     (oldCapacity &gt;&gt; <span class="number">1</span>));
    <span class="comment">// 溢出处理</span>
    <span class="keyword">if</span> (newCapacity - MAX_ARRAY_SIZE &gt; <span class="number">0</span>)
        newCapacity = hugeCapacity(minCapacity);
    <span class="comment">// 使用新容量</span>
    <span class="built_in">queue</span> = Arrays.copyOf(<span class="built_in">queue</span>, newCapacity);
}
</code></pre><p>remove，删除队列元素操作：</p>
<pre><code><span class="function"><span class="keyword">public</span> boolean <span class="title">remove</span><span class="params">(Object o)</span> </span>{
    <span class="keyword">int</span> i = indexOf(o); <span class="comment">// 找到数据对应的索引</span>
    <span class="keyword">if</span> (i == -<span class="number">1</span>) <span class="comment">// 不存在的话返回false</span>
        <span class="keyword">return</span> <span class="literal">false</span>;
    <span class="keyword">else</span> { <span class="comment">// 存在的话调用removeAt方法，返回true</span>
        removeAt(i);
        <span class="keyword">return</span> <span class="literal">true</span>;
    }
}

<span class="function"><span class="keyword">private</span> E <span class="title">removeAt</span><span class="params">(<span class="keyword">int</span> i)</span> </span>{
    modCount++;
    <span class="keyword">int</span> s = --size; <span class="comment">// 元素个数-1</span>
    <span class="keyword">if</span> (s == i) <span class="comment">// 如果是删除最后一个叶子节点</span>
        <span class="built_in">queue</span>[i] = null; <span class="comment">// 直接置空，删除即可，堆还是保持特质，不需要调整</span>
    <span class="keyword">else</span> { <span class="comment">// 如果是删除的不是最后一个叶子节点</span>
        E moved = (E) <span class="built_in">queue</span>[s]; <span class="comment">// 获得最后1个叶子节点元素</span>
        <span class="built_in">queue</span>[s] = null; <span class="comment">// 最后1个叶子节点置空</span>
        siftDown(i, moved); <span class="comment">// 从上往下调整</span>
        <span class="keyword">if</span> (<span class="built_in">queue</span>[i] == moved) { <span class="comment">// 如果从上往下调整完毕之后发现元素位置没变，从下往上调整</span>
            siftUp(i, moved); <span class="comment">// 从下往上调整</span>
            <span class="keyword">if</span> (<span class="built_in">queue</span>[i] != moved)
                <span class="keyword">return</span> moved;
        }
    }
    <span class="keyword">return</span> null;
}
</code></pre><p>下图这个堆如果删除红色节点100的时候，siftDown之后元素位置没变，所以还得siftUp：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/priorityqueue05.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<h2 id="总结">总结</h2><ol>
<li>jdk内置的优先队列PriorityQueue内部使用一个堆维护数据，每当有数据add进来或者poll出去的时候会对堆做从下往上的调整和从上往下的调整</li>
<li>PriorityQueue不是一个线程安全的类，如果要在多线程环境下使用，可以使用PriorityBlockingQueue这个优先阻塞队列</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>优先队列跟普通的队列不一样，普通队列是一种遵循FIFO规则的队列，拿数据的时候按照加入队列的顺序拿取。   而优先队列每次拿数据的时候都会拿出优先级最高的数据。</p>
<p>优先队列内部维护着一个堆，每次取数据的时候都从堆顶拿数据，这就是优先队列的原理。</p>
<p>jdk的优先队列使用PriorityQueue这个类，使用者可以自己定义优先级规则。</p>]]>
    
    </summary>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="queue" scheme="http://fangjian0423.github.io/tags/queue/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[堆、二叉堆、堆排序]]></title>
    <link href="http://fangjian0423.github.io/2016/04/09/heap-heapsort/"/>
    <id>http://fangjian0423.github.io/2016/04/09/heap-heapsort/</id>
    <published>2016-04-09T09:42:18.000Z</published>
    <updated>2016-04-09T09:52:48.000Z</updated>
    <content type="html"><![CDATA[<p>堆的概念：</p>
<p>n个元素序列 { k1, k2, k3, k4, k5, k6 …. kn } 当且仅当满足以下关系时才会被称为堆：</p>
<pre><code>ki &lt;= k2i,ki &lt;= k2i+<span class="number">1</span> 或者 ki &gt;= k2i,ki &gt;= k2i+<span class="number">1</span> (i = <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span> .. n/<span class="number">2</span>)
</code></pre><p>如果数组的下表是从0开始，那么需要满足 </p>
<pre><code>ki &lt;= k2i+<span class="number">1</span>,ki &lt;= k2i+<span class="number">2</span> 或者 ki &gt;= k2i+<span class="number">1</span>,ki &gt;= k2i+<span class="number">2</span> (i = <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> .. n/<span class="number">2</span>)
</code></pre><p>比如 { 1,3,5,10,15,9 } 这个序列就满足 [1 &lt;= 3; 1 &lt;= 5],  [3 &lt;= 10; 3 &lt;= 15], [5 &lt;= 9] 这3个条件，这个序列就是一个堆。</p>
<p>所以堆其实是一个序列(数组)，如果这个序列满足上述条件，那么就把这个序列看成堆。</p>
<p>堆的实现通常是通过构造二叉堆，因为二叉堆应用很普遍，当不加限定时，堆通常指的就是二叉堆。</p>
<a id="more"></a>
<p>二叉堆的概念：</p>
<p>二叉堆是一种特殊的堆，是一棵完全二叉树或者是近似完全二叉树，同时二叉堆还满足堆的特性：父节点的键值总是保持固定的序关系于任何一个子节点的键值，且每个节点的左子树和右子树都是一个二叉堆。</p>
<p>当父节点的键值总是大于或等于任何一个子节点的键值时为最大堆。 当父节点的键值总是小于或等于任何一个子节点的键值时为最小堆。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/heap01.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>上图中的最小堆对应的序列是： [1,3,5,9,10,15]  满足最小堆的特性(父节点的键值小于或等于任何一个子节点的键值，并且也满足堆的性质 [1 &lt;= 3; 1 &lt;= 5], [3 &lt;= 9; 3 &lt;= 10], [5 &lt;= 15])</p>
<p>上图中的最大堆对应的序列是： [15,10,9,7,5,3]  满足最大堆的特性(父节点的键值大于或等于任何一个子节点的键值，并且也满足堆的性质 [15 &gt;= 10; 15 &gt;= 9], [10 &gt;= 7; 10 &gt;= 5], [9 &gt;= 3])</p>
<h2 id="堆的操作">堆的操作</h2><h3 id="堆排序">堆排序</h3><p>堆排序指的是对堆这种数据结构进行排序的一种算法。其基本思想如下，以最大堆为例：</p>
<ol>
<li>将数组序列构建成最大堆[ A1, A2, A3 .. An]，这个堆是一个刚初始化无序区，同时有序区为空</li>
<li>堆顶元素A1与最后一个元素An进行交换，得到新的有序区[An]，无序区变成[A1 … An-1]</li>
<li>交换之后可能导致[A1 … An-1]这个无序区不是一个最大堆，[A1 … An-1]无序区重新调整成最大堆。重复步骤2，A1与An-1进行交换，得到新的有序区[An,An-1]，无序区变成[A1 … An-2].. 不断重复，直到有序区的个数为n-1才结束排序过程</li>
</ol>
<p>构造堆的过程如下(以最大堆为例)：</p>
<p>从最后一个非叶子节点开始调整，遍历节点和2个子节点，选择键值最大的节点的键值代替父节点的键值，如果进行了调整，调整之后的两个子节点可能不符合堆特性，递归调整。一直直到调整完根节点。</p>
<p>以序列[3,5,15,9,10,1]为例进行的堆排序：</p>
<p>首先第1步先把数组转换成完全二叉树：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/heap02.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>接下来是第2、3步构造有序区和无序区：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/heap03.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>构造完之后有序区的元素依次是：1，3，5，9，10，15</p>
<p>简单地使用java写一下堆排序：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">class</span> HeapSort {

    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">maxHeapify</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> size, <span class="keyword">int</span> index)</span> </span>{
        <span class="keyword">int</span> leftSonIndex = <span class="number">2</span> * index + <span class="number">1</span>;
        <span class="keyword">int</span> rightSonIndex = <span class="number">2</span> * index + <span class="number">2</span>;
        <span class="keyword">int</span> temp = index;
        <span class="keyword">if</span>(index &lt;= size / <span class="number">2</span>) {
            <span class="keyword">if</span>(leftSonIndex &lt; size &amp;&amp; arr[temp] &lt; arr[leftSonIndex]) {
                temp = leftSonIndex;
            }
            <span class="keyword">if</span>(rightSonIndex &lt; size &amp;&amp; arr[temp] &lt; arr[rightSonIndex]) {
                temp = rightSonIndex;
            }
            <span class="comment">// 左右子节点的值存在比父节点的值更大</span>
            <span class="keyword">if</span>(temp != index) {
                swap(arr, index, temp); <span class="comment">// 交换值</span>
                maxHeapify(arr, size, temp); <span class="comment">// 递归调整</span>
            }
        }
    }

    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">heapSort</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> size)</span> </span>{
        <span class="comment">// 构造成最大堆</span>
        buildMaxHeap(arr, arr.length);
        <span class="keyword">for</span>(<span class="keyword">int</span> i = size - <span class="number">1</span>; i &gt; <span class="number">0</span>; i --) {
            <span class="comment">// 先交换堆顶元素和无序区最后一个元素</span>
            swap(arr, <span class="number">0</span>, i);
            <span class="comment">// 重新调整无序区</span>
            buildMaxHeap(arr, i - <span class="number">1</span>);
        }
    }

    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">buildMaxHeap</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> size)</span> </span>{
        <span class="keyword">for</span>(<span class="keyword">int</span> i = size / <span class="number">2</span>; i &gt;= <span class="number">0</span>; i --) { <span class="comment">// 最后一个非叶子节点开始调整</span>
            maxHeapify(arr, size, i);
        }
    }

    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>{
        <span class="keyword">int</span> temp = arr[i];
        arr[i] = arr[j];
        arr[j] = temp;
    }

    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>{
        <span class="keyword">int</span>[] arr = { <span class="number">3</span>, <span class="number">5</span>, <span class="number">15</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">1</span>};
        System.out.println(<span class="string">"before build: "</span> + Arrays.toString(arr)); <span class="comment">// before build: [3, 5, 15, 9, 10, 1]</span>
        buildMaxHeap(arr, arr.length);
        System.out.println(<span class="string">"after build: "</span> + Arrays.toString(arr)); <span class="comment">// after build: [15, 10, 3, 9, 5, 1]</span>
        heapSort(arr, arr.length);
        System.out.println(<span class="string">"after sort: "</span> + Arrays.toString(arr)); <span class="comment">// after sort: [1, 3, 5, 9, 10, 15]</span>
    }

}
</code></pre><h3 id="添加">添加</h3><p>在最大堆[ 15,10,9,7,5,3 ]上添加一个新的元素 11 ，执行的步骤如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/heap04.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<h3 id="删除">删除</h3><p>在最大堆[ 15,10,9,7,5,3 ]上删除元素 10 ，执行的步骤如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/heap05.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>堆的概念：</p>
<p>n个元素序列 { k1, k2, k3, k4, k5, k6 …. kn } 当且仅当满足以下关系时才会被称为堆：</p>
<pre><code>ki &lt;= k2i,ki &lt;= k2i+<span class="number">1</span> 或者 ki &gt;= k2i,ki &gt;= k2i+<span class="number">1</span> (i = <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span> .. n/<span class="number">2</span>)
</code></pre><p>如果数组的下表是从0开始，那么需要满足 </p>
<pre><code>ki &lt;= k2i+<span class="number">1</span>,ki &lt;= k2i+<span class="number">2</span> 或者 ki &gt;= k2i+<span class="number">1</span>,ki &gt;= k2i+<span class="number">2</span> (i = <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> .. n/<span class="number">2</span>)
</code></pre><p>比如 { 1,3,5,10,15,9 } 这个序列就满足 [1 &lt;= 3; 1 &lt;= 5],  [3 &lt;= 10; 3 &lt;= 15], [5 &lt;= 9] 这3个条件，这个序列就是一个堆。</p>
<p>所以堆其实是一个序列(数组)，如果这个序列满足上述条件，那么就把这个序列看成堆。</p>
<p>堆的实现通常是通过构造二叉堆，因为二叉堆应用很普遍，当不加限定时，堆通常指的就是二叉堆。</p>]]>
    
    </summary>
    
      <category term="algorithm" scheme="http://fangjian0423.github.io/tags/algorithm/"/>
    
      <category term="algorithm" scheme="http://fangjian0423.github.io/categories/algorithm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk TreeSet工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/04/08/jdk_treeset/"/>
    <id>http://fangjian0423.github.io/2016/04/08/jdk_treeset/</id>
    <published>2016-04-07T16:20:23.000Z</published>
    <updated>2016-04-07T16:20:26.000Z</updated>
    <content type="html"><![CDATA[<p>TreeSet跟HashSet，LinkedHashSet一样，都是Set接口的实现类。</p>
<p>HashSet内部使用的HashMap，LinkedHashSet继承HashSet，内部使用的是LinkedHashMap。</p>
<p>TreeSet实现的是NavigableSet接口，而不是HashSet和LinkedHashSet实现的Set接口。</p>
<p>NavigableSet接口继承自SortedSet接口，SortedSet接口继承自Set接口。</p>
<p>NavigableSet接口比Set更方便，可以使用firstKey[最小关键字]，lastKey[最大关键字]，pollFirstEntry[最小键值对]，pollLastEntry[最大键值对]，higherEntry[比参数关键字要大的键值对]，lowerEntry[比参数关键字要小的键值对]等等方便方法，可以使用这些方法方便地获取期望位置上的键值对。</p>
<a id="more"></a>
<h2 id="TreeSet原理分析">TreeSet原理分析</h2><p>TreeSet跟HashSet一样，内部都使用Map，HashSet内部使用的是HashMap，但是TreeSet使用的是NavigableMap。</p>
<p>TreeSet的几个构造方法会构造NavigableMap，如果使用没有参数的构造函数，NavigableMap是TreeMap：</p>
<pre><code>TreeSet(NavigableMap&lt;E,Object&gt; m) {
    <span class="keyword">this</span>.m = m;
}

<span class="function"><span class="keyword">public</span> <span class="title">TreeSet</span><span class="params">()</span> </span>{
    <span class="keyword">this</span>(<span class="keyword">new</span> TreeMap&lt;E,Object&gt;());
}

<span class="function"><span class="keyword">public</span> <span class="title">TreeSet</span><span class="params">(Comparator&lt;? <span class="keyword">super</span> E&gt; comparator)</span> </span>{
    <span class="keyword">this</span>(<span class="keyword">new</span> TreeMap&lt;&gt;(comparator));
}
</code></pre><p>add方法调用Map的put方法：</p>
<pre><code><span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>{
    <span class="keyword">return</span> m.put(e, PRESENT)==<span class="keyword">null</span>;
}
</code></pre><p>remove方法调用Map的remove方法：</p>
<pre><code><span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">remove</span><span class="params">(Object o)</span> </span>{
    <span class="keyword">return</span> m.remove(o)==PRESENT;
}
</code></pre><p>原理基本跟HashSet一样。</p>
<pre><code><span class="comment">// 最小的关键字</span>
public <span class="keyword">E</span> first() {
    <span class="keyword">return</span> <span class="keyword">m</span>.firstKey();
}

<span class="comment">// 最大的关键字</span>
public <span class="keyword">E</span> last() {
    <span class="keyword">return</span> <span class="keyword">m</span>.lastKey();
}

<span class="comment">// 比参数小的关键字</span>
public <span class="keyword">E</span> <span class="literal">lower</span>(<span class="keyword">E</span> <span class="keyword">e</span>) {
    <span class="keyword">return</span> <span class="keyword">m</span>.lowerKey(<span class="keyword">e</span>);
}
</code></pre><h2 id="一个TreeSet例子">一个TreeSet例子</h2><p>使用没有参数的TreeMap构造函数，内部的Map使用TreeMap红黑树：</p>
<pre><code>TreeSet&lt;<span class="keyword">String</span>&gt; <span class="built_in">set</span> = <span class="keyword">new</span> TreeSet&lt;<span class="keyword">String</span>&gt;();
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"1:语文"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"2:数学"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"3:英语"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"4:政治"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"5:物理"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"6:化学"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"7:生物"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"8:体育"</span>);
</code></pre><p>内部红黑树结构如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treeset01.jpg" alt=""></p>
<p>还可以调用TreeSet的其他方法：</p>
<pre><code><span class="built_in">set</span>.<span class="keyword">first</span>();<span class="comment"> // 1:语文</span>
<span class="built_in">set</span>.<span class="keyword">last</span>();<span class="comment"> // 8:体育</span>
<span class="built_in">set</span>.higher(<span class="string">"5:物理"</span>);<span class="comment"> // 6:化学</span>
<span class="built_in">set</span>.<span class="built_in">lower</span>(<span class="string">"5:物理"</span>);<span class="comment"> // 4:政治</span>
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p>TreeSet跟HashSet，LinkedHashSet一样，都是Set接口的实现类。</p>
<p>HashSet内部使用的HashMap，LinkedHashSet继承HashSet，内部使用的是LinkedHashMap。</p>
<p>TreeSet实现的是NavigableSet接口，而不是HashSet和LinkedHashSet实现的Set接口。</p>
<p>NavigableSet接口继承自SortedSet接口，SortedSet接口继承自Set接口。</p>
<p>NavigableSet接口比Set更方便，可以使用firstKey[最小关键字]，lastKey[最大关键字]，pollFirstEntry[最小键值对]，pollLastEntry[最大键值对]，higherEntry[比参数关键字要大的键值对]，lowerEntry[比参数关键字要小的键值对]等等方便方法，可以使用这些方法方便地获取期望位置上的键值对。</p>]]>
    
    </summary>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="set" scheme="http://fangjian0423.github.io/tags/set/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk TreeMap工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/04/07/jdk_treemap/"/>
    <id>http://fangjian0423.github.io/2016/04/07/jdk_treemap/</id>
    <published>2016-04-06T16:55:31.000Z</published>
    <updated>2016-04-06T16:55:32.000Z</updated>
    <content type="html"><![CDATA[<p>TreeMap是jdk中基于红黑树的一种map实现。HashMap底层是使用链表法解决冲突的哈希表，LinkedHashMap继承自HashMap，内部同样也是使用链表法解决冲突的哈希表，但是额外添加了一个双向链表用于处理元素的插入顺序或访问访问。</p>
<p>既然TreeMap底层使用的是红黑树，首先先来简单了解一下红黑树的定义。</p>
<p>红黑树是一棵平衡二叉查找树，同时还需要满足以下5个规则：</p>
<ol>
<li>每个节点只能是红色或者黑点</li>
<li>根节点是黑点</li>
<li>叶子节点(Nil节点，空节点)是黑色节点</li>
<li>如果一个节点是红色节点，那么它的两个子节点必须是黑色节点(一条路径上不能出现相邻的两个红色节点)</li>
<li>从任一节点到其每个叶子节点的所有路径都包含相同数目的黑色节点</li>
</ol>
<p>红黑树的这些特性决定了它的查询、插入、删除操作的时间复杂度均为O(log n)。</p>
<a id="more"></a>
<h2 id="一个TreeMap例子">一个TreeMap例子</h2><p>一段TreeMap代码：</p>
<pre><code>TreeMap&lt;Integer, String&gt; treeMap = <span class="keyword">new</span> TreeMap&lt;Integer, String&gt;();
treeMap.put(<span class="number">1</span>, <span class="string">"语文"</span>);
treeMap.put(<span class="number">2</span>, <span class="string">"数学"</span>);
treeMap.put(<span class="number">3</span>, <span class="string">"英语"</span>);
treeMap.put(<span class="number">4</span>, <span class="string">"政治"</span>);
treeMap.put(<span class="number">5</span>, <span class="string">"物理"</span>);
treeMap.put(<span class="number">6</span>, <span class="string">"化学"</span>);
treeMap.put(<span class="number">7</span>, <span class="string">"生物"</span>);
treeMap.put(<span class="number">8</span>, <span class="string">"体育"</span>);
</code></pre><p>执行过程：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treemap01.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>从上面这个例子看到，红黑树添加新节点的时候可能会对节点进行旋转，以保证树的局部平衡。</p>
<h2 id="TreeMap原理分析">TreeMap原理分析</h2><p>TreeMap内部类Entry表示红黑树中的节点：</p>
<pre><code><span class="keyword">static</span> <span class="keyword">final</span> class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; {
    K <span class="variable">key</span>; <span class="comment">// 关键字</span>
    V value; <span class="comment">// 值</span>
    Entry&lt;K,V&gt; left; <span class="comment">// 左节点</span>
    Entry&lt;K,V&gt; right; <span class="comment">// 右节点</span>
    Entry&lt;K,V&gt; parent; <span class="comment">// 父节点</span>
    <span class="built_in">boolean</span> <span class="built_in">color</span> = BLACK; <span class="comment">// 颜色，默认为黑色</span>

    Entry(K <span class="variable">key</span>, V value, Entry&lt;K,V&gt; parent) {
        <span class="keyword">this</span>.<span class="variable">key</span> = <span class="variable">key</span>;
        <span class="keyword">this</span>.value = value;
        <span class="keyword">this</span>.parent = parent;
    }

    ...
}
</code></pre><p>TreeMap的属性：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">transient</span> Entry&lt;K,V&gt; root; <span class="comment">// 根节点</span>

<span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">int</span> <span class="keyword">size</span> = <span class="number">0</span>; <span class="comment">// 节点个数</span>
</code></pre><h3 id="put操作">put操作</h3><p>红黑树新节点的添加一定是红色节点，添加完新的节点之后会进行旋转操作以保持红黑树的特性。</p>
<p>为什么新添加的节点一定是红色节点，如果添加的是黑色节点，那么就会导致根到叶子的路径上有一条路上，多一个额外的黑节点，这个是很难调整的；但是如果插入的是红色节点，只需要解决其父节点也为红色节点的这个冲突即可。</p>
<p>以N为新插入节点，P为其父节点，U为其父节点的兄弟节点，R为P和U的父亲节点进行分析。如果N的父节点为黑色节点，那直接添加新节点即可，没有产生冲突。如果出现P节点是红色节点，那便产生冲突，可以分为以下几种冲突：</p>
<p>(1) P为红色节点，且U也为红色节点，P不论是R的左节点还是右节点</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treemap04.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>将P和U接口变成黑色节点，R节点变成红色节点。修改之后如果R节点的父节点也是红色节点，那么在R节点上执行相同操作，形成了一个递归过程。如果R节点是根节点的话，那么直接把R节点修改成黑色节点。</p>
<p>(2) P为红色节点，U为黑色节点或缺少，且N是P的右节点、P是R的左节点 或者 P为红色节点，U为黑色节点或缺少，且N是P的左节点、P是R的右节点</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treemap07.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>这两种情况分别对P进行左旋和右旋操作。操作结果就变成了冲突3。 (总结起来就是左右变左左，右左变右右)</p>
<p>(3) P为红色节点，U为黑色节点或缺少，且N是P的左节点、P是R的左节点 或者 P为红色节点，U为黑色节点或缺少，且N是P的右节点、P是R的右节点</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treemap08.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>这两种情况分别对祖父R进行右旋和左旋操作。完美解决冲突。(总结起来就是左左祖右，右右祖左)</p>
<p>这3个新增节点的冲突处理方法了解之后，我们回过头来看本文一开始的例子中添加最后一个[8:体育]节点是如何处理冲突的：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treemap09.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>接下来我们看TreeMap是如何实现新增节点并处理冲突的。</p>
<p>TreeMap对应的put方法：</p>
<pre><code><span class="keyword">public</span> V put(K <span class="variable">key</span>, V value) {
    Entry&lt;K,V&gt; t = root;
    <span class="keyword">if</span> (t == <span class="keyword">null</span>) { <span class="comment">// 如果根节点是空的，说明是第一次插入数据</span>
        compare(<span class="variable">key</span>, <span class="variable">key</span>);

        root = <span class="keyword">new</span> Entry&lt;&gt;(<span class="variable">key</span>, value, <span class="keyword">null</span>); <span class="comment">// 构造根节点，并赋值给属性root，默认颜色是黑色</span>
        <span class="built_in">size</span> = <span class="number">1</span>; <span class="comment">// 节点数 = 1</span>
        modCount++;
        <span class="keyword">return</span> <span class="keyword">null</span>;
    }
    <span class="built_in">int</span> cmp;
    Entry&lt;K,V&gt; parent;
    <span class="comment">// split comparator and comparable paths</span>
    Comparator&lt;? <span class="keyword">super</span> K&gt; cpr = comparator;
    <span class="keyword">if</span> (cpr != <span class="keyword">null</span>) { <span class="comment">// 比较器存在</span>
        do { <span class="comment">// 遍历寻找节点，关键字比节点小找左节点，比节点大的找右节点，直到找到那个叶子节点，会保存需要新构造节点的父节点到parent变量里</span>
            parent = t;
            cmp = cpr.compare(<span class="variable">key</span>, t.<span class="variable">key</span>);
            <span class="keyword">if</span> (cmp &lt; <span class="number">0</span>)
                t = t.left;
            <span class="keyword">else</span> <span class="keyword">if</span> (cmp &gt; <span class="number">0</span>)
                t = t.right;
            <span class="keyword">else</span>
                <span class="keyword">return</span> t.setValue(value); <span class="comment">// 关键字存在的话，直接用值覆盖原节点的关键字的值，并返回</span>
        } <span class="keyword">while</span> (t != <span class="keyword">null</span>);
    }
    <span class="keyword">else</span> { <span class="comment">// 比较器不存在</span>
        <span class="keyword">if</span> (<span class="variable">key</span> == <span class="keyword">null</span>)
            <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();
        @SuppressWarnings(<span class="string">"unchecked"</span>)
            Comparable&lt;? <span class="keyword">super</span> K&gt; k = (Comparable&lt;? <span class="keyword">super</span> K&gt;) <span class="variable">key</span>; <span class="comment">// 比较器不存在直接将关键字转换成比较器，如果关键字不是一个Comparable接口实现类，将会报错</span>
        do { <span class="comment">// 遍历寻找节点，关键字比节点小找左节点，比节点大的找右节点，直到找到那个叶子节点，会保存需要新构造节点的父节点到parent变量里</span>
            parent = t;
            cmp = k.compareTo(t.<span class="variable">key</span>);
            <span class="keyword">if</span> (cmp &lt; <span class="number">0</span>)
                t = t.left;
            <span class="keyword">else</span> <span class="keyword">if</span> (cmp &gt; <span class="number">0</span>)
                t = t.right;
            <span class="keyword">else</span>
                <span class="keyword">return</span> t.setValue(value); <span class="comment">// 关键字存在的话，直接用值覆盖原节点的关键字的值，并返回</span>
        } <span class="keyword">while</span> (t != <span class="keyword">null</span>);
    }
    Entry&lt;K,V&gt; e = <span class="keyword">new</span> Entry&lt;&gt;(<span class="variable">key</span>, value, parent); <span class="comment">// 构造新的关键字节点</span>
    <span class="keyword">if</span> (cmp &lt; <span class="number">0</span>) <span class="comment">// 需要在左节点构造</span>
        parent.left = e;
    <span class="keyword">else</span> <span class="comment">// 需要在右节点构造</span>
        parent.right = e;
    fixAfterInsertion(e); <span class="comment">// 插入节点之后，处理冲突以保持树符合红黑树的特性</span>
    <span class="built_in">size</span>++;
    modCount++;
    <span class="keyword">return</span> <span class="keyword">null</span>;
}
</code></pre><p>fixAfterInsertion方法处理红黑树冲突实现如下：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">void</span> fixAfterInsertion(Entry&lt;K,V&gt; x) {
    x.<span class="built_in">color</span> = RED; <span class="comment">// 新增的节点一定是红色节点</span>

    <span class="keyword">while</span> (x != <span class="keyword">null</span> &amp;&amp; x != root &amp;&amp; x.parent.<span class="built_in">color</span> == RED) { <span class="comment">// P节点是红色节点并且N节点不是根节点的话一直循环</span>
        <span class="keyword">if</span> (parentOf(x) == leftOf(parentOf(parentOf(x)))) { <span class="comment">// P节点是R节点的左节点</span>
            Entry&lt;K,V&gt; y = rightOf(parentOf(parentOf(x))); <span class="comment">// y就是U节点</span>
            <span class="keyword">if</span> (colorOf(y) == RED) { <span class="comment">// 如果U节点是红色节点，说明P和U这两个节点都是红色节点，满足冲突(1)</span>
                setColor(parentOf(x), BLACK); <span class="comment">// 冲突(1)解决方案 把P设置为黑色</span>
                setColor(y, BLACK); <span class="comment">// 冲突(1)解决方案 把U设置为黑色</span>
                setColor(parentOf(parentOf(x)), RED); <span class="comment">// 冲突(1)解决方案 把R设置为红色</span>
                x = parentOf(parentOf(x)); <span class="comment">// 递归处理R节点</span>
            } <span class="keyword">else</span> { <span class="comment">// 如果U节点是黑色节点，满足冲突(2)或(3)</span>
                <span class="keyword">if</span> (x == rightOf(parentOf(x))) { <span class="comment">// 如果N节点是P节点的右节点，满足冲突(2)的第一种情况</span>
                    x = parentOf(x);
                    rotateLeft(x); <span class="comment">// P节点进行左旋操作</span>
                }
                <span class="comment">// P节点左旋操作之后，满足了冲突(3)的第一种情况或者N一开始就是P节点的左节点，这本来就是冲突(3)的第一种情况</span>
                setColor(parentOf(x), BLACK);  <span class="comment">// P节点和R节点交换颜色，P节点变成黑色</span>
                setColor(parentOf(parentOf(x)), RED); <span class="comment">// P节点和R节点交换颜色，R节点变成红色</span>
                rotateRight(parentOf(parentOf(x))); <span class="comment">// R节点右旋操作</span>
            }
        } <span class="keyword">else</span> { <span class="comment">// P节点是R节点的右节点</span>
            Entry&lt;K,V&gt; y = leftOf(parentOf(parentOf(x))); <span class="comment">// y就是U节点</span>
            <span class="keyword">if</span> (colorOf(y) == RED) { <span class="comment">// 如果U节点是红色节点，说明P和U这两个节点都是红色节点，满足冲突(1)</span>
                setColor(parentOf(x), BLACK); <span class="comment">// 冲突(1)解决方案 把P设置为黑色</span>
                setColor(y, BLACK); <span class="comment">// 冲突(1)解决方案 把U设置为黑色</span>
                setColor(parentOf(parentOf(x)), RED); <span class="comment">// 冲突(1)解决方案 把R设置为红色</span>
                x = parentOf(parentOf(x)); <span class="comment">// 递归处理R节点</span>
            } <span class="keyword">else</span> { <span class="comment">// 如果U节点是黑色节点，满足冲突(2)或(3)</span>
                <span class="keyword">if</span> (x == leftOf(parentOf(x))) { <span class="comment">// 如果N节点是P节点的左节点，满足冲突(2)的第二种情况</span>
                    x = parentOf(x);
                    rotateRight(x); <span class="comment">// P节点右旋</span>
                }
                <span class="comment">// P节点右旋操作之后，满足了冲突(3)的第二种情况或者N一开始就是P节点的右节点，这本来就是冲突(3)的第二种情况</span>
                setColor(parentOf(x), BLACK); <span class="comment">// P节点和R节点交换颜色，P节点变成黑色</span>
                setColor(parentOf(parentOf(x)), RED); <span class="comment">// P节点和R节点交换颜色，R节点变成红色</span>
                rotateLeft(parentOf(parentOf(x))); <span class="comment">// R节点左旋操作</span>
            }
        }
    }
    root.<span class="built_in">color</span> = BLACK; <span class="comment">// 根节点是黑色节点</span>
}
</code></pre><p>fixAfterInsertion方法的代码跟之前分析的冲突解决方案一模一样。</p>
<h3 id="get操作">get操作</h3><p>红黑树的get操作相比add操作简单不少，只需要比较关键字即可，要查找的关键字比节点关键字要小的话找左节点，否则找右节点，一直递归操作，直到找到或找不到。代码如下：</p>
<pre><code><span class="keyword">final</span> Entry&lt;K,V&gt; getEntry(<span class="keyword">Object</span> <span class="variable">key</span>) {
    <span class="keyword">if</span> (comparator != <span class="keyword">null</span>)
        <span class="keyword">return</span> getEntryUsingComparator(<span class="variable">key</span>);
    <span class="keyword">if</span> (<span class="variable">key</span> == <span class="keyword">null</span>)
        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();
    @SuppressWarnings(<span class="string">"unchecked"</span>)
        Comparable&lt;? <span class="keyword">super</span> K&gt; k = (Comparable&lt;? <span class="keyword">super</span> K&gt;) <span class="variable">key</span>;
    Entry&lt;K,V&gt; p = root;
    <span class="keyword">while</span> (p != <span class="keyword">null</span>) {
        <span class="built_in">int</span> cmp = k.compareTo(p.<span class="variable">key</span>); <span class="comment">// 得到比较值</span>
        <span class="keyword">if</span> (cmp &lt; <span class="number">0</span>) <span class="comment">// 小的话找左节点</span>
            p = p.left;
        <span class="keyword">else</span> <span class="keyword">if</span> (cmp &gt; <span class="number">0</span>) <span class="comment">// 大的话找右节点</span>
            p = p.right;
        <span class="keyword">else</span>
            <span class="keyword">return</span> p;
    }
    <span class="keyword">return</span> <span class="keyword">null</span>;
}
</code></pre><h3 id="remove操作">remove操作</h3><p>红黑树的删除节点跟添加节点一样，比较复杂，删除节点也会让树不符合红黑树的特性，也需要解决这些冲突。</p>
<p>删除操作分为2个步骤：</p>
<ol>
<li>将红黑树当作一颗二叉查找树，将节点删除</li>
<li>通过”旋转和重新着色”等一系列来修正该树，使之重新成为一棵红黑树</li>
</ol>
<p>步骤1的删除操作可分为几种情况：</p>
<ol>
<li>删除节点没有儿子：直接删除该节点</li>
<li>删除节点有1个儿子：删除该节点，并用该节点的儿子节点顶替它的位置</li>
<li>删除节点有2个儿子：可以转成成删除节点只有1个儿子的情况，跟二叉查找树一样，找出节点的右子树的最小元素(或者左子树的最大元素，这种节点称为后继节点)，并把它的值转移到删除节点，然后删除这个后继节点。这个后继节点最多只有1个子节点(如果有2个子节点，说明还能找出右子树更小的值)，所以这样删除2个儿子的节点就演变成了删除没有儿子的节点和删除只有1个儿子的节点的情况</li>
</ol>
<p>删除节点之后要考虑的问题就是红黑树失衡的调整问题。</p>
<p>步骤2遇到的调整问题只有2种情况：</p>
<ol>
<li>删除节点没有儿子节点</li>
<li>删除节点只有1个儿子节点</li>
</ol>
<p>删除节点没有儿子节点的话，直接把节点删除即可。如果节点是黑色节点，需要进行平衡性调整，否则，不用调整平衡性。这里的平衡性调整跟删除只有1个儿子节点一样，删除只有1个儿子的调整会先把节点删除，然后儿子节点顶上来，顶上来之后再进行平衡性调整。而删除没有儿子节点的节点的话，先进行调整，调整之后再把这个节点删除。他们的调整策略是一样的，只不过没有儿子节点的情况下先进行调整，然后再删除节点，而有儿子节点的情况下，先把节点删除，删除之后儿子节点顶上来，然后再做平衡性调整。</p>
<p>删除节点只有1个儿子节点还分几种情况：</p>
<ol>
<li>如果被删除的节点是红色节点，那说明它的父节点是黑色节点，儿子节点也是黑色节点，那么删除这个节点就不会影响红黑树的属性，直接使用它的黑色子节点代替它即可</li>
<li>如果被删除的节点是黑色节点，而它的儿子节点是红色节点。删除这个黑色节点之后，它的红色儿子节点顶替之后，会破坏性质5，只需要把儿子节点重绘为黑色节点即可，这样原先通过黑色删除节点的所有路径被现在的重绘后的儿子节点所代替</li>
<li>如果被删除的节点是黑色节点，而它的儿子节点也是黑色节点。这是一种复杂的情况，因为路径路过被删除节点的黑色节点路径少了1个，导致违反了性质5，所以需要对红黑树进行平衡调整。可分为以下几种情况进行调整：</li>
</ol>
<p>以N为删除节点的儿子节点(删除之后，处于新的位置上)，它的兄弟节点为S，它们的父节点为P，Sl和Sr为S节点的左右子节点为例，进行讲解，其中<strong>N是父节点P的左子节点</strong>，如果N是父节点P的右子节点，做对称处理。</p>
<p>3.1：N是新的根节点。这种情况下不用做任何处理，因为原先的节点也是一个根节点，相当于所有的路径都需要经过这个根节点，删除之后没有什么影响，而且新根也是黑色节点，符合所有特性，不需要进行调整</p>
<p>3.2: S节点是红色节点，那么P节点，Sl，Sr节点是黑色节点。在这种情况下，对P节点进行左选操作并且交换P和S的颜色。完成这2个操作之后，所有路径上的黑色节点没有变化，但是N节点有了一个黑色兄弟节点Sl和一个红色的父亲节点P，左子树删除节点后还有存在着少1个黑色节点路径的问题。接下来按照N节点新的位置(兄弟节点S是个黑色节点，父节点P是个红色节点)进行3.4、3.5或3.6情况处理<br><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treemap10.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>3.3：N的父亲节点P、兄弟节点S，还有S的两个子节点Sl，Sr均为黑色节点。在这种情况下，重绘S为红色。重绘之后路过S节点这边的路径跟N节点一样也少了一个黑色节点，但是出现了另外一个问题：不经过P节点的路径还是少了一个黑色节点。 接下来，要调整以P作为N递归调整树</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treemap11.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>3.4：S和S的儿子节点Sl、Sr为黑色节点，但是N的父亲节点P为红色节点。在这种情况下，交换N的兄弟S与父亲P的颜色，颜色交换之后左子树多了1个黑色节点路径，刚好填补了左子树删除节点的少一个黑色节点路径的问题，而右子树的黑色路径没有改变，解决平衡问题</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treemap12.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>3.5：S是黑色节点，S的左儿子节点Sl是红色，S的右儿子节点Sr是黑色节点。在这种情况下，在S上做右旋操作交换S和它新父亲的颜色。操作之后，左子树的黑色节点路径和右子树的黑色节点路径没有改变。但是现在N节点有了一个黑色的兄弟节点，黑色的兄弟节点有个红色的右儿子节点，满足了3.6的情况，按照3.6处理</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treemap13.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>3.6：S是黑色节点，S的右儿子节点Sr为红色节点，S的左儿子Sl是黑色节点，P是红色或黑色节点。在这种情况下，N的父亲P做左旋操作，交换N父亲P和S的颜色，S的右子节点Sr变成黑色。这样操作以后，左子树的黑色路径+1，补了删除节点的黑色路径，右子树黑色路径不变，解决平衡问题</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treemap14.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>了解了删除节点之后的平衡性调整之后，我们回过头来看本文一开始的例子进行节点删除的操作过程：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/treemap15.jpg?imageView2/1/w/10000/h/10000/q/100|watermark/2/text/ZmFuZ2ppYW4wNDIzLmdpdGh1Yi5pbw==/font/5a6L5L2T/fontsize/500/fill/I0VGRUZFRg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt=""></p>
<p>TreeMap删除方法如下：</p>
<pre><code><span class="keyword">private</span> <span class="literal">void</span> deleteEntry(Entry&lt;K,V&gt; p) {
    modCount++;
    size--; <span class="comment">// 节点个数 -1</span>

    <span class="keyword">if</span> (p<span class="built_in">.</span>left != <span class="built_in">null</span> <span class="subst">&amp;&amp;</span> p<span class="built_in">.</span>right != <span class="built_in">null</span>) { <span class="comment">// 如果要删除的节点有2个子节点，去找后继节点</span>
        Entry&lt;K,V&gt; s = success<span class="subst">or</span>(p); <span class="comment">// 找出后继节点</span>
        p<span class="built_in">.</span>key = s<span class="built_in">.</span>key; <span class="comment">// 后继节点的关键字赋值给删除节点</span>
        p<span class="built_in">.</span>value = s<span class="built_in">.</span>value; <span class="comment">// 后继节点的值赋值给删除节点</span>
        p = s; <span class="comment">// 改为删除后继节点</span>
    }

    Entry&lt;K,V&gt; replacement = (p<span class="built_in">.</span>left != <span class="built_in">null</span> ? p<span class="built_in">.</span>left : p<span class="built_in">.</span>right); <span class="comment">// 找出替代节点，左子树存在的话使用左子树，否则使用右子树。这个替代节点就是被删除节点的左子节点或右子节点</span>

    <span class="keyword">if</span> (replacement != <span class="built_in">null</span>) { <span class="comment">// 替代节点如果存在的话</span>
        replacement<span class="built_in">.</span><span class="keyword">parent</span> = p<span class="built_in">.</span><span class="keyword">parent</span>; <span class="comment">// 删除要删除的节点</span>
        <span class="comment">// 有子节点的删除节点先删除节点，然后再做平衡性调整</span>
        <span class="keyword">if</span> (p<span class="built_in">.</span><span class="keyword">parent</span> == <span class="built_in">null</span>) <span class="comment">// 如果被删除节点的父节点为空，说明被删除节点是根节点</span>
            root = replacement; <span class="comment">// 用替代节点替代根节点</span>
        <span class="keyword">else</span> <span class="keyword">if</span> (p == p<span class="built_in">.</span><span class="keyword">parent</span><span class="built_in">.</span>left)
            p<span class="built_in">.</span><span class="keyword">parent</span><span class="built_in">.</span>left  = replacement; <span class="comment">// 用替代节点替代原先被删除的节点</span>
        <span class="keyword">else</span>
            p<span class="built_in">.</span><span class="keyword">parent</span><span class="built_in">.</span>right = replacement; <span class="comment">// 用替代节点替代原先被删除的节点</span>

        p<span class="built_in">.</span>left = p<span class="built_in">.</span>right = p<span class="built_in">.</span><span class="keyword">parent</span> = <span class="built_in">null</span>;

        <span class="keyword">if</span> (p<span class="built_in">.</span>col<span class="subst">or</span> == BLACK) <span class="comment">// 被删除节点如果是黑色节点，需要进行平衡性调整</span>
            fixAfterDeletion(replacement);
    } <span class="keyword">else</span> <span class="keyword">if</span> (p<span class="built_in">.</span><span class="keyword">parent</span> == <span class="built_in">null</span>) { <span class="comment">// 如果被删除节点的父节点为空，说明被删除节点是根节点</span>
        root = <span class="built_in">null</span>; <span class="comment">// 根节点的删除直接把根节点置空即可</span>
    } <span class="keyword">else</span> { <span class="comment">//   如果要删除的节点没有子节点</span>
        <span class="keyword">if</span> (p<span class="built_in">.</span>col<span class="subst">or</span> == BLACK) <span class="comment">// 如果要删除的节点是个黑色节点，需要进行平衡性调整</span>
            fixAfterDeletion(p); <span class="comment">// 调整平衡性，没有子节点的删除节点先进行平衡性调整</span>

        <span class="keyword">if</span> (p<span class="built_in">.</span><span class="keyword">parent</span> != <span class="built_in">null</span>) { <span class="comment">// 没有子节点的删除节点平衡性调整完毕之后再进行节点删除</span>
            <span class="keyword">if</span> (p == p<span class="built_in">.</span><span class="keyword">parent</span><span class="built_in">.</span>left)
                p<span class="built_in">.</span><span class="keyword">parent</span><span class="built_in">.</span>left = <span class="built_in">null</span>;
            <span class="keyword">else</span> <span class="keyword">if</span> (p == p<span class="built_in">.</span><span class="keyword">parent</span><span class="built_in">.</span>right)
                p<span class="built_in">.</span><span class="keyword">parent</span><span class="built_in">.</span>right = <span class="built_in">null</span>;
            p<span class="built_in">.</span><span class="keyword">parent</span> = <span class="built_in">null</span>;
        }
    }
}

<span class="comment">// 删除节点后的平衡性调整，对应之前分析的节点昵称，N、S、P、Sl、Sr</span>
<span class="keyword">private</span> <span class="literal">void</span> fixAfterDeletion(Entry&lt;K,V&gt; x) {
    <span class="keyword">while</span> (x != root <span class="subst">&amp;&amp;</span> colorOf(x) == BLACK) { <span class="comment">// N节点是黑色节点并且不是根节点就一直循环</span>
        <span class="keyword">if</span> (x == leftOf(parentOf(x))) { <span class="comment">// 如果N是P的左子节点</span>
            Entry&lt;K,V&gt; sib = rightOf(parentOf(x)); <span class="comment">// sib就是N节点的兄弟节点S</span>

            <span class="keyword">if</span> (colorOf(sib) == RED) { <span class="comment">// 如果S节点是红色节点，满足删除冲突3.2，对P节点进行左旋操作并交换P和S的颜色</span>
                <span class="comment">// 交换P和S的颜色，S原先为红色，P原先为黑色(2个红色节点不能相连)</span>
                setCol<span class="subst">or</span>(sib, BLACK); <span class="comment">// S节点从红色变成黑色</span>
                setCol<span class="subst">or</span>(parentOf(x), RED); <span class="comment">// P节点从黑色变成红色</span>
                rotateLeft(parentOf(x)); <span class="comment">// 删除冲突3.2中P节点进行左旋</span>
                sib = rightOf(parentOf(x)); <span class="comment">// 左旋之后N节点有了一个黑色的兄弟节点和红色的父亲节点，S节点重新赋值成N节点现在的兄弟节点。接下来按照删除冲突3.4、3.5、3.6处理</span>
            }

            <span class="comment">// 执行到这里S节点一定是黑色节点，如果是红色节点，会按照冲突3.2交换成黑色节点</span>
            <span class="comment">// 如果S节点的左右子节点Sl、Sr均为黑色节点并且S节点也为黑色节点</span>
            <span class="keyword">if</span> (colorOf(leftOf(sib))  == BLACK <span class="subst">&amp;&amp;</span>
                colorOf(rightOf(sib)) == BLACK) {
                <span class="comment">// 按照删除冲突3.3和3.4进行处理</span>
                <span class="comment">// 如果是冲突3.3，说明P节点也是黑色节点</span>
                <span class="comment">// 如果是冲突3.4，说明P节点是红色节点，P节点和S节点需要交换颜色</span>
                <span class="comment">// 3.3和3.4冲突的处理结果S节点都为红色节点，但是3.4冲突处理完毕之后直接结束，而3.3冲突处理完毕之后继续调整</span>
                setCol<span class="subst">or</span>(sib, RED); <span class="comment">// S节点变成红色节点，如果是3.4冲突需要交换颜色，N节点的颜色交换在跳出循环进行</span>
                x = parentOf(x); <span class="comment">// N节点重新赋值成N节点的父节点P之后继续递归处理</span>
            } <span class="keyword">else</span> { <span class="comment">// S节点的2个子节点Sl，Sr中存在红色节点</span>
                <span class="keyword">if</span> (colorOf(rightOf(sib)) == BLACK) { <span class="comment">// 如果S节点的右子节点Sr为黑色节点，Sl为红色节点[Sl如果为黑色节点的话就在上一个if逻辑里处理了]，满足删除冲突3.5</span>
                    <span class="comment">// 删除冲突3.5，对S节点做右旋操作，交换S和Sl的颜色，S变成红色节点，Sl变成黑色节点</span>
                    setCol<span class="subst">or</span>(leftOf(sib), BLACK); <span class="comment">// Sl节点变成黑色节点</span>
                    setCol<span class="subst">or</span>(sib, RED); <span class="comment">// S节点变成红色节点</span>
                    rotateRight(sib); <span class="comment">// S节点进行右旋操作</span>
                    sib = rightOf(parentOf(x)); <span class="comment">// S节点赋值现在N节点的兄弟节点</span>
                }
                <span class="comment">// 删除冲突3.5处理之后变成了删除冲突3.6或者一开始就是删除冲突3.6</span>
                <span class="comment">// 删除冲突3.6，P节点做左旋操作，P节点和S接口交换颜色，Sr节点变成黑色</span>
                setCol<span class="subst">or</span>(sib, colorOf(parentOf(x))); <span class="comment">// S节点颜色变成P节点颜色，红色</span>
                setCol<span class="subst">or</span>(parentOf(x), BLACK); <span class="comment">// P节点变成S节点颜色，也就是黑色</span>
                setCol<span class="subst">or</span>(rightOf(sib), BLACK); <span class="comment">// Sr节点变成黑色</span>
                rotateLeft(parentOf(x)); <span class="comment">// P节点做左旋操作</span>
                x = root; <span class="comment">// 准备跳出循环</span>
            }
        } <span class="keyword">else</span> { <span class="comment">// 如果N是P的右子节点，处理过程跟N是P的左子节点一样，左右对换即可</span>
            Entry&lt;K,V&gt; sib = leftOf(parentOf(x));

            <span class="keyword">if</span> (colorOf(sib) == RED) {
                setCol<span class="subst">or</span>(sib, BLACK);
                setCol<span class="subst">or</span>(parentOf(x), RED);
                rotateRight(parentOf(x));
                sib = leftOf(parentOf(x));
            }

            <span class="keyword">if</span> (colorOf(rightOf(sib)) == BLACK <span class="subst">&amp;&amp;</span>
                colorOf(leftOf(sib)) == BLACK) {
                setCol<span class="subst">or</span>(sib, RED);
                x = parentOf(x);
            } <span class="keyword">else</span> {
                <span class="keyword">if</span> (colorOf(leftOf(sib)) == BLACK) {
                    setCol<span class="subst">or</span>(rightOf(sib), BLACK);
                    setCol<span class="subst">or</span>(sib, RED);
                    rotateLeft(sib);
                    sib = leftOf(parentOf(x));
                }
                setCol<span class="subst">or</span>(sib, colorOf(parentOf(x)));
                setCol<span class="subst">or</span>(parentOf(x), BLACK);
                setCol<span class="subst">or</span>(leftOf(sib), BLACK);
                rotateRight(parentOf(x));
                x = root;
            }
        }
    }

    setCol<span class="subst">or</span>(x, BLACK); <span class="comment">// 删除冲突3.4循环调出来之后N节点颜色设置为黑色 或者 删除节点只有1个红色子节点的时候，将顶上来的红色节点设置为黑色</span>
}
</code></pre><h2 id="参考资料">参考资料</h2><p><a href="http://dongxicheng.org/structure/red-black-tree/" target="_blank" rel="external">http://dongxicheng.org/structure/red-black-tree/</a></p>
<p><a href="http://blog.csdn.net/chenssy/article/details/26668941" target="_blank" rel="external">http://blog.csdn.net/chenssy/article/details/26668941</a></p>
<p><a href="http://zh.wikipedia.org/wiki/%E7%BA%A2%E9%BB%91%E6%A0%91" target="_blank" rel="external">http://zh.wikipedia.org/wiki/%E7%BA%A2%E9%BB%91%E6%A0%91</a></p>
<p><a href="http://www.cnblogs.com/fanzhidongyzby/p/3187912.html" target="_blank" rel="external">http://www.cnblogs.com/fanzhidongyzby/p/3187912.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>TreeMap是jdk中基于红黑树的一种map实现。HashMap底层是使用链表法解决冲突的哈希表，LinkedHashMap继承自HashMap，内部同样也是使用链表法解决冲突的哈希表，但是额外添加了一个双向链表用于处理元素的插入顺序或访问访问。</p>
<p>既然TreeMap底层使用的是红黑树，首先先来简单了解一下红黑树的定义。</p>
<p>红黑树是一棵平衡二叉查找树，同时还需要满足以下5个规则：</p>
<ol>
<li>每个节点只能是红色或者黑点</li>
<li>根节点是黑点</li>
<li>叶子节点(Nil节点，空节点)是黑色节点</li>
<li>如果一个节点是红色节点，那么它的两个子节点必须是黑色节点(一条路径上不能出现相邻的两个红色节点)</li>
<li>从任一节点到其每个叶子节点的所有路径都包含相同数目的黑色节点</li>
</ol>
<p>红黑树的这些特性决定了它的查询、插入、删除操作的时间复杂度均为O(log n)。</p>]]>
    
    </summary>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="map" scheme="http://fangjian0423.github.io/tags/map/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk ArrayDeque工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/04/03/jdk_arraydeque/"/>
    <id>http://fangjian0423.github.io/2016/04/03/jdk_arraydeque/</id>
    <published>2016-04-02T16:43:59.000Z</published>
    <updated>2016-04-02T16:44:58.000Z</updated>
    <content type="html"><![CDATA[<p>ArrayDeque双向队列是jdk中列表的一种实现，支持元素在头和尾这两端进行插入和删除操作。</p>
<p>Deque接口(双向队列)的两个主要实现类是ArrayDeque和LinkedList。</p>
<p>其中ArrayDeque底层使用循环数组实现双向队列，而LinkedList是使用链接实现，之前在<a href="http://fangjian0423.github.io/2016/03/27/jdk_linkedlist/">jdk LinkedList工作原理分析</a>这篇文章中，已经分析过了LinkedList的实现原理，本文分析ArrayDeque的实现原理。</p>
<a id="more"></a>
<h2 id="一个ArrayDeque例子">一个ArrayDeque例子</h2><p>一段ArrayDeque代码：</p>
<pre><code>ArrayDeque&lt;<span class="keyword">String</span>&gt; arrayDeque = <span class="keyword">new</span> ArrayDeque&lt;<span class="keyword">String</span>&gt;(<span class="number">4</span>);
arrayDeque.<span class="built_in">add</span>(<span class="string">"1"</span>);
arrayDeque.<span class="built_in">add</span>(<span class="string">"2"</span>);
arrayDeque.<span class="built_in">add</span>(<span class="string">"3"</span>);
arrayDeque.addFirst(<span class="string">"0.5"</span>);
arrayDeque.<span class="built_in">add</span>(<span class="string">"4"</span>);
</code></pre><p>ArrayDeque内部使用的循环数组的容量，当首次进行初始化的时候，最小容量为8，如果超过8，扩大成2的幂。</p>
<pre><code><span class="comment">// 调用带有容量参数的构造函数后，数组初始化过程</span>
<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">allocateElements</span><span class="params">(<span class="keyword">int</span> numElements)</span> </span>{
    <span class="keyword">int</span> initialCapacity = MIN_INITIAL_CAPACITY; <span class="comment">// 最小容量为8</span>
    <span class="keyword">if</span> (numElements &gt;= initialCapacity) { <span class="comment">// 如果要分配的容量大于等于8，扩大成2的幂；否则使用最小容量8</span>
        initialCapacity = numElements;
        initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">1</span>);
        initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">2</span>);
        initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">4</span>);
        initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">8</span>);
        initialCapacity |= (initialCapacity &gt;&gt;&gt; <span class="number">16</span>);
        initialCapacity++;

        <span class="keyword">if</span> (initialCapacity &lt; <span class="number">0</span>)
            initialCapacity &gt;&gt;&gt;= <span class="number">1</span>;
    }
    elements = <span class="keyword">new</span> Object[initialCapacity]; <span class="comment">// 构造数组</span>
}
</code></pre><p>上面例子构造容量为4的数组，但是由于最小容量为8，所以构造的数组的容量是8。</p>
<p>执行过程如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/arraydeque01.jpg" alt=""></p>
<h2 id="ArrayDeque原理分析">ArrayDeque原理分析</h2><p>ArrayDeque使用的是循环数组，内部有3个属性，分别是：</p>
<pre><code><span class="built_in">Object</span>[] elements; <span class="comment">// 数组</span>
<span class="built_in">int</span> head; <span class="comment">// 头索引</span>
<span class="built_in">int</span> tail; <span class="comment">// 尾索引</span>
</code></pre><h3 id="add操作">add操作</h3><p>上面例子使用的add方法，其实内部使用了addLast方法，addLast也就添加数据到双向队列尾端：</p>
<pre><code><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addLast</span><span class="params">(E e)</span> </span>{
    <span class="keyword">if</span> (e == null)
        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();
    elements[tail] = e; <span class="comment">// 根据尾索引，添加到尾端</span>
    <span class="keyword">if</span> ( (tail = (tail + <span class="number">1</span>) &amp; (elements.length - <span class="number">1</span>)) == head) <span class="comment">// 尾索引+1，如果尾索引和头索引重复了，说明数组满了，进行扩容</span>
        doubleCapacity();
}
</code></pre><p>addFirst方法跟addLast方法相反，添加数据到双向队列头端：</p>
<pre><code><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addFirst</span><span class="params">(E e)</span> </span>{
    <span class="keyword">if</span> (e == null)
        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();
    elements[head = (head - <span class="number">1</span>) &amp; (elements.length - <span class="number">1</span>)] = e; <span class="comment">// 根据头索引，添加到头端，头索引-1</span>
    <span class="keyword">if</span> (head == tail) <span class="comment">// 如果头索引和尾索引重复了，说明数组满了，进行扩容</span>
        doubleCapacity();            
}
</code></pre><h3 id="remove操作">remove操作</h3><p>remove操作分别removeFirst和removeLast，removeLast代码如下：</p>
<pre><code>public E removeLast() {
    E x = pollLast(); // 调用pollLast方法
    <span class="keyword">if</span> (x == null)
        throw new <span class="type">NoSuchElementException</span>();
    <span class="keyword">return</span> x;
}

public E pollLast() {
    <span class="type">int</span> t = (tail - <span class="number">1</span>) &amp; (elements.length - <span class="number">1</span>); // 尾索引 -<span class="number">1</span>
    @<span class="type">SuppressWarnings</span>(<span class="string">"unchecked"</span>)
    E <span class="literal">result</span> = (E) elements[t]; // 根据尾索引，得到尾元素
    <span class="keyword">if</span> (<span class="literal">result</span> == null)
        <span class="keyword">return</span> null;
    elements[t] = null; // 尾元素置空
    tail = t;
    <span class="keyword">return</span> <span class="literal">result</span>;
}
</code></pre><p>removeFirst方法原理一样，remove头元素。 头索引 +1</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/arraydeque02.jpg" alt=""></p>
<h3 id="扩容">扩容</h3><p>ArrayDeque的扩容会把数组容量扩大2倍，同时还会重置头索引和尾索引，头索引置为0，尾索引置为原容量的值。</p>
<p>比如容量为8，扩容为16，头索引变成0，尾索引变成8。</p>
<p>扩容代码如下：</p>
<pre><code><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doubleCapacity</span><span class="params">()</span> </span>{
    assert head == tail;
    <span class="keyword">int</span> p = head;
    <span class="keyword">int</span> n = elements.length;
    <span class="keyword">int</span> r = n - p;
    <span class="keyword">int</span> newCapacity = n &lt;&lt; <span class="number">1</span>;
    <span class="keyword">if</span> (newCapacity &lt; <span class="number">0</span>)
        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Sorry, deque too big"</span>);
    Object[] a = <span class="keyword">new</span> Object[newCapacity];
    System.arraycopy(elements, p, a, <span class="number">0</span>, r);
    System.arraycopy(elements, <span class="number">0</span>, a, r, p);
    elements = a;
    head = <span class="number">0</span>;  <span class="comment">// 头索引重置</span>
    tail = n;  <span class="comment">// 尾索引重置</span>
}
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/arraydeque03.jpg" alt=""></p>
<h2 id="其他">其他</h2><p>Deque接口同时还附带了Stack的功能。</p>
<pre><code>ArrayDeque&lt;<span class="built_in">String</span>&gt; <span class="built_in">stack</span> = <span class="literal">new</span> ArrayDeque&lt;<span class="built_in">String</span>&gt;(<span class="number">4</span>);
<span class="built_in">stack</span><span class="built_in">.</span>push(<span class="string">"1"</span>);
<span class="built_in">stack</span><span class="built_in">.</span>push(<span class="string">"2"</span>);
<span class="built_in">stack</span><span class="built_in">.</span>push(<span class="string">"3"</span>);
<span class="built_in">String</span> pop = <span class="built_in">stack</span><span class="built_in">.</span>pop(); <span class="comment">// 3</span>
</code></pre><p>push方法内部调用addFirst方法，pop方法内部调用removeFirst方法。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/arraydeque04.jpg" alt=""></p>
<h2 id="注意点">注意点</h2><ol>
<li>ArrayDeque是一个使用循环数组实现的双向队列，LinkedList也是一个双向队列，不过它的底层实现是使用链表</li>
<li>ArrayDeque的扩容会把数组容量扩大2倍，同时还会重置头索引和尾索引</li>
<li>Deque双向队列接口同时也实现了Stack接口，可以把Deque当成Stack使用，它的速度比java.util.Stack要快，因为Stack底层操作数据会加锁，而Deque不会加锁</li>
<li>ArrayDeque不是一个线程安全的类</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>ArrayDeque双向队列是jdk中列表的一种实现，支持元素在头和尾这两端进行插入和删除操作。</p>
<p>Deque接口(双向队列)的两个主要实现类是ArrayDeque和LinkedList。</p>
<p>其中ArrayDeque底层使用循环数组实现双向队列，而LinkedList是使用链接实现，之前在<a href="http://fangjian0423.github.io/2016/03/27/jdk_linkedlist/">jdk LinkedList工作原理分析</a>这篇文章中，已经分析过了LinkedList的实现原理，本文分析ArrayDeque的实现原理。</p>]]>
    
    </summary>
    
      <category term="collection" scheme="http://fangjian0423.github.io/tags/collection/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk HashSet, LinkedHashSet工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/30/jdk_hashset_linkedhashset/"/>
    <id>http://fangjian0423.github.io/2016/03/30/jdk_hashset_linkedhashset/</id>
    <published>2016-03-29T16:46:33.000Z</published>
    <updated>2016-03-29T16:46:28.000Z</updated>
    <content type="html"><![CDATA[<p>Set是一个没有包括重复数据的集合，跟List一样，他们都继承自Collection。</p>
<p>Java中的Set接口最主要的实现类就是HashSet和LinkedHashSet。</p>
<a id="more"></a>
<h2 id="HashSet原理分析">HashSet原理分析</h2><p>首先看下HashSet的属性。</p>
<p>HashSet内部有个HashMap属性和一个对象属性：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">HashMap</span>&lt;E,<span class="keyword">Object</span>&gt; <span class="built_in">map</span>;

<span class="comment">// HashSet内部使用HashMap进行处理，由于Set只需要键值对中的键，而不需要值，所有的值都用这个对象</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">Object</span> PRESENT = <span class="keyword">new</span> <span class="keyword">Object</span>();
</code></pre><p>HashSet的构造函数中也提供了HashMap的capacity，loadFactor这些参数。</p>
<h3 id="add方法">add方法</h3><p>调用HashMap的put操作完成Set的add操作。</p>
<pre><code><span class="keyword">public</span> <span class="built_in">boolean</span> <span class="built_in">add</span>(E e) {
    <span class="keyword">return</span> <span class="built_in">map</span>.put(e, PRESENT)==<span class="keyword">null</span>;  <span class="comment">// HashMap put成功返回true，否则false</span>
}
</code></pre><p>HashMap相关的put操作在之前的博客中已经介绍过了，这里就不分析了。</p>
<h3 id="boolean_remove(Object_o)">boolean remove(Object o)</h3><p>调用HashMap的remove操作完成。</p>
<pre><code><span class="keyword">public</span> <span class="built_in">boolean</span> remove(<span class="keyword">Object</span> o) {
    <span class="keyword">return</span> <span class="built_in">map</span>.remove(o)==PRESENT; <span class="comment">// 对应的节点移除成功返回true，否则false</span>
}
</code></pre><h3 id="一个HashSet例子">一个HashSet例子</h3><pre><code>Set&lt;<span class="keyword">String</span>&gt; <span class="built_in">set</span> = <span class="keyword">new</span> HashSet&lt;<span class="keyword">String</span>&gt;(<span class="number">5</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"java"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"golang"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"python"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"ruby"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"scala"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"c"</span>);

<span class="keyword">for</span>(<span class="keyword">String</span> <span class="built_in">str</span> : <span class="built_in">set</span>) {
    System.out.<span class="built_in">println</span>(<span class="built_in">str</span>);
}
</code></pre><p>这个例子中set中的HashMap内部结构如下图所示：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap03.jpg" alt=""></p>
<h3 id="HashSet总结">HashSet总结</h3><ol>
<li>HashSet内部使用HashMap，HashSet集合内部所有的操作基本上都是基于HashMap完成的</li>
<li>HashSet中的元素是无序的，这是因为它内部使用HashMap进行存储，而HashMap添加键值对的时候是根据hash函数得到数组的下标的</li>
</ol>
<h2 id="LinkedHashSet原理分析">LinkedHashSet原理分析</h2><p>LinkedHashSet继承自HashSet，它的构造函数会调用父类HashSet的构造函数：</p>
<pre><code><span class="function"><span class="keyword">public</span> <span class="title">LinkedHashSet</span><span class="params">(<span class="keyword">int</span> initialCapacity, <span class="keyword">float</span> loadFactor)</span> </span>{
    <span class="keyword">super</span>(initialCapacity, loadFactor, <span class="keyword">true</span>);
}

HashSet(<span class="keyword">int</span> initialCapacity, <span class="keyword">float</span> loadFactor, <span class="keyword">boolean</span> dummy) {
    <span class="comment">// map使用LinkedHashMap构造，LinkedHashMap是HashMap的子类，accessOrder为false，即使用插入顺序</span>
    map = <span class="keyword">new</span> LinkedHashMap&lt;&gt;(initialCapacity, loadFactor);
}
</code></pre><h3 id="一个LinkedHashSet例子">一个LinkedHashSet例子</h3><pre><code>Set&lt;<span class="keyword">String</span>&gt; <span class="built_in">set</span> = <span class="keyword">new</span> LinkedHashSet&lt;<span class="keyword">String</span>&gt;(<span class="number">5</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"java"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"golang"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"python"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"ruby"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"scala"</span>);
<span class="keyword">for</span>(<span class="keyword">String</span> <span class="built_in">str</span> : <span class="built_in">set</span>) {
    System.out.<span class="built_in">println</span>(<span class="built_in">str</span>);
}
</code></pre><p>这个例子中set中的LinkedHashMap内部结构如下图所示：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedhashmap02.jpg" alt=""></p>
<h3 id="LinkedHashSet总结">LinkedHashSet总结</h3><ol>
<li>LinkedHashSet继自HashSet，但是内部的map是使用LinkedHashMap构造的，并且accessOrder为false，使用查询顺序。所以LinkedHashSet遍历的顺序就是插入顺序。</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>Set是一个没有包括重复数据的集合，跟List一样，他们都继承自Collection。</p>
<p>Java中的Set接口最主要的实现类就是HashSet和LinkedHashSet。</p>]]>
    
    </summary>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="set" scheme="http://fangjian0423.github.io/tags/set/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk LinkedHashMap工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/29/jdk_linkedhashmap/"/>
    <id>http://fangjian0423.github.io/2016/03/29/jdk_linkedhashmap/</id>
    <published>2016-03-29T11:23:23.000Z</published>
    <updated>2016-03-29T16:35:14.000Z</updated>
    <content type="html"><![CDATA[<p>LinkedHashMap是一种会记录插入顺序的Map，内部维护着一个accessOrder属性，用于表示map数据的迭代顺序是基于访问顺序还是插入顺序。</p>
<a id="more"></a>
<h2 id="LinkedHashMap原理分析">LinkedHashMap原理分析</h2><p>首先是LinkedHashMap的定义：</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LinkedHashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;
    <span class="keyword">extends</span> <span class="title">HashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;
        <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;</span>
</code></pre><p>LinkedHashMap继承HashMap，实现Map接口，所以它的结构跟HashMap是一样的，使用链表法解决哈希冲突的哈希表，基本操作跟HashMap也是一样的，就是多了一点额外的步骤用于处理链表。</p>
<p>LinkedHashMap有个内部类Entry，这个Entry就是链表中的节点，继承自HashMap.Node，多出了2个属性before和after，所以LinkedHashMap内部链表的节点是双向的，代码如下：</p>
<pre><code>static <span class="class"><span class="keyword">class</span> <span class="title">Entry&lt;K</span>,<span class="title">V&gt;</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">HashMap</span>.<span class="title">Node&lt;K</span>,<span class="title">V&gt;</span> {</span>
    <span class="type">Entry</span>&lt;<span class="type">K</span>,<span class="type">V</span>&gt; before, after;
    <span class="type">Entry</span>(int hash, <span class="type">K</span> key, <span class="type">V</span> value, <span class="type">Node</span>&lt;<span class="type">K</span>,<span class="type">V</span>&gt; next) {
        <span class="keyword">super</span>(hash, key, value, next);
    }
}
</code></pre><p>另外LinkedHashMap还有两个重要的属性head，tail，这2个属性用于存储插入的节点，形成一个双向链表：</p>
<pre><code><span class="comment">// 首节点</span>
<span class="keyword">transient</span> LinkedHashMap.Entry&lt;K,V&gt; head;

<span class="comment">// 尾节点</span>
<span class="keyword">transient</span> LinkedHashMap.Entry&lt;K,V&gt; tail;
</code></pre><p>跟HashMap一样，下面这个例子对应的LinkedHashMap结构图示如下所示，accessOrder为false，使用插入顺序：</p>
<pre><code>Map&lt;String, Integer&gt; <span class="built_in">map</span> = <span class="keyword">new</span> LinkedHashMap&lt;String, Integer&gt;(<span class="number">5</span>);
<span class="built_in">map</span>.put(<span class="string">"java"</span>, <span class="number">1</span>);
<span class="built_in">map</span>.put(<span class="string">"golang"</span>, <span class="number">2</span>);
<span class="built_in">map</span>.put(<span class="string">"python"</span>, <span class="number">3</span>);
<span class="built_in">map</span>.put(<span class="string">"ruby"</span>, <span class="number">4</span>);
<span class="built_in">map</span>.put(<span class="string">"scala"</span>, <span class="number">5</span>);
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedhashmap02.jpg" alt=""></p>
<h3 id="put操作">put操作</h3><p>LinkedHashMap没有覆盖HashMap的put方法，所以put操作跟HashMap是一样的。但是它覆盖了newNode方法，也就是说构造新节点的时候，LinkedHashMap跟HashMap是不一样的：</p>
<pre><code><span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;K</span>,V&gt; newNode(int hash, K key, V value, <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;K</span>,V&gt; e) {
    // 使用Entry双向链表构造节点，而不是HashMap的<span class="keyword">Node</span><span class="identifier"></span><span class="title">单向链表
    LinkedHashMap</span>.Entry<span class="tag">&lt;K,V&gt;</span> p =
        new LinkedHashMap.Entry<span class="tag">&lt;K,V&gt;</span>(hash, key, value, e);
    linkNodeLast(p); // 更新双向链表，这一操作在HashMap里面是没有的
    return p;
}
</code></pre><p>另外，LinkedHashMap重写了afterNodeInsertion这个钩子方法，在put一个关键字不存在的节点之后会调用这个方法：</p>
<pre><code><span class="keyword">void</span> afterNodeInsertion(<span class="built_in">boolean</span> evict) { <span class="comment">// possibly remove eldest</span>
    LinkedHashMap.Entry&lt;K,V&gt; first;
    <span class="comment">// removeEldestEntry方法LinkedHashMap永远返回false，一些使用缓存策略的Map会覆盖这个方法，比如jackson的LRUMap，会移除最老的节点，也就是首节点</span>
    <span class="keyword">if</span> (evict &amp;&amp; (first = head) != <span class="keyword">null</span> &amp;&amp; removeEldestEntry(first)) {
        K <span class="variable">key</span> = first.<span class="variable">key</span>;
        removeNode(hash(<span class="variable">key</span>), <span class="variable">key</span>, <span class="keyword">null</span>, <span class="keyword">false</span>, <span class="keyword">true</span>);
    }
}
</code></pre><p>put操作如果关键字已经存在，会调用afterNodeAccess这个钩子方法：</p>
<pre><code><span class="label">void</span> afterNodeAccess(Node&lt;K,V&gt; e) { // <span class="keyword">move </span>node to last
    LinkedHashMap.Entry&lt;K,V&gt; last<span class="comment">;</span>
    <span class="preprocessor">if</span> (accessOrder &amp;&amp; (last = tail) != e) { // 如果使用访问顺序并且访问的不是尾节点
        LinkedHashMap.Entry&lt;K,V&gt; p =
            (LinkedHashMap.Entry&lt;K,V&gt;)e, <span class="keyword">b </span>= p.<span class="keyword">before, </span>a = p.after<span class="comment">;</span>
        p.after = null<span class="comment">;</span>
        <span class="preprocessor">if</span> (<span class="keyword">b </span>== null)
            head = a<span class="comment">;</span>
        <span class="preprocessor">else</span>
            <span class="keyword">b.after </span>= a<span class="comment">;</span>
        <span class="preprocessor">if</span> (a != null)
            a.<span class="keyword">before </span>= <span class="keyword">b;
</span>        <span class="preprocessor">else</span>
            last = <span class="keyword">b;
</span>        <span class="preprocessor">if</span> (last == null)
            head = p<span class="comment">;</span>
        <span class="preprocessor">else</span> {
            p.<span class="keyword">before </span>= last<span class="comment">;</span>
            last.after = p<span class="comment">;</span>
        }
        tail = p<span class="comment">;</span>
        ++modCount<span class="comment">;</span>
    }
}
</code></pre><h3 id="get操作">get操作</h3><p>LinkedHashMap复写了get方法：</p>
<pre><code><span class="keyword">public</span> V <span class="built_in">get</span>(<span class="keyword">Object</span> <span class="variable">key</span>) {
    Node&lt;K,V&gt; e;
    <span class="keyword">if</span> ((e = getNode(hash(<span class="variable">key</span>), <span class="variable">key</span>)) == <span class="keyword">null</span>)
        <span class="keyword">return</span> <span class="keyword">null</span>;
    <span class="keyword">if</span> (accessOrder) <span class="comment">// 使用访问顺序的话，调用afterNodeAccess方法</span>
        afterNodeAccess(e);
    <span class="keyword">return</span> e.value;
}
</code></pre><h3 id="remove操作">remove操作</h3><p>LinkedHashMap的remove方法没有复写HashMap的remove方法，但是同样实现了afterNodeRemoval这个钩子方法：</p>
<pre><code>// 更新双向链表
<span class="label">void</span> afterNodeRemoval(Node&lt;K,V&gt; e) { // unlink
    LinkedHashMap.Entry&lt;K,V&gt; p =
        (LinkedHashMap.Entry&lt;K,V&gt;)e, <span class="keyword">b </span>= p.<span class="keyword">before, </span>a = p.after<span class="comment">;</span>
    p.<span class="keyword">before </span>= p.after = null<span class="comment">;</span>
    <span class="preprocessor">if</span> (<span class="keyword">b </span>== null)
        head = a<span class="comment">;</span>
    <span class="preprocessor">else</span>
        <span class="keyword">b.after </span>= a<span class="comment">;</span>
    <span class="preprocessor">if</span> (a == null)
        tail = <span class="keyword">b;
</span>    <span class="preprocessor">else</span>
        a.<span class="keyword">before </span>= <span class="keyword">b;
</span>}
</code></pre><h3 id="accessOrder属性分析">accessOrder属性分析</h3><p>LinkedHashMap默认情况下，accessOrder属性为false，也就是使用插入顺序，这个插入顺序是根据LinkedHashMap内部的一个双向链表实现的。如果accessOrder为true，也就是使用访问顺序，那么afterNodeAccess这个钩子方法内部的逻辑会被执行，将会修改双向链表的结构，再来看一下这个方法的具体逻辑：</p>
<pre><code><span class="function"><span class="keyword">void</span> <span class="title">afterNodeAccess</span><span class="params">(Node&lt;K,V&gt; e)</span> </span>{ <span class="comment">// move node to last</span>
    LinkedHashMap.Entry&lt;K,V&gt; last;
    <span class="keyword">if</span> (accessOrder &amp;&amp; (last = tail) != e) { <span class="comment">// 使用访问顺序，把节点移动到双向链表的最后面，如果已经在最后面了，不需要进行移动</span>
        LinkedHashMap.Entry&lt;K,V&gt; p =
            (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after;
        p.after = <span class="keyword">null</span>;
        <span class="keyword">if</span> (b == <span class="keyword">null</span>)
            head = a; <span class="comment">// 特殊情况，处理头节点</span>
        <span class="keyword">else</span>
            b.after = a; <span class="comment">// 节点处理</span>
        <span class="keyword">if</span> (a != <span class="keyword">null</span>)
            a.before = b; <span class="comment">// 节点处理</span>
        <span class="keyword">else</span>
            last = b; <span class="comment">// 特殊情况，处理尾节点</span>
        <span class="keyword">if</span> (last == <span class="keyword">null</span>)
            head = p;
        <span class="keyword">else</span> {
            p.before = last; <span class="comment">// 尾节点处理</span>
            last.after = p;
        }
        tail = p;
        ++modCount;
    }
}
</code></pre><p>afterNodeAccess在使用get方法或者put方法遇到关键字已经存在的情况下，会被触发，一个例子如下：</p>
<pre><code>Map&lt;String, Integer&gt; <span class="built_in">map</span> = <span class="keyword">new</span> LinkedHashMap&lt;String, Integer&gt;(<span class="number">5</span>, <span class="number">0.75f</span>, <span class="literal">true</span>);
<span class="built_in">map</span>.put(<span class="string">"java"</span>, <span class="number">1</span>);
<span class="built_in">map</span>.put(<span class="string">"golang"</span>, <span class="number">2</span>);
<span class="built_in">map</span>.put(<span class="string">"python"</span>, <span class="number">3</span>);
<span class="built_in">map</span>.put(<span class="string">"ruby"</span>, <span class="number">4</span>);
<span class="built_in">map</span>.put(<span class="string">"scala"</span>, <span class="number">5</span>);
System.out.println(<span class="built_in">map</span>.get(<span class="string">"ruby"</span>));
</code></pre><p>上面这段代码，LinkedHashMap的accessOrder属性为true，使用访问顺序，最后调用了get方法，触发afterNodeAccess方法，修改双向链表，效果如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedhashmap03.jpg" alt=""></p>
<h2 id="注意点">注意点</h2><p>LinkedHashMap使用访问顺序并且进行遍历的时候，如果使用如下代码，会发生ConcurrentModificationException异常：</p>
<pre><code><span class="keyword">for</span>(<span class="keyword">String</span> <span class="built_in">str</span> : <span class="built_in">map</span>.keySet()) {
    System.out.<span class="built_in">println</span>(<span class="built_in">map</span>.<span class="built_in">get</span>(<span class="built_in">str</span>));
}
</code></pre><p>不应该这么使用，而是应该直接读取value：</p>
<pre><code><span class="function">for</span>(Integer it <span class="value">: map.<span class="function">values</span>()) {
    System.out.<span class="function">println</span>(it);</span>
}
</code></pre><p>具体可以参考<a href="http://stackoverflow.com/questions/16180568/concurrentmodificationexception-with-linkedhashmap" target="_blank" rel="external">stackoverflow上的这篇帖子</a>。</p>
<h2 id="总结">总结</h2><ol>
<li><p>LinkedHashMap也是一种使用拉链式哈希表的数据结构，除了哈希表，它内部还维护着一个双向链表，用于处理访问顺序和插入顺序的问题</p>
</li>
<li><p>LinkedHashMap继承自HashMap，大多数的方法都是跟HashMap一样的，不过覆盖了一些方法</p>
</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>LinkedHashMap是一种会记录插入顺序的Map，内部维护着一个accessOrder属性，用于表示map数据的迭代顺序是基于访问顺序还是插入顺序。</p>]]>
    
    </summary>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="map" scheme="http://fangjian0423.github.io/tags/map/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk HashMap工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/29/jdk_hashmap/"/>
    <id>http://fangjian0423.github.io/2016/03/29/jdk_hashmap/</id>
    <published>2016-03-28T17:49:58.000Z</published>
    <updated>2016-03-29T16:34:57.000Z</updated>
    <content type="html"><![CDATA[<p>Map是一个映射键和值的对象。类似于Python中的字典。</p>
<p>HashMap为什么会出现呢?</p>
<p>因为数组这种数据结构，虽然遍历简单，但是插入和删除操作复杂，需要移动数组内部的元素；链表这种数据结构，插入和删除操作简单，但是查找复杂，只能一个一个地遍历。</p>
<p>有没有一种新的数据结构，插入数据简单，同时查找也简单？ 这个时候就出现了哈希表这种数据结构。 这是一种折中的方式，插入没链表快，查询没数组快。</p>
<p>wiki上就是这么定义哈希表的：</p>
<p>散列表（Hash table，也叫哈希表），是根据关键字（Key value）而直接访问在内存存储位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录，这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做散列表。</p>
<a id="more"></a>
<p>有几个概念要解释一下：</p>
<ol>
<li>如果有1个关键字为k，它是通过一种函数f(k)得到散列表的地址，然后把值放到这个地址上。这个函数f就称为散列函数，也叫哈希函数。</li>
<li>对于不同的关键字，得到了同一地址，即k1 != k2，但是f(k1) = f(k2)。这种现象称为冲突，</li>
<li>若对于关键字集合中的任一个关键字，经散列函数映象到地址集合中任何一个地址的概率是相等的，则称此类散列函数为均匀散列函数</li>
</ol>
<p>散列函数有好几种实现，分别有直接定址法、随机数法、除留余数法等，在<a href="https://zh.wikipedia.org/wiki/%E5%93%88%E5%B8%8C%E8%A1%A8" target="_blank" rel="external">wiki散列表</a>上都有介绍。</p>
<p>散列表的冲突解决方法，也有好几种，有开放定址法、单独链表法、再散列等。</p>
<p>Java中的HashMap采用的冲突解决方法是使用单独链表法，如下图所示：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap01.png" alt=""></p>
<h2 id="HashMap原理分析">HashMap原理分析</h2><p>HashMap是jdk中Map接口的实现类之一，是一个散列表的实现。</p>
<p>HashMap中的key和value都可以为null，且它的方法都没有synchronized。 其他方法的实现大部分跟HashTable一致。HashTable的相关源码不在这里介绍，基本上跟HashTable一致。</p>
<p>HashMap有个内部静态类Node，这个Node就是为了解决冲突而设计的链表中的节点的概念。它有4个属性，hash表示哈希地址，key表示关键字，value表示值, next表示这个节点的下一个节点，是一个单项链表：</p>
<pre><code>static class <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;K</span>,V&gt; implements Map.Entry<span class="tag">&lt;K,V&gt;</span> {
    final int hash;
    final K key;
    V value;
    <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;K</span>,V&gt; next;

    <span class="keyword">Node</span><span class="identifier"></span><span class="title">(int</span> hash, K key, V value, <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;K</span>,V&gt; next) {
        this.hash = hash;
        this.key = key;
        this.value = value;
        this.next = next;
    }

    ...
}
</code></pre><h3 id="在分析HashMap源码之前，先看一个HashMap使用例子">在分析HashMap源码之前，先看一个HashMap使用例子</h3><pre><code>Map&lt;String, Integer&gt; <span class="built_in">map</span> = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;(<span class="number">5</span>);
<span class="built_in">map</span>.put(<span class="string">"java"</span>, <span class="number">1</span>);
<span class="built_in">map</span>.put(<span class="string">"golang"</span>, <span class="number">2</span>);
<span class="built_in">map</span>.put(<span class="string">"python"</span>, <span class="number">3</span>);
<span class="built_in">map</span>.put(<span class="string">"ruby"</span>, <span class="number">4</span>);
<span class="built_in">map</span>.put(<span class="string">"scala"</span>, <span class="number">5</span>);
</code></pre><p>上面这段代码执行之后会生成下面这张哈希表。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap05.jpg" alt=""></p>
<p>至于为什么会生成这样的哈希表，会在后面分析源码中讲解。</p>
<h3 id="HashMap的属性">HashMap的属性</h3><p>HashMap的几个重要的属性:</p>
<pre><code><span class="keyword">transient</span> Node&lt;K,V&gt;[] table; <span class="comment">// 哈希表数组</span>

<span class="keyword">transient</span> <span class="keyword">int</span> <span class="keyword">size</span>; <span class="comment">// 键值对个数</span>

<span class="keyword">int</span> threshold; <span class="comment">// 阀值。 值 = 容量 * 加载因子。默认值为12(16(默认容量) * 0.75(默认加载因子))。当哈希表中的键值对个数超过该值时，会进行扩容</span>

<span class="keyword">final</span> <span class="keyword">float</span> loadFactor; <span class="comment">// 加载因子，默认是0.75</span>
</code></pre><p>有2个重要的特性影响着HashMap的性能，分别是capacity(容量)和load factor(加载因子)。</p>
<p>其中capacity表示哈希表bucket的数量，HashMap的默认值是16。load factor加载因子表示当一个map填满了达到这个比例之后的bucket时候，和ArrayList一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程也叫做重哈希。默认的load factor为0.75 。</p>
<h3 id="HashMap的操作">HashMap的操作</h3><p>分析一下HashMapput键值对的过程，是如何找到bucket的，遇到哈希冲突的时候是如何使用链表法的。</p>
<h4 id="put操作">put操作</h4><pre><code><span class="keyword">public</span> V put(K <span class="variable">key</span>, V value) {
    <span class="comment">// 第一个参数就是关键字key的哈希值</span>
    <span class="keyword">return</span> putVal(hash(<span class="variable">key</span>), <span class="variable">key</span>, value, <span class="keyword">false</span>, <span class="keyword">true</span>);
}

<span class="keyword">final</span> V putVal(<span class="built_in">int</span> hash, K <span class="variable">key</span>, V value, <span class="built_in">boolean</span> onlyIfAbsent,
               <span class="built_in">boolean</span> evict) {
    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="built_in">int</span> n, i;
    <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)
        n = (tab = resize()).length; <span class="comment">// 哈希表是空的话，重新构建，进行扩容</span>
    <span class="keyword">if</span> ((p = tab[i = (n - <span class="number">1</span>) &amp; hash]) == <span class="keyword">null</span>)
        tab[i] = newNode(hash, <span class="variable">key</span>, value, <span class="keyword">null</span>); <span class="comment">// 没有hash冲突的话，直接在对应位置上构造一个新的节点即可</span>
    <span class="keyword">else</span> { <span class="comment">// 如果哈希表当前位置上已经有节点的话，说明有hash冲突</span>
        Node&lt;K,V&gt; e; K k;
        <span class="comment">// 关键字跟哈希表上的首个节点济宁比较</span>
        <span class="keyword">if</span> (p.hash == hash &amp;&amp;
            ((k = p.<span class="variable">key</span>) == <span class="variable">key</span> || (<span class="variable">key</span> != <span class="keyword">null</span> &amp;&amp; <span class="variable">key</span>.equals(k))))
            e = p;
        <span class="comment">// 如果使用的是红黑树，用红黑树的方式进行处理</span>
        <span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)
            e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, <span class="variable">key</span>, value);
        <span class="keyword">else</span> { <span class="comment">// 跟链表进行比较</span>
            <span class="keyword">for</span> (<span class="built_in">int</span> binCount = <span class="number">0</span>; ; ++binCount) {
                <span class="keyword">if</span> ((e = p.next) == <span class="keyword">null</span>) { <span class="comment">// 一直遍历链表，直到找到最后一个</span>
                    p.next = newNode(hash, <span class="variable">key</span>, value, <span class="keyword">null</span>); <span class="comment">// 构造链表上的新节点</span>
                    <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>) <span class="comment">// -1 for 1st</span>
                        treeifyBin(tab, hash);
                    <span class="keyword">break</span>;
                }
                <span class="keyword">if</span> (e.hash == hash &amp;&amp;
                    ((k = e.<span class="variable">key</span>) == <span class="variable">key</span> || (<span class="variable">key</span> != <span class="keyword">null</span> &amp;&amp; <span class="variable">key</span>.equals(k))))
                    <span class="keyword">break</span>;
                p = e;
            }
        }
        <span class="keyword">if</span> (e != <span class="keyword">null</span>) { <span class="comment">// 如果找到了节点，说明关键字相同，进行覆盖操作，直接返回旧的关键字的值</span>
            V oldValue = e.value;
            <span class="keyword">if</span> (!onlyIfAbsent || oldValue == <span class="keyword">null</span>)
                e.value = value;
            afterNodeAccess(e);
            <span class="keyword">return</span> oldValue;
        }
    }
    ++modCount;
    <span class="keyword">if</span> (++<span class="built_in">size</span> &gt; threshold) <span class="comment">// 如果目前键值对个数已经超过阀值，重新构建</span>
        resize();
    afterNodeInsertion(evict); <span class="comment">// 节点插入以后的钩子方法</span>
    <span class="keyword">return</span> <span class="keyword">null</span>;
}
</code></pre><h4 id="get操作">get操作</h4><p>get操作关键点就是怎么在哈希表上取数据，理解了put操作之后，get方法很容易理解了：</p>
<pre><code><span class="keyword">public</span> V <span class="built_in">get</span>(<span class="keyword">Object</span> <span class="variable">key</span>) {
    Node&lt;K,V&gt; e;
    <span class="keyword">return</span> (e = getNode(hash(<span class="variable">key</span>), <span class="variable">key</span>)) == <span class="keyword">null</span> ? <span class="keyword">null</span> : e.value;
}
</code></pre><p>getNode方法就说明了如何取数据：</p>
<pre><code><span class="keyword">final</span> Node&lt;K,V&gt; getNode(<span class="built_in">int</span> hash, <span class="keyword">Object</span> <span class="variable">key</span>) {
    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; <span class="built_in">int</span> n; K k;
    <span class="keyword">if</span> ((tab = table) != <span class="keyword">null</span> &amp;&amp; (n = tab.length) &gt; <span class="number">0</span> &amp;&amp;
        (first = tab[(n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) { <span class="comment">// 如果哈希表容量为0或者关键字没有命中，直接返回null</span>
        <span class="keyword">if</span> (first.hash == hash &amp;&amp;  <span class="comment">// 关键字命中的话比较第一个节点</span>
            ((k = first.<span class="variable">key</span>) == <span class="variable">key</span> || (<span class="variable">key</span> != <span class="keyword">null</span> &amp;&amp; <span class="variable">key</span>.equals(k)))) 
            <span class="keyword">return</span> first;
        <span class="keyword">if</span> ((e = first.next) != <span class="keyword">null</span>) {
            <span class="keyword">if</span> (first <span class="keyword">instanceof</span> TreeNode) <span class="comment">// 以红黑树的方式查找</span>
                <span class="keyword">return</span> ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, <span class="variable">key</span>);
            do { <span class="comment">// 遍历链表查找</span>
                <span class="keyword">if</span> (e.hash == hash &amp;&amp;
                    ((k = e.<span class="variable">key</span>) == <span class="variable">key</span> || (<span class="variable">key</span> != <span class="keyword">null</span> &amp;&amp; <span class="variable">key</span>.equals(k))))
                    <span class="keyword">return</span> e;
            } <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>);
        }
    }
    <span class="keyword">return</span> <span class="keyword">null</span>;
}
</code></pre><h4 id="hash过程和resize过程分析">hash过程和resize过程分析</h4><p>hash过程在HashMap里就是一个hash方法：</p>
<pre><code><span class="keyword">static</span> <span class="keyword">final</span> <span class="built_in">int</span> hash(<span class="keyword">Object</span> <span class="variable">key</span>) {
    <span class="built_in">int</span> h;
    <span class="comment">// 使用hashCode的值和hashCode的值无符号右移16位做异或操作</span>
    <span class="keyword">return</span> (<span class="variable">key</span> == <span class="keyword">null</span>) ? <span class="number">0</span> : (h = <span class="variable">key</span>.hashCode()) ^ (h &gt;&gt;&gt; <span class="number">16</span>);
}
</code></pre><p>这段代码是什么意思呢？ 我们以文中的那个demo为例，说明”java”这个关键字是如何找到对应bucket的过程。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap06.jpg" alt=""></p>
<p>从上图可以看到，hash方法得到的hash值是根据关键字的hashCode的高16位和低16位进行异或操作得到的一个值。</p>
<p>这个值再与哈希表容量-1值进行与操作得到最终的bucket索引值。</p>
<pre><code><span class="list">(<span class="keyword">n</span> - <span class="number">1</span>)</span> &amp; hash
</code></pre><p>hashCode的高16位与低16位进行异或操作主要是设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)来做的。</p>
<p>如果链表的数量大了，HashMap会把哈希表转换成红黑树来进行处理，本文不讨论这部分内容。</p>
<p>现在回过头来看例子，为什么初始化了一个容量为5的HashMap，但是哈希表的容量为8，而且阀值为6？</p>
<p>因为HashMap的构造函数初始化threshold的时候调用了tableSizeFor方法，这个方法会把容量改成2的幂的整数，主要是为了哈希表散列更均匀。</p>
<pre><code><span class="comment">// 定位bucket索引的最后操作。如果n为奇数，n-1就是偶数，偶数的话转成二进制最后一位是0，相反如果是奇数，最后一位是1，这样产生的索引值将更均匀</span>
(n - <span class="number">1</span>) &amp; hash
</code></pre><p>tableSizeFor方法如下：</p>
<pre><code>this.threshold = tableSizeFor(initialCapacity);

<span class="comment">// 保证thresold为2的幂</span>
static final int tableSizeFor(int <span class="keyword">cap</span>) {
    int <span class="keyword">n</span> = <span class="keyword">cap</span> - 1;
    <span class="keyword">n</span> |= <span class="keyword">n</span> &gt;&gt;&gt; 1;
    <span class="keyword">n</span> |= <span class="keyword">n</span> &gt;&gt;&gt; 2;
    <span class="keyword">n</span> |= <span class="keyword">n</span> &gt;&gt;&gt; 4;
    <span class="keyword">n</span> |= <span class="keyword">n</span> &gt;&gt;&gt; 8;
    <span class="keyword">n</span> |= <span class="keyword">n</span> &gt;&gt;&gt; 16;
    <span class="keyword">return</span> (<span class="keyword">n</span> &lt; 0) ? 1 : (<span class="keyword">n</span> &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : <span class="keyword">n</span> + 1;
}
</code></pre><p>阀值为6是因为之后进行resize操作的时候更新了阀值</p>
<pre><code>阀值 = 容量 * 加载因子 = <span class="number">8</span> * <span class="number">0.75</span> = <span class="number">6</span>
</code></pre><p>HashMap的扩容会把原先哈希表的容量扩大两倍。扩大之后，会对节点重新进行处理。</p>
<p>哈希表上的节点的状态有3种，分别是单节点，无节点，链表，扩容对于这3种状态的处理方式如下：</p>
<p>以8节点为原先容量，扩容为16容量讲解。</p>
<ol>
<li>单节点：由于容量扩大两倍，相当于左移1位。扩容前与00000111[7，n - 1 = 8 - 1]进行与操作。扩容后与00001111[15, n - 1 = 16 - 1]进行与操作。所以最终的结果要是还是在原位置，要么在原位置 +8(+old capacity) 位置</li>
<li>无节点：不处理</li>
<li>链表：遍历各个节点，每个节点的处理方式跟单节点一样，结果分成2种，还在原位置和原位置 +8 位置</li>
</ol>
<p>单节点处理示意图如下，这么设计的原因就是不需要再次计算hash值，只需要移动位置(+old capacity)即可：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap07.jpg" alt="">   </p>
<p>下图是一个HashMap扩容之后的效果图（省去了索引为7橙色链表的虚线，太多线条了）：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap08.jpg" alt="">   </p>
<p>哈希表扩容是使用resize方法完成：</p>
<pre><code><span class="keyword">final</span> Node&lt;K,V&gt;[] resize() {
    Node&lt;K,V&gt;[] oldTab = table;
    <span class="keyword">int</span> oldCap = (oldTab == <span class="keyword">null</span>) ? <span class="number">0</span> : oldTab.length;
    <span class="keyword">int</span> oldThr = threshold;
    <span class="keyword">int</span> newCap, newThr = <span class="number">0</span>;
    <span class="keyword">if</span> (oldCap &gt; <span class="number">0</span>) { <span class="comment">// 如果老容量大于0，说明哈希表中已经有数据了，然后进行扩容</span>
        <span class="keyword">if</span> (oldCap &gt;= MAXIMUM_CAPACITY) { <span class="comment">// 超过最大容量的话，不扩容</span>
            threshold = Integer.MAX_VALUE;
            <span class="keyword">return</span> oldTab;
        }
        <span class="keyword">else</span> <span class="keyword">if</span> ((newCap = oldCap &lt;&lt; <span class="number">1</span>) &lt; MAXIMUM_CAPACITY &amp;&amp; <span class="comment">// 容量加倍</span>
                 oldCap &gt;= DEFAULT_INITIAL_CAPACITY) <span class="comment">// 如果老的容量超过默认容量的话</span>
            newThr = oldThr &lt;&lt; <span class="number">1</span>; <span class="comment">// 阀值加倍</span>
    }
    <span class="keyword">else</span> <span class="keyword">if</span> (oldThr &gt; <span class="number">0</span>) <span class="comment">// 根据thresold初始化数组</span>
        newCap = oldThr;
    <span class="keyword">else</span> {               <span class="comment">// 使用默认配置</span>
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (<span class="keyword">int</span>)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
    <span class="keyword">if</span> (newThr == <span class="number">0</span>) {
        <span class="keyword">float</span> ft = (<span class="keyword">float</span>)newCap * loadFactor;
        newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (<span class="keyword">float</span>)MAXIMUM_CAPACITY ?
                  (<span class="keyword">int</span>)ft : Integer.MAX_VALUE);
    }
    threshold = newThr;
    @SuppressWarnings({<span class="string">"rawtypes"</span>,<span class="string">"unchecked"</span>})
        Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap];
    table = newTab;
    <span class="keyword">if</span> (oldTab != <span class="keyword">null</span>) {
        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; oldCap; ++j) { <span class="comment">// 扩容之后进行rehash操作</span>
            Node&lt;K,V&gt; e;
            <span class="keyword">if</span> ((e = oldTab[j]) != <span class="keyword">null</span>) {
                oldTab[j] = <span class="keyword">null</span>;
                <span class="keyword">if</span> (e.<span class="keyword">next</span> == <span class="keyword">null</span>)
                    newTab[e.hash &amp; (newCap - <span class="number">1</span>)] = e; <span class="comment">// 单节点扩容</span>
                <span class="keyword">else</span> <span class="keyword">if</span> (e <span class="keyword">instanceof</span> TreeNode)
                    ((TreeNode&lt;K,V&gt;)e).split(<span class="keyword">this</span>, newTab, j, oldCap); <span class="comment">// 红黑树方式处理</span>
                <span class="keyword">else</span> { <span class="comment">// 链表扩容</span>
                    Node&lt;K,V&gt; loHead = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;
                    Node&lt;K,V&gt; hiHead = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;
                    Node&lt;K,V&gt; <span class="keyword">next</span>;
                    <span class="keyword">do</span> {
                        <span class="keyword">next</span> = e.<span class="keyword">next</span>;
                        <span class="keyword">if</span> ((e.hash &amp; oldCap) == <span class="number">0</span>) {
                            <span class="keyword">if</span> (loTail == <span class="keyword">null</span>)
                                loHead = e;
                            <span class="keyword">else</span>
                                loTail.<span class="keyword">next</span> = e;
                            loTail = e;
                        } 
                        <span class="keyword">else</span> {
                            <span class="keyword">if</span> (hiTail == <span class="keyword">null</span>)
                                hiHead = e;
                            <span class="keyword">else</span>
                                hiTail.<span class="keyword">next</span> = e;
                            hiTail = e;
                        }
                    } <span class="keyword">while</span> ((e = <span class="keyword">next</span>) != <span class="keyword">null</span>);
                    <span class="keyword">if</span> (loTail != <span class="keyword">null</span>) {
                        loTail.<span class="keyword">next</span> = <span class="keyword">null</span>;
                        newTab[j] = loHead;
                    }
                    <span class="keyword">if</span> (hiTail != <span class="keyword">null</span>) {
                        hiTail.<span class="keyword">next</span> = <span class="keyword">null</span>;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    <span class="keyword">return</span> newTab;
}
</code></pre><h2 id="HashMap注意的地方">HashMap注意的地方</h2><ol>
<li>HashMap底层是个哈希表，使用拉链法解决冲突</li>
<li>HashMap内部存储的数据是无序的，这是因为HashMap内部的数组的下表是根据hash值算出来的</li>
<li>HashMap允许key为null</li>
<li>HashMap不是一个线程安全的类</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>Map是一个映射键和值的对象。类似于Python中的字典。</p>
<p>HashMap为什么会出现呢?</p>
<p>因为数组这种数据结构，虽然遍历简单，但是插入和删除操作复杂，需要移动数组内部的元素；链表这种数据结构，插入和删除操作简单，但是查找复杂，只能一个一个地遍历。</p>
<p>有没有一种新的数据结构，插入数据简单，同时查找也简单？ 这个时候就出现了哈希表这种数据结构。 这是一种折中的方式，插入没链表快，查询没数组快。</p>
<p>wiki上就是这么定义哈希表的：</p>
<p>散列表（Hash table，也叫哈希表），是根据关键字（Key value）而直接访问在内存存储位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录，这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做散列表。</p>]]>
    
    </summary>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="map" scheme="http://fangjian0423.github.io/tags/map/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk LinkedList工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/27/jdk_linkedlist/"/>
    <id>http://fangjian0423.github.io/2016/03/27/jdk_linkedlist/</id>
    <published>2016-03-27T09:35:27.000Z</published>
    <updated>2016-03-29T16:34:48.000Z</updated>
    <content type="html"><![CDATA[<p>List接口的实现类之一ArrayList的内部实现是一个数组，而另外一个实现LinkedList内部实现是使用双向链表。</p>
<p>LinkedList在内部定义了一个叫做Node类型的内部类，这个Node就是一个节点，链表中的节点，这个节点有3个属性，分别是元素item(当前节点要表示的值), 前节点prev(当前节点之前位置上的一个节点)，后节点next(当前节点后面位置的一个节点)。 </p>
<p>LinkedList关于数据的插入，删除操作都会处理这些节点的前后关系。而不像ArrayList那样只需要移动元素的位置即可。</p>
<a id="more"></a>
<h2 id="源码分析">源码分析</h2><p>在分析LinkedList之前，我们先看下它里面的内部类Node，也就是节点的定义：</p>
<pre><code>private static class <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; {
    E item; // 节点所表示的值
    <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; next; // 后节点
    <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; prev; // 前节点

    <span class="keyword">Node</span><span class="identifier"></span><span class="title">(Node</span><span class="tag">&lt;E&gt;</span> prev, E element, <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; next) {
        this.item = element;
        this.next = next;
        this.prev = prev;
    }
}
</code></pre><p>LinkedList的3个属性：</p>
<pre><code>transient int size = <span class="number">0</span>; // 集合链表内节点数量

transient <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; first; // 集合链表的首节点

transient <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; last; // 集合链表的尾节点
</code></pre><h3 id="add(E_e)">add(E e)</h3><p><strong> 添加元素到链表的最后一个位置 </strong></p>
<pre><code>public boolean add(<span class="keyword">E</span> <span class="keyword">e</span>) {
    linkLast(<span class="keyword">e</span>);
    <span class="keyword">return</span> true;
}

void linkLast(<span class="keyword">E</span> <span class="keyword">e</span>) {
    final Node&lt;<span class="keyword">E</span>&gt; <span class="keyword">l</span> = last;
    final Node&lt;<span class="keyword">E</span>&gt; newNode = new Node&lt;&gt;(<span class="keyword">l</span>, <span class="keyword">e</span>, null); <span class="comment">// 由于是添加元素的链表尾部，所以也就是这个新的节点是最后1个节点，它的前节点肯定是目前链表的尾节点，它的后节点为null</span>
    last = newNode; <span class="comment">// 尾节点变成新的节点</span>
    <span class="keyword">if</span> (<span class="keyword">l</span> == null) <span class="comment">// 如果一开始尾节点还没设置，那么说明这个新的节点是第一个节点，那么首节点也就是这个第一个节点</span>
        first = newNode;
    <span class="keyword">else</span> <span class="comment">// 否则，说明新节点不是第一个节点，处理节点前后关系</span>
        <span class="keyword">l</span>.next = newNode;
    size++; <span class="comment">// 节点数量+1</span>
    modCount++;
}
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedlist01.jpg" alt=""></p>
<p>上图第一个图就表示一个已经有1，2这2个节点的LinkedList调用add方法，第二个图表示添加一个值为3的元素后的情况。原先的尾节点2的后节点变成的新节点3，新节点3的前节点是原先的尾节点2，新节点3的后节点为null。同时链表的尾节点变成了3.</p>
<h3 id="add(int_index,_E_element)">add(int index, E element)</h3><p><strong> 添加元素到列表中的指定位置 </strong></p>
<pre><code><span class="keyword">public</span> <span class="keyword">void</span> add(<span class="keyword">int</span> <span class="keyword">index</span>, E element) {
    checkPositionIndex(<span class="keyword">index</span>); <span class="comment">// 检查索引的合法性，不能超过链表个数，不能小于0</span>

    <span class="keyword">if</span> (<span class="keyword">index</span> == size) <span class="comment">// 如果是在链表尾部插入节点，那么直接调用linkLast方法，上面已经分析过</span>
        linkLast(element);
    <span class="keyword">else</span> <span class="comment">// 不在链表尾部插入节点的话，调用linkBefore方法，参数为要插入的元素值和节点对象</span>
        linkBefore(element, node(<span class="keyword">index</span>));
}
</code></pre><p>先看一下node方法是如何根据索引找到对应的节点的：</p>
<pre><code>Node&lt;E&gt; node(<span class="keyword">int</span> index) {
    <span class="comment">// 用了一个小算法，如果索引比链表数量的一半还要小，从前往后找，这样只需要花O(n/2)的时间获取节点</span>
    <span class="keyword">if</span> (index &lt; (<span class="keyword">size</span> &gt;&gt; <span class="number">1</span>)) {
        Node&lt;E&gt; x = first;
        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index; i++)
            x = x.<span class="keyword">next</span>;
        <span class="keyword">return</span> x;
    } <span class="keyword">else</span> { <span class="comment">// 否则从后往前找</span>
        Node&lt;E&gt; x = last;
        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="keyword">size</span> - <span class="number">1</span>; i &gt; index; i--)
            x = x.prev;
        <span class="keyword">return</span> x;
    }
}

<span class="keyword">void</span> linkBefore(E e, Node&lt;E&gt; succ) { <span class="comment">// succ节点表示要新插入节点应该在的位置</span>
    <span class="keyword">final</span> Node&lt;E&gt; pred = succ.prev;
    <span class="keyword">final</span> Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(pred, e, succ); <span class="comment">// 1：新节点的前节点就是succ节点的前节点，新节点的后节点是succ节点</span>
    succ.prev = newNode; <span class="comment">// 2：succ的前节点就是新节点</span>
    <span class="keyword">if</span> (pred == <span class="keyword">null</span>)    <span class="comment">// prev=null表示succ节点就是head首节点，这样的话只需要重新set一下首节点即可，首节点的后节点在步骤1以及设置过了</span>
        first = newNode;
    <span class="keyword">else</span> <span class="comment">// succ不是首节点的话执行步骤3</span>
        pred.<span class="keyword">next</span> = newNode; <span class="comment">// 3：succ节点的前节点的后节点就是新节点</span>
    <span class="keyword">size</span>++; <span class="comment">// 节点数量+1</span>
    modCount++;
} 
</code></pre><p>上面代码中注释的1，2，3点就在下图中表示：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedlist02.jpg" alt=""></p>
<p>LinkedList还提供了2种特殊的add方法，分别是addFirst和addLast方法，处理添加首节点和尾节点，原理都是差不多的，处理链表之间的关联关系即可。</p>
<h3 id="remove(int_index)">remove(int index)</h3><p><strong> 移除指定位置上的节点 </strong></p>
<pre><code><span class="keyword">public</span> E remove(<span class="keyword">int</span> index) {
    checkElementIndex(index); <span class="comment">// 检查索引的合法性，不能超过链表个数，不能小于0 </span>
    <span class="keyword">return</span> unlink(node(index));
}

E unlink(Node&lt;E&gt; x) {
    <span class="keyword">final</span> E element = x.item;
    <span class="keyword">final</span> Node&lt;E&gt; <span class="keyword">next</span> = x.<span class="keyword">next</span>;
    <span class="keyword">final</span> Node&lt;E&gt; prev = x.prev;

    <span class="keyword">if</span> (prev == <span class="keyword">null</span>) { <span class="comment">// 特殊情况，删除的是头节点</span>
        first = <span class="keyword">next</span>;
    } <span class="keyword">else</span> {
        prev.<span class="keyword">next</span> = <span class="keyword">next</span>; <span class="comment">// 1</span>
        x.prev = <span class="keyword">null</span>; <span class="comment">// 1</span>
    }

    <span class="keyword">if</span> (<span class="keyword">next</span> == <span class="keyword">null</span>) { <span class="comment">// 特殊情况，删除的是尾节点</span>
        last = prev;
    } <span class="keyword">else</span> {
        <span class="keyword">next</span>.prev = prev; <span class="comment">// 2</span>
        x.<span class="keyword">next</span> = <span class="keyword">null</span>; <span class="comment">// 2</span>
    }

    x.item = <span class="keyword">null</span>; <span class="comment">// 3</span>
    <span class="keyword">size</span>--; <span class="comment">// 链表数量减一</span>
    modCount++;
    <span class="keyword">return</span> element;
}
</code></pre><p>上面代码中注释的1，2，3点就在下图中表示：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedlist03.jpg" alt=""></p>
<h3 id="get(int_index)">get(int index)</h3><p><strong> 得到索引位置上的元素 </strong></p>
<pre><code><span class="keyword">public</span> E get(<span class="keyword">int</span> <span class="keyword">index</span>) {
    checkElementIndex(<span class="keyword">index</span>); <span class="comment">// 检查索引的合法性，不能超过链表个数，不能小于0 </span>
    <span class="keyword">return</span> node(<span class="keyword">index</span>).item; <span class="comment">// 直接找到节点，返回节点的元素值即可</span>
}
</code></pre><h2 id="LinkedList和ArrayList的比较">LinkedList和ArrayList的比较</h2><ol>
<li>LinkedList和ArrayList的设计理念完全不一样，ArrayList基于数组，而LinkedList基于节点，也就是链表。所以LinkedList内部没有容量这个概念，因为是链表，链表是无界的</li>
<li>两者的使用场景不同，ArrayList适用于读多写少的场合。LinkedList适用于写多读少的场合。 刚好相反。 那是因为LinkedList要找节点的话必须要遍历一个一个节点，直到找到为止。而ArrayList完全不需要，因为ArrayList内部维护着一个数组，直接根据索引拿到需要的元素即可。</li>
<li>两个都是List接口的实现类，都是一种集合</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>List接口的实现类之一ArrayList的内部实现是一个数组，而另外一个实现LinkedList内部实现是使用双向链表。</p>
<p>LinkedList在内部定义了一个叫做Node类型的内部类，这个Node就是一个节点，链表中的节点，这个节点有3个属性，分别是元素item(当前节点要表示的值), 前节点prev(当前节点之前位置上的一个节点)，后节点next(当前节点后面位置的一个节点)。 </p>
<p>LinkedList关于数据的插入，删除操作都会处理这些节点的前后关系。而不像ArrayList那样只需要移动元素的位置即可。</p>]]>
    
    </summary>
    
      <category term="collection" scheme="http://fangjian0423.github.io/tags/collection/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk ArrayList工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/27/jdk_arraylist/"/>
    <id>http://fangjian0423.github.io/2016/03/27/jdk_arraylist/</id>
    <published>2016-03-27T05:33:17.000Z</published>
    <updated>2016-03-29T16:34:38.000Z</updated>
    <content type="html"><![CDATA[<p>list是一种有序的集合(an ordered collection), 通常也会被称为序列(sequence)，使用list可以精确地控制每个元素的插入，可以通过索引值找到对应list中的各个项，也可以在list中查询元素。</p>
<p>以前的几段话摘自jdk文档的说明。</p>
<p>其实list就相当于一个动态的数组，也就是链表，普通的数组长度大小都是固定的，而list是一个动态的数组，当list的长度满了，再次插入数据到list当中的时候，list会自动地扩展它的长度。</p>
<a id="more"></a>
<h2 id="ArrayList源码分析">ArrayList源码分析</h2><p>首先我们先分析一个List接口的实现类之一，也是最常用的ArrayList的源码。</p>
<p>ArrayList底层使用一个数组完成数据的添加，查询，删除，修改。这个数组就是下面提到的elementData。</p>
<p>这里分析的代码是基于jdk1.7的。</p>
<p>ArrayList类的属性如下：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="built_in">int</span> DEFAULT_CAPACITY = <span class="number">10</span>; <span class="comment">// 集合的默认容量</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">Object</span>[] EMPTY_ELEMENTDATA = {}; <span class="comment">// 一个空集合数组，容量为0</span>
<span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">Object</span>[] elementData; <span class="comment">// 存储集合数据的数组，默认值为null</span>
<span class="keyword">private</span> <span class="built_in">int</span> <span class="built_in">size</span>; <span class="comment">// ArrayList集合中数组的当前有效长度，比如数组的容量是5，size是1 表示容量为5的数组目前已经有1条记录了，其余4条记录还是为空</span>
</code></pre><p>接下来看一下ArrayList的构造函数：</p>
<p>ArrayList有3个构造函数，分别是</p>
<pre><code><span class="keyword">public</span> ArrayList(<span class="keyword">int</span> initialCapacity) { <span class="comment">// 带有集合容量参数的构造函数</span>
    <span class="comment">// 调用父类AbstractList的方法构造函数</span>
    <span class="keyword">super</span>();
    <span class="keyword">if</span> (initialCapacity &lt; <span class="number">0</span>) <span class="comment">// 如果集合的容量小于0，这明显是个错误数值，直接抛出异常</span>
        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Illegal Capacity: "</span>+
                                           initialCapacity);
    <span class="keyword">this</span>.elementData = <span class="keyword">new</span> Object[initialCapacity]; <span class="comment">// 初始化elementData属性，确定容量</span>
}

<span class="keyword">public</span> ArrayList() { <span class="comment">// 没有参数的构造函数</span>
    <span class="keyword">super</span>(); <span class="comment">// 调用父类AbstractList的方法构造函数</span>
    <span class="keyword">this</span>.elementData = EMPTY_ELEMENTDATA; <span class="comment">// 让elementData和ArrayList的EMPTY_ELEMENTDATA这个空数组使用同一个引用</span>
}

<span class="keyword">public</span> ArrayList(Collection&lt;? <span class="keyword">extends</span> E&gt; c) { <span class="comment">// 参数是一个集合的构造函数</span>
    elementData = c.toArray(); <span class="comment">// elementData直接使用参数集合内部的数组</span>
    <span class="keyword">size</span> = elementData.length; <span class="comment">// 初始化数组当前有效长度</span>
    <span class="comment">// c.toArray方法可能不会返回一个Object[]结果，需要做一层判断。这个一个Java的bug，可以在http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6260652查看</span>
    <span class="keyword">if</span> (elementData.getClass() != Object[].<span class="keyword">class</span>)
        elementData = Arrays.copyOf(elementData, <span class="keyword">size</span>, Object[].<span class="keyword">class</span>);
}
</code></pre><p>接下来挑几个重要的方法讲解一下：</p>
<h3 id="add(E_e)_方法">add(E e) 方法</h3><p>这个方法的作用就是把 <strong>元素添加到集合的最后面</strong> </p>
<p>源码：</p>
<pre><code><span class="function"><span class="keyword">public</span> boolean <span class="title">add</span><span class="params">(E e)</span> </span>{
    ensureCapacityInternal(size + <span class="number">1</span>);  <span class="comment">// 调用ensureCapacityInternal，参数是集合当前的长度。确保集合容量够大，不够的话需要扩容</span>
    elementData[size++] = e; <span class="comment">// 数组容量够的话，直接添加元素到数组最后一个位置即可，同时修改集合当前有效长度</span>
    <span class="keyword">return</span> <span class="literal">true</span>;
}

<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureCapacityInternal</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>{
    <span class="keyword">if</span> (elementData == EMPTY_ELEMENTDATA) { <span class="comment">// 如果数组是个空数组，说明调用的是无参的构造函数</span>
        <span class="comment">// 如果调用的是无参构造函数，说明数组容量为0，那就需要使用默认容量</span>
        minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity);
    }

    ensureExplicitCapacity(minCapacity);
}    

<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureExplicitCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>{
    modCount++;

    <span class="comment">// 如果集合需要的最小长度比数组容量要大，那么就需要扩容，已经放不下了</span>
    <span class="keyword">if</span> (minCapacity - elementData.length &gt; <span class="number">0</span>)
        grow(minCapacity);
}

<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">grow</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>{ <span class="comment">// 扩容的实现</span>
    <span class="keyword">int</span> oldCapacity = elementData.length;
    <span class="keyword">int</span> newCapacity = oldCapacity + (oldCapacity &gt;&gt; <span class="number">1</span>); <span class="comment">// 长度扩大1.5倍</span>
    <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>)
        newCapacity = minCapacity;
    <span class="keyword">if</span> (newCapacity - MAX_ARRAY_SIZE &gt; <span class="number">0</span>)
        newCapacity = hugeCapacity(minCapacity);
    <span class="comment">// 将数组拷贝到新长度的数组中</span>
    elementData = Arrays.copyOf(elementData, newCapacity);
}
</code></pre><p>以下面这段代码讲解一下扩容的机制：</p>
<pre><code><span class="comment">// 初始化一个容量为5的数组</span>
ArrayList&lt;Integer&gt; <span class="built_in">list</span> = <span class="keyword">new</span> ArrayList&lt;Integer&gt;(<span class="number">5</span>);
<span class="built_in">list</span>.add(<span class="number">1</span>);
<span class="built_in">list</span>.add(<span class="number">2</span>);
<span class="built_in">list</span>.add(<span class="number">3</span>);
<span class="built_in">list</span>.add(<span class="number">4</span>);
<span class="built_in">list</span>.add(<span class="number">5</span>);
<span class="comment">// 当添加第6个元素的时候，数组进行了扩容，扩容1.5倍(5+5/2=7)</span>
<span class="built_in">list</span>.add(<span class="number">6</span>);
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/arraylist01.jpg" alt=""></p>
<p>上图2个白色的空间就是扩容出来的，添加第6个元素之后，最后一个元素没被设置。</p>
<h3 id="add(int_index,_E_element)_方法">add(int index, E element) 方法</h3><p>这个方法的作用是 <strong>在指定位置插入数据</strong>，该方法的缺点就是如果集合数据量很大，移动元素位置将会话费不少时间：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">void</span> add(<span class="keyword">int</span> <span class="keyword">index</span>, E element) {
    rangeCheckForAdd(<span class="keyword">index</span>); <span class="comment">// 检查索引位置的正确的，不能小于0也不能大于数组有效长度</span>

    ensureCapacityInternal(size + <span class="number">1</span>);  <span class="comment">// 扩容检测</span>
    System.arraycopy(elementData, <span class="keyword">index</span>, elementData, <span class="keyword">index</span> + <span class="number">1</span>,
                     size - <span class="keyword">index</span>); <span class="comment">// 移动数组位置，数据量很大的话，性能变差</span>
    elementData[<span class="keyword">index</span>] = element; <span class="comment">// 指定的位置插入数据</span>
    size++; <span class="comment">// 数组有效长度+1</span>
}
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/arraylist03.jpg" alt=""></p>
<p>上图就表示要在容量为5的数组中的第4个位置插入6这个元素，会进行3个步骤：</p>
<ol>
<li>容量为5，再次加入元素，需要扩容，扩容出2个白色的空间</li>
<li>扩容之后，5和4这2个元素都移到后面那个位置上</li>
<li>移动完毕之后空出了第4个位置，插入元素6</li>
</ol>
<h3 id="remove(int_index)">remove(int index)</h3><p>remove方法就是 <strong>移除对应坐标值上的数据</strong></p>
<pre><code><span class="keyword">public</span> E remove(<span class="keyword">int</span> <span class="keyword">index</span>) {
  rangeCheck(<span class="keyword">index</span>); <span class="comment">// 检查索引值是否合法</span>

  modCount++;
  E oldValue = elementData(<span class="keyword">index</span>); <span class="comment">// 得到对应索引位置上的元素</span>

  <span class="keyword">int</span> numMoved = size - <span class="keyword">index</span> - <span class="number">1</span>; <span class="comment">// 需要移动的数量</span>
  <span class="keyword">if</span> (numMoved &gt; <span class="number">0</span>)
      System.arraycopy(elementData, <span class="keyword">index</span>+<span class="number">1</span>, elementData, <span class="keyword">index</span>,
                       numMoved); <span class="comment">// 从后往前移，留出最后一个元素</span>
  elementData[--size] = <span class="keyword">null</span>; <span class="comment">// 清楚对应位置上的对象，让gc回收</span>

  <span class="keyword">return</span> oldValue;
}
</code></pre><p>比如要移除5个元素中的第3个元素，首先要把4和5这2个位置的元素分别set到3和4这2个位置上，set完之后最后一个位置也就是第5个位置set为null。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/arraylist02.jpg" alt=""></p>
<h3 id="remove(Object_o)">remove(Object o)</h3><p> <strong>找出数组中的元素，然后移除</strong></p>
<pre><code><span class="comment">// 跟remove索引元素一样，这个方法是根据equals比较</span>
<span class="keyword">public</span> <span class="keyword">boolean</span> remove(Object o) {
    <span class="keyword">if</span> (o == <span class="keyword">null</span>) {
        <span class="comment">// ArrayList允许元素为null，所以对null值的删除在这个分支里进行</span>
        <span class="keyword">for</span> (<span class="keyword">int</span> <span class="keyword">index</span> = <span class="number">0</span>; <span class="keyword">index</span> &lt; size; <span class="keyword">index</span>++)
            <span class="keyword">if</span> (elementData[<span class="keyword">index</span>] == <span class="keyword">null</span>) {
                fastRemove(<span class="keyword">index</span>);
                <span class="keyword">return</span> <span class="keyword">true</span>;
            }
    } <span class="keyword">else</span> {
        <span class="comment">// 效率比较低，需要从第1个元素开始遍历直到找到equals相等的元素后才进行删除，删除同样需要移动元素</span>
        <span class="keyword">for</span> (<span class="keyword">int</span> <span class="keyword">index</span> = <span class="number">0</span>; <span class="keyword">index</span> &lt; size; <span class="keyword">index</span>++)
            <span class="keyword">if</span> (o.equals(elementData[<span class="keyword">index</span>])) {
                fastRemove(<span class="keyword">index</span>);
                <span class="keyword">return</span> <span class="keyword">true</span>;
            }
    }
    <span class="keyword">return</span> <span class="keyword">false</span>;
}
</code></pre><h3 id="clear">clear</h3><p><strong>清除list中的所有数据</strong></p>
<pre><code><span class="keyword">public</span> <span class="keyword">void</span> <span class="built_in">clear</span>() {
  modCount++;

  <span class="comment">// 遍历集合数据，全部set为null</span>
  <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">size</span>; i++)
      elementData[i] = <span class="keyword">null</span>;

  <span class="built_in">size</span> = <span class="number">0</span>; <span class="comment">// 数组有效长度变成0</span>
}
</code></pre><h3 id="set(int_index,_E_element)">set(int index, E element)</h3><p><strong>用element值替换下标值为index的值</strong></p>
<pre><code><span class="keyword">public</span> E set(<span class="keyword">int</span> <span class="keyword">index</span>, E element) {
  rangeCheck(<span class="keyword">index</span>); <span class="comment">// 检查索引值是否合法</span>

  E oldValue = elementData(<span class="keyword">index</span>); 
  elementData[<span class="keyword">index</span>] = element; <span class="comment">// 直接替换</span>
  <span class="keyword">return</span> oldValue;
}
</code></pre><h3 id="get(int_index)">get(int index)</h3><p><strong>得到下标值为index的元素</strong></p>
<pre><code><span class="keyword">public</span> E get(<span class="keyword">int</span> <span class="keyword">index</span>) {
    rangeCheck(<span class="keyword">index</span>); <span class="comment">// 检查索引值是否合法</span>

    <span class="keyword">return</span> elementData(<span class="keyword">index</span>); <span class="comment">// 直接返回下标值</span>
}
</code></pre><h3 id="addAll">addAll</h3><p><strong>在列表的结尾添加一个Collection集合</strong></p>
<pre><code><span class="keyword">public</span> <span class="keyword">boolean</span> addAll(Collection&lt;? <span class="keyword">extends</span> E&gt; c) {
    Object[] a = c.toArray();
    <span class="keyword">int</span> numNew = a.length;
    ensureCapacityInternal(<span class="keyword">size</span> + numNew);  <span class="comment">// 扩容检测</span>
    System.arraycopy(a, <span class="number">0</span>, elementData, <span class="keyword">size</span>, numNew); <span class="comment">// 直接在数组后面添加新的数组中的所有元素</span>
    <span class="keyword">size</span> += numNew; <span class="comment">// 更新有效长度</span>
    <span class="keyword">return</span> numNew != <span class="number">0</span>;
}
</code></pre><h3 id="toArray">toArray</h3><p><strong>根据elementData数组拷贝一份新的数组</strong></p>
<pre><code><span class="keyword">public</span> <span class="keyword">Object</span>[] toArray() {
    <span class="keyword">return</span> Arrays.copyOf(elementData, <span class="built_in">size</span>);
}
</code></pre><h2 id="ArrayList的注意点">ArrayList的注意点</h2><ol>
<li>当数据量很大的时候，ArrayList内部操作元素的时候会移动位置，很耗性能</li>
<li>ArrayList虽然可以自动扩展长度，但是数据量一大，扩展的也多，会造成很多空间的浪费</li>
<li>ArrayList有一个内部私有类，SubList。ArrayList提供一个subList方法用于构造这个SubList。这里需要注意的是SubList和ArrayList使用的数据引用是同一个对象，在SubList中操作数据和在ArrayList中操作数据都会影响双方。</li>
<li>ArrayList允许加入null元素</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>list是一种有序的集合(an ordered collection), 通常也会被称为序列(sequence)，使用list可以精确地控制每个元素的插入，可以通过索引值找到对应list中的各个项，也可以在list中查询元素。</p>
<p>以前的几段话摘自jdk文档的说明。</p>
<p>其实list就相当于一个动态的数组，也就是链表，普通的数组长度大小都是固定的，而list是一个动态的数组，当list的长度满了，再次插入数据到list当中的时候，list会自动地扩展它的长度。</p>]]>
    
    </summary>
    
      <category term="collection" scheme="http://fangjian0423.github.io/tags/collection/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java线程池ThreadPoolExecutor源码分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/22/java-threadpool-analysis/"/>
    <id>http://fangjian0423.github.io/2016/03/22/java-threadpool-analysis/</id>
    <published>2016-03-22T12:56:11.000Z</published>
    <updated>2016-03-22T12:56:15.000Z</updated>
    <content type="html"><![CDATA[<p>ThreadPoolExecutor是jdk内置线程池的一个实现，基本上大部分情况都会使用这个线程池完成各项操作。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/thread-pool.jpeg" alt=""></p>
<a id="more"></a>
<p>本文分析ThreadPoolExecutor的实现原理。</p>
<h2 id="ThreadPoolExecutor的状态和属性">ThreadPoolExecutor的状态和属性</h2><p>ThreadPoolExecutor的属性在之前的一篇<a href="http://fangjian0423.github.io/2015/07/24/java-poolthread/">java内置的线程池笔记</a>文章中解释过了，本文不再解释。</p>
<p>ThreadPoolExecutor线程池有5个状态，分别是：</p>
<ol>
<li>RUNNING：可以接受新的任务，也可以处理阻塞队列里的任务</li>
<li>SHUTDOWN：不接受新的任务，但是可以处理阻塞队列里的任务</li>
<li>STOP：不接受新的任务，不处理阻塞队列里的任务，中断正在处理的任务</li>
<li>TIDYING：过渡状态，也就是说所有的任务都执行完了，当前线程池已经没有有效的线程，这个时候线程池的状态将会TIDYING，并且将要调用terminated方法</li>
<li>TERMINATED：终止状态。terminated方法调用完成以后的状态</li>
</ol>
<p>状态之间可以进行转换：</p>
<p>RUNNING -&gt; SHUTDOWN：手动调用shutdown方法，或者ThreadPoolExecutor要被GC回收的时候调用finalize方法，finalize方法内部也会调用shutdown方法</p>
<p>(RUNNING or SHUTDOWN) -&gt; STOP：调用shutdownNow方法</p>
<p>SHUTDOWN -&gt; TIDYING：当队列和线程池都为空的时候</p>
<p>STOP -&gt; TIDYING：当线程池为空的时候</p>
<p>TIDYING -&gt; TERMINATED：terminated方法调用完成之后</p>
<p>ThreadPoolExecutor内部还保存着线程池的有效线程个数。</p>
<p>状态和线程数在ThreadPoolExecutor内部使用一个整型变量保存，没错，一个变量表示两种含义。</p>
<p>为什么一个整型变量既可以保存状态，又可以保存数量？ 分析一下：</p>
<p>首先，我们知道java中1个整型占4个字节，也就是32位，所以1个整型有32位。</p>
<p>所以整型1用二进制表示就是：00000000000000000000000000000001</p>
<p>整型-1用二进制表示就是：11111111111111111111111111111111(这个是补码，不懂的同学可以看下原码，反码，补码的知识)</p>
<p>在ThreadPoolExecutor，整型中32位的前3位用来表示线程池状态，后3位表示线程池中有效的线程数。</p>
<pre><code><span class="comment">// 前3位表示状态，所有线程数占29位</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> COUNT_BITS = Integer.<span class="keyword">SIZE</span> - <span class="number">3</span>;
</code></pre><p>线程池容量大小为 1 &lt;&lt; 29 - 1 = 00011111111111111111111111111111(二进制)，代码如下</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> final <span class="keyword">int</span> CAPACITY   = (<span class="number">1</span> &lt;&lt; COUNT_BITS) - <span class="number">1</span>;
</code></pre><p>RUNNING状态 -1 &lt;&lt; 29 = 11111111111111111111111111111111 &lt;&lt; 29 = 11100000000000000000000000000000(前3位为111)：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> RUNNING    = -<span class="number">1</span> &lt;&lt; COUNT_BITS;
</code></pre><p>SHUTDOWN状态 0 &lt;&lt; 29 = 00000000000000000000000000000000 &lt;&lt; 29 = 00000000000000000000000000000000(前3位为000)</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SHUTDOWN   =  <span class="number">0</span> &lt;&lt; COUNT_BITS;
</code></pre><p>STOP状态 1 &lt;&lt; 29 = 00000000000000000000000000000001 &lt;&lt; 29 = 00100000000000000000000000000000(前3位为001)：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> STOP       =  <span class="number">1</span> &lt;&lt; COUNT_BITS;
</code></pre><p>TIDYING状态 2 &lt;&lt; 29 = 00000000000000000000000000000010 &lt;&lt; 29 = 01000000000000000000000000000000(前3位为010)：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TIDYING    =  <span class="number">2</span> &lt;&lt; COUNT_BITS;
</code></pre><p>TERMINATED状态 3 &lt;&lt; 29 = 00000000000000000000000000000011 &lt;&lt; 29 = 01100000000000000000000000000000(前3位为011)：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TERMINATED =  <span class="number">3</span> &lt;&lt; COUNT_BITS;    
</code></pre><p>清楚状态位之后，下面是获得状态和线程数的内部方法：</p>
<pre><code><span class="comment">// 得到线程数，也就是后29位的数字。 直接跟CAPACITY做一个与操作即可，CAPACITY就是的值就 1 &lt;&lt; 29 - 1 = 00011111111111111111111111111111。 与操作的话前面3位肯定为0，相当于直接取后29位的值</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="function"><span class="keyword">int</span> <span class="title">workerCountOf</span><span class="params">(<span class="keyword">int</span> c)</span>  </span>{ <span class="keyword">return</span> c &amp; CAPACITY; }

<span class="comment">// 得到状态，CAPACITY的非操作得到的二进制位11100000000000000000000000000000，然后做在一个与操作，相当于直接取前3位的的值</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="function"><span class="keyword">int</span> <span class="title">runStateOf</span><span class="params">(<span class="keyword">int</span> c)</span>     </span>{ <span class="keyword">return</span> c &amp; ~CAPACITY; }

<span class="comment">// 或操作。相当于更新数量和状态两个操作</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="function"><span class="keyword">int</span> <span class="title">ctlOf</span><span class="params">(<span class="keyword">int</span> rs, <span class="keyword">int</span> wc)</span> </span>{ <span class="keyword">return</span> rs | wc; }
</code></pre><p>线程池初始化状态线程数变量：</p>
<pre><code><span class="comment">// 初始化状态和数量，状态为RUNNING，线程数为0</span>
<span class="keyword">private</span> <span class="keyword">final</span> AtomicInteger ctl = <span class="keyword">new</span> AtomicInteger(ctlOf(RUNNING, <span class="number">0</span>));
</code></pre><h2 id="ThreadPoolExecutor执行任务">ThreadPoolExecutor执行任务</h2><p>使用ThreadPoolExecutor执行任务的时候，可以使用execute或submit方法，submit方法如下：</p>
<pre><code><span class="keyword">public</span> Future&lt;?&gt; submit(Runnable <span class="keyword">task</span>) {
    <span class="keyword">if</span> (<span class="keyword">task</span> == <span class="keyword">null</span>) <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();
    RunnableFuture&lt;<span class="keyword">Void</span>&gt; ftask = newTaskFor(<span class="keyword">task</span>, <span class="keyword">null</span>);
    execute(ftask);
    <span class="keyword">return</span> ftask;
}
</code></pre><p>很明显地看到，submit方法内部使用了execute方法，而且submit方法是有返回值的。在调用execute方法之前，使用FutureTask包装一个Runnable，这个FutureTask就是返回值。</p>
<p>由于submit方法内部调用execute方法，所以execute方法就是执行任务的方法，来看一下execute方法，execute方法内部分3个步骤进行处理。</p>
<ol>
<li>如果当前正在执行的Worker数量比corePoolSize(基本大小)要小。直接创建一个新的Worker执行任务，会调用addWorker方法</li>
<li>如果当前正在执行的Worker数量大于等于corePoolSize(基本大小)。将任务放到阻塞队列里，如果阻塞队列没满并且状态是RUNNING的话，直接丢到阻塞队列，否则执行第3步。丢到阻塞队列之后，还需要再做一次验证(丢到阻塞队列之后可能另外一个线程关闭了线程池或者刚刚加入到队列的线程死了)。如果这个时候线程池不在RUNNING状态，把刚刚丢入队列的任务remove掉，调用reject方法，否则查看Worker数量，如果Worker数量为0，起一个新的Worker去阻塞队列里拿任务执行</li>
<li>丢到阻塞失败的话，会调用addWorker方法尝试起一个新的Worker去阻塞队列拿任务并执行任务，如果这个新的Worker创建失败，调用reject方法</li>
</ol>
<p>上面说的Worker可以暂时理解为一个执行任务的线程。</p>
<p>execute方法源码如下，上面提到的3个步骤对应源码中的3个注释：</p>
<pre><code><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span>(<span class="params">Runnable command</span>) </span>{
    <span class="keyword">if</span> (command == <span class="keyword">null</span>)
        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();
    <span class="keyword">int</span> c = ctl.<span class="keyword">get</span>();
    <span class="keyword">if</span> (workerCountOf(c) &lt; corePoolSize) {   <span class="comment">// 第一个步骤，满足线程池中的线程大小比基本大小要小</span>
        <span class="keyword">if</span> (addWorker(command, <span class="keyword">true</span>)) <span class="comment">// addWorker方法第二个参数true表示使用基本大小</span>
            <span class="keyword">return</span>;
        c = ctl.<span class="keyword">get</span>();
    }
    <span class="keyword">if</span> (isRunning(c) &amp;&amp; workQueue.offer(command)) { <span class="comment">// 第二个步骤，线程池的线程大小比基本大小要大，并且线程池还在RUNNING状态，阻塞队列也没满的情况，加到阻塞队列里</span>
        <span class="keyword">int</span> recheck = ctl.<span class="keyword">get</span>();
        <span class="keyword">if</span> (! isRunning(recheck) &amp;&amp; remove(command)) <span class="comment">// 虽然满足了第二个步骤，但是这个时候可能突然线程池关闭了，所以再做一层判断</span>
            reject(command);
        <span class="function"><span class="keyword">else</span> <span class="title">if</span> (<span class="params">workerCountOf(recheck</span>) </span>== <span class="number">0</span>)
            addWorker(<span class="keyword">null</span>, <span class="keyword">false</span>);
    }
    <span class="function"><span class="keyword">else</span> <span class="title">if</span> (<span class="params">!addWorker(command, <span class="keyword">false</span></span>)) <span class="comment">// 第三个步骤，直接使用线程池最大大小。addWorker方法第二个参数false表示使用最大大小</span>
        <span class="title">reject</span>(<span class="params">command</span>)</span>;
}
</code></pre><p>addWorker关系着如何起一个线程，再看addWorker方法之前，先看一下ThreadPoolExecutor的一个内部类Worker, Worker是一个AQS的实现类(为何设计成一个AQS在闲置Worker里会说明)，同时也是一个实现Runnable的类，使用独占锁，它的构造函数只接受一个Runnable参数，内部保存着这个Runnable属性，还有一个thread线程属性用于包装这个Runnable(这个thread属性使用ThreadFactory构造，在构造函数内完成thread线程的构造)，另外还有一个completedTasks计数器表示这个Worker完成的任务数。Worker类复写了run方法，使用ThreadPoolExecutor的runWorker方法(在addWorker方法里调用)，直接启动Worker的话，会调用ThreadPoolExecutor的runWork方法。<strong>需要特别注意的是这个Worker是实现了Runnable接口的，thread线程属性使用ThreadFactory构造Thread的时候，构造的Thread中使用的Runnable其实就是Worker。</strong>下面的Worker的源码：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Worker</span>    
    <span class="keyword">extends</span> <span class="title">AbstractQueuedSynchronizer</span>
    <span class="keyword">implements</span> <span class="title">Runnable</span>
</span>{
    <span class="comment">/**
     * This class will never be serialized, but we provide a
     * serialVersionUID to suppress a javac warning.
     */</span>
    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">6138294804551838833</span>L;

    <span class="comment">/** Thread this worker is running in.  Null if factory fails. */</span>
    <span class="keyword">final</span> Thread thread;
    <span class="comment">/** Initial task to run.  Possibly null. */</span>
    Runnable firstTask;
    <span class="comment">/** Per-thread task counter */</span>
    <span class="keyword">volatile</span> <span class="keyword">long</span> completedTasks;

    <span class="comment">/**
     * Creates with given first task and thread from ThreadFactory.
     * <span class="doctag">@param</span> firstTask the first task (null if none)
     */</span>
    Worker(Runnable firstTask) {
        <span class="comment">// 使用ThreadFactory构造Thread，这个构造的Thread内部的Runnable就是本身，也就是Worker。所以得到Worker的thread并start的时候，会执行Worker的run方法，也就是执行ThreadPoolExecutor的runWorker方法</span>
        setState(-<span class="number">1</span>); 把状态位设置成-<span class="number">1</span>，这样任何线程都不能得到Worker的锁，除非调用了unlock方法。这个unlock方法会在runWorker方法中一开始就调用，这是为了确保Worker构造出来之后，没有任何线程能够得到它的锁，除非调用了runWorker之后，其他线程才能获得Worker的锁
        <span class="keyword">this</span>.firstTask = firstTask;
        <span class="keyword">this</span>.thread = getThreadFactory().newThread(<span class="keyword">this</span>);
    }

    <span class="comment">/** Delegates main run loop to outer runWorker  */</span>
    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
        runWorker(<span class="keyword">this</span>);
    }

    <span class="comment">// Lock methods</span>
    <span class="comment">//</span>
    <span class="comment">// The value 0 represents the unlocked state.</span>
    <span class="comment">// The value 1 represents the locked state.</span>

    <span class="keyword">protected</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isHeldExclusively</span><span class="params">()</span> </span>{
        <span class="keyword">return</span> getState() != <span class="number">0</span>;
    }

    <span class="keyword">protected</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> unused)</span> </span>{
        <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, <span class="number">1</span>)) {
            setExclusiveOwnerThread(Thread.currentThread());
            <span class="keyword">return</span> <span class="keyword">true</span>;
        }
        <span class="keyword">return</span> <span class="keyword">false</span>;
    }

    <span class="keyword">protected</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryRelease</span><span class="params">(<span class="keyword">int</span> unused)</span> </span>{
        setExclusiveOwnerThread(<span class="keyword">null</span>);
        setState(<span class="number">0</span>);
        <span class="keyword">return</span> <span class="keyword">true</span>;
    }

    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span>        </span>{ acquire(<span class="number">1</span>); }
    <span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryLock</span><span class="params">()</span>  </span>{ <span class="function"><span class="keyword">return</span> <span class="title">tryAcquire</span><span class="params">(<span class="number">1</span>)</span></span>; }
    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">unlock</span><span class="params">()</span>      </span>{ release(<span class="number">1</span>); }
    <span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isLocked</span><span class="params">()</span> </span>{ <span class="function"><span class="keyword">return</span> <span class="title">isHeldExclusively</span><span class="params">()</span></span>; }

    <span class="function"><span class="keyword">void</span> <span class="title">interruptIfStarted</span><span class="params">()</span> </span>{
        Thread t;
        <span class="keyword">if</span> (getState() &gt;= <span class="number">0</span> &amp;&amp; (t = thread) != <span class="keyword">null</span> &amp;&amp; !t.isInterrupted()) {
            <span class="keyword">try</span> {
                t.interrupt();
            } <span class="keyword">catch</span> (SecurityException ignore) {
            }
        }
    }
}
</code></pre><p>接下来看一下addWorker源码：</p>
<pre><code><span class="comment">// 两个参数，firstTask表示需要跑的任务。boolean类型的core参数为true的话表示使用线程池的基本大小，为false使用线程池最大大小</span>
<span class="comment">// 返回值是boolean类型，true表示新任务被接收了，并且执行了。否则是false</span>
<span class="keyword">private</span> <span class="built_in">boolean</span> addWorker(Runnable firstTask, <span class="built_in">boolean</span> core) {
    retry:
    <span class="keyword">for</span> (;;) {
        <span class="built_in">int</span> c = ctl.<span class="built_in">get</span>();
        <span class="built_in">int</span> rs = runStateOf(c); <span class="comment">// 线程池当前状态</span>

        <span class="comment">// 这个判断转换成 rs &gt;= SHUTDOWN &amp;&amp; (rs != SHUTDOWN || firstTask != null || workQueue.isEmpty)。 </span>
        <span class="comment">// 概括为3个条件：</span>
        <span class="comment">// 1. 线程池不在RUNNING状态并且状态是STOP、TIDYING或TERMINATED中的任意一种状态</span>

        <span class="comment">// 2. 线程池不在RUNNING状态，线程池接受了新的任务 </span>

        <span class="comment">// 3. 线程池不在RUNNING状态，阻塞队列为空。  满足这3个条件中的任意一个的话，拒绝执行任务</span>

        <span class="keyword">if</span> (rs &gt;= SHUTDOWN &amp;&amp;
            ! (rs == SHUTDOWN &amp;&amp;
               firstTask == <span class="keyword">null</span> &amp;&amp;
               ! workQueue.isEmpty()))
            <span class="keyword">return</span> <span class="keyword">false</span>;

        <span class="keyword">for</span> (;;) {
            <span class="built_in">int</span> wc = workerCountOf(c); <span class="comment">// 线程池线程个数</span>
            <span class="keyword">if</span> (wc &gt;= CAPACITY ||
                wc &gt;= (core ? corePoolSize : maximumPoolSize)) <span class="comment">// 如果线程池线程数量超过线程池最大容量或者线程数量超过了基本大小(core参数为true，core参数为false的话判断超过最大大小)</span>
                <span class="keyword">return</span> <span class="keyword">false</span>; <span class="comment">// 超过直接返回false</span>
            <span class="keyword">if</span> (compareAndIncrementWorkerCount(c)) <span class="comment">// 没有超过各种大小的话，cas操作线程池线程数量+1，cas成功的话跳出循环</span>
                <span class="keyword">break</span> retry;
            c = ctl.<span class="built_in">get</span>();  <span class="comment">// 重新检查状态</span>
            <span class="keyword">if</span> (runStateOf(c) != rs) <span class="comment">// 如果状态改变了，重新循环操作</span>
                <span class="keyword">continue</span> retry;
            <span class="comment">// else CAS failed due to workerCount change; retry inner loop</span>
        }
    }
    <span class="comment">// 走到这一步说明cas操作成功了，线程池线程数量+1</span>
    <span class="built_in">boolean</span> workerStarted = <span class="keyword">false</span>; <span class="comment">// 任务是否成功启动标识</span>
    <span class="built_in">boolean</span> workerAdded = <span class="keyword">false</span>; <span class="comment">// 任务是否添加成功标识</span>
    Worker w = <span class="keyword">null</span>;
    <span class="keyword">try</span> {
        <span class="keyword">final</span> ReentrantLock mainLock = <span class="keyword">this</span>.mainLock; <span class="comment">// 得到线程池的可重入锁</span>
        w = <span class="keyword">new</span> Worker(firstTask); <span class="comment">// 基于任务firstTask构造worker</span>
        <span class="keyword">final</span> Thread t = w.thread; <span class="comment">// 使用Worker的属性thread，这个thread是使用ThreadFactory构造出来的</span>
        <span class="keyword">if</span> (t != <span class="keyword">null</span>) { <span class="comment">// ThreadFactory构造出的Thread有可能是null，做个判断</span>
            mainLock.lock(); <span class="comment">// 锁住，防止并发</span>
            <span class="keyword">try</span> {
                <span class="comment">// 在锁住之后再重新检测一下状态</span>
                <span class="built_in">int</span> c = ctl.<span class="built_in">get</span>();
                <span class="built_in">int</span> rs = runStateOf(c);

                <span class="keyword">if</span> (rs &lt; SHUTDOWN ||
                    (rs == SHUTDOWN &amp;&amp; firstTask == <span class="keyword">null</span>)) { <span class="comment">// 如果线程池在RUNNING状态或者线程池在SHUTDOWN状态并且任务是个null</span>
                    <span class="keyword">if</span> (t.isAlive()) <span class="comment">// 判断线程是否还活着，也就是说线程已经启动并且还没死掉</span>
                        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalThreadStateException(); <span class="comment">// 如果存在已经启动并且还没死的线程，抛出异常</span>
                    workers.<span class="built_in">add</span>(w); <span class="comment">// worker添加到线程池的workers属性中，是个HashSet</span>
                    <span class="built_in">int</span> s = workers.<span class="built_in">size</span>(); <span class="comment">// 得到目前线程池中的线程个数</span>
                    <span class="keyword">if</span> (s &gt; largestPoolSize) <span class="comment">// 如果线程池中的线程个数超过了线程池中的最大线程数时，更新一下这个最大线程数</span>
                        largestPoolSize = s;
                    workerAdded = <span class="keyword">true</span>; <span class="comment">// 标识一下任务已经添加成功</span>
                }
            } <span class="keyword">finally</span> {
                mainLock.unlock(); <span class="comment">// 解锁</span>
            }
            <span class="keyword">if</span> (workerAdded) { <span class="comment">// 如果任务添加成功，运行任务，改变一下任务成功启动标识</span>
                t.start(); <span class="comment">// 启动线程，这里的t是Worker中的thread属性，所以相当于就是调用了Worker的run方法</span>
                workerStarted = <span class="keyword">true</span>;
            }
        }
    } <span class="keyword">finally</span> {
        <span class="keyword">if</span> (! workerStarted) <span class="comment">// 如果任务启动失败，调用addWorkerFailed方法</span>
            addWorkerFailed(w);
    }
    <span class="keyword">return</span> workerStarted;
}
</code></pre><p>Worker中的线程start的时候，调用Worker本身run方法，这个run方法之前分析过，调用外部类ThreadPoolExecutor的runWorker方法，直接看runWorker方法：</p>
<pre><code><span class="keyword">final</span> <span class="keyword">void</span> runWorker(Worker w) {
    Thread wt = Thread.currentThread(); <span class="comment">// 得到当前线程</span>
    Runnable <span class="keyword">task</span> = w.firstTask; <span class="comment">// 得到Worker中的任务task，也就是用户传入的task</span>
    w.firstTask = <span class="keyword">null</span>; <span class="comment">// 将Worker中的任务置空</span>
    w.unlock(); <span class="comment">// allow interrupts。 </span>
    <span class="keyword">boolean</span> completedAbruptly = <span class="keyword">true</span>;
    <span class="keyword">try</span> {
        <span class="comment">// 如果worker中的任务不为空，继续知否，否则使用getTask获得任务。一直死循环，除非得到的任务为空才退出</span>
        <span class="keyword">while</span> (<span class="keyword">task</span> != <span class="keyword">null</span> || (<span class="keyword">task</span> = getTask()) != <span class="keyword">null</span>) {
            w.lock();  <span class="comment">// 如果拿到了任务，给自己上锁，表示当前Worker已经要开始执行任务了，已经不是闲置Worker(闲置Worker的解释请看下面的线程池关闭)</span>
            <span class="comment">// 在执行任务之前先做一些处理。 1. 如果线程池已经处于STOP状态并且当前线程没有被中断，中断线程 2. 如果线程池还处于RUNNING或SHUTDOWN状态，并且当前线程已经被中断了，重新检查一下线程池状态，如果处于STOP状态并且没有被中断，那么中断线程</span>
            <span class="keyword">if</span> ((runStateAtLeast(ctl.get(), STOP) ||
                 (Thread.interrupted() &amp;&amp;
                  runStateAtLeast(ctl.get(), STOP))) &amp;&amp;
                !wt.isInterrupted())
                wt.interrupt();
            <span class="keyword">try</span> {
                beforeExecute(wt, <span class="keyword">task</span>); <span class="comment">// 任务执行前需要做什么，ThreadPoolExecutor是个空实现</span>
                Throwable thrown = <span class="keyword">null</span>;
                <span class="keyword">try</span> {
                    <span class="keyword">task</span>.run(); <span class="comment">// 真正的开始执行任务，调用的是run方法，而不是start方法。这里run的时候可能会被中断，比如线程池调用了shutdownNow方法</span>
                } <span class="keyword">catch</span> (RuntimeException x) { <span class="comment">// 任务执行发生的异常全部抛出，不在runWorker中处理</span>
                    thrown = x; <span class="keyword">throw</span> x;
                } <span class="keyword">catch</span> (Error x) {
                    thrown = x; <span class="keyword">throw</span> x;
                } <span class="keyword">catch</span> (Throwable x) {
                    thrown = x; <span class="keyword">throw</span> <span class="keyword">new</span> Error(x);
                } <span class="keyword">finally</span> {
                    afterExecute(<span class="keyword">task</span>, thrown); <span class="comment">// 任务执行结束需要做什么，ThreadPoolExecutor是个空实现</span>
                }
            } <span class="keyword">finally</span> {
                <span class="keyword">task</span> = <span class="keyword">null</span>;
                w.completedTasks++; <span class="comment">// 记录执行任务的个数</span>
                w.unlock(); <span class="comment">// 执行完任务之后，解锁，Worker变成闲置Worker</span>
            }
        }
        completedAbruptly = <span class="keyword">false</span>;
    } <span class="keyword">finally</span> {
        processWorkerExit(w, completedAbruptly); <span class="comment">// 回收Worker方法</span>
    }
}
</code></pre><p>我们看一下getTask方法是如何获得任务的：</p>
<pre><code><span class="comment">// 如果发生了以下四件事中的任意一件，那么Worker需要被回收：</span>
<span class="comment">// 1. Worker个数比线程池最大大小要大</span>
<span class="comment">// 2. 线程池处于STOP状态</span>
<span class="comment">// 3. 线程池处于SHUTDOWN状态并且阻塞队列为空</span>
<span class="comment">// 4. 使用超时时间从阻塞队列里拿数据，并且超时之后没有拿到数据(allowCoreThreadTimeOut || workerCount &gt; corePoolSize)</span>
<span class="keyword">private</span> <span class="function">Runnable <span class="title">getTask</span><span class="params">()</span> </span>{
    <span class="keyword">boolean</span> timedOut = <span class="keyword">false</span>; <span class="comment">// 如果使用超时时间并且也没有拿到任务的标识</span>

    retry:
    <span class="keyword">for</span> (;;) {
        <span class="keyword">int</span> c = ctl.get();
        <span class="keyword">int</span> rs = runStateOf(c);

        <span class="comment">// 如果线程池是SHUTDOWN状态并且阻塞队列为空的话，worker数量减一，直接返回null(SHUTDOWN状态还会处理阻塞队列任务，但是阻塞队列为空的话就结束了)，如果线程池是STOP状态的话，worker数量建议，直接返回null(STOP状态不处理阻塞队列任务)[方法一开始注释的2，3两点，返回null，开始Worker回收]</span>
        <span class="keyword">if</span> (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) {
            decrementWorkerCount();
            <span class="keyword">return</span> <span class="keyword">null</span>;
        }

        <span class="keyword">boolean</span> timed;      <span class="comment">// 标记从队列中取任务时是否设置超时时间，如果为true说明这个worker可能需要回收，为false的话这个worker会一直存在，并且阻塞当前线程等待阻塞队列中有数据</span>

        <span class="keyword">for</span> (;;) {
            <span class="keyword">int</span> wc = workerCountOf(c); <span class="comment">// 得到当前线程池Worker个数</span>
            <span class="comment">// allowCoreThreadTimeOut属性默认为false，表示线程池中的核心线程在闲置状态下还保留在池中；如果是true表示核心线程使用keepAliveTime这个参数来作为超时时间</span>
            <span class="comment">// 如果worker数量比基本大小要大的话，timed就为true，需要进行回收worker</span>
            timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; 

            <span class="keyword">if</span> (wc &lt;= maximumPoolSize &amp;&amp; ! (timedOut &amp;&amp; timed)) <span class="comment">// 方法一开始注释的1，4两点，会进行下一步worker数量减一</span>
                <span class="keyword">break</span>;
            <span class="keyword">if</span> (compareAndDecrementWorkerCount(c)) <span class="comment">// worker数量减一，返回null，之后会进行Worker回收工作</span>
                <span class="keyword">return</span> <span class="keyword">null</span>;
            c = ctl.get();  <span class="comment">// 重新检查线程池状态</span>
            <span class="keyword">if</span> (runStateOf(c) != rs) <span class="comment">// 线程池状态改变的话重新开始外部循环，否则继续内部循环</span>
                <span class="keyword">continue</span> retry;
            <span class="comment">// else CAS failed due to workerCount change; retry inner loop</span>
        }

        <span class="keyword">try</span> {
            <span class="comment">// 如果需要设置超时时间，使用poll方法，否则使用take方法一直阻塞等待阻塞队列新进数据</span>
            Runnable r = timed ?
                workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :
                workQueue.take();
            <span class="keyword">if</span> (r != <span class="keyword">null</span>)
                <span class="keyword">return</span> r;
            timedOut = <span class="keyword">true</span>;
        } <span class="keyword">catch</span> (InterruptedException retry) {
            timedOut = <span class="keyword">false</span>; <span class="comment">// 闲置Worker被中断</span>
        }
    }
}
</code></pre><p>如果getTask返回的是null，那说明阻塞队列已经没有任务并且当前调用getTask的Worker需要被回收，那么会调用processWorkerExit方法进行回收：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">void</span> processWorkerExit(Worker w, <span class="built_in">boolean</span> completedAbruptly) {
    <span class="keyword">if</span> (completedAbruptly) <span class="comment">// 如果Worker没有正常结束流程调用processWorkerExit方法，worker数量减一。如果是正常结束的话，在getTask方法里worker数量已经减一了</span>
        decrementWorkerCount();

    <span class="keyword">final</span> ReentrantLock mainLock = <span class="keyword">this</span>.mainLock;
    mainLock.lock(); <span class="comment">// 加锁，防止并发问题</span>
    <span class="keyword">try</span> {
        completedTaskCount += w.completedTasks; <span class="comment">// 记录总的完成任务数</span>
        workers.remove(w); <span class="comment">// 线程池的worker集合删除掉需要回收的Worker</span>
    } <span class="keyword">finally</span> {
        mainLock.unlock(); <span class="comment">// 解锁</span>
    }

    tryTerminate(); <span class="comment">// 尝试结束线程池</span>

    <span class="built_in">int</span> c = ctl.<span class="built_in">get</span>();
    <span class="keyword">if</span> (runStateLessThan(c, STOP)) {  <span class="comment">// 如果线程池还处于RUNNING或者SHUTDOWN状态</span>
        <span class="keyword">if</span> (!completedAbruptly) { <span class="comment">// Worker是正常结束流程的话</span>
            <span class="built_in">int</span> <span class="built_in">min</span> = allowCoreThreadTimeOut ? <span class="number">0</span> : corePoolSize;
            <span class="keyword">if</span> (<span class="built_in">min</span> == <span class="number">0</span> &amp;&amp; ! workQueue.isEmpty())
                <span class="built_in">min</span> = <span class="number">1</span>;
            <span class="keyword">if</span> (workerCountOf(c) &gt;= <span class="built_in">min</span>)
                <span class="keyword">return</span>; <span class="comment">// 不需要新开一个Worker</span>
        }
        <span class="comment">// 新开一个Worker代替原先的Worker</span>
        <span class="comment">// 新开一个Worker需要满足以下3个条件中的任意一个：</span>
        <span class="comment">// 1. 用户执行的任务发生了异常</span>
        <span class="comment">// 2. Worker数量比线程池基本大小要小</span>
        <span class="comment">// 3. 阻塞队列不空但是没有任何Worker在工作</span>
        addWorker(<span class="keyword">null</span>, <span class="keyword">false</span>);
    }
}
</code></pre><p>在回收Worker的时候线程池会尝试结束自己的运行，tryTerminate方法：</p>
<pre><code><span class="keyword">final</span> void tryTerminate() {
    <span class="keyword">for</span> (;;) {
        int <span class="built_in">c</span> = ctl.<span class="keyword">get</span>();
        <span class="comment">// 满足3个条件中的任意一个，不终止线程池</span>
        <span class="comment">// 1. 线程池还在运行，不能终止</span>
        <span class="comment">// 2. 线程池处于TIDYING或TERMINATED状态，说明已经在关闭了，不允许继续处理</span>
        <span class="comment">// 3. 线程池处于SHUTDOWN状态并且阻塞队列不为空，这时候还需要处理阻塞队列的任务，不能终止线程池</span>
        <span class="keyword">if</span> (isRunning(<span class="built_in">c</span>) ||
            runStateAtLeast(<span class="built_in">c</span>, <span class="type">TIDYING</span>) ||
            (runStateOf(<span class="built_in">c</span>) == <span class="type">SHUTDOWN</span> &amp;&amp; ! workQueue.isEmpty()))
            <span class="keyword">return</span>;
        <span class="comment">// 走到这一步说明线程池已经不在运行，阻塞队列已经没有任务，但是还要回收正在工作的Worker</span>
        <span class="keyword">if</span> (workerCountOf(<span class="built_in">c</span>) != <span class="number">0</span>) {
             <span class="comment">// 由于线程池不运行了，调用了线程池的关闭方法，在解释线程池的关闭原理的时候会说道这个方法</span>
            interruptIdleWorkers(<span class="type">ONLY_ONE</span>); <span class="comment">// 中断闲置Worker，直到回收全部的Worker。这里没有那么暴力，只中断一个，中断之后退出方法，中断了Worker之后，Worker会回收，然后还是会调用tryTerminate方法，如果还有闲置线程，那么继续中断</span>
            <span class="keyword">return</span>;
        }
        <span class="comment">// 走到这里说明worker已经全部回收了，并且线程池已经不在运行，阻塞队列已经没有任务。可以准备结束线程池了</span>
        <span class="keyword">final</span> <span class="type">ReentrantLock</span> mainLock = this.mainLock;
        mainLock.lock(); <span class="comment">// 加锁，防止并发</span>
        <span class="keyword">try</span> {
            <span class="keyword">if</span> (ctl.compareAndSet(<span class="built_in">c</span>, ctlOf(<span class="type">TIDYING</span>, <span class="number">0</span>))) { <span class="comment">// cas操作，将线程池状态改成TIDYING</span>
                <span class="keyword">try</span> {
                    terminated(); <span class="comment">// 调用terminated方法</span>
                } finally {
                    ctl.<span class="keyword">set</span>(ctlOf(<span class="type">TERMINATED</span>, <span class="number">0</span>)); <span class="comment">// terminated方法调用完毕之后，状态变为TERMINATED</span>
                    termination.signalAll();
                }
                <span class="keyword">return</span>;
            }
        } finally {
            mainLock.unlock(); <span class="comment">// 解锁</span>
        }
        <span class="comment">// else retry on failed CAS</span>
    }
}
</code></pre><p>解释了这么多，对线程池的启动并且执行任务做一个总结：</p>
<p>首先，构造线程池的时候，需要一些参数。一些重要的参数解释在 <a href="http://fangjian0423.github.io/2015/07/24/java-poolthread/">java内置的线程池笔记</a> 文章中的结尾已经说明了一下重要参数的意义。</p>
<p>线程池构造完毕之后，如果用户调用了execute或者submit方法的时候，最后都会使用execute方法执行。</p>
<p>execute方法内部分3种情况处理任务：</p>
<ol>
<li>如果当前正在执行的Worker数量比corePoolSize(基本大小)要小。直接创建一个新的Worker执行任务，会调用addWorker方法</li>
<li>如果当前正在执行的Worker数量大于等于corePoolSize(基本大小)。将任务放到阻塞队列里，如果阻塞队列没满并且状态是RUNNING的话，直接丢到阻塞队列，否则执行第3步</li>
<li>丢到阻塞失败的话，会调用addWorker方法尝试起一个新的Worker去阻塞队列拿任务并执行任务，如果这个新的Worker创建失败，调用reject方法</li>
</ol>
<p>线程池中的这个基本大小指的是Worker的数量。一个Worker是一个Runnable的实现类，会被当做一个线程进行启动。Worker内部带有一个Runnable属性firstTask，这个firstTask可以为null，为null的话Worker会去阻塞队列拿任务执行，否则会先执行这个任务，执行完毕之后再去阻塞队列继续拿任务执行。</p>
<p>所以说如果Worker数量超过了基本大小，那么任务都会在阻塞队列里，当Worker执行完了它的第一个任务之后，就会去阻塞队列里拿其他任务继续执行。</p>
<p>Worker在执行的时候会根据一些参数进行调节，比如Worker数量超过了线程池基本大小或者超时时间到了等因素，这个时候Worker会被线程池回收，线程池会尽量保持内部的Worker数量不超过基本大小。</p>
<p>另外Worker执行任务的时候调用的是Runnable的run方法，而不是start方法，调用了start方法就相当于另外再起一个线程了。</p>
<p>Worker在回收的时候会尝试终止线程池。尝试关闭线程池的时候，会检查是否还有Worker在工作，检查线程池的状态，没问题的话会将状态过度到TIDYING状态，之后调用terminated方法，terminated方法调用完成之后将线程池状态更新到TERMINATED。</p>
<h2 id="ThreadPoolExecutor的关闭">ThreadPoolExecutor的关闭</h2><p>线程池的启动过程分析好了之后，接下来看线程池的关闭操作：</p>
<p>shutdown方法，关闭线程池，关闭之后阻塞队列里的任务不受影响，会继续被Worker处理，但是新的任务不会被接受：</p>
<pre><code><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">shutdown</span><span class="params">()</span> </span>{
    <span class="keyword">final</span> ReentrantLock mainLock = <span class="keyword">this</span>.mainLock;
    mainLock.lock(); <span class="comment">// 关闭的时候需要加锁，防止并发</span>
    <span class="keyword">try</span> {
        checkShutdownAccess(); <span class="comment">// 检查关闭线程池的权限</span>
        advanceRunState(SHUTDOWN); <span class="comment">// 把线程池状态更新到SHUTDOWN</span>
        interruptIdleWorkers(); <span class="comment">// 中断闲置的Worker</span>
        onShutdown(); <span class="comment">// 钩子方法，默认不处理。ScheduledThreadPoolExecutor会做一些处理</span>
    } <span class="keyword">finally</span> {
        mainLock.unlock(); <span class="comment">// 解锁</span>
    }
    tryTerminate(); <span class="comment">// 尝试结束线程池，上面已经分析过了</span>
}
</code></pre><p>interruptIdleWorkers方法，注意，这个方法打断的是闲置Worker，打断闲置Worker之后，getTask方法会返回null，然后Worker会被回收。那什么是闲置Worker呢？</p>
<p>闲置Worker是这样解释的：Worker运行的时候会去阻塞队列拿数据(getTask方法)，拿的时候如果没有设置超时时间，那么会一直阻塞等待阻塞队列进数据，这样的Worker就被称为闲置Worker。由于Worker也是一个AQS，在runWorker方法里会有一对lock和unlock操作，这对lock操作是为了确保Worker不是一个闲置Worker。</p>
<p>所以Worker被设计成一个AQS是为了根据Worker的锁来判断是否是闲置线程，是否可以被强制中断。</p>
<p>下面我们看下interruptIdleWorkers方法：</p>
<pre><code><span class="comment">// 调用他的一个重载方法，传入了参数false，表示要中断所有的正在运行的闲置Worker，如果为true表示只打断一个闲置Worker</span>
<span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">interruptIdleWorkers</span><span class="params">()</span> </span>{
    interruptIdleWorkers(<span class="keyword">false</span>);
}

<span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">interruptIdleWorkers</span><span class="params">(<span class="keyword">boolean</span> onlyOne)</span> </span>{
    <span class="keyword">final</span> ReentrantLock mainLock = <span class="keyword">this</span>.mainLock;
    mainLock.lock(); <span class="comment">// 中断闲置Worker需要加锁，防止并发</span>
    <span class="keyword">try</span> {
        <span class="keyword">for</span> (Worker w : workers) { 
            Thread t = w.thread; <span class="comment">// 拿到worker中的线程</span>
            <span class="keyword">if</span> (!t.isInterrupted() &amp;&amp; w.tryLock()) { <span class="comment">// Worker中的线程没有被打断并且Worker可以获取锁，这里Worker能获取锁说明Worker是个闲置Worker，在阻塞队列里拿数据一直被阻塞，没有数据进来。如果没有获取到Worker锁，说明Worker还在执行任务，不进行中断(shutdown方法不会中断正在执行的任务)</span>
                <span class="keyword">try</span> {
                    t.interrupt();  <span class="comment">// 中断Worker线程</span>
                } <span class="keyword">catch</span> (SecurityException ignore) {
                } <span class="keyword">finally</span> {
                    w.unlock(); <span class="comment">// 释放Worker锁</span>
                }
            }
            <span class="keyword">if</span> (onlyOne) <span class="comment">// 如果只打断1个Worker的话，直接break退出，否则，遍历所有的Worker</span>
                <span class="keyword">break</span>;
        }
    } <span class="keyword">finally</span> {
        mainLock.unlock(); <span class="comment">// 解锁</span>
    }
}
</code></pre><p>shutdown方法将线程池状态改成SHUTDOWN，线程池还能继续处理阻塞队列里的任务，并且会回收一些闲置的Worker。但是shutdownNow方法不一样，它会把线程池状态改成STOP状态，这样不会处理阻塞队列里的任务，也不会处理新的任务：</p>
<pre><code><span class="comment">// shutdownNow方法会有返回值的，返回的是一个任务列表，而shutdown方法没有返回值</span>
<span class="function"><span class="keyword">public</span> List&lt;Runnable&gt; <span class="title">shutdownNow</span>(<span class="params"></span>) </span>{
    List&lt;Runnable&gt; tasks;
    final ReentrantLock mainLock = <span class="keyword">this</span>.mainLock;
    mainLock.<span class="keyword">lock</span>(); <span class="comment">// shutdownNow操作也需要加锁，防止并发</span>
    <span class="keyword">try</span> {
        checkShutdownAccess(); <span class="comment">// 检查关闭线程池的权限</span>
        advanceRunState(STOP); <span class="comment">// 把线程池状态更新到STOP</span>
        interruptWorkers(); <span class="comment">// 中断Worker的运行</span>
        tasks = drainQueue();
    } <span class="keyword">finally</span> {
        mainLock.unlock(); <span class="comment">// 解锁</span>
    }
    tryTerminate(); <span class="comment">// 尝试结束线程池，上面已经分析过了</span>
    <span class="keyword">return</span> tasks;
}
</code></pre><p>shutdownNow的中断和shutdown方法不一样，调用的是interruptWorkers方法：</p>
<pre><code><span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">interruptWorkers</span><span class="params">()</span> </span>{
    <span class="keyword">final</span> ReentrantLock mainLock = <span class="keyword">this</span>.mainLock;
    mainLock.lock(); <span class="comment">// 中断Worker需要加锁，防止并发</span>
    <span class="keyword">try</span> {
        <span class="keyword">for</span> (Worker w : workers)
            w.interruptIfStarted(); <span class="comment">// 中断Worker的执行</span>
    } <span class="keyword">finally</span> {
        mainLock.unlock(); <span class="comment">// 解锁</span>
    }
}
</code></pre><p>Worker的interruptIfStarted方法中断Worker的执行：</p>
<pre><code><span class="function"><span class="keyword">void</span> <span class="title">interruptIfStarted</span><span class="params">()</span> </span>{
   Thread t;
   <span class="comment">// Worker无论是否被持有锁，只要还没被中断，那就中断Worker</span>
   <span class="keyword">if</span> (getState() &gt;= <span class="number">0</span> &amp;&amp; (t = thread) != <span class="keyword">null</span> &amp;&amp; !t.isInterrupted()) {
       <span class="keyword">try</span> {
           t.interrupt(); <span class="comment">// 强行中断Worker的执行</span>
       } <span class="keyword">catch</span> (SecurityException ignore) {
       }
   }
}
</code></pre><p>线程池关闭总结：    </p>
<p>线程池的关闭主要是两个方法，shutdown和shutdownNow方法。</p>
<p>shutdown方法会更新状态到SHUTDOWN，不会影响阻塞队列里任务的执行，但是不会执行新进来的任务。同时也会回收闲置的Worker，闲置Worker的定义上面已经说过了。</p>
<p>shutdownNow方法会更新状态到STOP，会影响阻塞队列的任务执行，也不会执行新进来的任务。同时会回收所有的Worker。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>ThreadPoolExecutor是jdk内置线程池的一个实现，基本上大部分情况都会使用这个线程池完成各项操作。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/thread-pool.jpeg" alt=""></p>]]>
    
    </summary>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="thread" scheme="http://fangjian0423.github.io/tags/thread/"/>
    
      <category term="java" scheme="http://fangjian0423.github.io/categories/java/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java可重入锁ReentrantLock分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/19/java-ReentrantLock-analysis/"/>
    <id>http://fangjian0423.github.io/2016/03/19/java-ReentrantLock-analysis/</id>
    <published>2016-03-19T06:31:58.000Z</published>
    <updated>2016-03-19T06:59:23.000Z</updated>
    <content type="html"><![CDATA[<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/ReentrantLock.png" alt=""><br>Java中的可重入锁ReentrantLock很常见，可以用它来代替内置锁synchronized，ReentrantLock是语法级别的锁，所以比内置锁更加灵活。</p>
<a id="more"></a>
<p>下面这段代码是ReentrantLock的一个例子：</p>
<pre><code><span class="keyword">class</span> <span class="title">Context</span> {
    <span class="keyword">private</span> ReentrantLock <span class="keyword">lock</span> = <span class="keyword">new</span> ReentrantLock();
    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">method</span>(<span class="params"></span>) </span>{
        <span class="keyword">lock</span>.<span class="keyword">lock</span>();
        System.<span class="keyword">out</span>.println(<span class="string">"do atomic operation"</span>);
        <span class="keyword">try</span> {
            Thread.sleep(<span class="number">3000</span>l);
        } <span class="keyword">catch</span> (InterruptedException e) {
            e.printStackTrace();
        } <span class="keyword">finally</span> {
            <span class="keyword">lock</span>.unlock();
        }
    }
}

<span class="keyword">class</span> <span class="title">MyThread</span> <span class="title">implements</span> <span class="title">Runnable</span> {
    <span class="keyword">private</span> Context context;
    <span class="function"><span class="keyword">public</span> <span class="title">MyThread</span>(<span class="params">Context context</span>) </span>{
        <span class="keyword">this</span>.context = context;
    }
    @<span class="function">Override
    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span>(<span class="params"></span>) </span>{
        context.method();
    }
}
</code></pre><p>main方法：</p>
<pre><code><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>{
    Context context = <span class="keyword">new</span> Context();
    ExecutorService executorService = Executors.newFixedThreadPool(<span class="number">5</span>);
    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i ++ ) {
        executorService.submit(<span class="keyword">new</span> MyThread(context));
    }
}
</code></pre><p>输出结果，每隔3秒输出：</p>
<pre><code><span class="keyword">do</span> atomic operation   
</code></pre><p>如果没有使用可重入锁的话，那么一次性输出5条 do atomic operation。</p>
<p>ReentrantLock中有3个内部类，分别是Sync、FairSync和NonfairSync。</p>
<p>Sync是一个继承AQS的抽象类，使用独占锁，复写了tryRelease方法。tryAcquire方法由它的两个FairSync(公平锁)和NonfairSync(非公平锁)实现。</p>
<p>AQS相关的内容可以参考文章末尾的参考资料，这篇文章写得非常棒。</p>
<p>ReentrantLock的lock方法使用sync的lock方法，Sync的lock方法是个抽象方法，由公平锁和非公平锁去实现。unlock方法直接使用AQS的release方法。所以说公平锁和非公平锁的释放锁过程是一样的，不一样的是获取锁过程。</p>
<p>先来看一下unlock方法，unlock方法调用的AQS的release方法，也就是调用了tryRelease方法，tryRelease方法调完之后恢复第一个挂起的线程：</p>
<pre><code><span class="keyword">protected</span> <span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryRelease</span><span class="params">(<span class="keyword">int</span> releases)</span> </span>{
    <span class="keyword">int</span> c = getState() - releases; <span class="comment">// 释放</span>
    <span class="keyword">if</span> (Thread.currentThread() != getExclusiveOwnerThread()) <span class="comment">// 如果当前线程不是独占线程，直接抛出异常</span>
        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalMonitorStateException();
    <span class="keyword">boolean</span> free = <span class="keyword">false</span>;
    <span class="keyword">if</span> (c == <span class="number">0</span>) { <span class="comment">// 由于是可重入锁，需要判断是否全部释放了</span>
        free = <span class="keyword">true</span>;
        setExclusiveOwnerThread(<span class="keyword">null</span>); <span class="comment">// 全部释放的话直接把独占线程设置为null</span>
    }
    setState(c);
    <span class="keyword">return</span> free;
}

<span class="comment">// 恢复线程</span>
<span class="keyword">public</span> <span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">release</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>{
    <span class="keyword">if</span> (tryRelease(arg)) {
        Node h = head;  <span class="comment">// 恢复第一个挂起的线程</span>
        <span class="keyword">if</span> (h != <span class="keyword">null</span> &amp;&amp; h.waitStatus != <span class="number">0</span>)
            unparkSuccessor(h);
        <span class="keyword">return</span> <span class="keyword">true</span>;
    }
    <span class="keyword">return</span> <span class="keyword">false</span>;
}
</code></pre><p>ReentrantLock的lock方法就是获取锁的方法，AQS中线程对锁的的竞争结果只有两种，要么获取到了锁；要么没有获取到锁，没有获取的锁线程被挂起等待被唤醒。</p>
<p>公平锁FairSync的lock方法：</p>
<pre><code><span class="keyword">final</span> <span class="function"><span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>{
    <span class="comment">// acquire方法内部调用tryAcquire方法</span>
    <span class="comment">// 公平锁的获取锁方法，对于没有获取到的线程，会按照队列的方式挂起线程</span>
    acquire(<span class="number">1</span>);
}

<span class="keyword">protected</span> <span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>{
    <span class="keyword">final</span> Thread current = Thread.currentThread();
    <span class="keyword">int</span> c = getState();
    <span class="keyword">if</span> (c == <span class="number">0</span>) {
        <span class="comment">// 公平锁这里多了一个!hasQueuedPredecessors()判断，表示是否有线程在队列里等待的时间比当前线程要长，如果有等待时间更长的线程，那么放弃获取锁</span>
        <span class="keyword">if</span> (!hasQueuedPredecessors() &amp;&amp;
            compareAndSetState(<span class="number">0</span>, acquires)) {
            setExclusiveOwnerThread(current);
            <span class="keyword">return</span> <span class="keyword">true</span>;
        }
    }
    <span class="keyword">else</span> <span class="keyword">if</span> (current == getExclusiveOwnerThread()) {
        <span class="keyword">int</span> nextc = c + acquires;
        <span class="keyword">if</span> (nextc &lt; <span class="number">0</span>)
            <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">"Maximum lock count exceeded"</span>);
        setState(nextc);
        <span class="keyword">return</span> <span class="keyword">true</span>;
    }
    <span class="keyword">return</span> <span class="keyword">false</span>;
}
</code></pre><p>非公平锁NonfairSync的lock方法：</p>
<pre><code><span class="keyword">final</span> <span class="function"><span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>{
    <span class="comment">// 非公平锁的获取锁</span>
    <span class="comment">// 跟公平锁的区别就在这里。直接对状态位state进行cas操作，成功就获取锁，这是一种抢占式的方式。不成功跟公平锁一样进入队列挂起线程</span>
    <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, <span class="number">1</span>))
        setExclusiveOwnerThread(Thread.currentThread());
    <span class="keyword">else</span>
        acquire(<span class="number">1</span>);
}

<span class="comment">// 调用Sync的nonfairTryAcquire方法</span>
<span class="keyword">protected</span> <span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>{
    <span class="function"><span class="keyword">return</span> <span class="title">nonfairTryAcquire</span><span class="params">(acquires)</span></span>;
}

<span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">nonfairTryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>{
    <span class="keyword">final</span> Thread current = Thread.currentThread();
    <span class="keyword">int</span> c = getState();
    <span class="keyword">if</span> (c == <span class="number">0</span>) {
        <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, acquires)) {
            setExclusiveOwnerThread(current);
            <span class="keyword">return</span> <span class="keyword">true</span>;
        }
    }
    <span class="keyword">else</span> <span class="keyword">if</span> (current == getExclusiveOwnerThread()) {
        <span class="keyword">int</span> nextc = c + acquires;
        <span class="keyword">if</span> (nextc &lt; <span class="number">0</span>) <span class="comment">// overflow</span>
            <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">"Maximum lock count exceeded"</span>);
        setState(nextc);
        <span class="keyword">return</span> <span class="keyword">true</span>;
    }
    <span class="keyword">return</span> <span class="keyword">false</span>;
}
</code></pre><p>如上述源码和注释所说，公平锁和非公平锁的最主要区别就是获取锁的方式不一样。</p>
<p>公平锁获取锁的时候，首先先读取状态位state，然后再做判断，之后使用cas设置状态位。能获取锁的线程就获取锁，不能获取锁的线程被挂起进入队列。之后再来的线程的等待时间没有已经在队列里的线程等待时间长，所以会一直进入等待队列。 公平锁类似于排队买火车票一样，后面来的人没有前面来的人等待时间长，会一直在队尾被加入到队列里。</p>
<p>非公平锁获取锁的时候，立马就使用cas判断设置状态位，是一种抢占式的方式。同时非公平锁也没有等待时间长的线程会优先获取锁这个概念。非公平锁类似吃饭排队，但是总会有那么几个人试图插队。</p>
<p>公平锁和非公平锁的还有另外一个差别，前面已经分析过了。就是公平锁获取锁多了一个判断条件，当前线程的等待时间没有队列里的线程等待时间长的话，不能获取锁；而非公平锁没有这个条件。</p>
<p>ReentrantLock的默认构造函数使用的是NonfairSync，如果想使用FairSync，使用带有boolean参数的构造函数，传入true表示FairSync，否则是NonfairSync。</p>
<p>ReentrantLock内部还提供了一些有用的方法：</p>
<p>hasQueuedThreads： 查询是否有线程在等待队列里<br>hasQueuedThread(Thread thread)：查询线程是否在等待队列里<br>isHeldByCurrentThread：当前线程是否持有锁<br>getQueueLength：队列中的挂起线程个数</p>
<p>等等还有其他的一些有用方法。</p>
<p>总结：</p>
<p>ReentrantLock可重入锁内部有3个类，Sync、FairSync和NonfairSync。</p>
<p>Sync是一个继承AQS的抽象类，并发的控制就是通过Sync实现的(当然是使用AQS实现的，AQS是Java并发包的一个同步基础类)，它复写了tryRelease方法，它有2个子类FairSync和NonfairSync，也就是公平锁和非公平锁。</p>
<p>由于Sync复写了tryRelease方法，它的2个子类公平锁和非公平锁没有再次复写这个方法，所以公平锁和非公平锁的释放锁操作是一样的，释放锁也就是唤醒等待队列中的第一个被挂起的线程。</p>
<p>虽然公平锁和非公平锁的释放锁方式一样，但是它们的获取锁方式不一样，公平锁获取锁的时候，如果1个线程获取到了锁，其他线程都会被挂起并且进入等待队列，后面来的线程的等待时间没有队列里的线程等待时间长的话，那么就放弃获取锁，进入等待队列。非公平锁获取锁的方式是一种抢占式的方式，不考虑等待时间的问题，无论哪个线程获取到了锁，其他线程就进入等待队列。</p>
<p>参考资料：</p>
<p><a href="http://ifeve.com/jdk1-8-abstractqueuedsynchronizer/" target="_blank" rel="external">http://ifeve.com/jdk1-8-abstractqueuedsynchronizer/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/ReentrantLock.png" alt=""><br>Java中的可重入锁ReentrantLock很常见，可以用它来代替内置锁synchronized，ReentrantLock是语法级别的锁，所以比内置锁更加灵活。</p>]]>
    
    </summary>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="thread" scheme="http://fangjian0423.github.io/tags/thread/"/>
    
      <category term="java" scheme="http://fangjian0423.github.io/categories/java/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java AtomicInteger原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/16/java-AtomicInteger-analysis/"/>
    <id>http://fangjian0423.github.io/2016/03/16/java-AtomicInteger-analysis/</id>
    <published>2016-03-16T12:32:35.000Z</published>
    <updated>2016-03-19T07:36:38.000Z</updated>
    <content type="html"><![CDATA[<p>Java中的AtomicInteger大家应该很熟悉，它是为了解决多线程访问Integer变量导致结果不正确所设计的一个基于多线程并且支持原子操作的Integer类。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/java-AtomicInteger-analysis.jpeg" alt=""></p>
<a id="more"></a>
<p>它的使用也非常简单：</p>
<pre><code>AtomicInteger ai = <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>);
ai.addAndGet(<span class="number">5</span>); <span class="comment">// 5</span>
ai.getAndAdd(<span class="number">1</span>); <span class="comment">// 5</span>
ai.get(); <span class="comment">// 6</span>
</code></pre><p>AtomicInteger内部有一个变量UnSafe：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">Unsafe</span> <span class="keyword">unsafe</span> = <span class="keyword">Unsafe</span>.getUnsafe();
</code></pre><p>Unsafe类是一个可以执行不安全、容易犯错的操作的一个特殊类。虽然Unsafe类中所有方法都是public的，但是这个类只能在一些被信任的代码中使用。Unsafe的源码可以在这里看 -&gt; <a href="http://www.docjar.com/html/api/sun/misc/Unsafe.java.html" target="_blank" rel="external">UnSafe源码</a>。</p>
<p>Unsafe类可以执行以下几种操作：</p>
<ol>
<li>分配内存，释放内存：在方法allocateMemory，reallocateMemory，freeMemory中，有点类似c中的malloc，free方法</li>
<li>可以定位对象的属性在内存中的位置，可以修改对象的属性值。使用objectFieldOffset方法</li>
<li>挂起和恢复线程，被封装在LockSupport类中供使用</li>
<li>CAS操作(CompareAndSwap，比较并交换，是一个原子操作)</li>
</ol>
<p>AtomicInteger中用的就是Unsafe的CAS操作。</p>
<p>Unsafe中的int类型的CAS操作方法：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">native</span> <span class="function"><span class="keyword">boolean</span> <span class="title">compareAndSwapInt</span><span class="params">(Object o, <span class="keyword">long</span> offset,
                                                <span class="keyword">int</span> expected,
                                                <span class="keyword">int</span> x)</span></span>;
</code></pre><p>参数o就是要进行cas操作的对象，offset参数是内存位置，expected参数就是期望的值，x参数是需要更新到的值。</p>
<p>也就是说，如果我把1这个数字属性更新到2的话，需要这样调用：</p>
<pre><code>compareAndSwapInt(<span class="keyword">this</span>, valueOffset, <span class="number">1</span>, <span class="number">2</span>)
</code></pre><p>valueOffset字段表示内存位置，可以在AtomicInteger对象中使用unsafe得到：</p>
<pre><code><span class="keyword">static</span> {
  <span class="keyword">try</span> {
    valueOffset = unsafe.objectFieldOffset
        (AtomicInteger.<span class="keyword">class</span>.getDeclaredField(<span class="string">"value"</span>));
  } <span class="keyword">catch</span> (Exception ex) { <span class="keyword">throw</span> <span class="keyword">new</span> Error(ex); }
}
</code></pre><p>AtomicInteger内部使用变量value表示当前的整型值，这个整型变量还是volatile的，表示内存可见性，一个线程修改value之后保证对其他线程的可见性：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">int</span> <span class="keyword">value</span>;
</code></pre><p>AtomicInteger内部还封装了一下CAS，定义了一个compareAndSet方法，只需要2个参数：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">compareAndSet</span><span class="params">(<span class="keyword">int</span> expect, <span class="keyword">int</span> update)</span> </span>{
    <span class="function"><span class="keyword">return</span> unsafe.<span class="title">compareAndSwapInt</span><span class="params">(<span class="keyword">this</span>, valueOffset, expect, update)</span></span>;
}
</code></pre><p>addAndGet方法，addAndGet方法内部使用一个死循环，先得到当前的值value，然后再把当前的值加一，加完之后使用cas原子操作让当前值加一处理正确。当然cas原子操作不一定是成功的，所以做了一个死循环，当cas操作成功的时候返回数据。这里由于使用了cas原子操作，所以不会出现多线程处理错误的问题。比如线程A得到current为1，线程B也得到current为1；线程A的next值为2，进行cas操作并且成功的时候，将value修改成了2；这个时候线程B也得到next值为2，当进行cas操作的时候由于expected值已经是2，而不是1了；所以cas操作会失败，下一次循环的时候得到的current就变成了2；也就不会出现多线程处理问题了：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> addAndGet(<span class="keyword">int</span> delta) {
    <span class="keyword">for</span> (;;) {
        <span class="keyword">int</span> current = get();
        <span class="keyword">int</span> <span class="keyword">next</span> = current + delta;
        <span class="keyword">if</span> (compareAndSet(current, <span class="keyword">next</span>))
            <span class="keyword">return</span> <span class="keyword">next</span>;
    }
}
</code></pre><p>incrementAndGet方法，跟addAndGet方法类似，只不过next值变成了current+1：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> incrementAndGet() {
    <span class="keyword">for</span> (;;) {
        <span class="keyword">int</span> current = get();
        <span class="keyword">int</span> <span class="keyword">next</span> = current + <span class="number">1</span>;
        <span class="keyword">if</span> (compareAndSet(current, <span class="keyword">next</span>))
            <span class="keyword">return</span> <span class="keyword">next</span>;
    }
}
</code></pre><p>getAndAdd方法，跟addAndGet方法一样，返回值变成了current：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> getAndAdd(<span class="keyword">int</span> delta) {
    <span class="keyword">for</span> (;;) {
        <span class="keyword">int</span> current = get();
        <span class="keyword">int</span> <span class="keyword">next</span> = current + delta;
        <span class="keyword">if</span> (compareAndSet(current, <span class="keyword">next</span>))
            <span class="keyword">return</span> current;
    }
}
</code></pre><p>缺点：</p>
<p>虽然AtomicInteger中的cas操作可以实现非阻塞的原子操作，但是会产生ABA问题，</p>
<p>参考资料：</p>
<p><a href="http://blog.csdn.net/ghsau/article/details/38471987" target="_blank" rel="external">http://blog.csdn.net/ghsau/article/details/38471987</a></p>
<p><a href="http://blog.csdn.net/aesop_wubo/article/details/7537278" target="_blank" rel="external">http://blog.csdn.net/aesop_wubo/article/details/7537278</a></p>
<p><a href="http://ifeve.com/sun-misc-unsafe/" target="_blank" rel="external">http://ifeve.com/sun-misc-unsafe/</a></p>
<p><a href="http://ifeve.com/java-atomic/" target="_blank" rel="external">http://ifeve.com/java-atomic/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Java中的AtomicInteger大家应该很熟悉，它是为了解决多线程访问Integer变量导致结果不正确所设计的一个基于多线程并且支持原子操作的Integer类。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/java-AtomicInteger-analysis.jpeg" alt=""></p>]]>
    
    </summary>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="java" scheme="http://fangjian0423.github.io/categories/java/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java根类Object的方法说明]]></title>
    <link href="http://fangjian0423.github.io/2016/03/12/java-Object-method/"/>
    <id>http://fangjian0423.github.io/2016/03/12/java-Object-method/</id>
    <published>2016-03-12T09:44:59.000Z</published>
    <updated>2016-03-12T09:45:05.000Z</updated>
    <content type="html"><![CDATA[<p>Java中的Object类是所有类的父类，它提供了以下11个方法：</p>
<ol>
<li>public final native Class&lt;?&gt; getClass()</li>
<li>public native int hashCode()</li>
<li>public boolean equals(Object obj)</li>
<li>protected native Object clone() throws CloneNotSupportedException</li>
<li>public String toString()</li>
<li>public final native void notify()</li>
<li>public final native void notifyAll()</li>
<li>public final native void wait(long timeout) throws InterruptedException</li>
<li>public final void wait(long timeout, int nanos) throws InterruptedException </li>
<li>public final void wait() throws InterruptedException</li>
<li>protected void finalize() throws Throwable { }</li>
</ol>
<p>下面我们一个个方法进行分析，看这些方法到底有什么作用：</p>
<h3 id="getClass方法">getClass方法</h3><p>getClass方法是一个final方法，不允许子类重写，并且也是一个native方法。</p>
<p>返回当前运行时对象的Class对象，注意这里是运行时，比如以下代码中n是一个Number类型的实例，但是java中数值默认是Integer类型，所以getClass方法返回的是java.lang.Integer：</p>
<pre><code><span class="string">"str"</span>.getClass() // <span class="class"><span class="keyword">class</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">String</span></span>
<span class="string">"str"</span>.getClass == String.class // <span class="literal">true</span>
Number n = <span class="number">0</span>;
<span class="class"><span class="keyword">Class</span>&lt;? <span class="keyword">extends</span> <span class="title">Number</span>&gt; <span class="title">c</span> = <span class="title">n</span>.<span class="title">getClass</span>(); // <span class="title">class</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Integer</span></span>
</code></pre><h3 id="hashCode方法">hashCode方法</h3><p>hashCode方法也是一个native方法。</p>
<p>该方法返回对象的哈希码，主要使用在哈希表中，比如JDK中的HashMap。</p>
<p>哈希码的通用约定如下：</p>
<ol>
<li>在java程序执行过程中，在一个对象没有被改变的前提下，无论这个对象被调用多少次，hashCode方法都会返回相同的整数值。对象的哈希码没有必要在不同的程序中保持相同的值。</li>
<li>如果2个对象使用equals方法进行比较并且相同的话，那么这2个对象的hashCode方法的值也必须相等。</li>
<li>如果根据equals方法，得到两个对象不相等，那么这2个对象的hashCode值不需要必须不相同。但是，不相等的对象的hashCode值不同的话可以提高哈希表的性能。</li>
</ol>
<p>通常情况下，不同的对象产生的哈希码是不同的。默认情况下，对象的哈希码是通过将该对象的内部地址转换成一个整数来实现的。</p>
<p>String的hashCode方法实现如下， 计算方法是 s[0]<em>31^(n-1) + s[1]</em>31^(n-2) + … + s[n-1]，其中s[0]表示字符串的第一个字符，n表示字符串长度：</p>
<pre><code><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>{
    <span class="keyword">int</span> h = hash;
    <span class="keyword">if</span> (h == <span class="number">0</span> &amp;&amp; value.length &gt; <span class="number">0</span>) {
        <span class="keyword">char</span> val[] = value;

        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; value.length; i++) {
            h = <span class="number">31</span> * h + val[i];
        }
        hash = h;
    }
    <span class="keyword">return</span> h;
}
</code></pre><p>比如”fo”的hashCode = 102 <em> 31^1 + 111 = 3273， “foo”的hashCode = 102 </em> 31^2 + 111 * 31^1 + 111 = 101574 (‘f’的ascii码为102, ‘o’的ascii码为111)</p>
<p>hashCode在哈希表HashMap中的应用：</p>
<pre><code><span class="comment">// Student类，只重写了hashCode方法</span>
<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> Student {

    <span class="keyword">private</span> String name;
    <span class="keyword">private</span> <span class="keyword">int</span> age;

    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>{
        <span class="keyword">this</span>.name = name;
        <span class="keyword">this</span>.age = age;
    }

    @<span class="function">Override
    <span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>{
        <span class="keyword">return</span> name.hashCode();
    }
}

Map&lt;Student, String&gt; <span class="built_in">map</span> = <span class="keyword">new</span> HashMap&lt;Student, String&gt;();
Student stu1 = <span class="keyword">new</span> Student(<span class="string">"fo"</span>, <span class="number">11</span>);
Student stu2 = <span class="keyword">new</span> Student(<span class="string">"fo"</span>, <span class="number">22</span>);
<span class="built_in">map</span>.put(stu1, <span class="string">"fo"</span>);
<span class="built_in">map</span>.put(stu2, <span class="string">"fo"</span>);
</code></pre><p>上面这段代码中，map中有2个元素stu1和stu2。但是这2个元素是在哈希表中的同一个数组项中的位置，也就是在同一串链表中。 但是为什么stu1和stu2的hashCode相同，但是两条元素都插到map里了，这是因为map判断重复数据的条件是 <strong>两个对象的哈希码相同并且(两个对象是同一个对象或者两个对象相等[equals为true])</strong>。 所以再给Student重写equals方法，并且只比较name的话，这样map就只有1个元素了。</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>{
    <span class="keyword">if</span> (<span class="keyword">this</span> == o) <span class="keyword">return</span> <span class="keyword">true</span>;
    <span class="keyword">if</span> (o == <span class="keyword">null</span> || getClass() != o.getClass()) <span class="keyword">return</span> <span class="keyword">false</span>;
    Student student = (Student) o;
    <span class="keyword">return</span> <span class="keyword">this</span>.name.equals(student.name);
}
</code></pre><p>这个例子直接说明了hashCode中通用约定的第三点：</p>
<p>第三点：如果根据equals方法，得到两个对象不相等，那么这2个对象的hashCode值不需要必须不相同。但是，不相等的对象的hashCode值不同的话可以提高哈希表的性能。 –&gt; 上面例子一开始没有重写equals方法，导致两个对象不相等，但是这两个对象的hashCode值一样，所以导致这两个对象在同一串链表中，影响性能。</p>
<p>当然，还有第三种情况，那就是equals方法相等，但是hashCode的值不相等。</p>
<p>这种情况也就是违反了通用约定的第二点：</p>
<p>第二点：<strong>如果2个对象使用equals方法进行比较并且相同的话，那么这2个对象的hashCode方法的值也必须相等。</strong> 违反这一点产生的后果就是如果一个stu1实例是Student(“fo”, 11)，stu2实例是Student(“fo”, 11)，那么这2个实例是相等的，但是他们的hashCode不一样，这样是导致哈希表中都会存入stu1实例和stu2实例，但是实际情况下，stu1和stu2是重复数据，只允许存在一条数据在哈希表中。所以这一点是非常重点的，再强调一下：<strong>如果2个对象的equals方法相等，那么他们的hashCode值也必须相等，反之，如果2个对象hashCode值相等，但是equals不相等，这样会影响性能，所以还是建议2个方法都一起重写。</strong></p>
<h3 id="equals方法">equals方法</h3><p>比较两个对象是否相等。Object类的默认实现，即比较2个对象的内存地址是否相等：</p>
<pre><code><span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span> </span>{
    <span class="keyword">return</span> (<span class="keyword">this</span> == obj);
}
</code></pre><p>equals方法在非空对象引用上的特性：</p>
<ol>
<li>reflexive，自反性。任何非空引用值x，对于x.equals(x)必须返回true</li>
<li>symmetric，对称性。任何非空引用值x和y，如果x.equals(y)为true，那么y.equals(x)也必须为true</li>
<li>transitive，传递性。任何非空引用值x、y和z，如果x.equals(y)为true并且y.equals(z)为true，那么x.equals(z)也必定为true</li>
<li>consistent，一致性。任何非空引用值x和y，多次调用 x.equals(y) 始终返回 true 或始终返回 false，前提是对象上 equals 比较中所用的信息没有被修改</li>
<li>对于任何非空引用值 x，x.equals(null) 都应返回 false</li>
</ol>
<p>Object类的equals方法对于任何非空引用值x和y，当x和y引用同一个对象时，此方法才返回true。这个也就是我们常说的地址相等。</p>
<p>注意点：如果重写了equals方法，通常有必要重写hashCode方法，这点已经在hashCode方法中说明了。</p>
<h3 id="clone方法">clone方法</h3><p>创建并返回当前对象的一份拷贝。一般情况下，对于任何对象 x，表达式 x.clone() != x 为true，x.clone().getClass() == x.getClass() 也为true。</p>
<p>Object类的clone方法是一个protected的native方法。</p>
<p>由于Object本身没有实现Cloneable接口，所以不重写clone方法并且进行调用的话会发生CloneNotSupportedException异常。</p>
<h3 id="toString方法">toString方法</h3><p>Object对象的默认实现，即输出类的名字@实例的哈希码的16进制：</p>
<pre><code><span class="keyword">public</span> <span class="built_in">String</span> toString() {
    <span class="keyword">return</span> getClass()<span class="built_in">.</span>getName() + <span class="string">"@"</span> + <span class="built_in">Integer</span><span class="built_in">.</span>toHexString(hashCode());
}
</code></pre><p>toString方法的结果应该是一个简明但易于读懂的字符串。建议Object所有的子类都重写这个方法。</p>
<h3 id="notify方法">notify方法</h3><p>notify方法是一个native方法，并且也是final的，不允许子类重写。</p>
<p>唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果所有的线程都在此对象上等待，那么只会选择一个线程。选择是任意性的，并在对实现做出决定时发生。一个线程在对象监视器上等待可以调用wait方法。</p>
<p>直到当前线程放弃对象上的锁之后，被唤醒的线程才可以继续处理。被唤醒的线程将以常规方式与在该对象上主动同步的其他所有线程进行竞争。例如，唤醒的线程在作为锁定此对象的下一个线程方面没有可靠的特权或劣势。</p>
<p>notify方法只能被作为此对象监视器的所有者的线程来调用。一个线程要想成为对象监视器的所有者，可以使用以下3种方法：</p>
<ol>
<li>执行对象的同步实例方法</li>
<li>使用synchronized内置锁</li>
<li>对于Class类型的对象，执行同步静态方法</li>
</ol>
<p>一次只能有一个线程拥有对象的监视器。</p>
<p>如果当前线程不是此对象监视器的所有者的话会抛出IllegalMonitorStateException异常</p>
<p>注意点：</p>
<p><strong>因为notify只能在拥有对象监视器的所有者线程中调用，否则会抛出IllegalMonitorStateException异常</strong></p>
<h3 id="notifyAll方法">notifyAll方法</h3><p>跟notify一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。</p>
<p>同样，如果当前线程不是对象监视器的所有者，那么调用notifyAll同样会发生IllegalMonitorStateException异常。</p>
<p>以下这段代码直接调用notify或者notifyAll方法会发生IllegalMonitorStateException异常，这是因为调用这两个方法需要当前线程是对象监视器的所有者：</p>
<pre><code>Factory <span class="literal">factory</span> = <span class="keyword">new</span> Factory();
<span class="literal">factory</span>.notify();
<span class="literal">factory</span>.notifyAll();
</code></pre><h3 id="wait(long_timeout)_throws_InterruptedException方法">wait(long timeout) throws InterruptedException方法</h3><p>wait(long timeout)方法同样是一个native方法，并且也是final的，不允许子类重写。</p>
<p>wait方法会让当前线程等待直到另外一个线程调用对象的notify或notifyAll方法，或者超过参数设置的timeout超时时间。</p>
<p><strong>跟notify和notifyAll方法一样，当前线程必须是此对象的监视器所有者，否则还是会发生IllegalMonitorStateException异常。</strong></p>
<p>wait方法会让当前线程(我们先叫做线程T)将其自身放置在对象的等待集中，并且放弃该对象上的所有同步要求。出于线程调度目的，线程T是不可用并处于休眠状态，直到发生以下四件事中的任意一件：</p>
<ol>
<li>其他某个线程调用此对象的notify方法，并且线程T碰巧被任选为被唤醒的线程</li>
<li>其他某个线程调用此对象的notifyAll方法</li>
<li>其他某个线程调用Thread.interrupt方法中断线程T</li>
<li>时间到了参数设置的超时时间。如果timeout参数为0，则不会超时，会一直进行等待</li>
</ol>
<p>所以可以理解wait方法相当于放弃了当前线程对对象监视器的所有者(也就是说释放了对象的锁)</p>
<p>之后，线程T会被等待集中被移除，并且重新进行线程调度。然后，该线程以常规方式与其他线程竞争，以获得在该对象上同步的权利；一旦获得对该对象的控制权，该对象上的所有其同步声明都将被恢复到以前的状态，这就是调用wait方法时的情况。然后，线程T从wait方法的调用中返回。所以，从wait方法返回时，该对象和线程T的同步状态与调用wait方法时的情况完全相同。</p>
<p>在没有被通知、中断或超时的情况下，线程还可以唤醒一个所谓的虚假唤醒 (spurious wakeup)。虽然这种情况在实践中很少发生，但是应用程序必须通过以下方式防止其发生，即对应该导致该线程被提醒的条件进行测试，如果不满足该条件，则继续等待。换句话说，等待应总是发生在循环中，如下面的示例：</p>
<pre><code><span class="keyword">synchronized</span> (obj) {
    <span class="keyword">while</span> (&lt;condition does not hold&gt;)
        obj.wait(timeout);
        ... <span class="comment">// Perform action appropriate to condition</span>
}
</code></pre><p>如果当前线程在等待之前或在等待时被任何线程中断，则会抛出InterruptedException异常。在按上述形式恢复此对象的锁定状态时才会抛出此异常。</p>
<h3 id="wait(long_timeout,_int_nanos)_throws_InterruptedException方法">wait(long timeout, int nanos) throws InterruptedException方法</h3><p>跟wait(long timeout)方法类似，多了一个nanos参数，这个参数表示额外时间（以毫微秒为单位，范围是 0-999999）。 所以超时的时间还需要加上nanos毫秒。</p>
<p>需要注意的是 wait(0, 0)和wait(0)效果是一样的，即一直等待。</p>
<h3 id="wait()_throws_InterruptedException方法">wait() throws InterruptedException方法</h3><p>跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念。</p>
<p>以下这段代码直接调用wait方法会发生IllegalMonitorStateException异常，这是因为调用wait方法需要当前线程是对象监视器的所有者：</p>
<pre><code>Factory <span class="literal">factory</span> = <span class="keyword">new</span> Factory();
<span class="literal">factory</span>.wait();
</code></pre><p>一般情况下，wait方法和notify方法会一起使用的，wait方法阻塞当前线程，notify方法唤醒当前线程，一个使用wait和notify方法的生产者消费者例子代码如下：</p>
<pre><code>public <span class="class"><span class="keyword">class</span> <span class="title">WaitNotifyTest</span> </span>{

    public <span class="literal">static</span> <span class="keyword">void</span> main(<span class="built_in">String</span>[] args) {
        Factory <span class="literal">factory</span> = <span class="keyword">new</span> Factory();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Producer(<span class="literal">factory</span>, <span class="number">5</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Producer(<span class="literal">factory</span>, <span class="number">5</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Producer(<span class="literal">factory</span>, <span class="number">20</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Producer(<span class="literal">factory</span>, <span class="number">30</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Consumer(<span class="literal">factory</span>, <span class="number">10</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Consumer(<span class="literal">factory</span>, <span class="number">20</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Consumer(<span class="literal">factory</span>, <span class="number">5</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Consumer(<span class="literal">factory</span>, <span class="number">5</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Consumer(<span class="literal">factory</span>, <span class="number">20</span>)).start();
    }

}

<span class="class"><span class="keyword">class</span> <span class="title">Factory</span> </span>{

    public <span class="literal">static</span> <span class="keyword">final</span> Integer MAX_NUM = <span class="number">50</span>;

    private <span class="built_in">int</span> currentNum = <span class="number">0</span>;

    public <span class="keyword">void</span> consume(<span class="built_in">int</span> <span class="built_in">num</span>) throws InterruptedException {
        synchronized (<span class="keyword">this</span>) {
            <span class="keyword">while</span>(currentNum - <span class="built_in">num</span> &lt; <span class="number">0</span>) {
                <span class="keyword">this</span>.wait();
            }
            currentNum -= <span class="built_in">num</span>;
            System.out.println(<span class="string">"consume "</span> + <span class="built_in">num</span> + <span class="string">", left: "</span> + currentNum);
            <span class="keyword">this</span>.notifyAll();
        }
    }

    public <span class="keyword">void</span> produce(<span class="built_in">int</span> <span class="built_in">num</span>) throws InterruptedException {
        synchronized (<span class="keyword">this</span>) {
            <span class="keyword">while</span>(currentNum + <span class="built_in">num</span> &gt; MAX_NUM) {
                <span class="keyword">this</span>.wait();
            }
            currentNum += <span class="built_in">num</span>;
            System.out.println(<span class="string">"produce "</span> + <span class="built_in">num</span> + <span class="string">", left: "</span> + currentNum);
            <span class="keyword">this</span>.notifyAll();
        }
    }

}

<span class="class"><span class="keyword">class</span> <span class="title">Producer</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>{
    private Factory <span class="literal">factory</span>;
    private <span class="built_in">int</span> <span class="built_in">num</span>;
    public Producer(Factory <span class="literal">factory</span>, <span class="built_in">int</span> <span class="built_in">num</span>) {
        <span class="keyword">this</span>.<span class="literal">factory</span> = <span class="literal">factory</span>;
        <span class="keyword">this</span>.<span class="built_in">num</span> = <span class="built_in">num</span>;
    }
    <span class="annotation">@Override</span>
    public <span class="keyword">void</span> run() {
        <span class="keyword">try</span> {
            <span class="literal">factory</span>.produce(<span class="built_in">num</span>);
        } <span class="keyword">catch</span> (InterruptedException e) {
            e.printStackTrace();
        }
    }
}


<span class="class"><span class="keyword">class</span> <span class="title">Consumer</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>{
    private Factory <span class="literal">factory</span>;
    private <span class="built_in">int</span> <span class="built_in">num</span>;
    public Consumer(Factory <span class="literal">factory</span>, <span class="built_in">int</span> <span class="built_in">num</span>) {
        <span class="keyword">this</span>.<span class="literal">factory</span> = <span class="literal">factory</span>;
        <span class="keyword">this</span>.<span class="built_in">num</span> = <span class="built_in">num</span>;
    }
    <span class="annotation">@Override</span>
    public <span class="keyword">void</span> run() {
        <span class="keyword">try</span> {
            <span class="literal">factory</span>.consume(<span class="built_in">num</span>);
        } <span class="keyword">catch</span> (InterruptedException e) {
            e.printStackTrace();
        }
    }
}
</code></pre><p>注意的是Factory类的produce和consume方法都将Factory实例锁住了，锁住之后线程就成为了对象监视器的所有者，然后才能调用wait和notify方法。</p>
<p>输出：</p>
<pre><code>produce <span class="number">5</span>, left: <span class="number">5</span>
produce <span class="number">20</span>, left: <span class="number">25</span>
produce <span class="number">5</span>, left: <span class="number">30</span>
consume <span class="number">10</span>, left: <span class="number">20</span>
produce <span class="number">30</span>, left: <span class="number">50</span>
consume <span class="number">20</span>, left: <span class="number">30</span>
consume <span class="number">5</span>, left: <span class="number">25</span>
consume <span class="number">5</span>, left: <span class="number">20</span>
consume <span class="number">20</span>, left: <span class="number">0</span>
</code></pre><h3 id="finalize方法">finalize方法</h3><p>finalize方法是一个protected方法，Object类的默认实现是不进行任何操作。 </p>
<p>该方法的作用是实例被垃圾回收器回收的时候触发的操作，就好比 “死前的最后一波挣扎”。</p>
<p>直接写个弱引用例子：</p>
<pre><code>Car car = <span class="keyword">new</span> Car(<span class="number">9999</span>, <span class="string">"black"</span>);
WeakReference&lt;Car&gt; carWeakReference = <span class="keyword">new</span> WeakReference&lt;Car&gt;(car);

<span class="keyword">int</span> i = <span class="number">0</span>;
<span class="keyword">while</span>(<span class="keyword">true</span>) {
    <span class="keyword">if</span>(carWeakReference.<span class="keyword">get</span>() != <span class="keyword">null</span>) {
        i++;
        System.<span class="keyword">out</span>.println(<span class="string">"Object is alive for "</span>+i+<span class="string">" loops - "</span>+carWeakReference);
    } <span class="keyword">else</span> {
        System.<span class="keyword">out</span>.println(<span class="string">"Object has been collected."</span>);
        <span class="keyword">break</span>;
    }
}

<span class="keyword">class</span> <span class="title">Car</span> {
    <span class="keyword">private</span> <span class="keyword">double</span> price;
    <span class="keyword">private</span> String colour;

    <span class="function"><span class="keyword">public</span> <span class="title">Car</span>(<span class="params"><span class="keyword">double</span> price, String colour</span>)</span>{
        <span class="keyword">this</span>.price = price;
        <span class="keyword">this</span>.colour = colour;
    }

    <span class="comment">// get set method</span>

    @<span class="function">Override
    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">finalize</span>(<span class="params"></span>) throws Throwable </span>{
        System.<span class="keyword">out</span>.println(<span class="string">"i will be destroyed"</span>);
    }
}
</code></pre><p>输出：</p>
<pre><code>....
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26417</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26418</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26419</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26420</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26421</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26422</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> has been collected.
i will be destroyed
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Java中的Object类是所有类的父类，它提供了11个方法，分别是getClass，hashCode，clone，toString，notify，notifyAll，wait，finalze等方法 ...]]>
    
    </summary>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="java" scheme="http://fangjian0423.github.io/categories/java/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Avro介绍]]></title>
    <link href="http://fangjian0423.github.io/2016/02/21/avro-intro/"/>
    <id>http://fangjian0423.github.io/2016/02/21/avro-intro/</id>
    <published>2016-02-20T17:23:22.000Z</published>
    <updated>2016-03-16T06:55:43.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Avro介绍">Avro介绍</h2><p>Apache Avro是一个数据序列化系统。</p>
<p>Avro所提供的属性：</p>
<p>1.丰富的数据结构<br>2.使用快速的压缩二进制数据格式<br>3.提供容器文件用于持久化数据<br>4.远程过程调用RPC<br>5.简单的动态语言结合功能，Avro 和动态语言结合后，读写数据文件和使用 RPC 协议都不需要生成代码，而代码生成作为一种可选的优化只值得在静态类型语言中实现。</p>
<h2 id="Avro的Schema">Avro的Schema</h2><p>Avro的Schema用JSON表示。Schema定义了简单数据类型和复杂数据类型。</p>
<h3 id="基本类型">基本类型</h3><p>其中简单数据类型有以下8种：</p>
<table>
<thead>
<tr>
<th style="text-align:center">类型</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">null</td>
<td style="text-align:center">没有值</td>
</tr>
<tr>
<td style="text-align:center">boolean</td>
<td style="text-align:center">布尔值</td>
</tr>
<tr>
<td style="text-align:center">int</td>
<td style="text-align:center">32位有符号整数</td>
</tr>
<tr>
<td style="text-align:center">long</td>
<td style="text-align:center">64位有符号整数</td>
</tr>
<tr>
<td style="text-align:center">float</td>
<td style="text-align:center">单精度（32位）的IEEE 754浮点数</td>
</tr>
<tr>
<td style="text-align:center">double</td>
<td style="text-align:center">双精度（64位）的IEEE 754浮点数</td>
</tr>
<tr>
<td style="text-align:center">bytes</td>
<td style="text-align:center">8位无符号字节序列</td>
</tr>
<tr>
<td style="text-align:center">string</td>
<td style="text-align:center">字符串</td>
</tr>
</tbody>
</table>
<p>基本类型没有属性，基本类型的名字也就是类型的名字，比如：</p>
<pre><code>{"<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>}
</code></pre><h3 id="复杂类型">复杂类型</h3><p>Avro提供了6种复杂类型。分别是Record，Enum，Array，Map，Union和Fixed。</p>
<h4 id="Record">Record</h4><p>Record类型使用的类型名字是 “record”，还支持其它属性的设置：</p>
<p>name：record类型的名字(必填)</p>
<p>namespace：命名空间(可选)</p>
<p>doc：这个类型的文档说明(可选)</p>
<p>aliases：record类型的别名，是个字符串数组(可选)</p>
<p>fields：record类型中的字段，是个对象数组(必填)。每个字段需要以下属性：</p>
<ol>
<li>name：字段名字(必填)</li>
<li>doc：字段说明文档(可选)</li>
<li>type：一个schema的json对象或者一个类型名字(必填)</li>
<li>default：默认值(可选)</li>
<li>order：排序(可选)，只有3个值ascending(默认)，descending或ignore</li>
<li>aliases：别名，字符串数组(可选)</li>
</ol>
<p>一个Record类型例子，定义一个元素类型是Long的链表：</p>
<pre><code><span class="collection">{
  <span class="string">"type"</span>: <span class="string">"record"</span>, 
  <span class="string">"name"</span>: <span class="string">"LongList"</span>,
  <span class="string">"aliases"</span>: <span class="collection">[<span class="string">"LinkedLongs"</span>]</span>,                      // old name for this
  <span class="string">"fields"</span> : <span class="collection">[
    <span class="collection">{<span class="string">"name"</span>: <span class="string">"value"</span>, <span class="string">"type"</span>: <span class="string">"long"</span>}</span>,             // each element has a long
    <span class="collection">{<span class="string">"name"</span>: <span class="string">"next"</span>, <span class="string">"type"</span>: <span class="collection">[<span class="string">"null"</span>, <span class="string">"LongList"</span>]</span>}</span> // optional next element
  ]</span>
}</span>
</code></pre><h4 id="Enum">Enum</h4><p>枚举类型的类型名字是”enum”，还支持其它属性的设置：</p>
<p>name：枚举类型的名字(必填)<br>namespace：命名空间(可选)<br>aliases：字符串数组，别名(可选)<br>doc：说明文档(可选)<br>symbols：字符串数组，所有的枚举值(必填)，不允许重复数据。</p>
<p>一个枚举类型的例子：</p>
<pre><code>{ "<span class="attribute">type</span>": <span class="value"><span class="string">"enum"</span></span>,
  "<span class="attribute">name</span>": <span class="value"><span class="string">"Suit"</span></span>,
  "<span class="attribute">symbols</span>" : <span class="value">[<span class="string">"SPADES"</span>, <span class="string">"HEARTS"</span>, <span class="string">"DIAMONDS"</span>, <span class="string">"CLUBS"</span>]
</span>}
</code></pre><h4 id="Array">Array</h4><p>数组类型的类型名字是”array”并且只支持一个属性：</p>
<p>items：数组元素的schema</p>
<p>一个数组例子：</p>
<pre><code>{"<span class="attribute">type</span>": <span class="value"><span class="string">"array"</span></span>, "<span class="attribute">items</span>": <span class="value"><span class="string">"string"</span></span>}
</code></pre><h4 id="Map">Map</h4><p>Map类型的类型名字是”map”并且只支持一个属性：</p>
<p>values：map值的schema</p>
<p>Map的key必须是字符串。</p>
<p>一个Map例子：</p>
<pre><code>{"<span class="attribute">type</span>": <span class="value"><span class="string">"map"</span></span>, "<span class="attribute">values</span>": <span class="value"><span class="string">"long"</span></span>}
</code></pre><h4 id="Union">Union</h4><p>组合类型，表示各种类型的组合，使用数组进行组合。比如[“null”, “string”]表示类型可以为null或者string。</p>
<p>组合类型的默认值是看组合类型的第一个元素，因此如果一个组合类型包括null类型，那么null类型一般都会放在第一个位置，这样子的话这个组合类型的默认值就是null。</p>
<p>组合类型中不允许同一种类型的元素的个数不会超过1个，除了record，fixed和enum。比如组合类中有2个array类型或者2个map类型，这是不允许的。</p>
<p>组合类型不允许嵌套组合类型。</p>
<h4 id="Fixed">Fixed</h4><p>混合类型的类型名字是fixed，支持以下属性：</p>
<p>name：名字(必填)<br>namespace：命名空间(可选)<br>aliases：字符串数组，别名(可选)<br>size：一个整数，表示每个值的字节数(必填)</p>
<p>比如16个字节数的fixed类型例子如下：</p>
<pre><code>{"<span class="attribute">type</span>": <span class="value"><span class="string">"fixed"</span></span>, "<span class="attribute">size</span>": <span class="value"><span class="number">16</span></span>, "<span class="attribute">name</span>": <span class="value"><span class="string">"md5"</span></span>}
</code></pre><h2 id="1个Avro例子">1个Avro例子</h2><p>首先定义一个User的schema：</p>
<pre><code>{
"<span class="attribute">namespace</span>": <span class="value"><span class="string">"example.avro"</span></span>,
 "<span class="attribute">type</span>": <span class="value"><span class="string">"record"</span></span>,
 "<span class="attribute">name</span>": <span class="value"><span class="string">"User"</span></span>,
 "<span class="attribute">fields</span>": <span class="value">[
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"name"</span></span>, "<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_number"</span></span>,  "<span class="attribute">type</span>": <span class="value"><span class="string">"int"</span></span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_color"</span></span>, "<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>}
 ]
</span>}
</code></pre><p>User有3个属性，分别是name，favorite_number和favorite_color。</p>
<p>json文件内容：</p>
<pre><code>{"<span class="attribute">name</span>":<span class="value"><span class="string">"format"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value"><span class="number">1</span></span>,"<span class="attribute">favorite_color</span>":<span class="value"><span class="string">"red"</span></span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"format2"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value"><span class="number">2</span></span>,"<span class="attribute">favorite_color</span>":<span class="value"><span class="string">"black"</span></span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"format3"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value"><span class="number">666</span></span>,"<span class="attribute">favorite_color</span>":<span class="value"><span class="string">"blue"</span></span>}
</code></pre><p>使用avro工具将json文件转换成avro文件：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">fromjson</span> <span class="tag">--schema-file</span> <span class="tag">user</span><span class="class">.avsc</span> <span class="tag">user</span><span class="class">.json</span> &gt; <span class="tag">user</span><span class="class">.avro</span>
</code></pre><p>可以设置压缩格式：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">fromjson</span> <span class="tag">--codec</span> <span class="tag">snappy</span> <span class="tag">--schema-file</span> <span class="tag">user</span><span class="class">.avsc</span> <span class="tag">user</span><span class="class">.json</span> &gt; <span class="tag">user2</span><span class="class">.avro</span>
</code></pre><p>将avro文件反转换成json文件：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">tojson</span> <span class="tag">user</span><span class="class">.avro</span>
<span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">--pretty</span> <span class="tag">tojson</span> <span class="tag">user</span><span class="class">.avro</span>
</code></pre><p>得到avro文件的meta：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">getmeta</span> <span class="tag">user</span><span class="class">.avro</span>
</code></pre><p>输出：</p>
<pre><code>avro.codec    null
avro.<span class="keyword">schema</span>    {<span class="string">"type"</span>:<span class="string">"record"</span>,<span class="string">"name"</span>:<span class="string">"User"</span>,<span class="string">"namespace"</span>:<span class="string">"example.avro"</span>,<span class="string">"fields"</span>:[{<span class="string">"name"</span>:<span class="string">"name"</span>,<span class="string">"type"</span>:<span class="string">"string"</span>},{<span class="string">"name"</span>:<span class="string">"favorite_number"</span>,<span class="string">"type"</span>:<span class="string">"int"</span>},{<span class="string">"name"</span>:<span class="string">"favorite_color"</span>,<span class="string">"type"</span>:<span class="string">"string"</span>}]}
</code></pre><p>得到avro文件的schema：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">getschema</span> <span class="tag">user</span><span class="class">.avro</span>
</code></pre><p>将文本文件转换成avro文件：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">fromtext</span> <span class="tag">user</span><span class="class">.txt</span> <span class="tag">usertxt</span><span class="class">.avro</span>
</code></pre><h2 id="Avro使用生成的代码进行序列化和反序列化">Avro使用生成的代码进行序列化和反序列化</h2><p>以上面一个例子的schema为例讲解。</p>
<p>Avro可以根据schema自动生成对应的类：</p>
<pre><code>java -jar /path/to/avro-tools-<span class="number">1.8</span>.<span class="number">0</span><span class="class">.jar</span> compile schema user<span class="class">.avsc</span> .
</code></pre><p>user.avsc的namespace为example.avro，name为User。最终在当前目录生成的example/avro目录下有个User.java文件。</p>
<pre><code>├── <span class="tag">example</span>
│   └── <span class="tag">avro</span>
│       └── <span class="tag">User</span><span class="class">.java</span>
</code></pre><p><strong>使用Avro生成的代码创建User：</strong></p>
<pre><code><span class="keyword">User</span> <span class="title">user1</span> = new User();
user1.setName(<span class="string">"Format"</span>);
user1.setFavoriteColor(<span class="string">"red"</span>);
user1.setFavoriteNumber(<span class="number">666</span>);

<span class="keyword">User</span> <span class="title">user2</span> = new User(<span class="string">"Format2"</span>, <span class="number">66</span>, <span class="string">"blue"</span>);

<span class="keyword">User</span> <span class="title">user3</span> = User.newBuilder()
                .setName(<span class="string">"Format3"</span>)
                .setFavoriteNumber(<span class="number">6</span>)
                .setFavoriteColor(<span class="string">"black"</span>).build();
</code></pre><p>可以使用有参的构造函数和无参的构造函数，也可以使用Builder构造User。</p>
<p><strong>序列化：</strong></p>
<p>DatumWrite接口用来把java对象转换成内存中的序列化格式，SpecificDatumWriter用来生成类并且指定生成的类型。</p>
<p>最后使用DataFileWriter来进行具体的序列化，create方法指定文件和schema信息，append方法用来写数据，最后写完后close文件。</p>
<pre><code>DatumWriter&lt;User&gt; userDatumWriter = <span class="keyword">new</span> SpecificDatumWriter&lt;User&gt;(User.<span class="keyword">class</span>);
        DataFileWriter&lt;User&gt; dataFileWriter = <span class="keyword">new</span> DataFileWriter&lt;User&gt;(userDatumWriter);
dataFileWriter.create(user1.getSchema(), <span class="keyword">new</span> <span class="keyword">File</span>(<span class="string">"users.avro"</span>));
dataFileWriter.<span class="keyword">append</span>(user1);
dataFileWriter.<span class="keyword">append</span>(user2);
dataFileWriter.<span class="keyword">append</span>(user3);
dataFileWriter.close();
</code></pre><p><strong>反序列化：</strong></p>
<p>反序列化跟序列化很像，相应的Writer换成Reader。这里只创建一个User对象是为了性能优化，每次都重用这个User对象，如果文件量很大，对象分配和垃圾收集处理的代价很昂贵。如果不考虑性能，可以使用 for (User user : dataFileReader) 循环遍历对象</p>
<pre><code><span class="keyword">File</span> <span class="keyword">file</span> = <span class="keyword">new</span> <span class="keyword">File</span>(<span class="string">"users.avro"</span>);
DatumReader&lt;User&gt; userDatumReader = <span class="keyword">new</span> SpecificDatumReader&lt;User&gt;(User.<span class="keyword">class</span>);
DataFileReader&lt;User&gt; dataFileReader = <span class="keyword">new</span> DataFileReader&lt;User&gt;(<span class="keyword">file</span>, userDatumReader);
User user = <span class="keyword">null</span>;
<span class="keyword">while</span>(dataFileReader.hasNext()) {
    user = dataFileReader.<span class="keyword">next</span>(user);
    System.out.<span class="keyword">println</span>(user);
}
</code></pre><p>打印出：</p>
<pre><code>{"<span class="attribute">name</span>": <span class="value"><span class="string">"Format"</span></span>, "<span class="attribute">favorite_number</span>": <span class="value"><span class="number">666</span></span>, "<span class="attribute">favorite_color</span>": <span class="value"><span class="string">"red"</span></span>}
{"<span class="attribute">name</span>": <span class="value"><span class="string">"Format2"</span></span>, "<span class="attribute">favorite_number</span>": <span class="value"><span class="number">66</span></span>, "<span class="attribute">favorite_color</span>": <span class="value"><span class="string">"blue"</span></span>}
{"<span class="attribute">name</span>": <span class="value"><span class="string">"Format3"</span></span>, "<span class="attribute">favorite_number</span>": <span class="value"><span class="number">6</span></span>, "<span class="attribute">favorite_color</span>": <span class="value"><span class="string">"black"</span></span>}
</code></pre><h2 id="Avro不使用生成的代码进行序列化和反序列化">Avro不使用生成的代码进行序列化和反序列化</h2><p>虽然Avro为我们提供了根据schema自动生成类的方法，我们也可以自己创建类，不使用Avro的自动生成工具。</p>
<p><strong>创建User：</strong></p>
<p>首先使用Parser读取schema信息并且创建Schema类：</p>
<pre><code>Schema schema = <span class="keyword">new</span> Schema.Parser().parse(<span class="keyword">new</span> <span class="keyword">File</span>(<span class="string">"user.avsc"</span>));
</code></pre><p>有了Schema之后可以创建record：</p>
<pre><code>GenericRecord user1 = <span class="keyword">new</span> GenericData.Record(schema);
user1.<span class="keyword">put</span>(<span class="string">"name"</span>, <span class="string">"Format"</span>);
user1.<span class="keyword">put</span>(<span class="string">"favorite_number"</span>, <span class="number">666</span>);
user1.<span class="keyword">put</span>(<span class="string">"favorite_color"</span>, <span class="string">"red"</span>);

GenericRecord user2 = <span class="keyword">new</span> GenericData.Record(schema);
user2.<span class="keyword">put</span>(<span class="string">"name"</span>, <span class="string">"Format2"</span>);
user2.<span class="keyword">put</span>(<span class="string">"favorite_number"</span>, <span class="number">66</span>);
user2.<span class="keyword">put</span>(<span class="string">"favorite_color"</span>, <span class="string">"blue"</span>);
</code></pre><p>使用GenericRecord表示User，GenericRecord会根据schema验证字段是否正确，如果put进了不存在的字段 user1.put(“favorite_animal”, “cat”) ，那么运行的时候会得到AvroRuntimeException异常。</p>
<p><strong>序列化：</strong></p>
<p>序列化跟生成的User类似，只不过schema是自己构造的，不是User中拿的。</p>
<pre><code>Schema schema = <span class="keyword">new</span> Schema.Parser().parse(<span class="keyword">new</span> File(<span class="string">"user.avsc"</span>));
GenericRecord user1 = <span class="keyword">new</span> GenericData.Record(schema);
user1.<span class="keyword">put</span>(<span class="string">"name"</span>, <span class="string">"Format"</span>);
user1.<span class="keyword">put</span>(<span class="string">"favorite_number"</span>, <span class="number">666</span>);
user1.<span class="keyword">put</span>(<span class="string">"favorite_color"</span>, <span class="string">"red"</span>);

GenericRecord user2 = <span class="keyword">new</span> GenericData.Record(schema);
user2.<span class="keyword">put</span>(<span class="string">"name"</span>, <span class="string">"Format2"</span>);
user2.<span class="keyword">put</span>(<span class="string">"favorite_number"</span>, <span class="number">66</span>);
user2.<span class="keyword">put</span>(<span class="string">"favorite_color"</span>, <span class="string">"blue"</span>);

DatumWriter&lt;GenericRecord&gt; datumWriter = <span class="keyword">new</span> SpecificDatumWriter&lt;GenericRecord&gt;(schema);
DataFileWriter&lt;GenericRecord&gt; dataFileWriter = <span class="keyword">new</span> DataFileWriter&lt;GenericRecord&gt;(datumWriter);
dataFileWriter.create(schema, <span class="keyword">new</span> File(<span class="string">"users2.avro"</span>));
dataFileWriter.<span class="built_in">append</span>(user1);
dataFileWriter.<span class="built_in">append</span>(user2);
dataFileWriter.<span class="keyword">close</span>();
</code></pre><p><strong>反序列化：</strong></p>
<p>反序列化跟生成的User类似，只不过schema是自己构造的，不是User中拿的。</p>
<pre><code>Schema schema = <span class="keyword">new</span> Schema.Parser().parse(<span class="keyword">new</span> <span class="keyword">File</span>(<span class="string">"user.avsc"</span>));
<span class="keyword">File</span> <span class="keyword">file</span> = <span class="keyword">new</span> <span class="keyword">File</span>(<span class="string">"users2.avro"</span>);
DatumReader&lt;GenericRecord&gt; datumReader = <span class="keyword">new</span> SpecificDatumReader&lt;GenericRecord&gt;(schema);
DataFileReader&lt;GenericRecord&gt; dataFileReader = <span class="keyword">new</span> DataFileReader&lt;GenericRecord&gt;(<span class="keyword">file</span>, datumReader);
GenericRecord user = <span class="keyword">null</span>;
<span class="keyword">while</span>(dataFileReader.hasNext()) {
    user = dataFileReader.<span class="keyword">next</span>(user);
    System.out.<span class="keyword">println</span>(user);
}
</code></pre><p>打印出：</p>
<pre><code>{"<span class="attribute">name</span>": <span class="value"><span class="string">"Format"</span></span>, "<span class="attribute">favorite_number</span>": <span class="value"><span class="number">666</span></span>, "<span class="attribute">favorite_color</span>": <span class="value"><span class="string">"red"</span></span>}
{"<span class="attribute">name</span>": <span class="value"><span class="string">"Format2"</span></span>, "<span class="attribute">favorite_number</span>": <span class="value"><span class="number">66</span></span>, "<span class="attribute">favorite_color</span>": <span class="value"><span class="string">"blue"</span></span>}
</code></pre><h2 id="一些注意点">一些注意点</h2><p>Avro解析json文件的时候，如果类型是Record并且里面有字段是union并且允许空值的话，需要进行转换。因为[“bytes”, “string”]和[“int”,”long”]这2个union类型在json中是有歧义的，第一个union在json中都会被转换成string类型，第二个union在json中都会被转换成数字类型。</p>
<p>所以如果json值的null的话，在avro提供的json中直接写null，否则使用只有一个键值对的对象，键是类型，值的具体的值。</p>
<p>比如：</p>
<pre><code>{
"<span class="attribute">namespace</span>": <span class="value"><span class="string">"example.avro"</span></span>,
 "<span class="attribute">type</span>": <span class="value"><span class="string">"record"</span></span>,
 "<span class="attribute">name</span>": <span class="value"><span class="string">"User"</span></span>,
 "<span class="attribute">fields</span>": <span class="value">[
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"name"</span></span>, "<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_number"</span></span>,  "<span class="attribute">type</span>": <span class="value">[<span class="string">"int"</span>,<span class="string">"null"</span>]</span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_color"</span></span>, "<span class="attribute">type</span>": <span class="value">[<span class="string">"string"</span>,<span class="string">"null"</span>]</span>}
 ]
</span>}
</code></pre><p>在要转换成json文件的时候要写成这样：</p>
<pre><code>{"<span class="attribute">name</span>":<span class="value"><span class="string">"format"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value">{"<span class="attribute">int</span>":<span class="value"><span class="number">1</span></span>}</span>,"<span class="attribute">favorite_color</span>":<span class="value">{"<span class="attribute">string</span>":<span class="value"><span class="string">"red"</span></span>}</span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"format2"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value"><span class="literal">null</span></span>,"<span class="attribute">favorite_color</span>":<span class="value">{"<span class="attribute">string</span>":<span class="value"><span class="string">"black"</span></span>}</span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"format3"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value">{"<span class="attribute">int</span>":<span class="value"><span class="number">66</span></span>}</span>,"<span class="attribute">favorite_color</span>":<span class="value"><span class="literal">null</span></span>}
</code></pre><h2 id="Spark读取Avro文件">Spark读取Avro文件</h2><p>直接遍历avro文件，得到GenericRecord进行处理：</p>
<pre><code>val conf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(<span class="string">"local"</span>)</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(<span class="string">"AvroTest"</span>)</span></span>

val sc = new <span class="function"><span class="title">SparkContext</span><span class="params">(conf)</span></span>

val rdd = sc<span class="class">.hadoopFile</span>[AvroWrapper[GenericRecord], NullWritable, AvroInputFormat[GenericRecord]](this<span class="class">.getClass</span><span class="class">.getResource</span>(<span class="string">"/"</span>)<span class="class">.toString</span> + <span class="string">"users.avro"</span>)

val nameRdd = rdd.<span class="function"><span class="title">map</span><span class="params">(s =&gt; s._1.datum()</span></span>.<span class="function"><span class="title">get</span><span class="params">(<span class="string">"name"</span>)</span></span>.toString)

nameRdd.<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>
</code></pre><h2 id="使用Avro需要注意的地方">使用Avro需要注意的地方</h2><p>笔者使用Avro的时候暂时遇到了下面2个坑。先记录一下，以后遇到新的坑会更新这篇文章。</p>
<p>1.如果定义了unions类型的字段，而且unions中有null选项的schema，比如如下schema：</p>
<pre><code>{
"<span class="attribute">namespace</span>": <span class="value"><span class="string">"example.avro"</span></span>,
 "<span class="attribute">type</span>": <span class="value"><span class="string">"record"</span></span>,
 "<span class="attribute">name</span>": <span class="value"><span class="string">"User2"</span></span>,
 "<span class="attribute">fields</span>": <span class="value">[
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"name"</span></span>, "<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_number"</span></span>,  "<span class="attribute">type</span>": <span class="value">[<span class="string">"null"</span>,<span class="string">"int"</span>]</span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_color"</span></span>, "<span class="attribute">type</span>": <span class="value">[<span class="string">"null"</span>,<span class="string">"string"</span>]</span>}
 ]
</span>}
</code></pre><p>这样的schema，如果不使用Avro自动生成的model代码进行insert，并且insert中的model数据有null数据的话。然后用spark读avro文件的话，会报org.apache.avro.AvroTypeException: Found null, expecting int … 这样的错误。</p>
<p>这一点很奇怪，但是使用Avro生成的Model进行insert的话，sprak读取就没有任何问题。 很困惑。</p>
<p>2.如果使用了Map类型的字段，avro生成的model中的Map的Key默认类型为CharSequence。这种model我们insert数据的话，用String是没有问题的。但是spark读取之后要根据Key拿这个Map数据的时候，永远得到的是null。</p>
<p>stackoverflow上有一个页面说到了这个问题。<a href="http://stackoverflow.com/questions/19728853/apache-avro-map-uses-charsequence-as-key" target="_blank" rel="external">http://stackoverflow.com/questions/19728853/apache-avro-map-uses-charsequence-as-key
</a></p>
<p>需要在map类型的字段里加上”avro.java.string”: “String”这个选项, 然后compile的时候使用-string参数即可。</p>
<p>比如以下这个schema：</p>
<pre><code>{
"<span class="attribute">namespace</span>": <span class="value"><span class="string">"example.avro"</span></span>,
 "<span class="attribute">type</span>": <span class="value"><span class="string">"record"</span></span>,
 "<span class="attribute">name</span>": <span class="value"><span class="string">"User3"</span></span>,
 "<span class="attribute">fields</span>": <span class="value">[
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"name"</span></span>, "<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_number"</span></span>,  "<span class="attribute">type</span>": <span class="value">[<span class="string">"null"</span>,<span class="string">"int"</span>]</span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_color"</span></span>, "<span class="attribute">type</span>": <span class="value">[<span class="string">"null"</span>,<span class="string">"string"</span>]</span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"scores"</span></span>, "<span class="attribute">type</span>": <span class="value">[<span class="string">"null"</span>, {"<span class="attribute">type</span>": <span class="value"><span class="string">"map"</span></span>, "<span class="attribute">values</span>": <span class="value"><span class="string">"string"</span></span>, "<span class="attribute">avro.java.string</span>": <span class="value"><span class="string">"String"</span></span>}]</span>}
 ]
</span>}
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Apache Avro是一个数据序列化系统, 提供丰富的数据结构，使用快速的压缩二进制数据格式，提供容器文件用于持久化数据 ...]]>
    
    </summary>
    
      <category term="avro" scheme="http://fangjian0423.github.io/tags/avro/"/>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="avro" scheme="http://fangjian0423.github.io/categories/avro/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark DataFrame介绍]]></title>
    <link href="http://fangjian0423.github.io/2016/02/17/spark-sql/"/>
    <id>http://fangjian0423.github.io/2016/02/17/spark-sql/</id>
    <published>2016-02-17T01:22:22.000Z</published>
    <updated>2016-02-17T03:25:25.000Z</updated>
    <content type="html"><![CDATA[<h2 id="DataFrame是什么">DataFrame是什么</h2><p>DataFrame是一个以命名列方式组织的分布式数据集。在概念上，它跟关系型数据库中的一张表或者1个Python(或者R)中的data frame一样，但是比他们更优化。DataFrame可以根据结构化的数据文件、hive表、外部数据库或者已经存在的RDD构造。</p>
<h2 id="DataFrame的创建">DataFrame的创建</h2><p>Spark DataFrame可以从一个已经存在的RDD、hive表或者数据源中创建。</p>
<p>以下一个例子就表示一个DataFrame基于一个json文件创建：</p>
<pre><code>val sc: SparkContext <span class="comment">// An existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SQLContext</span>(sc)

val df = sqlContext<span class="class">.read</span><span class="class">.json</span>(<span class="string">"examples/src/main/resources/people.json"</span>)

<span class="comment">// Displays the content of the DataFrame to stdout</span>
df.<span class="function"><span class="title">show</span><span class="params">()</span></span>
</code></pre><h2 id="DataFrame的操作">DataFrame的操作</h2><p>直接以1个例子来说明DataFrame的操作：</p>
<p>json文件内容：</p>
<pre><code>{"<span class="attribute">name</span>":<span class="value"><span class="string">"Michael"</span></span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"Andy"</span></span>, "<span class="attribute">age</span>":<span class="value"><span class="number">30</span></span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"Justin"</span></span>, "<span class="attribute">age</span>":<span class="value"><span class="number">19</span></span>}
</code></pre><p>程序内容：</p>
<pre><code>val conf = new SparkConf().setMaster("local").setAppName("DataFrameTest")

val sc = new SparkContext(conf)

val sqlContext = new SQLContext(sc)

val df = sqlContext.read.json(this.getClass.getResource("/").toString + "people.json")

<span class="header">  /** 展示DataFrame的内容
+----+-------+</span>
<span class="header">| age|   name|
+----+-------+</span>
|null|Michael|
|  30|   Andy|
|  19| Justin|
<span class="code">+----+</span>-------+  
<span class="code">  **/</span>
df.show()

/** 以树的形式打印出DataFrame的schema
root
<span class="code"> |-- age: long (nullable = true)</span>
<span class="code"> |-- name: string (nullable = true)</span>
*<span class="strong">*/
df.printSchema()

</span><span class="header">/** 打印出name列的数据
+-------+</span>
<span class="header">|   name|
+-------+</span>
|Michael|
|   Andy|
| Justin|
<span class="code">+-------+</span>   
*<span class="strong">*/
df.select("name").show()

</span><span class="header">/** 打印出name列和age列+1的数据，DataFrame的apply方法返回Column
+-------+---------+</span>
<span class="header">|   name|(age + 1)|
+-------+---------+</span>
|Michael|     null|
|   Andy|       31|
<span class="header">| Justin|       20|
+-------+---------+</span>
*<span class="strong">*/
df.select(df("name"), df("age") + 1).show()

</span><span class="header">/** 添加过滤条件，过滤出age字段大于21的数据
+---+----+</span>
<span class="header">|age|name|
+---+----+</span>
<span class="header">| 30|Andy|
+---+----+</span>
*<span class="strong">*/
df.filter(df("age") &gt; 21).show()

</span><span class="header">/** 以age字段分组进行统计
+----+-----+</span>
<span class="header">| age|count|
+----+-----+</span>
|null|    1|
|  19|    1|
<span class="header">|  30|    1|
+----+-----+</span>
*<span class="strong">*/
df.groupBy(df("age")).count().show()</span>
</code></pre><h2 id="使用反射推断出Schema">使用反射推断出Schema</h2><p>Spark SQL的Scala接口支持将包括case class数据的RDD转换成DataFrame。</p>
<p>case class定义表的schema，case class的属性会被读取并且成为列的名字，这里case class也可以被当成别的case class的属性或者是复杂的类型，比如Sequence或Array。</p>
<p>RDD会被隐式转换成DataFrame并且被注册成一个表，这个表可以被用在查询语句中：</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SQLContext</span>(sc)
<span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span>
import sqlContext<span class="class">.implicits</span>._

<span class="comment">// Define the schema using a case class.</span>
<span class="comment">// <span class="doctag">Note:</span> Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span>
<span class="comment">// you can use custom classes that implement the Product interface.</span>
case class <span class="function"><span class="title">Person</span><span class="params">(name: String, age: Int)</span></span>

<span class="comment">// Create an RDD of Person objects and register it as a table.</span>
val people = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"examples/src/main/resources/people.txt"</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(_.split(<span class="string">","</span>)</span></span>).<span class="function"><span class="title">map</span><span class="params">(p =&gt; Person(p(<span class="number">0</span>)</span></span>, <span class="function"><span class="title">p</span><span class="params">(<span class="number">1</span>)</span></span><span class="class">.trim</span><span class="class">.toInt</span>)).<span class="function"><span class="title">toDF</span><span class="params">()</span></span>
people.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"people"</span>)</span></span>

<span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span>
val teenagers = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span></span>

<span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span>
<span class="comment">// The columns of a row in the result can be accessed by field index:</span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>

<span class="comment">// or by field name:</span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t.getAs[String](<span class="string">"name"</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>

<span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(_.getValuesMap[Any](List(<span class="string">"name"</span>, <span class="string">"age"</span>)</span></span>)).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>
<span class="comment">// Map("name" -&gt; "Justin", "age" -&gt; 19)</span>
</code></pre><h2 id="使用编程指定Schema">使用编程指定Schema</h2><p>当case class不能提前确定（例如，记录的结构是经过编码的字符串，或者一个文本集合将会被解析，不同的字段投影给不同的用户），一个 DataFrame 可以通过三步来创建。</p>
<p>1.从原来的 RDD 创建一个行的 RDD<br>2.创建由一个 StructType 表示的模式与第一步创建的 RDD 的行结构相匹配<br>3.在行 RDD 上通过 applySchema 方法应用模式</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
<span class="variable"><span class="keyword">val</span> sqlContext</span> = new org.apache.spark.sql.SQLContext(sc)

<span class="comment">// Create an RDD</span>
<span class="variable"><span class="keyword">val</span> people</span> = sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)

<span class="comment">// The schema is encoded in a string</span>
<span class="variable"><span class="keyword">val</span> schemaString</span> = <span class="string">"name age"</span>

<span class="comment">// Import Row.</span>
<span class="keyword">import</span> org.apache.spark.sql.Row;

<span class="comment">// Import Spark SQL data types</span>
<span class="keyword">import</span> org.apache.spark.sql.types.{StructType,StructField,StringType};

<span class="comment">// Generate the schema based on the string of schema</span>
<span class="variable"><span class="keyword">val</span> schema</span> =
  StructType(
    schemaString.split(<span class="string">" "</span>).map(fieldName =&gt; StructField(fieldName, StringType, <span class="literal">true</span>)))

<span class="comment">// Convert records of the RDD (people) to Rows.</span>
<span class="variable"><span class="keyword">val</span> rowRDD</span> = people.map(_.split(<span class="string">","</span>)).map(p =&gt; Row(p(<span class="number">0</span>), p(<span class="number">1</span>).trim))

<span class="comment">// Apply the schema to the RDD.</span>
<span class="variable"><span class="keyword">val</span> peopleDataFrame</span> = sqlContext.createDataFrame(rowRDD, schema)

<span class="comment">// Register the DataFrames as a table.</span>
peopleDataFrame.registerTempTable(<span class="string">"people"</span>)

<span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span>
<span class="variable"><span class="keyword">val</span> results</span> = sqlContext.sql(<span class="string">"SELECT name FROM people"</span>)

<span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span>
<span class="comment">// The columns of a row in the result can be accessed by field index or by field name.</span>
results.map(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)).collect().foreach(println)
</code></pre><h2 id="数据源">数据源</h2><p>Spark SQL默认使用的数据源是parquet(可以通过spark.sql.sources.default修改)。</p>
<pre><code>val df = sqlContext<span class="class">.read</span><span class="class">.load</span>(<span class="string">"examples/src/main/resources/users.parquet"</span>)
df.<span class="function"><span class="title">select</span><span class="params">(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>)</span></span><span class="class">.write</span><span class="class">.save</span>(<span class="string">"namesAndFavColors.parquet"</span>)
</code></pre><p>可以在读取数据源的时候指定一些往外的参数。数据源也可以使用全名称，比如org.apache.spark.sql.parquet，但是内置的数据源可以使用短名称，比如json, parquet, jdbc。任何类型的DataFrame都可以使用这种方式转换成其他类型：</p>
<pre><code>val df = sqlContext<span class="class">.read</span><span class="class">.format</span>(<span class="string">"json"</span>).<span class="function"><span class="title">load</span><span class="params">(<span class="string">"examples/src/main/resources/people.json"</span>)</span></span>
df.<span class="function"><span class="title">select</span><span class="params">(<span class="string">"name"</span>, <span class="string">"age"</span>)</span></span><span class="class">.write</span><span class="class">.format</span>(<span class="string">"parquet"</span>).<span class="function"><span class="title">save</span><span class="params">(<span class="string">"namesAndAges.parquet"</span>)</span></span>
</code></pre><p>使用read方法读取数据源得到DataFrame，还可以使用sql直接查询文件的方式：</p>
<pre><code>val df = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`"</span>)</span></span>
</code></pre><p>保存模式：</p>
<p>保存方法会需要一个可选参数SaveMode，用于处理已经存在的数据。这些保存模式内部不会用到锁的概念，也不是一个原子操作。如果使用了Overwrite这种保存模式，那么写入数据前会清空之前的老数据。</p>
<table>
<thead>
<tr>
<th style="text-align:center">Scala/Java</th>
<th style="text-align:center">具体值</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">SaveMode.ErrorIfExists (默认值)</td>
<td style="text-align:center">“error” (默认值)</td>
<td style="text-align:center">当保存DataFrame到数据源的时候，如果数据源文件已经存在，那么会抛出异常</td>
</tr>
<tr>
<td style="text-align:center">SaveMode.Append</td>
<td style="text-align:center">“append”</td>
<td style="text-align:center">如果数据源文件已经存在，append到文件末尾</td>
</tr>
<tr>
<td style="text-align:center">SaveMode.Overwrite</td>
<td style="text-align:center">“overwrite”</td>
<td style="text-align:center">如果数据源文件已经存在，清空数据</td>
</tr>
<tr>
<td style="text-align:center">SaveMode.Ignore</td>
<td style="text-align:center">“ignore”</td>
<td style="text-align:center">如果数据源文件已经存在，不做任何处理。跟SQL中的 CREATE TABLE IF NOT EXISTS 类似</td>
</tr>
</tbody>
</table>
<p>持久化表：</p>
<p>当使用HiveContext的时候，使用saveAsTable方法可以把DataFrame持久化成表。跟registerTempTable方法不一样，saveAsTable方法会把DataFrame持久化成表，并且创建一个数据的指针到HiveMetastore对象中。只要获得了同一个HiveMetastore对象的链接，当Spark程序重启的时候，saveAsTable持久化后的表依然会存在。一个DataFrame持久化成一个table也可以通过SQLContext的table方法，参数就是表的名字。</p>
<p>默认情况下，saveAsTable方法会创建一个”被管理的表”，被管理的表的意思是说表中数据的位置会被HiveMetastore所控制，如果表被删除了，HiveMetastore中的数据也相当于被删除了。</p>
<h3 id="Parquet_Files">Parquet Files</h3><p>parquet是一种基于列的存储格式，并且可以被很多框架所支持。Spark SQL支持parquet文件的读和写操作，并且会自动维护原始数据的schema，当写一个parquet文件的时候，所有的列都允许为空。</p>
<h4 id="加载Parquet文件">加载Parquet文件</h4><pre><code><span class="comment">// sqlContext from the previous example is used in this example.</span>
<span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span>
import sqlContext<span class="class">.implicits</span>._

val people: RDD[Person] = ... <span class="comment">// An RDD of case class objects, from the previous example.</span>

<span class="comment">// The RDD is implicitly converted to a DataFrame by implicits, allowing it to be stored using Parquet.</span>
people<span class="class">.write</span><span class="class">.parquet</span>(<span class="string">"people.parquet"</span>)

<span class="comment">// Read in the parquet file created above. Parquet files are self-describing so the schema is preserved.</span>
<span class="comment">// The result of loading a Parquet file is also a DataFrame.</span>
val parquetFile = sqlContext<span class="class">.read</span><span class="class">.parquet</span>(<span class="string">"people.parquet"</span>)

<span class="comment">//Parquet files can also be registered as tables and then used in SQL statements.</span>
parquetFile.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"parquetFile"</span>)</span></span>
val teenagers = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span></span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>
</code></pre><h4 id="Parquet文件的Partition">Parquet文件的Partition</h4><p>Parquet文件可以根据列自动进行分区，只需要调用DataFrameWriter的partitionBy方法即可，该方法需要的参数是需要进行分区的列。比如需要分区成这样：</p>
<pre><code>path
└── <span class="keyword">to</span>
    └── table
        ├── gender=male
        │   ├── <span class="attribute">...</span>
        │   │
        │   ├── country=US
        │   │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
        │   ├── country=<span class="literal">CN</span>
        │   │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
        │   └── <span class="attribute">...</span>
        └── gender=female
            ├── <span class="attribute">...</span>
            │
            ├── country=US
            │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
            ├── country=<span class="literal">CN</span>
            │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
            └── <span class="attribute">...</span>
</code></pre><p>这个需要DataFrame就需要4列，分别是name，age，gender和country，write的时候如下：</p>
<pre><code>dataFrame<span class="class">.write</span><span class="class">.partitionBy</span>(<span class="string">"gender"</span>, <span class="string">"country"</span>).<span class="function"><span class="title">parquet</span><span class="params">(<span class="string">"path"</span>)</span></span>
</code></pre><h4 id="Schema_Merging">Schema Merging</h4><p>像ProtocolBuffer，Avro，Thrift一样，Parquet也支持schema的扩展。</p>
<p>由于schema的自动扩展是一次昂贵的操作，所以默认情况下不是开启的，可以根据以下设置打开：</p>
<p>读parquet文件的时候设置参数mergeSchema为true或者设置全局的sql属性spark.sql.parquet.mergeSchema为true：</p>
<pre><code><span class="comment">// sqlContext from the previous example is used in this example.</span>
<span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span>
import sqlContext<span class="class">.implicits</span>._

<span class="comment">// Create a simple DataFrame, stored into a partition directory</span>
val df1 = sc.<span class="function"><span class="title">makeRDD</span><span class="params">(<span class="number">1</span> to <span class="number">5</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(i =&gt; (i, i * <span class="number">2</span>)</span></span>).<span class="function"><span class="title">toDF</span><span class="params">(<span class="string">"single"</span>, <span class="string">"double"</span>)</span></span>
df1<span class="class">.write</span><span class="class">.parquet</span>(<span class="string">"data/test_table/key=1"</span>)

<span class="comment">// Create another DataFrame in a new partition directory,</span>
<span class="comment">// adding a new column and dropping an existing column</span>
val df2 = sc.<span class="function"><span class="title">makeRDD</span><span class="params">(<span class="number">6</span> to <span class="number">10</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(i =&gt; (i, i * <span class="number">3</span>)</span></span>).<span class="function"><span class="title">toDF</span><span class="params">(<span class="string">"single"</span>, <span class="string">"triple"</span>)</span></span>
df2<span class="class">.write</span><span class="class">.parquet</span>(<span class="string">"data/test_table/key=2"</span>)

<span class="comment">// Read the partitioned table</span>
val df3 = sqlContext<span class="class">.read</span><span class="class">.option</span>(<span class="string">"mergeSchema"</span>, <span class="string">"true"</span>).<span class="function"><span class="title">parquet</span><span class="params">(<span class="string">"data/test_table"</span>)</span></span>
df3.<span class="function"><span class="title">printSchema</span><span class="params">()</span></span>

<span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span>
<span class="comment">// with the partitioning column appeared in the partition directory paths.</span>
<span class="comment">// root</span>
<span class="comment">// |-- single: int (nullable = true)</span>
<span class="comment">// |-- double: int (nullable = true)</span>
<span class="comment">// |-- triple: int (nullable = true)</span>
<span class="comment">// |-- key : int (nullable = true)</span>
</code></pre><h3 id="JSON数据源">JSON数据源</h3><p>本文之前的一个例子就是使用的JSON数据源，使用SQLContext.read.json()读取一个带有String类型的RDD或者一个json文件。</p>
<p>需要注意的是json文件不是一个典型的json格式的文件，每一行都是一个json对象。</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SQLContext</span>(sc)

<span class="comment">// A JSON dataset is pointed to by path.</span>
<span class="comment">// The path can be either a single text file or a directory storing text files.</span>
val path = <span class="string">"examples/src/main/resources/people.json"</span>
val people = sqlContext<span class="class">.read</span><span class="class">.json</span>(path)

<span class="comment">// The inferred schema can be visualized using the printSchema() method.</span>
people.<span class="function"><span class="title">printSchema</span><span class="params">()</span></span>
<span class="comment">// root</span>
<span class="comment">//  |-- age: integer (nullable = true)</span>
<span class="comment">//  |-- name: string (nullable = true)</span>

<span class="comment">// Register this DataFrame as a table.</span>
people.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"people"</span>)</span></span>

<span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span>
val teenagers = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span></span>

<span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span>
<span class="comment">// an RDD[String] storing one JSON object per string.</span>
val anotherPeopleRDD = sc.parallelize(
  <span class="string">""</span><span class="string">"{"</span>name<span class="string">":"</span>Yin<span class="string">","</span>address<span class="string">":{"</span>city<span class="string">":"</span>Columbus<span class="string">","</span>state<span class="string">":"</span>Ohio<span class="string">"}}"</span><span class="string">""</span> :: Nil)
val anotherPeople = sqlContext<span class="class">.read</span><span class="class">.json</span>(anotherPeopleRDD)
</code></pre><h3 id="Hive表">Hive表</h3><p>需要使用HiveContext。</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.hive</span><span class="class">.HiveContext</span>(sc)

sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"</span>)</span></span>
sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)</span></span>

<span class="comment">// Queries are expressed in HiveQL</span>
sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"FROM src SELECT key, value"</span>)</span></span>.<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>
</code></pre><h3 id="JDBC">JDBC</h3><p>直接使用load方法加载：</p>
<pre><code>sqlContext<span class="built_in">.</span>load(<span class="string">"jdbc"</span>, <span class="built_in">Map</span>(<span class="string">"url"</span> <span class="subst">-&gt; </span><span class="string">"jdbc:mysql://localhost:3306/your_database?user=your_user&amp;password=your_password"</span>, <span class="string">"dbtable"</span> <span class="subst">-&gt; </span><span class="string">"your_table"</span>))
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[DataFrame是一个以命名列方式组织的分布式数据集。在概念上，它跟关系型数据库中的一张表或者1个Python(或者R)中的data frame一样，但是比他们更优化。DataFrame可以根据结构化的数据文件、hive表、外部数据库或者已经存在的RDD构造 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/tags/spark/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark Streaming编程指南笔记]]></title>
    <link href="http://fangjian0423.github.io/2016/02/10/sparkstreaming-programming-guide/"/>
    <id>http://fangjian0423.github.io/2016/02/10/sparkstreaming-programming-guide/</id>
    <published>2016-02-09T17:36:17.000Z</published>
    <updated>2016-02-09T17:46:12.000Z</updated>
    <content type="html"><![CDATA[<h2 id="概述">概述</h2><p>Spark Streaming是Spark核心API的扩展，用于处理实时数据流。Spark Streaming处理的数据源可以是Kafka，Flume，Twitter，ZeroMQ，Kinesis或者Tcp Sockets，这些数据可以使用map，reduce，join，window方法进行处转换，还可以直接使用Spark内置的机器学习算法，图算法包来处理数据。</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt=""></p>
<p>最终处理后的数据可以存入文件系统，数据库。</p>
<p>Spark Streaming内部接收到实时数据之后，会把数据分成几个批次，这些批次数据会被Spark引擎处理并生成各个批次的结果。</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-flow.png" alt=""></p>
<p>Spark Streaming提供了一个叫做<strong>discretized stream 或者 DStream</strong>的抽象概念，表示一段连续的数据流。DStream会在数据源中的数据流中创建，或者在别的DStream中使用类似map，join方法创建。一个DStream表示一个RDD序列。</p>
<h2 id="一个快速例子">一个快速例子</h2><p>以一个TCP Socket监听接收数据，并计算单词的个数为例子讲解。</p>
<p>首先，需要import Spark Streaming中的一些类和StreamingContext中的一些隐式转换。我们会创建一个带有2个线程，1秒一个批次的StreamingContext。</p>
<pre><code><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>
<span class="keyword">import</span> org.apache.spark.streaming.{<span class="type">Seconds</span>, <span class="type">StreamingContext</span>}

<span class="class"><span class="keyword">object</span> <span class="title">SparkStreamTest</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">App</span> {</span>

  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)

  <span class="comment">// 创建一个StreamingContext，每1秒钟处理一次计算程序</span>
  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))

  <span class="comment">// 使用StreamingContext创建DStream，DStream表示TCP源中的流数据. lines这个DStream表示接收到的服务器数据，每一行都是文本</span>
  <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)

  <span class="comment">// 使用flatMap将每一行中的文本转换成每个单词，并产生一个新的DStream。</span>
  <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))

  <span class="comment">// 使用map方法将每个单词转换成tuple</span>
  <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))

  <span class="comment">// 使用reduceByKey计算出每个单词的出现次数</span>
  <span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)

  wordCounts.print()

  ssc.start() <span class="comment">// 开始计算</span>
  ssc.awaitTermination() <span class="comment">// 等待计算结束</span>
}
</code></pre><p>在运行这段代码之前，首先先起一个netcat服务：</p>
<pre><code>nc -lk <span class="number">9999</span>
</code></pre><p>之后比如输入hello world之后，控制台会打印出如下数据：</p>
<pre><code><span class="code">-------------------------------------------
Time: 1454684570000 ms
-------------------------------------------</span>
(hello,1)
(world,1)
</code></pre><h2 id="基础概念">基础概念</h2><h3 id="Linking(SparkStreaming的连接)">Linking(SparkStreaming的连接)</h3><p>写Spark Streaming程序需要一些依赖。使用maven的话加入以下依赖：</p>
<pre><code><span class="tag">&lt;<span class="title">dependency</span>&gt;</span>
    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span>
    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-streaming_2.10<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span>
    <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.6.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span>
<span class="tag">&lt;/<span class="title">dependency</span>&gt;</span>
</code></pre><p>使用sbt的话，加入以下依赖：</p>
<pre><code><span class="title">libraryDependencies</span> += <span class="string">"org.apache.spark"</span> % <span class="string">"spark-streaming_2.10"</span> % <span class="string">"1.6.0"</span>
</code></pre><p>SparkStreaming核心不提供一些数据源的依赖，需要手动添加，一些数据源对应的Artifact如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">数据源</th>
<th style="text-align:center">Artifact</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Kafka</td>
<td style="text-align:center">spark-streaming-kafka_2.10</td>
</tr>
<tr>
<td style="text-align:center">Flume</td>
<td style="text-align:center">spark-streaming-flume_2.10</td>
</tr>
<tr>
<td style="text-align:center">Kinesis</td>
<td style="text-align:center">spark-streaming-kinesis-asl_2.10 [Amazon Software License]</td>
</tr>
<tr>
<td style="text-align:center">Twitter</td>
<td style="text-align:center">spark-streaming-twitter_2.10</td>
</tr>
<tr>
<td style="text-align:center">ZeroMQ</td>
<td style="text-align:center">spark-streaming-zeromq_2.10</td>
</tr>
<tr>
<td style="text-align:center">MQTT</td>
<td style="text-align:center">spark-streaming-mqtt_2.10</td>
</tr>
</tbody>
</table>
<h3 id="StreamingContext的初始化">StreamingContext的初始化</h3><p>StreamingContext的创建是Spark Streaming程序中最重要的一环。</p>
<p>可以根据SparkConf对象创建出StreamingContext对象：</p>
<pre><code>import org<span class="class">.apache</span><span class="class">.spark</span>._
import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.streaming</span>._

val conf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(appName)</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(master)</span></span>
val ssc = new <span class="function"><span class="title">StreamingContext</span><span class="params">(conf, Seconds(<span class="number">1</span>)</span></span>)
</code></pre><p>appName参数是应用程序的名字，在cluster UI中显示的就是这个名字。master参数的意义跟spark中master参数的意义是一样的。</p>
<p>StreamingContext内部会创建SparkContext，可以使用StreamingContext内部的sparkContext获得。</p>
<pre><code>ssc<span class="class">.sparkContext</span> <span class="comment">// 得到SparkContext</span>
</code></pre><p>StreamingContext也可以根据SparkContext创建：</p>
<pre><code>import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.streaming</span>._

val sc = ...                <span class="comment">// existing SparkContext</span>
val ssc = new <span class="function"><span class="title">StreamingContext</span><span class="params">(sc, Seconds(<span class="number">1</span>)</span></span>)
</code></pre><p>StreamingContext创建之后，可以做以下几点：</p>
<p>1.创建DStreams定义数据源<br>2.使用DStreams的transformation和output operations用于计算<br>3.使用streamingContext的start方法接收数据<br>4.使用streamingContext的awaitTermination方法等待处理结果<br>5.可以使用streamingContext的stop方法停止程序</p>
<p>一些需要注意的点：</p>
<p>1.context开始启动之后，一些streaming的计算不允许发生<br>2.context停掉之后不能重启<br>3.一个JVM在同一时刻只能有一个StreamingContext可以激活<br>4.StreamingContext中的stop方法内部也会stop SparkContext。如果只想stop StreamingContext，那么调用stop方法的时候参数设置为false<br>5.一个SparkContext可以用来创建多个StreamingContexts，只要上一个StreamingContext在下一个StreamingContext创建之前停掉</p>
<h3 id="Discretized_Streams_(DStreams)">Discretized Streams (DStreams)</h3><p>DStreams和Discretized Streams在Spark Streaming中代表相同的意思：</p>
<p>1.一段连续的数据流<br>2.数据源中接收到的数据流<br>3.使用transforming处理过的流数据</p>
<p>Spark内部一个DStream表示一段连续的RDD。DStream中每段RDD表示一段时间内的RDD，效果如下：</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream.png" alt=""></p>
<p>DStream可以使用一些transformation操作将内部的RDD转换成另外一种RDD。比如之前的一个单词统计例子中就将一行文本的DStream转换成每个单词的DStream，过程如下：</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream-ops.png" alt=""></p>
<h3 id="Input_DStreams_and_Receivers(数据源和接收器)">Input DStreams and Receivers(数据源和接收器)</h3><p>Input DStreams是DStreams从streaming source中接收到的输入流数据。在之前分析的一个单词统计例子中，lines就是个Input DStream，表示接收到的服务器数据，每一行都是文本。</p>
<p>每一个Input DStream(除了file stream)都会关联一个Receiver对象，这个Receiver对象的作用是接收数据源中的数据并存储在内存中。</p>
<p>Spark Streaming提供了两种类型的内置数据源：</p>
<p>1.基础数据源。可以直接使用StreamingContext的API，比如文件系统，socket连接，Akka。<br>2.高级数据源。比如Flume，Kafka，Kinesis，Twitter等可以使用工具类的数据源。使用这些数据源需要对应的依赖，在Linking章节中已经介绍过。</p>
<p>如果想在streaming程序中并行地接收多个数据源，需要创建多个Input DStream，有个多个Input DStream的话那就会对应地有多个Receiver。但是需要记住的是，Spark的worker/executor模式是一个相当耗时的任务，因此服务器的配置需要够好才能支撑多个Input DStream。</p>
<p>一些注意点：</p>
<p>1.当本地跑Spark Streaming程序的时候，不要使用”local”或者”local[1]”设置master URL。因为这两种master URL只会使用1个线程。当使用比如Flume，Kafka，socket这些数据源的时候，因为只有一个线程跑receiver接收数据，那么没有其他线程去处理接收后的数据了。所以，当在本地跑Spark Streaming程序的时候，需要将master URL设置为local[n]，n需要大于receiver的个数。<br>2.服务器的核数需要大于receiver的个数。否则程序只会接收数据，而不会处理数据。</p>
<h4 id="Basic_Sources(基础数据源)">Basic Sources(基础数据源)</h4><p>基础数据源刚刚分析过，StreamingContext的API可以使用如文件系统，socket连接，Akka作为输入源。socket连接本文以开始的例子中已经使用过了。</p>
<p>文件系统的输入源会读取文件或任何支持HDFS API(比如HDFS，S3，NFS)的文件系统的数据：</p>
<pre><code>streamingContext.fileStream[<span class="link_label">KeyClass, ValueClass, InputFormatClass</span>](<span class="link_url">dataDirectory</span>)
</code></pre><p>Spark Streaming会监测dataDirectory目录并且会处理这个目录中新创建的文件(老文件写新数据的话不会被支持)。使用文件数据源还需要这几点：</p>
<p>1.所有文件的数据格式必须相同<br>2.dataDirectory目录中的文件必须是新创建的，也可以是从别的目录move进来的<br>3.文件内部的数据更改之后，新更改的数据不会被处理</p>
<p>对于简单的文件，可以使用streamingContext的textFileStream方法处理。</p>
<h4 id="Advanced_Sources(高级数据源)">Advanced Sources(高级数据源)</h4><p>高级数据源需要一些非Spark依赖。Spark Streaming把创建DStream的API移到了各自的API里。如果想创建一个使用Twitter的数据源，需要做以下三步：</p>
<p>1.添加对应的Twitter依赖spark-streaming-twitter_2.10到项目里<br>2.import这个类TwitterUtils，使用TwitterUtils.createStream创建DStream<br>3.部署</p>
<h4 id="Custom_Sources(自定义数据源)">Custom Sources(自定义数据源)</h4><p>要实现一个自定义的数据源，需要实现一个自定义的receiver</p>
<h4 id="Receiver_Reliability(接收器的可靠性)">Receiver Reliability(接收器的可靠性)</h4><p>基于可靠性的数据源分为两种。</p>
<p>1.可靠的接收器(Reliable Receiver)：一个可靠的接收器接收到数据之后会给数据源发送消息表示自己已经接收到数据<br>2.不可靠的接收器(Unreliable Receiver)：一个不可靠的接收器不会发送消息给数据源。</p>
<p>想写出一个可靠的接收器可以参考 <a href="http://spark.apache.org/docs/latest/streaming-custom-receivers.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/streaming-custom-receivers.html</a></p>
<h3 id="DStreams的Transformations操作">DStreams的Transformations操作</h3><p>DStream的Transformations操作跟RDD的Transformations操作类似，</p>
<table>
<thead>
<tr>
<th style="text-align:center">Transformation</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">map(func)</td>
<td style="text-align:center">根据func函数生成一个新的DStream</td>
</tr>
<tr>
<td style="text-align:center">flatMap(func)</td>
<td style="text-align:center">跟map方法类似，但是每一项可以返回多个值。func函数的返回值是一个集合</td>
</tr>
<tr>
<td style="text-align:center">filter(func)</td>
<td style="text-align:center">根据func函数返回true的数据集</td>
</tr>
<tr>
<td style="text-align:center">repartition(numPartitions)</td>
<td style="text-align:center">重新给 DStream 分区</td>
</tr>
<tr>
<td style="text-align:center">union(otherStream)</td>
<td style="text-align:center">取2个DStream的并集，得到一个新的DStream</td>
</tr>
<tr>
<td style="text-align:center">count()</td>
<td style="text-align:center">返回一个新的只有一个元素的DStream，这个元素就是DStream中的所有RDD的个数</td>
</tr>
<tr>
<td style="text-align:center">reduce(func)</td>
<td style="text-align:center">返回一个新的只有一个元素的DStream，这个元素就是DStream中的所有RDD通过func函数聚合得到的结果</td>
</tr>
<tr>
<td style="text-align:center">countByValue()</td>
<td style="text-align:center">如果DStream的类型为K，那么返回一个新的DStream，这个新的DStream中的元素类型是(K, Long)，K是原先DStream的值，Long表示这个Key有多少次</td>
</tr>
<tr>
<td style="text-align:center">reduceByKey(func, [numTasks])</td>
<td style="text-align:center">本文的例子使用过这个方法，对于是键值对(K,V)的DStream，返回一个新的DStream以K为键，各个value使用func函数操作得到的聚合结果为value</td>
</tr>
<tr>
<td style="text-align:center">join(otherStream, [numTasks])</td>
<td style="text-align:center">基于(K, V)键值对的DStream，如果对(K, W)的键值对DStream使用join操作，可以产生(K, (V, W))键值对的DStream</td>
</tr>
<tr>
<td style="text-align:center">cogroup(otherStream, [numTasks])</td>
<td style="text-align:center">跟join方法类似，不过是基于(K, V)的DStream，cogroup基于(K, W)的DStream，产生(K, (Seq[V], Seq[W]))的DStream</td>
</tr>
<tr>
<td style="text-align:center">transform(func)</td>
<td style="text-align:center">基于DStream中的每个RDD调用func函数，func函数的参数是个RDD，返回值也是个RDD</td>
</tr>
<tr>
<td style="text-align:center">updateStateByKey(func)</td>
<td style="text-align:center">对于每个key都会调用func函数处理先前的状态和所有新的状态。比如就可以用来做累加，这个方法跟reduceByKey类似，但比它更加灵活</td>
</tr>
</tbody>
</table>
<h4 id="UpdateStateByKey操作">UpdateStateByKey操作</h4><p>使用UpdateStateByKey方法需要做以下两步：</p>
<p>1.定义状态：状态可以是任意的数据类型<br>2.定义状态更新函数：这个函数需要根据输入流把先前的状态和所有新的状态</p>
<p>不管有没有新数据进来，在每个批次中，Spark都会对所有存在的key调用func方法，如果func函数返回None，那么key-value键值对不会被处理。</p>
<p>以一个例子来讲解updateStateByKey方法，这个例子会统计每个单词的个数在一个文本输入流里：</p>
<p>runningCount是一个状态并且是Int类型，所以这个状态的类型是Int，runningCount是先前的状态，newValues是所有新的状态，是一个集合，函数如下：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span><span class="params">(newValues: Seq[Int], runningCount: Option[Int])</span>:</span> Option[Int] = {
    val newCount = ...  // add the new values <span class="keyword">with</span> the previous running count to get the new count
    Some(newCount)
}
</code></pre><p>updateStateByKey方法的调用：</p>
<pre><code>val runningCounts = pairs.updateStateByKey[<span class="link_label">Int</span>](<span class="link_url">updateFunction _</span>)
</code></pre><h4 id="Transform操作">Transform操作</h4><p>Transform操作针对的是RDD-RDD的操作，所以可以用来处理那些没有在DStream API中暴露的处理任意的RDD操作。比如在DStream中的每次批次没有join rdd的API，所以可以使用transform操作：</p>
<pre><code><span class="variable"><span class="keyword">val</span> spamInfoRDD</span> = ssc.sparkContext.newAPIHadoopRDD(...) <span class="comment">// RDD containing spam information</span>

<span class="variable"><span class="keyword">val</span> cleanedDStream</span> = wordCounts.transform(rdd =&gt; {
  rdd.join(spamInfoRDD).filter(...) <span class="comment">// join data stream with spam information to do data cleaning</span>
  ...
})
</code></pre><h4 id="Window操作">Window操作</h4><p>window操作效果图如下图所示，把几个批次的DStream合并成一个DStream：</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream-window.png" alt=""></p>
<p>每个window操作都需要2个参数：</p>
<p>1.window length。每个window对应的批次数(上图中是3，time1-time3是一个window, time3-time5也是一个window)<br>2.sliding interval。每个window之间的间隔时间，上图下方的window1，window3，window5的间隔。上图这个值为2</p>
<p>这两个参数必须是批次间隔的倍数。上个批次间隔值为1。</p>
<p>以1个例子来讲解window操作，基于本文一开始的那个例子，生成最后30秒的数据，每10秒为单位，这里就需要使用reduceByKeyAndWindow方法：</p>
<pre><code>val windowedWordCounts = pairs.<span class="function"><span class="title">reduceByKeyAndWindow</span><span class="params">((a:Int,b:Int)</span></span> =&gt; (<span class="tag">a</span> + b), <span class="function"><span class="title">Seconds</span><span class="params">(<span class="number">30</span>)</span></span>, <span class="function"><span class="title">Seconds</span><span class="params">(<span class="number">10</span>)</span></span>)
</code></pre><p>其他的一些window操作：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Transformation</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">window(windowLength, slideInterval)</td>
<td style="text-align:center">根据window操作的2个参数得到新的DStream</td>
</tr>
<tr>
<td style="text-align:center">countByWindow(windowLength, slideInterval)</td>
<td style="text-align:center">基于window操作的count操作</td>
</tr>
<tr>
<td style="text-align:center">reduceByWindow(func, windowLength, slideInterval)</td>
<td style="text-align:center">基于window操作的reduce操作</td>
</tr>
<tr>
<td style="text-align:center">reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</td>
<td style="text-align:center">基于window操作的reduceByKey操作</td>
</tr>
<tr>
<td style="text-align:center">reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</td>
<td style="text-align:center">跟reduceByKeyAndWindow方法类似，更有效率，invFunc方法跟func方法的参数返回值一样，表示从window离开的数据</td>
</tr>
<tr>
<td style="text-align:center">countByValueAndWindow(windowLength, slideInterval, [numTasks])</td>
<td style="text-align:center">基于window操作的countByValue操作</td>
</tr>
</tbody>
</table>
<h4 id="Join操作">Join操作</h4><p>DStream可以很容易地join其他DStream：</p>
<pre><code><span class="label">val</span> <span class="keyword">stream1: </span>DStream[<span class="keyword">String, </span><span class="keyword">String] </span>= ...
<span class="label">val</span> <span class="keyword">stream2: </span>DStream[<span class="keyword">String, </span><span class="keyword">String] </span>= ...
<span class="label">val</span> joinedStream = <span class="keyword">stream1.join(stream2)</span>
</code></pre><p>还可以使用leftOuterJoin，rightOuterJoin，fullOuterJoin等。同样地，也可以在window操作后的DStream中使用join：</p>
<pre><code>val windowedStream1 = stream1.<span class="function"><span class="title">window</span><span class="params">(Seconds(<span class="number">20</span>)</span></span>)
val windowedStream2 = stream2.<span class="function"><span class="title">window</span><span class="params">(Minutes(<span class="number">1</span>)</span></span>)
val joinedStream = windowedStream1.<span class="function"><span class="title">join</span><span class="params">(windowedStream2)</span></span>
</code></pre><p>基于rdd的join：</p>
<pre><code><span class="variable"><span class="keyword">val</span> dataset</span>: RDD[String, String] = ...
<span class="variable"><span class="keyword">val</span> windowedStream</span> = stream.window(Seconds(<span class="number">20</span>))...
<span class="variable"><span class="keyword">val</span> joinedStream</span> = windowedStream.transform { rdd =&gt; rdd.join(dataset) }
</code></pre><h3 id="DStream的输出操作">DStream的输出操作</h3><p>输出操作允许DStream中的数据输出到外部系统，比如像数据库、文件系统等。</p>
<table>
<thead>
<tr>
<th style="text-align:center">输出操作</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">print()</td>
<td style="text-align:center">打印出DStream中每个批次的前10条数据</td>
</tr>
<tr>
<td style="text-align:center">saveAsTextFiles(prefix, [suffix])</td>
<td style="text-align:center">把DStream中的数据保存到文本文件里。每次批次的文件名根据参数prefix和suffix生成：”prefix-TIME_IN_MS[.suffix]”</td>
</tr>
<tr>
<td style="text-align:center">saveAsObjectFiles(prefix, [suffix])</td>
<td style="text-align:center">把DStream中的数据按照Java序列化的方式保存Sequence文件里，文件名规则跟saveAsTextFiles方法一样</td>
</tr>
<tr>
<td style="text-align:center">saveAsHadoopFiles(prefix, [suffix])</td>
<td style="text-align:center">把DStream中的数据保存到Hadoop文件里，文件名规则跟saveAsTextFiles方法一样</td>
</tr>
<tr>
<td style="text-align:center">foreachRDD(func)</td>
<td style="text-align:center">遍历DStream中的每段RDD，遍历的过程中可以将RDD中的数据保存到外部系统中</td>
</tr>
</tbody>
</table>
<h4 id="foreachRDD中的设计模式">foreachRDD中的设计模式</h4><p>foreachRDD方法会遍历DStream中的每段RDD，遍历的过程中可以将RDD中的数据保存到外部系统中。这个方法很实用，所以理解foreachRDD方法显得很重要。</p>
<p>将数据写到外部系统通常都需要一个connection对象，所以很多时候都会不经意地创建这个connection对象：</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  val connection = createNewConnection()  <span class="comment">// executed at the driver</span>
  rdd.<span class="keyword">foreach</span> { record =&gt;
    connection.send(record) <span class="comment">// executed at the worker</span>
  }
}
</code></pre><p>这种写法是不正确的。这里connection需要被序列化并且发送到worker，而且connection对象会跨机器传递，会发生序列化错误(connection对象是不可序列化的)，初始化错误(connection对象需要在worker中初始化)。这个错误的解决方案就是在worker中创建connection对象。</p>
<p>为每条记录创建connection也是一个很常见的错误：</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  rdd.foreach { record =&gt;
    <span class="keyword">val</span> connection = createNewConnection<span class="literal">()</span>
    connection.send(record)
    connection.close<span class="literal">()</span>
  }
}
</code></pre><p>因为创建connection对象是一种很耗资源，很耗时间的操作。对于每条数据都创建一个connection代驾更大。所有可以使用rdd.foreachPartition方法，这个方法会创建单一的connection并且在一个RDD分区中所有数据都使用这个connection：</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  rdd.foreachPartition { partitionOfRecords =&gt;
    <span class="keyword">val</span> connection = createNewConnection<span class="literal">()</span>
    partitionOfRecords.foreach(record =&gt; connection.send(record))
    connection.close<span class="literal">()</span>
  }
}
</code></pre><p>一种更好的方式就是使用ConnectionPool，ConnectionPool可以重用connection对象在多个批次和RDD中。</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  rdd.foreachPartition { partitionOfRecords =&gt;
    <span class="comment">// ConnectionPool is a static, lazily initialized pool of connections</span>
    val connection = ConnectionPool.getConnection()
    partitionOfRecords.<span class="keyword">foreach</span>(record =&gt; connection.send(record))
    ConnectionPool.returnConnection(connection)  <span class="comment">// return to the pool for future reuse</span>
  }
}
</code></pre><p>其他需要注意的点：</p>
<p>1.DStream的输出操作也是延迟执行的，就像RDD的action操作一样。RDD的action操作在DStream的输出操作内部执行的话会强制Spark Streaming执行。如果程序里没有任何输出操作，或者有比如像dstream.foreachRDD操作一样内部没有rdd的action操作的话，这样就不会执行任意操作，会被Spark忽略。<br>2.默认情况下，在一个时间点下，只有一个输出操作被执行。它们是根据程序里的编写顺序执行的。</p>
<h3 id="DataFrame_and_SQL_Operations">DataFrame and SQL Operations</h3><p>在Spark Streaming中可以使用DataFrames and SQL操作。</p>
<pre><code><span class="comment">/** DataFrame operations inside your streaming program */</span>

<span class="variable"><span class="keyword">val</span> words</span>: DStream[String] = ...

words.foreachRDD { rdd =&gt;

  <span class="comment">// Get the singleton instance of SQLContext</span>
  <span class="variable"><span class="keyword">val</span> sqlContext</span> = SQLContext.getOrCreate(rdd.sparkContext)
  <span class="keyword">import</span> sqlContext.implicits._

  <span class="comment">// Convert RDD[String] to DataFrame</span>
  <span class="variable"><span class="keyword">val</span> wordsDataFrame</span> = rdd.toDF(<span class="string">"word"</span>)

  <span class="comment">// Register as table</span>
  wordsDataFrame.registerTempTable(<span class="string">"words"</span>)

  <span class="comment">// Do word count on DataFrame using SQL and print it</span>
  <span class="variable"><span class="keyword">val</span> wordCountsDataFrame</span> = 
    sqlContext.sql(<span class="string">"select word, count(*) as total from words group by word"</span>)
  wordCountsDataFrame.show()
}
</code></pre><h3 id="Caching_/_Persistence">Caching / Persistence</h3><p>跟RDD类似，DStream也允许将数据保存到内存中，使用persist方法可以做到这一点。</p>
<p>但是基于window和state的操作，reduceByWindow,reduceByKeyAndWindow,updateStateByKey它们就是隐式的保存了，系统已经帮它自动保存了。</p>
<p>从网络接收的数据(比如Kafka, Flume, sockets等)，默认是保存在两个节点来实现容错性，以序列化的方式保存在内存当中。</p>
<h3 id="Checkpointing">Checkpointing</h3><p>一个Spark Streaming程序必须是全天工作的，所以如果万一系统挂掉了或者JVM挂掉之后是要有容错性的。Spark Streaming需在容错存储系统做checkpoint，这样才能够处理错误信息。有两种类型的数据需要做checkpoint：</p>
<p>1.metadata checkpointing：元数据检查点。主要包括3个元数据：<br>配置：创建streaming程序的的配置信息<br>DStream操作：streaming程序中DStream的操作集合<br>未完成的批次：在队列中未完成的批次<br>2.data checkpointing：数据检查点。保存已经生成的RDD数据。在一些有状态的transformation操作中，一些RDD数据会依赖之前批次的RDD数据，随时时间的推移，这种依赖情况就会越发严重。为了解决这个问题，需要保存这些有依赖关系的RDD数据到存储系统中(比如HDFS)来剪断这种依赖关系</p>
<p>什么时候需要启用checkpoint？</p>
<p>满足以下2个条件中的任意1个即可启用checkpoint:</p>
<p>1.使用了有状态的transformation。比如使用了updateStateByKey或reduceByKeyAndWindow方法后，就需要启用checkpoint<br>2.恢复挂掉的程序。可以根据metadata数据恢复程序</p>
<p>一些比较简单的streaming程序没有用到有状态的transformation，并且也可以接受程序挂掉之后丢失部分数据，那么就没有必要启用checkpoint。</p>
<p>如何配置checkpoint？</p>
<p>checkpoint的配置需要设置一个目录，使用streamingContext.checkpoint(checkpointDirectory)方法。</p>
<pre><code>// Function to <span class="operator"><span class="keyword">create</span> <span class="keyword">and</span> setup a <span class="keyword">new</span> StreamingContext
<span class="keyword">def</span> functionToCreateContext(): StreamingContext = {
    val ssc = <span class="keyword">new</span> StreamingContext(...)   // <span class="keyword">new</span> <span class="keyword">context</span>
    val <span class="keyword">lines</span> = ssc.socketTextStream(...) // <span class="keyword">create</span> DStreams
    ...
    ssc.checkpoint(checkpointDirectory)   // <span class="keyword">set</span> checkpoint <span class="keyword">directory</span>
    ssc
}

// <span class="keyword">Get</span> StreamingContext <span class="keyword">from</span> checkpoint <span class="keyword">data</span> <span class="keyword">or</span> <span class="keyword">create</span> a <span class="keyword">new</span> one
val <span class="keyword">context</span> = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _)

// <span class="keyword">Do</span> additional setup <span class="keyword">on</span> <span class="keyword">context</span> that needs <span class="keyword">to</span> be done,
// irrespective <span class="keyword">of</span> whether it <span class="keyword">is</span> being started <span class="keyword">or</span> restarted
<span class="keyword">context</span>. ...

// <span class="keyword">Start</span> the <span class="keyword">context</span>
<span class="keyword">context</span>.<span class="keyword">start</span>()
<span class="keyword">context</span>.awaitTermination()</span>
</code></pre><p>因为检查操作会导致保存到hdfs上的开销，所以设置这个时间间隔，要很慎重。对于小批次的数据，比如一秒的，检查操作会大大降低吞吐量。但是检查的间隔太长，会导致任务变大。通常来说，5-10秒的检查间隔时间是比较合适的。</p>
]]></content>
    <summary type="html">
    <![CDATA[Spark Streaming是Spark核心API的扩展，用于处理实时数据流。Spark Streaming处理的数据源可以是Kafka，Flume，Twitter，ZeroMQ，Kinesis或者Tcp Sockets ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/tags/spark/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark编程指南笔记]]></title>
    <link href="http://fangjian0423.github.io/2016/01/27/spark-programming-guide/"/>
    <id>http://fangjian0423.github.io/2016/01/27/spark-programming-guide/</id>
    <published>2016-01-26T16:22:21.000Z</published>
    <updated>2016-01-26T16:24:07.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Spark初始化">Spark初始化</h2><p>使用Spark编程第一件要做的事就是初始化SparkContext对象，SparkContext对象会告诉Spark如何使用Spark集群。</p>
<p>SparkContext会使用SparkConf中的一些配置信息，所以构造SparkContext对象之前需要构造一个SparkConf对象。</p>
<p>一个JVM上的SparkContext只有一个是激活的，如果要构造一个新的SparkContext，必须stop一个已经激活的SparkContext。</p>
<pre><code>val conf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(<span class="string">"local"</span>)</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(<span class="string">"Test"</span>)</span></span>
</code></pre><p>  val sc = new SparkContext(conf)</p>
<p>appName为Test，这个name会在cluster UI上展示，master是一个Spark，Mesos，YARN cluster URL或者local。 具体的值可以参考 <a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="external">master-url解释</a>。</p>
<h2 id="RDD(Resilient_Distributed_Datasets)">RDD(Resilient Distributed Datasets)</h2><p>Spark提出的最主要抽象概念是RDD(弹性分布式数据集)，它是一个有容错机制并且可以被并行操作的元素集合。</p>
<p>有两种方式可以创建RDD:</p>
<p>1.使用一个已存在的集合进行并行计算<br>2.使用外部数据集，比如共享的文件系统，HDFS，HBase以及任何支持Hadoop InputFormat的数据源</p>
<h3 id="并行集合">并行集合</h3><p>使用SparkContext的parallelize方法构造并行集合。</p>
<pre><code>val dataSet = <span class="function"><span class="title">Array</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span></span>
val rdd = sc.<span class="function"><span class="title">parallelize</span><span class="params">(dataSet)</span></span>
rdd.<span class="function"><span class="title">reduce</span><span class="params">(_ + _)</span></span> <span class="comment">// 15</span>
</code></pre><p>parallelize方法有一个参数slices，表示数据集切分的份数。Spark会在集群上为每一个分片起一个任务。如果不设置的话，Spark会根据集群的情况，自动设置slices的数字。</p>
<h3 id="外部数据集">外部数据集</h3><p>文本文件可以使用SparkContext的textFile方法构造RDD。这个方法接收一个URI参数(也可以包括本地的文件)，并且以每行的方式读取文件内容。</p>
<p>比如data.txt文件里一行一个数字，取所有数字的和：</p>
<pre><code>val rdd = sc.textFile(this.getClass.getResource(<span class="string">"/data.txt"</span>).<span class="built_in">toString</span>)

rdd.<span class="built_in">reduce</span> {
   (a, b) =&gt; (a.toInt + b.toInt).<span class="built_in">toString</span>
}

<span class="comment">// 另外一种方式</span>
rdd.<span class="built_in">map</span>(s =&gt; s.toInt).<span class="built_in">reduce</span>(<span class="number">_</span> + <span class="number">_</span>)
</code></pre><p>Spark中所有基于文件的输入方法，都支持目录，压缩文件，通配符读取文件。比如</p>
<pre><code>sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"/data/*.txt"</span>)</span></span>
sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"/data"</span>)</span></span>
</code></pre><p>textFile方法也可以传入第二个可选参数来控制文件的分片数量。默认情况下，Spark会为文件的每一个块（在HDFS中块的大小默认是64MB）创建一个分片。但是你也可以通过传入一个更大的值来要求Spark建立更多的分片。注意，分片的数量绝不能小于文件块的数量。</p>
<p>除了文本文件之外，Spark还支持其他格式的输入：</p>
<p>1.SparkContext的wholeTextFiles方法会读取一个包含很多小文件的目录，并以filename，content为键值对的方式返回结果。<br>2.对于SequenceFiles，可以使用SparkContext的sequenceFile[K, V]方法创建。像 IntWritable和Text一样，它们必须是 Hadoop 的 Writable 接口的子类。另外，对于几种通用 Writable 类型，Spark 允许你指定原生类型来替代。例如：sequencFile[Int, String] 将会自动读取 IntWritable 和 Texts。<br>3.对于其他类型的 Hadoop 输入格式，你可以使用 SparkContext.hadoopRDD 方法，它可以接收任意类型的 JobConf 和输入格式类，键类型和值类型。按照像 Hadoop 作业一样的方法设置输入源就可以了。<br>4.RDD.saveAsObjectFile 和 SparkContext.objectFile 提供了以 Java 序列化的简单方式来保存 RDD。虽然这种方式没有 Avro 高效，但也是一种简单的方式来保存任意的 RDD。</p>
<h2 id="RDD操作">RDD操作</h2><h3 id="RDD操作基础">RDD操作基础</h3><p>RDD支持两种类型的操作。</p>
<p>1.transformations。从一个数据集产生一个新的数据集。比如map方法，就可以根据旧的数据集产生新的数据集。<br>2.actions。在一个数据集中进行聚合操作，并且返回一个最终的结果。</p>
<p>Spark中所有的transformations操作都是lazy的，就是说它们并不会立刻真的计算出结果。相反，它们仅仅是记录下了转换操作的操作对象（比如：一个文件）。只有当一个启动操作被执行，要向驱动程序返回结果时，转化操作才会真的开始计算。这样的设计使得Spark运行更加高效——比如，我们会发觉由map操作产生的数据集将会在reduce操作中用到，之后仅仅是返回了reduce的最终的结果而不是map产生的庞大数据集。</p>
<p>在默认情况下，每一个由转化操作得到的RDD都会在每次执行启动操作时重新计算生成。但是，你也可以通过调用persist(或cache)方法来将RDD持久化到内存中，这样Spark就可以在下次使用这个数据集时快速获得。Spark同样提供了对将RDD持久化到硬盘上或在多个节点间复制的支持。</p>
<p>一个计算文件中每行的字符串个数和所有字符串个数的和例子：</p>
<pre><code>val rdd = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"data.txt"</span>)</span></span>
val lineLengths = rdd.<span class="function"><span class="title">map</span><span class="params">(s =&gt; s.length)</span></span>
val totalLength = lineLengths.<span class="function"><span class="title">reduce</span><span class="params">(_ + _)</span></span>
</code></pre><p>lineLengths对象是一个transformations结果，所以它不是马上就开始执行的，当运行lineLengths.reduce的时候lineLengths才会开始去计算。如果之后还会用到这个lineLengths。可以在reduce方法之前加上:</p>
<pre><code>lineLengths.<span class="function"><span class="title">persist</span><span class="params">()</span></span>
</code></pre><h3 id="使用函数">使用函数</h3><p>Spark很多方法都可以使用函数完成。</p>
<p>使用对象：</p>
<pre><code><span class="class"><span class="keyword">object</span> <span class="title">SparkFunction</span> {</span>
  <span class="function"><span class="keyword">def</span> <span class="title">strLength</span> =</span> (s: <span class="type">String</span>) =&gt; s.length
}

<span class="keyword">val</span> lineLengths = rdds.map(<span class="type">SparkFunction</span>.strLength)
lineLengths.reduce(_ + _)
</code></pre><p>使用类：</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">SparkCls</span> </span>{
  def <span class="func"><span class="keyword">func</span> = <span class="params">(s: String)</span></span> =&gt; s.length
  def buildRdd(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = rdd.<span class="built_in">map</span>(<span class="func"><span class="keyword">func</span>)
}
<span class="title">new</span> <span class="title">SparkCls</span><span class="params">()</span></span>.buildRdd(rdds).<span class="built_in">reduce</span>(<span class="number">_</span> + <span class="number">_</span>)
</code></pre><h3 id="闭包">闭包</h3><pre><code><span class="tag">var</span> counter = <span class="number">0</span>
<span class="tag">var</span> rdd = sc.<span class="function"><span class="title">parallelize</span><span class="params">(data)</span></span>

<span class="comment">// Wrong: Don't do this!!</span>
rdd.<span class="function"><span class="title">foreach</span><span class="params">(x =&gt; counter += x)</span></span>

<span class="function"><span class="title">println</span><span class="params">(<span class="string">"Counter value: "</span> + counter)</span></span>
</code></pre><p>上述代码如果在local模式并且在一个JVM的情况下使用是可以得到正确的值的。这是因为所有的RDD和变量counter都是同一块内存上。</p>
<p>然后在集群模式下，上述代码的结果可能就不会是我们想要的正确结果。集群模式下，Spark会在RDD分成多个任务，每个任务都会被对应的executor执行。在executor执行之前，Spark会计算每个闭包。上面这个例子foreach方法和counter就组成了一个闭包。这个闭包会被序列化并且发送给每个executor。在local模式下，因为只有一个executor，所以共享相同的闭包。然后在集群模式下，有多个executor，并且各个executor在不同的节点上都有自己的闭包的拷贝。</p>
<p>所以counter变量就已经不再是节点上的变量了。虽然counter变量在内存上依然存在，但是它对于executor已经不可见，executor只知道它是序列化后的闭包的一份拷贝。因此如果counter的操作都是在闭包下的话，counter的值还是为0。</p>
<p>Spark提供了一种Accumulator的概念用来处理集群模式下的变量更新问题。</p>
<p>另外一个要注意的是不要使用foreach或者map方法打印数据。在一台机器上，这个操作是没有问题的。但是如果在集群上，不一定会打印出全部的数据。可以使用collect方法将RDD放到调用节点上。所以rdd.collect().foreach(println)是可以打印出数据的，但是可能数据量过大，会导致OOM。所以最好的方式还是使用take方法：rdd.take(100).foreach(println)。</p>
<h3 id="使用键值对">使用键值对</h3><p>Spark也支持键值对的操作，这在分组和聚合操作时候用得到。当键值对中的键为自定义对象时，需要自定义该对象的equals()和hashCode()方法。</p>
<p>一个使用键值对的单词统计例子：</p>
<pre><code>// 使用map方法将单词文本转换成一个键值对，(word, num)。 num初始值为<span class="number">1</span>
val pairs = rdd.map(s =&gt; (s, <span class="number">1</span>))
val reduceRdd = pairs.reduceByKey(_ + _)
val <span class="literal">result</span> = reduceRdd.sortByKey().collect()
<span class="literal">result</span>.foreach(println)
</code></pre><h3 id="Spark内置的Transformations">Spark内置的Transformations</h3><table>
<thead>
<tr>
<th style="text-align:center">转换</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">map(func)</td>
<td style="text-align:center">根据func函数生成一个新的rdd数据集</td>
</tr>
<tr>
<td style="text-align:center">filter(func)</td>
<td style="text-align:center">根据func函数返回true的数据集</td>
</tr>
<tr>
<td style="text-align:center">flatMap(func)</td>
<td style="text-align:center">跟map方法类似，但是每一项可以返回多个值。func函数的返回值是一个集合</td>
</tr>
<tr>
<td style="text-align:center">mapPartitions(func)</td>
<td style="text-align:center">跟map方法类似，但是是在每个partition上运行的。func函数的参数是一个Iteraror，返回值也是一个Iterator。如果map方法需要创建一个额外的对象，使用mapPartitions方法比map方法高效得多</td>
</tr>
<tr>
<td style="text-align:center">mapPartitionsWithIndex(func)</td>
<td style="text-align:center">作用跟mapPartitions方法一样，只是func方法多了一个index参数。 func方法定义 (Int, Iterator[T]) =&gt; Iterator[U]</td>
</tr>
<tr>
<td style="text-align:center">sample(withReplacement, fraction, seed)</td>
<td style="text-align:center">根据 fraction 指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed 用于指定随机数生成器种子</td>
</tr>
<tr>
<td style="text-align:center">union(otherDataset)</td>
<td style="text-align:center">取2个rdd的并集，得到一个新的rdd</td>
</tr>
<tr>
<td style="text-align:center">intersection(otherDataset)</td>
<td style="text-align:center">取2个rdd的交集，得到一个新的rdd。这个新的rdd没有重复的数据</td>
</tr>
<tr>
<td style="text-align:center">distinct([numTasks])</td>
<td style="text-align:center">返回一个新的没有重复数据的数据集</td>
</tr>
<tr>
<td style="text-align:center">groupByKey([numTasks])</td>
<td style="text-align:center">将一个(K,V)的键值对RDD转换成一个(K, Iterable[V])的新的键值对RDD。注意点：如果group的目的是为了做聚合计算(比如总和或者平均值)，使用reduceByKey或者aggregateByKey性能更好。</td>
</tr>
<tr>
<td style="text-align:center">reduceByKey(func, [numTasks])</td>
<td style="text-align:center">跟groupByKey方法一样，也是操作(K, V)的键值对RDD。返回值同样是一个(K, V)的键值对RDD，func函数的定义：(V, V) =&gt; V，也就是每两个值的值</td>
</tr>
<tr>
<td style="text-align:center">aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</td>
<td style="text-align:center">跟reduceByKey作用类似，zeroValue参数表示初始值，这个初始值的类型可以跟rdd中的键值对的值的类型不同。seqOp参数是个函数，定义为(U, V) =&gt; U，U类型是初始化zeroValue的类型，V类型是一开始rdd的键值对的值的类型。这个函数表示用来与初始值zeroValue进行比较，取一个新的值，需要注意的是这个新的值会作为参数出现在下一次key相等的情况下。 combOp参数也是个函数，定义为(U, U) =&gt; U，U类型也是初始值的类型。这个函数相当于reduce方法中的函数，用来做聚合操作</td>
</tr>
<tr>
<td style="text-align:center">sortByKey([ascending], [numTasks])</td>
<td style="text-align:center">对一个(K, V)键值对的RDD进行排行，返回一个基于K排序的新的RDD</td>
</tr>
<tr>
<td style="text-align:center">join(otherDataset, [numTasks])</td>
<td style="text-align:center">基于(K, V)键值对的rdd，如果对(K, W)的键值对rdd使用join操作，可以产生(K, (V, W))键值对的rdd。类似数据库中的join操作，spark还提供leftOuterJoin, rightOuterJoin, fullOuterJoin方法</td>
</tr>
<tr>
<td style="text-align:center">cogroup(otherDataset, [numTasks])</td>
<td style="text-align:center">跟join方法类似，不过是基于(K, V)的rdd，cogroup基于(K, W)的rdd，产生(K, (Iterable[V], Iterable[W]))的rdd。这个方法也叫做groupWith</td>
</tr>
<tr>
<td style="text-align:center">cartesian(otherDataset)</td>
<td style="text-align:center">笛卡尔积。 有K的rdd与V的rdd进行笛卡尔积，会生成(K, V)的rdd</td>
</tr>
<tr>
<td style="text-align:center">pipe(command, [envVars])</td>
<td style="text-align:center">对rdd进行管道操作。 就像shell命令一样</td>
</tr>
<tr>
<td style="text-align:center">coalesce(numPartitions)</td>
<td style="text-align:center">减少 RDD 的分区数到指定值。在过滤大量数据之后，可以执行此操作</td>
</tr>
<tr>
<td style="text-align:center">repartition(numPartitions)</td>
<td style="text-align:center">重新给 RDD 分区</td>
</tr>
<tr>
<td style="text-align:center">repartitionAndSortWithinPartitions(partitioner)</td>
<td style="text-align:center">重新给 RDD 分区，并且每个分区内以记录的 key 排序</td>
</tr>
</tbody>
</table>
<p>以这个数据为例：</p>
<pre><code><span class="literal">i</span>
am
<span class="keyword">format</span>
let
<span class="keyword">us</span>
go
hoho
good
nice
<span class="keyword">format</span>
is
nice
haha
haha
haha
<span class="keyword">scala</span> is cool, nice
</code></pre><p>一些Transformations操作：</p>
<pre><code>rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; s<span class="built_in">.</span>length) <span class="comment">// 每一行文本转换成长度</span>
rdd<span class="built_in">.</span>filter(s =&gt; s<span class="built_in">.</span>length == <span class="number">1</span>) <span class="comment">// 取文本长度为1的数据</span>
rdd<span class="built_in">.</span>flatMap(s =&gt; s<span class="built_in">.</span>split(<span class="string">","</span>)) <span class="comment">// 把有 , 的字符串转换成多行</span>
rdd<span class="built_in">.</span>sample(<span class="literal">false</span>, <span class="number">1.0</span>)
rdd<span class="built_in">.</span>union(rdd2)
rdd<span class="built_in">.</span>intersection(rdd2)
rdd<span class="built_in">.</span>distinct(rdd2)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>groupByKey()<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    print(<span class="built_in">pair</span><span class="built_in">.</span>_1) ; print(<span class="string">" ** "</span>)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
  }
}
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>reduceByKey {
  (x, y) =&gt; x + y
}<span class="built_in">.</span>foreach {
  (<span class="built_in">pair</span>) =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>aggregateByKey(<span class="number">0</span>)(
  (a, b) =&gt; {
    math<span class="built_in">.</span><span class="keyword">max</span>(a, b)
  }, (a, b) =&gt; {
    a + b
  }
)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>sortByKey(<span class="literal">true</span>)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span><span class="keyword">join</span>(rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>charAt(<span class="number">0</span>)<span class="built_in">.</span>toUpper<span class="built_in">.</span>toString)))<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>cogroup(rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>charAt(<span class="number">0</span>)<span class="built_in">.</span>toUpper<span class="built_in">.</span>toString)))<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">"======"</span>)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_1<span class="built_in">.</span>toList<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_2<span class="built_in">.</span>toList<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
    println(<span class="string">"**"</span> * <span class="number">8</span>)
  }
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; s<span class="built_in">.</span>length)<span class="built_in">.</span>cartesian(rdd)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    println(<span class="built_in">pair</span><span class="built_in">.</span>_1)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2)
    println(<span class="string">"**"</span> * <span class="number">8</span>)
  }
}
</code></pre><h3 id="Spark内置的Actions">Spark内置的Actions</h3><table>
<thead>
<tr>
<th style="text-align:center">动作</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">reduce(func)</td>
<td style="text-align:center">聚合操作。之前很多例子都使用了reduce方法。这个功能必须可交换且可关联的，从而可以正确的被并行执行。</td>
</tr>
<tr>
<td style="text-align:center">collect()</td>
<td style="text-align:center">返回rdd中所有的元素，返回值类型是Array。这个方法经常用来取数据量比较小的集合</td>
</tr>
<tr>
<td style="text-align:center">count()</td>
<td style="text-align:center">rdd中的元素个数</td>
</tr>
<tr>
<td style="text-align:center">first()</td>
<td style="text-align:center">返回数据集的第一个元素</td>
</tr>
<tr>
<td style="text-align:center">take(n)</td>
<td style="text-align:center">返回数据集的前n个元素，返回值是个Array</td>
</tr>
<tr>
<td style="text-align:center">takeSample(withReplacement, num, [seed])</td>
<td style="text-align:center">返回一个数组，在数据集中随机采样 num 个元素组成，可以选择是否用随机数替换不足的部分，seed 用于指定的随机数生成器种子返回数据集的前n个元素，返回值是个Array</td>
</tr>
<tr>
<td style="text-align:center">takeOrdered(n, [ordering])</td>
<td style="text-align:center">返回自然顺序或者自定义顺序的前 n 个元素</td>
</tr>
<tr>
<td style="text-align:center">saveAsTextFile(path)</td>
<td style="text-align:center">把数据集中的元素写到文件里，可以写到本地文件系统上，hdfs上或者任意Hadoop支持的文件系统上。Spark会调用元素的toString方法将其转换成文本的一行</td>
</tr>
<tr>
<td style="text-align:center">saveAsSequenceFile(path)</td>
<td style="text-align:center">跟saveAsTextFile方法类似，但是是写成SequenceFile文件格式，也是支持写到本地文件系统上，hdfs上或者任意Hadoop支持的文件系统上。这个方法只能作用于键值对的RDD</td>
</tr>
<tr>
<td style="text-align:center">saveAsObjectFile(path)</td>
<td style="text-align:center">跟saveAsTextFile方法类似，是使用Java的序列化的方式保存文件</td>
</tr>
<tr>
<td style="text-align:center">countByKey()</td>
<td style="text-align:center">计算键值的数量。对键值对(K, V)的rdd数据集，返回(K, Int)的Map</td>
</tr>
<tr>
<td style="text-align:center">foreach(func)</td>
<td style="text-align:center">使用func遍历rdd数据集中的各个元素。这通常用于边缘效果，例如更新一个Accumulator，或者和外部存储系统进行交互</td>
</tr>
</tbody>
</table>
<p>一些Actions操作：</p>
<pre><code>rdd<span class="built_in">.</span>saveAsTextFile(<span class="string">"file:///tmp/data01"</span>)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>saveAsSequenceFile(<span class="string">"file:///tmp/data02"</span>)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>countByKey()<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}
</code></pre><h2 id="RDD的持久化(Persistence)">RDD的持久化(Persistence)</h2><p>Spark的一个重要功能就是在将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。当持久化一个RDD的时候，每个存储着这个RDD的分片节点都会计算然后保存到内存中以便下次再次使用。这使得接下来的计算过程速度能够加快（经常能加快超过十倍的速度）。缓存是加快迭代算法和快速交互过程速度的关键工具。</p>
<p>可以使用persist或者cache方法让rdd持久化。在第一次被计算产生之后，它就会始终停留在节点的内存中。Spark的缓存是具有容错性的——如果RDD的任意一个分片丢失了，Spark就会依照这个RDD产生的转化过程自动重算一遍。</p>
<p>另外，每个持久化后的RDD可以使用不用级别的存储级别。比如可以存在硬盘中，可以存在内存中，还可以将这个数据集在节点之间复制，或者使用 Tachyon 将它储存到堆外。这些存储级别都是通过向 persist() 传递一个 StorageLevel 对象（Scala, Java, Python）来设置的。</p>
<p>Spark的一些存储级别如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">存储级别</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">MEMORY_ONLY</td>
<td style="text-align:center">默认级别。将RDD作为反序列化的的对象存储在JVM中。如果不能被内存装下，一些分区将不会被缓存，并且在需要的时候被重新计算</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_AND_DISK</td>
<td style="text-align:center">默认级别。将RDD作为反序列化的的对象存储在JVM中。如果不能被内存装下，会存在硬盘上，并且在需要的时候被重新计算</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_ONLY_SER</td>
<td style="text-align:center">将RDD作为序列化的的对象进行存储（每一分区占用一个字节数组）。通常来说，这比将对象反序列化的空间利用率更高，尤其当使用fast serializer,但在读取时会比较占用CPU</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_AND_DISK_SER</td>
<td style="text-align:center">与MEMORY_ONLY_SER相似，但是把超出内存的分区将存储在硬盘上而不是在每次需要的时候重新计算</td>
</tr>
<tr>
<td style="text-align:center">DISK_ONLY</td>
<td style="text-align:center">只存储RDD分区到硬盘上</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_ONLY_2, MEMORY_AND_DISK_2 等</td>
<td style="text-align:center">与上述的存储级别一样，但是将每一个分区都复制到两个集群结点上</td>
</tr>
</tbody>
</table>
<p>存储级别的选择：</p>
<p>如果你的 RDD 可以很好的与默认的存储级别契合，就不需要做任何修改了。这已经是 CPU 使用效率最高的选项，它使得 RDD的操作尽可能的快。</p>
<p>如果不行，试着使用 MEMORY_ONLY_SER 并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</p>
<p>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</p>
<p>如果你想有快速故障恢复能力，使用复制存储级别。例如：用 Spark 来响应web应用的请求。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在 RDD 上持续的运行任务，而不需要等待丢失的分区被重新计算。</p>
<p>如果你想要定义你自己的存储级别，比如复制因子为3而不是2，可以使用 StorageLevel 单例对象的 apply()方法。</p>
<h2 id="共享变量">共享变量</h2><p>通常情况下，当一个函数在远程集群节点上通过Spark操作(比如map或者reduce)，Spark会对涉及到的变量的所有副本执行这个函数。这些变量都会被拷贝到每台机器上，而且这个过程不会被反馈到驱动程序。通常情况下，在任务之间读写共享变量是很低效的。但是Spark仍然提供了有限的两种共享变量类型用于常见的使用场景：broadcast variables 和 accumulators。</p>
<h3 id="broadcast_variables(广播变量)">broadcast variables(广播变量)</h3><p>广播变量允许程序员在每台机器上保持一个只读变量的缓存而不是将一个变量的拷贝传递给各个任务。这些变量是可以被使用的，比如，给每个节点传递一份大输入数据集的拷贝是很耗时的。Spark试图使用高效的广播算法来分布广播变量，以此来降低通信花销。可以通过SparkContext.broadcast(v)来从变量v创建一个广播变量。这个广播变量是v的一个包装，同时它的值可以调用value方法获得：</p>
<pre><code>val broadcastVar = sc.<span class="function"><span class="title">broadcast</span><span class="params">(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span></span>)

broadcastVar<span class="class">.value</span> <span class="comment">// Array(1, 2, 3)</span>
</code></pre><p>一个广播变量被创建之后，在所有函数中都应当使用它来代替原来的变量v，这样就可以包装v在节点之间只被传递一次。另外，v变量在被广播之后不应该再被修改，这样可以确保每一个节点上储存的广播变量的一致性。</p>
<h3 id="accumulators(累加器)">accumulators(累加器)</h3><p>累加器是在一个相关过程中只能被”累加”的变量，对这个变量的操作可以有效地被并行化。它们可以被用于实现计数器（就像在MapReduce过程中）或求和运算。Spark原生支持对数字类型的累加器，程序员也可以为其他新的类型添加支持。累加器被以一个名字创建之后，会在Spark的UI中显示出来。这有助于了解计算的累进过程（注意：目前Python中不支持这个特性）。</p>
<p>可以通过SparkContext.accumulator(v)来从变量v创建一个累加器。在集群中运行的任务随后可以使用add方法或+=操作符（在Scala和Python中）来向这个累加器中累加值。但是，他们不能读取累加器中的值。只有驱动程序可以读取累加器中的值，通过累加器的value方法。</p>
<p>以下的代码展示了向一个累加器中累加数组元素的过程：</p>
<pre><code>val accum = sc.<span class="function"><span class="title">accumulator</span><span class="params">(<span class="number">0</span>, <span class="string">"My Accumulator"</span>)</span></span>
sc.<span class="function"><span class="title">parallelize</span><span class="params">(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span></span>).<span class="function"><span class="title">foreach</span><span class="params">(x =&gt; accum += x)</span></span>
accum<span class="class">.value</span> <span class="comment">// 10</span>
</code></pre><p>这段代码利用了累加器对Int类型的内建支持，程序员可以通过继承 AccumulatorParam 类来创建自己想要的类型支持。AccumulatorParam 的接口提供了两个方法：zero 用于为你的数据类型提供零值；addInPlace 用于计算两个值得和。比如，假设我们有一个 Vector类表示数学中的向量，我们可以这样写：</p>
<pre><code>object VectorAccumulatorParam extends AccumulatorParam[Vector] {
  <span class="function"><span class="keyword">def</span> <span class="title">zero</span><span class="params">(initialValue: Vector)</span>:</span> Vector = {
    Vector.zeros(initialValue.size)
  }
  <span class="function"><span class="keyword">def</span> <span class="title">addInPlace</span><span class="params">(v1: Vector, v2: Vector)</span>:</span> Vector = {
    v1 += v2
  }
}

// Then, create an Accumulator of this type:
val vecAccum = sc.accumulator(new Vector(...))(VectorAccumulatorParam)
</code></pre><p>累加器的更新操作只会被运行一次，Spark 提供了保证，每个任务中对累加器的更新操作都只会被运行一次。比如，重启一个任务不会再次更新累加器。在转化过程中，用户应该留意每个任务的更新操作在任务或作业重新运算时是否被执行了超过一次。</p>
<p>累加器不会改变Spark的惰性求值模型。如果累加器在对RDD的操作中被更新了，它们的值只会在启动操作中作为 RDD 计算过程中的一部分被更新。所以，在一个懒惰的转化操作中调用累加器的更新，并没法保证会被及时运行。下面的代码段展示了这一点：</p>
<pre><code><span class="title">accum</span> = sc.accumulator(<span class="number">0</span>)
<span class="typedef"><span class="keyword">data</span>.map<span class="container">(<span class="title">lambda</span> <span class="title">x</span> =&gt; <span class="title">acc</span>.<span class="title">add</span>(<span class="title">x</span>)</span>; f<span class="container">(<span class="title">x</span>)</span>)</span>
</code></pre><h2 id="参考资料">参考资料</h2><p><a href="http://spark.apache.org/docs/latest/programming-guide.html/" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[Spark编程指南笔记，参考官方文档的编程指南，翻译再加上一些自己写的代码 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/tags/spark/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[记录Flume使用KafkaSource的时候Channel队列满了之后发生的怪异问题]]></title>
    <link href="http://fangjian0423.github.io/2016/01/19/flume-channel-full-exception/"/>
    <id>http://fangjian0423.github.io/2016/01/19/flume-channel-full-exception/</id>
    <published>2016-01-19T12:07:35.000Z</published>
    <updated>2016-03-04T17:54:42.000Z</updated>
    <content type="html"><![CDATA[<p>Flume的这个问题纠结了2个月，因为之前实在太忙了，没有时间来研究这个问题产生的原理，今天终于研究出来了，找出了这个问题所在。</p>
<p>先来描述一下这个问题的现象：</p>
<p>Flume的Source用的是KafkaSource，Sink用的是Custom Sink，由于这个Custom Sink写的有一点小问题，比如batchSize是5000次，第4000条就会发生exception，这样每次都会写入4000条数据。Sink处理的时候都会发生异常，每次都会rollback，rollback方面的知识可以参考<a href="http://fangjian0423.github.io/2016/01/03/flume-transaction/">Flume Transaction介绍</a>。</p>
<p>这样造成的后果有3个：</p>
<p>1.Channel中的数据满了。会发生以下异常：</p>
<pre><code>Caused by: org.apache.flume.ChannelFullException: Space <span class="keyword">for</span> commit <span class="keyword">to</span> queue couldn<span class="attribute">'t</span> be acquired. Sinks are likely <span class="keyword">not</span> keeping up <span class="keyword">with</span> sources, <span class="keyword">or</span> the <span class="keyword">buffer</span> size <span class="keyword">is</span> too tight
</code></pre><p>2.Sink会一直写数据，造成数据量暴增。</p>
<p>3.如果用了interceptor，且修改了event中的数据，那么会重复处理这些修改完后的event数据。</p>
<p>前面2个很容易理解，Sink发生异常，transaction rollback，导致channel中的队列满了。</p>
<p>关键是第三点，很让人费解。</p>
<p>以一段伪需求和伪代码为例，TestInterceptor的intercept方法：</p>
<p>比如处理一段json：</p>
<pre><code>{"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">languages</span>": <span class="value">[<span class="string">"java"</span>, <span class="string">"scala"</span>, <span class="string">"javascript"</span>]</span>}
</code></pre><p>使用interceptor处理成:</p>
<pre><code>[{"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"java"</span></span>}, {"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"scala"</span></span>}, {"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"javascript"</span></span>}]
</code></pre><p>interceptor代码如下：</p>
<pre><code><span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span>(<span class="params">Event <span class="keyword">event</span></span>) </span>{
    Model model = <span class="keyword">null</span>;
    String jsonStr = <span class="keyword">new</span> String(<span class="keyword">event</span>.getBody(), <span class="string">"UTF-8"</span>);
    <span class="keyword">try</span> {
        model = parseJsonStr(jsonStr);
    } <span class="keyword">catch</span> (Exception e) {
        log.error(<span class="string">"convert json data error"</span>);
    }
    <span class="keyword">event</span>.setBody(model.getJsonString().getBytes());
    <span class="keyword">return</span> <span class="keyword">event</span>;
}
</code></pre><p>当Channel中的队列已经满了以后，上述代码会打印出convert json data error，而且jsonStr的内容居然是转换后的数据，这一点一开始让我十分费解，误以为transaction rollback之后会修改source中的数据。后来debug源码发现错误在Source中。</p>
<p>后来发现并不是这样的。</p>
<p>KafkaSource中有一个属性eventList，是个ArrayList。用来接收kafka consume的message。</p>
<p>直接说明KafkaSource的process方法源码：</p>
<pre><code><span class="keyword">public</span> Status process() <span class="keyword">throws</span> EventDeliveryException {

  <span class="built_in">byte</span>[] kafkaMessage;
  <span class="built_in">byte</span>[] kafkaKey;
  Event event;
  Map&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; headers;
  <span class="keyword">long</span> batchStartTime = System.currentTimeMillis();
  <span class="keyword">long</span> batchEndTime = System.currentTimeMillis() + timeUpperLimit;
  <span class="keyword">try</span> {

    <span class="comment">/** 这里读取kafka中的message **/</span>
    <span class="built_in">boolean</span> iterStatus = <span class="keyword">false</span>;
    <span class="keyword">while</span> (eventList.<span class="built_in">size</span>() &lt; batchUpperLimit &amp;&amp;
            System.currentTimeMillis() &lt; batchEndTime) {
      iterStatus = hasNext();
      <span class="keyword">if</span> (iterStatus) {
        <span class="comment">// get next message</span>
        MessageAndMetadata&lt;<span class="built_in">byte</span>[], <span class="built_in">byte</span>[]&gt; messageAndMetadata = it.next();
        kafkaMessage = messageAndMetadata.message();
        kafkaKey = messageAndMetadata.<span class="variable">key</span>();

        <span class="comment">// Add headers to event (topic, timestamp, and key)</span>
        headers = <span class="keyword">new</span> <span class="keyword">HashMap</span>&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;();
        headers.put(KafkaSourceConstants.TIMESTAMP,
                <span class="keyword">String</span>.valueOf(System.currentTimeMillis()));
        headers.put(KafkaSourceConstants.TOPIC, topic);
        headers.put(KafkaSourceConstants.KEY, <span class="keyword">new</span> <span class="keyword">String</span>(kafkaKey));
        <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
          <span class="built_in">log</span>.debug(<span class="string">"Message: {}"</span>, <span class="keyword">new</span> <span class="keyword">String</span>(kafkaMessage));
        }
        event = EventBuilder.withBody(kafkaMessage, headers);
        eventList.<span class="built_in">add</span>(event);
      }
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Waited: {} "</span>, System.currentTimeMillis() - batchStartTime);
        <span class="built_in">log</span>.debug(<span class="string">"Event #: {}"</span>, eventList.<span class="built_in">size</span>());
      }
    }
    <span class="comment">/** 这里读取kafka中的message **/</span>

    <span class="comment">// If we have events, send events to channel</span>
    <span class="comment">// clear the event list</span>
    <span class="comment">// and commit if Kafka doesn't auto-commit</span>
    <span class="keyword">if</span> (eventList.<span class="built_in">size</span>() &gt; <span class="number">0</span>) {
      <span class="comment">// 使用ChannelProcess将Source中读取的数据给各个Channel</span>
      <span class="comment">// 如果getChannelProcessor().processEventBatch(eventList);发生了异常，eventList不会被清空，而且processEventBatch方法会调用Interceptor处理event中的数据，event中的数据已经被转换。所以下一次会将转换后的event数据再次传给Interceptor。</span>
      getChannelProcessor().processEventBatch(eventList);
      eventList.<span class="built_in">clear</span>();
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Wrote {} events to channel"</span>, eventList.<span class="built_in">size</span>());
      }
      <span class="keyword">if</span> (!kafkaAutoCommitEnabled) {
        <span class="comment">// commit the read transactions to Kafka to avoid duplicates</span>
        consumer.commitOffsets();
      }
    }
    <span class="keyword">if</span> (!iterStatus) {
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Returning with backoff. No more data to read"</span>);
      }
      <span class="keyword">return</span> Status.BACKOFF;
    }
    <span class="keyword">return</span> Status.READY;
  } <span class="keyword">catch</span> (Exception e) {
    <span class="built_in">log</span>.error(<span class="string">"KafkaSource EXCEPTION, {}"</span>, e);
    <span class="keyword">return</span> Status.BACKOFF;
  }
}
</code></pre><p>上述代码已经加了备注，再重申一下：ChannelProcess的processEventBatch方法会调用Interceptor处理event中的数据。所以如果Channel中的队列满了，那么processEventBatch方法会发生异常，发生异常之后eventList中的没有进入channel的数据已经被Interceptor修改，且不会被清空。因此下次还是会使用这些数据，所以会发生convert json data error错误。</p>
<p>画了一个序列图如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/flume-channel-full01.png" alt=""></p>
<p>第6步添加event到channel中的时候，队列已满，所以会抛出异常。最终异常被KafkaSource捕捉，但是eventList内部的部分数据已经被interceptor修改过。</p>
<p>多个channel的影响：</p>
<p>如果有多个channel，这个问题也会影响。比如有2个channel，c1和c2。c1的sink没有问题，一直稳定执行，c2对应的sink是一个CustomSink，会有问题。这样c2中的队列迟早会爆满，爆满之后，ChannelProcess批量处理event的时候，由于c2的队列满了，所以Source中的eventList永远不会被清空，eventList永远不会被清空的话，所有的channel都会被影响到，这就好比水源被污染之后，所有的用水都会受到影响。</p>
<p>举个例子：source为s1，c1对应的sink是k1，c2对应的sink是k2。k1和k2的batchSize都是5000，k2处理第4000条数据的时候总会发生异常，进行回滚。k1很稳定。这样c2迟早会爆满，爆满之后s1的eventList一直不能clear，这样也会导致c1一直在处理，所以k1的数据量跟k2一样也会暴增。</p>
<p>要避免本文所说的这一系列情况，最好的做法就是sink必须要加上很好的异常处理机制，不是任何情况都可以rollback的，要根据需求做对应的处理。</p>
]]></content>
    <summary type="html">
    <![CDATA[记录Flume使用KafkaSource的时候Channel队列满了之后发生的怪异问题。数据量暴增，Channel队列爆满 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
</feed>
