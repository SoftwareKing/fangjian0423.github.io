<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Format's Notes]]></title>
  <subtitle><![CDATA[吃饭睡觉撸代码]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://fangjian0423.github.io/"/>
  <updated>2016-02-17T03:25:25.000Z</updated>
  <id>http://fangjian0423.github.io/</id>
  
  <author>
    <name><![CDATA[Format]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Spark DataFrame介绍]]></title>
    <link href="http://fangjian0423.github.io/2016/02/17/spark-sql/"/>
    <id>http://fangjian0423.github.io/2016/02/17/spark-sql/</id>
    <published>2016-02-17T01:22:22.000Z</published>
    <updated>2016-02-17T03:25:25.000Z</updated>
    <content type="html"><![CDATA[<h2 id="DataFrame是什么">DataFrame是什么</h2><p>DataFrame是一个以命名列方式组织的分布式数据集。在概念上，它跟关系型数据库中的一张表或者1个Python(或者R)中的data frame一样，但是比他们更优化。DataFrame可以根据结构化的数据文件、hive表、外部数据库或者已经存在的RDD构造。</p>
<h2 id="DataFrame的创建">DataFrame的创建</h2><p>Spark DataFrame可以从一个已经存在的RDD、hive表或者数据源中创建。</p>
<p>以下一个例子就表示一个DataFrame基于一个json文件创建：</p>
<pre><code>val sc: SparkContext <span class="comment">// An existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SQLContext</span>(sc)

val df = sqlContext<span class="class">.read</span><span class="class">.json</span>(<span class="string">"examples/src/main/resources/people.json"</span>)

<span class="comment">// Displays the content of the DataFrame to stdout</span>
df.<span class="function"><span class="title">show</span><span class="params">()</span></span>
</code></pre><h2 id="DataFrame的操作">DataFrame的操作</h2><p>直接以1个例子来说明DataFrame的操作：</p>
<p>json文件内容：</p>
<pre><code>{"<span class="attribute">name</span>":<span class="value"><span class="string">"Michael"</span></span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"Andy"</span></span>, "<span class="attribute">age</span>":<span class="value"><span class="number">30</span></span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"Justin"</span></span>, "<span class="attribute">age</span>":<span class="value"><span class="number">19</span></span>}
</code></pre><p>程序内容：</p>
<pre><code>val conf = new SparkConf().setMaster("local").setAppName("DataFrameTest")

val sc = new SparkContext(conf)

val sqlContext = new SQLContext(sc)

val df = sqlContext.read.json(this.getClass.getResource("/").toString + "people.json")

<span class="header">  /** 展示DataFrame的内容
+----+-------+</span>
<span class="header">| age|   name|
+----+-------+</span>
|null|Michael|
|  30|   Andy|
|  19| Justin|
<span class="code">+----+</span>-------+  
<span class="code">  **/</span>
df.show()

/** 以树的形式打印出DataFrame的schema
root
<span class="code"> |-- age: long (nullable = true)</span>
<span class="code"> |-- name: string (nullable = true)</span>
*<span class="strong">*/
df.printSchema()

</span><span class="header">/** 打印出name列的数据
+-------+</span>
<span class="header">|   name|
+-------+</span>
|Michael|
|   Andy|
| Justin|
<span class="code">+-------+</span>   
*<span class="strong">*/
df.select("name").show()

</span><span class="header">/** 打印出name列和age列+1的数据，DataFrame的apply方法返回Column
+-------+---------+</span>
<span class="header">|   name|(age + 1)|
+-------+---------+</span>
|Michael|     null|
|   Andy|       31|
<span class="header">| Justin|       20|
+-------+---------+</span>
*<span class="strong">*/
df.select(df("name"), df("age") + 1).show()

</span><span class="header">/** 添加过滤条件，过滤出age字段大于21的数据
+---+----+</span>
<span class="header">|age|name|
+---+----+</span>
<span class="header">| 30|Andy|
+---+----+</span>
*<span class="strong">*/
df.filter(df("age") &gt; 21).show()

</span><span class="header">/** 以age字段分组进行统计
+----+-----+</span>
<span class="header">| age|count|
+----+-----+</span>
|null|    1|
|  19|    1|
<span class="header">|  30|    1|
+----+-----+</span>
*<span class="strong">*/
df.groupBy(df("age")).count().show()</span>
</code></pre><h2 id="使用反射推断出Schema">使用反射推断出Schema</h2><p>Spark SQL的Scala接口支持将包括case class数据的RDD转换成DataFrame。</p>
<p>case class定义表的schema，case class的属性会被读取并且成为列的名字，这里case class也可以被当成别的case class的属性或者是复杂的类型，比如Sequence或Array。</p>
<p>RDD会被隐式转换成DataFrame并且被注册成一个表，这个表可以被用在查询语句中：</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SQLContext</span>(sc)
<span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span>
import sqlContext<span class="class">.implicits</span>._

<span class="comment">// Define the schema using a case class.</span>
<span class="comment">// <span class="doctag">Note:</span> Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span>
<span class="comment">// you can use custom classes that implement the Product interface.</span>
case class <span class="function"><span class="title">Person</span><span class="params">(name: String, age: Int)</span></span>

<span class="comment">// Create an RDD of Person objects and register it as a table.</span>
val people = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"examples/src/main/resources/people.txt"</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(_.split(<span class="string">","</span>)</span></span>).<span class="function"><span class="title">map</span><span class="params">(p =&gt; Person(p(<span class="number">0</span>)</span></span>, <span class="function"><span class="title">p</span><span class="params">(<span class="number">1</span>)</span></span><span class="class">.trim</span><span class="class">.toInt</span>)).<span class="function"><span class="title">toDF</span><span class="params">()</span></span>
people.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"people"</span>)</span></span>

<span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span>
val teenagers = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span></span>

<span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span>
<span class="comment">// The columns of a row in the result can be accessed by field index:</span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>

<span class="comment">// or by field name:</span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t.getAs[String](<span class="string">"name"</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>

<span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(_.getValuesMap[Any](List(<span class="string">"name"</span>, <span class="string">"age"</span>)</span></span>)).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>
<span class="comment">// Map("name" -&gt; "Justin", "age" -&gt; 19)</span>
</code></pre><h2 id="使用编程指定Schema">使用编程指定Schema</h2><p>当case class不能提前确定（例如，记录的结构是经过编码的字符串，或者一个文本集合将会被解析，不同的字段投影给不同的用户），一个 DataFrame 可以通过三步来创建。</p>
<p>1.从原来的 RDD 创建一个行的 RDD<br>2.创建由一个 StructType 表示的模式与第一步创建的 RDD 的行结构相匹配<br>3.在行 RDD 上通过 applySchema 方法应用模式</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
<span class="variable"><span class="keyword">val</span> sqlContext</span> = new org.apache.spark.sql.SQLContext(sc)

<span class="comment">// Create an RDD</span>
<span class="variable"><span class="keyword">val</span> people</span> = sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)

<span class="comment">// The schema is encoded in a string</span>
<span class="variable"><span class="keyword">val</span> schemaString</span> = <span class="string">"name age"</span>

<span class="comment">// Import Row.</span>
<span class="keyword">import</span> org.apache.spark.sql.Row;

<span class="comment">// Import Spark SQL data types</span>
<span class="keyword">import</span> org.apache.spark.sql.types.{StructType,StructField,StringType};

<span class="comment">// Generate the schema based on the string of schema</span>
<span class="variable"><span class="keyword">val</span> schema</span> =
  StructType(
    schemaString.split(<span class="string">" "</span>).map(fieldName =&gt; StructField(fieldName, StringType, <span class="literal">true</span>)))

<span class="comment">// Convert records of the RDD (people) to Rows.</span>
<span class="variable"><span class="keyword">val</span> rowRDD</span> = people.map(_.split(<span class="string">","</span>)).map(p =&gt; Row(p(<span class="number">0</span>), p(<span class="number">1</span>).trim))

<span class="comment">// Apply the schema to the RDD.</span>
<span class="variable"><span class="keyword">val</span> peopleDataFrame</span> = sqlContext.createDataFrame(rowRDD, schema)

<span class="comment">// Register the DataFrames as a table.</span>
peopleDataFrame.registerTempTable(<span class="string">"people"</span>)

<span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span>
<span class="variable"><span class="keyword">val</span> results</span> = sqlContext.sql(<span class="string">"SELECT name FROM people"</span>)

<span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span>
<span class="comment">// The columns of a row in the result can be accessed by field index or by field name.</span>
results.map(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)).collect().foreach(println)
</code></pre><h2 id="数据源">数据源</h2><p>Spark SQL默认使用的数据源是parquet(可以通过spark.sql.sources.default修改)。</p>
<pre><code>val df = sqlContext<span class="class">.read</span><span class="class">.load</span>(<span class="string">"examples/src/main/resources/users.parquet"</span>)
df.<span class="function"><span class="title">select</span><span class="params">(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>)</span></span><span class="class">.write</span><span class="class">.save</span>(<span class="string">"namesAndFavColors.parquet"</span>)
</code></pre><p>可以在读取数据源的时候指定一些往外的参数。数据源也可以使用全名称，比如org.apache.spark.sql.parquet，但是内置的数据源可以使用短名称，比如json, parquet, jdbc。任何类型的DataFrame都可以使用这种方式转换成其他类型：</p>
<pre><code>val df = sqlContext<span class="class">.read</span><span class="class">.format</span>(<span class="string">"json"</span>).<span class="function"><span class="title">load</span><span class="params">(<span class="string">"examples/src/main/resources/people.json"</span>)</span></span>
df.<span class="function"><span class="title">select</span><span class="params">(<span class="string">"name"</span>, <span class="string">"age"</span>)</span></span><span class="class">.write</span><span class="class">.format</span>(<span class="string">"parquet"</span>).<span class="function"><span class="title">save</span><span class="params">(<span class="string">"namesAndAges.parquet"</span>)</span></span>
</code></pre><p>使用read方法读取数据源得到DataFrame，还可以使用sql直接查询文件的方式：</p>
<pre><code>val df = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`"</span>)</span></span>
</code></pre><p>保存模式：</p>
<p>保存方法会需要一个可选参数SaveMode，用于处理已经存在的数据。这些保存模式内部不会用到锁的概念，也不是一个原子操作。如果使用了Overwrite这种保存模式，那么写入数据前会清空之前的老数据。</p>
<table>
<thead>
<tr>
<th style="text-align:center">Scala/Java</th>
<th style="text-align:center">具体值</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">SaveMode.ErrorIfExists (默认值)</td>
<td style="text-align:center">“error” (默认值)</td>
<td style="text-align:center">当保存DataFrame到数据源的时候，如果数据源文件已经存在，那么会抛出异常</td>
</tr>
<tr>
<td style="text-align:center">SaveMode.Append</td>
<td style="text-align:center">“append”</td>
<td style="text-align:center">如果数据源文件已经存在，append到文件末尾</td>
</tr>
<tr>
<td style="text-align:center">SaveMode.Overwrite</td>
<td style="text-align:center">“overwrite”</td>
<td style="text-align:center">如果数据源文件已经存在，清空数据</td>
</tr>
<tr>
<td style="text-align:center">SaveMode.Ignore</td>
<td style="text-align:center">“ignore”</td>
<td style="text-align:center">如果数据源文件已经存在，不做任何处理。跟SQL中的 CREATE TABLE IF NOT EXISTS 类似</td>
</tr>
</tbody>
</table>
<p>持久化表：</p>
<p>当使用HiveContext的时候，使用saveAsTable方法可以把DataFrame持久化成表。跟registerTempTable方法不一样，saveAsTable方法会把DataFrame持久化成表，并且创建一个数据的指针到HiveMetastore对象中。只要获得了同一个HiveMetastore对象的链接，当Spark程序重启的时候，saveAsTable持久化后的表依然会存在。一个DataFrame持久化成一个table也可以通过SQLContext的table方法，参数就是表的名字。</p>
<p>默认情况下，saveAsTable方法会创建一个”被管理的表”，被管理的表的意思是说表中数据的位置会被HiveMetastore所控制，如果表被删除了，HiveMetastore中的数据也相当于被删除了。</p>
<h3 id="Parquet_Files">Parquet Files</h3><p>parquet是一种基于列的存储格式，并且可以被很多框架所支持。Spark SQL支持parquet文件的读和写操作，并且会自动维护原始数据的schema，当写一个parquet文件的时候，所有的列都允许为空。</p>
<h4 id="加载Parquet文件">加载Parquet文件</h4><pre><code><span class="comment">// sqlContext from the previous example is used in this example.</span>
<span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span>
import sqlContext<span class="class">.implicits</span>._

val people: RDD[Person] = ... <span class="comment">// An RDD of case class objects, from the previous example.</span>

<span class="comment">// The RDD is implicitly converted to a DataFrame by implicits, allowing it to be stored using Parquet.</span>
people<span class="class">.write</span><span class="class">.parquet</span>(<span class="string">"people.parquet"</span>)

<span class="comment">// Read in the parquet file created above. Parquet files are self-describing so the schema is preserved.</span>
<span class="comment">// The result of loading a Parquet file is also a DataFrame.</span>
val parquetFile = sqlContext<span class="class">.read</span><span class="class">.parquet</span>(<span class="string">"people.parquet"</span>)

<span class="comment">//Parquet files can also be registered as tables and then used in SQL statements.</span>
parquetFile.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"parquetFile"</span>)</span></span>
val teenagers = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span></span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>
</code></pre><h4 id="Parquet文件的Partition">Parquet文件的Partition</h4><p>Parquet文件可以根据列自动进行分区，只需要调用DataFrameWriter的partitionBy方法即可，该方法需要的参数是需要进行分区的列。比如需要分区成这样：</p>
<pre><code>path
└── <span class="keyword">to</span>
    └── table
        ├── gender=male
        │   ├── <span class="attribute">...</span>
        │   │
        │   ├── country=US
        │   │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
        │   ├── country=<span class="literal">CN</span>
        │   │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
        │   └── <span class="attribute">...</span>
        └── gender=female
            ├── <span class="attribute">...</span>
            │
            ├── country=US
            │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
            ├── country=<span class="literal">CN</span>
            │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
            └── <span class="attribute">...</span>
</code></pre><p>这个需要DataFrame就需要4列，分别是name，age，gender和country，write的时候如下：</p>
<pre><code>dataFrame<span class="class">.write</span><span class="class">.partitionBy</span>(<span class="string">"gender"</span>, <span class="string">"country"</span>).<span class="function"><span class="title">parquet</span><span class="params">(<span class="string">"path"</span>)</span></span>
</code></pre><h4 id="Schema_Merging">Schema Merging</h4><p>像ProtocolBuffer，Avro，Thrift一样，Parquet也支持schema的扩展。</p>
<p>由于schema的自动扩展是一次昂贵的操作，所以默认情况下不是开启的，可以根据以下设置打开：</p>
<p>读parquet文件的时候设置参数mergeSchema为true或者设置全局的sql属性spark.sql.parquet.mergeSchema为true：</p>
<pre><code><span class="comment">// sqlContext from the previous example is used in this example.</span>
<span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span>
import sqlContext<span class="class">.implicits</span>._

<span class="comment">// Create a simple DataFrame, stored into a partition directory</span>
val df1 = sc.<span class="function"><span class="title">makeRDD</span><span class="params">(<span class="number">1</span> to <span class="number">5</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(i =&gt; (i, i * <span class="number">2</span>)</span></span>).<span class="function"><span class="title">toDF</span><span class="params">(<span class="string">"single"</span>, <span class="string">"double"</span>)</span></span>
df1<span class="class">.write</span><span class="class">.parquet</span>(<span class="string">"data/test_table/key=1"</span>)

<span class="comment">// Create another DataFrame in a new partition directory,</span>
<span class="comment">// adding a new column and dropping an existing column</span>
val df2 = sc.<span class="function"><span class="title">makeRDD</span><span class="params">(<span class="number">6</span> to <span class="number">10</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(i =&gt; (i, i * <span class="number">3</span>)</span></span>).<span class="function"><span class="title">toDF</span><span class="params">(<span class="string">"single"</span>, <span class="string">"triple"</span>)</span></span>
df2<span class="class">.write</span><span class="class">.parquet</span>(<span class="string">"data/test_table/key=2"</span>)

<span class="comment">// Read the partitioned table</span>
val df3 = sqlContext<span class="class">.read</span><span class="class">.option</span>(<span class="string">"mergeSchema"</span>, <span class="string">"true"</span>).<span class="function"><span class="title">parquet</span><span class="params">(<span class="string">"data/test_table"</span>)</span></span>
df3.<span class="function"><span class="title">printSchema</span><span class="params">()</span></span>

<span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span>
<span class="comment">// with the partitioning column appeared in the partition directory paths.</span>
<span class="comment">// root</span>
<span class="comment">// |-- single: int (nullable = true)</span>
<span class="comment">// |-- double: int (nullable = true)</span>
<span class="comment">// |-- triple: int (nullable = true)</span>
<span class="comment">// |-- key : int (nullable = true)</span>
</code></pre><h3 id="JSON数据源">JSON数据源</h3><p>本文之前的一个例子就是使用的JSON数据源，使用SQLContext.read.json()读取一个带有String类型的RDD或者一个json文件。</p>
<p>需要注意的是json文件不是一个典型的json格式的文件，每一行都是一个json对象。</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SQLContext</span>(sc)

<span class="comment">// A JSON dataset is pointed to by path.</span>
<span class="comment">// The path can be either a single text file or a directory storing text files.</span>
val path = <span class="string">"examples/src/main/resources/people.json"</span>
val people = sqlContext<span class="class">.read</span><span class="class">.json</span>(path)

<span class="comment">// The inferred schema can be visualized using the printSchema() method.</span>
people.<span class="function"><span class="title">printSchema</span><span class="params">()</span></span>
<span class="comment">// root</span>
<span class="comment">//  |-- age: integer (nullable = true)</span>
<span class="comment">//  |-- name: string (nullable = true)</span>

<span class="comment">// Register this DataFrame as a table.</span>
people.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"people"</span>)</span></span>

<span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span>
val teenagers = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span></span>

<span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span>
<span class="comment">// an RDD[String] storing one JSON object per string.</span>
val anotherPeopleRDD = sc.parallelize(
  <span class="string">""</span><span class="string">"{"</span>name<span class="string">":"</span>Yin<span class="string">","</span>address<span class="string">":{"</span>city<span class="string">":"</span>Columbus<span class="string">","</span>state<span class="string">":"</span>Ohio<span class="string">"}}"</span><span class="string">""</span> :: Nil)
val anotherPeople = sqlContext<span class="class">.read</span><span class="class">.json</span>(anotherPeopleRDD)
</code></pre><h3 id="Hive表">Hive表</h3><p>需要使用HiveContext。</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.hive</span><span class="class">.HiveContext</span>(sc)

sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"</span>)</span></span>
sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)</span></span>

<span class="comment">// Queries are expressed in HiveQL</span>
sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"FROM src SELECT key, value"</span>)</span></span>.<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>
</code></pre><h3 id="JDBC">JDBC</h3><p>直接使用load方法加载：</p>
<pre><code>sqlContext<span class="built_in">.</span>load(<span class="string">"jdbc"</span>, <span class="built_in">Map</span>(<span class="string">"url"</span> <span class="subst">-&gt; </span><span class="string">"jdbc:mysql://localhost:3306/your_database?user=your_user&amp;password=your_password"</span>, <span class="string">"dbtable"</span> <span class="subst">-&gt; </span><span class="string">"your_table"</span>))
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[DataFrame是一个以命名列方式组织的分布式数据集。在概念上，它跟关系型数据库中的一张表或者1个Python(或者R)中的data frame一样，但是比他们更优化。DataFrame可以根据结构化的数据文件、hive表、外部数据库或者已经存在的RDD构造 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/tags/spark/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark Streaming编程指南笔记]]></title>
    <link href="http://fangjian0423.github.io/2016/02/10/sparkstreaming-programming-guide/"/>
    <id>http://fangjian0423.github.io/2016/02/10/sparkstreaming-programming-guide/</id>
    <published>2016-02-09T17:36:17.000Z</published>
    <updated>2016-02-09T17:46:12.000Z</updated>
    <content type="html"><![CDATA[<h2 id="概述">概述</h2><p>Spark Streaming是Spark核心API的扩展，用于处理实时数据流。Spark Streaming处理的数据源可以是Kafka，Flume，Twitter，ZeroMQ，Kinesis或者Tcp Sockets，这些数据可以使用map，reduce，join，window方法进行处转换，还可以直接使用Spark内置的机器学习算法，图算法包来处理数据。</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt=""></p>
<p>最终处理后的数据可以存入文件系统，数据库。</p>
<p>Spark Streaming内部接收到实时数据之后，会把数据分成几个批次，这些批次数据会被Spark引擎处理并生成各个批次的结果。</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-flow.png" alt=""></p>
<p>Spark Streaming提供了一个叫做<strong>discretized stream 或者 DStream</strong>的抽象概念，表示一段连续的数据流。DStream会在数据源中的数据流中创建，或者在别的DStream中使用类似map，join方法创建。一个DStream表示一个RDD序列。</p>
<h2 id="一个快速例子">一个快速例子</h2><p>以一个TCP Socket监听接收数据，并计算单词的个数为例子讲解。</p>
<p>首先，需要import Spark Streaming中的一些类和StreamingContext中的一些隐式转换。我们会创建一个带有2个线程，1秒一个批次的StreamingContext。</p>
<pre><code><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>
<span class="keyword">import</span> org.apache.spark.streaming.{<span class="type">Seconds</span>, <span class="type">StreamingContext</span>}

<span class="class"><span class="keyword">object</span> <span class="title">SparkStreamTest</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">App</span> {</span>

  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)

  <span class="comment">// 创建一个StreamingContext，每1秒钟处理一次计算程序</span>
  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))

  <span class="comment">// 使用StreamingContext创建DStream，DStream表示TCP源中的流数据. lines这个DStream表示接收到的服务器数据，每一行都是文本</span>
  <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)

  <span class="comment">// 使用flatMap将每一行中的文本转换成每个单词，并产生一个新的DStream。</span>
  <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))

  <span class="comment">// 使用map方法将每个单词转换成tuple</span>
  <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))

  <span class="comment">// 使用reduceByKey计算出每个单词的出现次数</span>
  <span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)

  wordCounts.print()

  ssc.start() <span class="comment">// 开始计算</span>
  ssc.awaitTermination() <span class="comment">// 等待计算结束</span>
}
</code></pre><p>在运行这段代码之前，首先先起一个netcat服务：</p>
<pre><code>nc -lk <span class="number">9999</span>
</code></pre><p>之后比如输入hello world之后，控制台会打印出如下数据：</p>
<pre><code><span class="code">-------------------------------------------
Time: 1454684570000 ms
-------------------------------------------</span>
(hello,1)
(world,1)
</code></pre><h2 id="基础概念">基础概念</h2><h3 id="Linking(SparkStreaming的连接)">Linking(SparkStreaming的连接)</h3><p>写Spark Streaming程序需要一些依赖。使用maven的话加入以下依赖：</p>
<pre><code><span class="tag">&lt;<span class="title">dependency</span>&gt;</span>
    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span>
    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-streaming_2.10<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span>
    <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.6.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span>
<span class="tag">&lt;/<span class="title">dependency</span>&gt;</span>
</code></pre><p>使用sbt的话，加入以下依赖：</p>
<pre><code><span class="title">libraryDependencies</span> += <span class="string">"org.apache.spark"</span> % <span class="string">"spark-streaming_2.10"</span> % <span class="string">"1.6.0"</span>
</code></pre><p>SparkStreaming核心不提供一些数据源的依赖，需要手动添加，一些数据源对应的Artifact如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">数据源</th>
<th style="text-align:center">Artifact</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Kafka</td>
<td style="text-align:center">spark-streaming-kafka_2.10</td>
</tr>
<tr>
<td style="text-align:center">Flume</td>
<td style="text-align:center">spark-streaming-flume_2.10</td>
</tr>
<tr>
<td style="text-align:center">Kinesis</td>
<td style="text-align:center">spark-streaming-kinesis-asl_2.10 [Amazon Software License]</td>
</tr>
<tr>
<td style="text-align:center">Twitter</td>
<td style="text-align:center">spark-streaming-twitter_2.10</td>
</tr>
<tr>
<td style="text-align:center">ZeroMQ</td>
<td style="text-align:center">spark-streaming-zeromq_2.10</td>
</tr>
<tr>
<td style="text-align:center">MQTT</td>
<td style="text-align:center">spark-streaming-mqtt_2.10</td>
</tr>
</tbody>
</table>
<h3 id="StreamingContext的初始化">StreamingContext的初始化</h3><p>StreamingContext的创建是Spark Streaming程序中最重要的一环。</p>
<p>可以根据SparkConf对象创建出StreamingContext对象：</p>
<pre><code>import org<span class="class">.apache</span><span class="class">.spark</span>._
import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.streaming</span>._

val conf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(appName)</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(master)</span></span>
val ssc = new <span class="function"><span class="title">StreamingContext</span><span class="params">(conf, Seconds(<span class="number">1</span>)</span></span>)
</code></pre><p>appName参数是应用程序的名字，在cluster UI中显示的就是这个名字。master参数的意义跟spark中master参数的意义是一样的。</p>
<p>StreamingContext内部会创建SparkContext，可以使用StreamingContext内部的sparkContext获得。</p>
<pre><code>ssc<span class="class">.sparkContext</span> <span class="comment">// 得到SparkContext</span>
</code></pre><p>StreamingContext也可以根据SparkContext创建：</p>
<pre><code>import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.streaming</span>._

val sc = ...                <span class="comment">// existing SparkContext</span>
val ssc = new <span class="function"><span class="title">StreamingContext</span><span class="params">(sc, Seconds(<span class="number">1</span>)</span></span>)
</code></pre><p>StreamingContext创建之后，可以做以下几点：</p>
<p>1.创建DStreams定义数据源<br>2.使用DStreams的transformation和output operations用于计算<br>3.使用streamingContext的start方法接收数据<br>4.使用streamingContext的awaitTermination方法等待处理结果<br>5.可以使用streamingContext的stop方法停止程序</p>
<p>一些需要注意的点：</p>
<p>1.context开始启动之后，一些streaming的计算不允许发生<br>2.context停掉之后不能重启<br>3.一个JVM在同一时刻只能有一个StreamingContext可以激活<br>4.StreamingContext中的stop方法内部也会stop SparkContext。如果只想stop StreamingContext，那么调用stop方法的时候参数设置为false<br>5.一个SparkContext可以用来创建多个StreamingContexts，只要上一个StreamingContext在下一个StreamingContext创建之前停掉</p>
<h3 id="Discretized_Streams_(DStreams)">Discretized Streams (DStreams)</h3><p>DStreams和Discretized Streams在Spark Streaming中代表相同的意思：</p>
<p>1.一段连续的数据流<br>2.数据源中接收到的数据流<br>3.使用transforming处理过的流数据</p>
<p>Spark内部一个DStream表示一段连续的RDD。DStream中每段RDD表示一段时间内的RDD，效果如下：</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream.png" alt=""></p>
<p>DStream可以使用一些transformation操作将内部的RDD转换成另外一种RDD。比如之前的一个单词统计例子中就将一行文本的DStream转换成每个单词的DStream，过程如下：</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream-ops.png" alt=""></p>
<h3 id="Input_DStreams_and_Receivers(数据源和接收器)">Input DStreams and Receivers(数据源和接收器)</h3><p>Input DStreams是DStreams从streaming source中接收到的输入流数据。在之前分析的一个单词统计例子中，lines就是个Input DStream，表示接收到的服务器数据，每一行都是文本。</p>
<p>每一个Input DStream(除了file stream)都会关联一个Receiver对象，这个Receiver对象的作用是接收数据源中的数据并存储在内存中。</p>
<p>Spark Streaming提供了两种类型的内置数据源：</p>
<p>1.基础数据源。可以直接使用StreamingContext的API，比如文件系统，socket连接，Akka。<br>2.高级数据源。比如Flume，Kafka，Kinesis，Twitter等可以使用工具类的数据源。使用这些数据源需要对应的依赖，在Linking章节中已经介绍过。</p>
<p>如果想在streaming程序中并行地接收多个数据源，需要创建多个Input DStream，有个多个Input DStream的话那就会对应地有多个Receiver。但是需要记住的是，Spark的worker/executor模式是一个相当耗时的任务，因此服务器的配置需要够好才能支撑多个Input DStream。</p>
<p>一些注意点：</p>
<p>1.当本地跑Spark Streaming程序的时候，不要使用”local”或者”local[1]”设置master URL。因为这两种master URL只会使用1个线程。当使用比如Flume，Kafka，socket这些数据源的时候，因为只有一个线程跑receiver接收数据，那么没有其他线程去处理接收后的数据了。所以，当在本地跑Spark Streaming程序的时候，需要将master URL设置为local[n]，n需要大于receiver的个数。<br>2.服务器的核数需要大于receiver的个数。否则程序只会接收数据，而不会处理数据。</p>
<h4 id="Basic_Sources(基础数据源)">Basic Sources(基础数据源)</h4><p>基础数据源刚刚分析过，StreamingContext的API可以使用如文件系统，socket连接，Akka作为输入源。socket连接本文以开始的例子中已经使用过了。</p>
<p>文件系统的输入源会读取文件或任何支持HDFS API(比如HDFS，S3，NFS)的文件系统的数据：</p>
<pre><code>streamingContext.fileStream[<span class="link_label">KeyClass, ValueClass, InputFormatClass</span>](<span class="link_url">dataDirectory</span>)
</code></pre><p>Spark Streaming会监测dataDirectory目录并且会处理这个目录中新创建的文件(老文件写新数据的话不会被支持)。使用文件数据源还需要这几点：</p>
<p>1.所有文件的数据格式必须相同<br>2.dataDirectory目录中的文件必须是新创建的，也可以是从别的目录move进来的<br>3.文件内部的数据更改之后，新更改的数据不会被处理</p>
<p>对于简单的文件，可以使用streamingContext的textFileStream方法处理。</p>
<h4 id="Advanced_Sources(高级数据源)">Advanced Sources(高级数据源)</h4><p>高级数据源需要一些非Spark依赖。Spark Streaming把创建DStream的API移到了各自的API里。如果想创建一个使用Twitter的数据源，需要做以下三步：</p>
<p>1.添加对应的Twitter依赖spark-streaming-twitter_2.10到项目里<br>2.import这个类TwitterUtils，使用TwitterUtils.createStream创建DStream<br>3.部署</p>
<h4 id="Custom_Sources(自定义数据源)">Custom Sources(自定义数据源)</h4><p>要实现一个自定义的数据源，需要实现一个自定义的receiver</p>
<h4 id="Receiver_Reliability(接收器的可靠性)">Receiver Reliability(接收器的可靠性)</h4><p>基于可靠性的数据源分为两种。</p>
<p>1.可靠的接收器(Reliable Receiver)：一个可靠的接收器接收到数据之后会给数据源发送消息表示自己已经接收到数据<br>2.不可靠的接收器(Unreliable Receiver)：一个不可靠的接收器不会发送消息给数据源。</p>
<p>想写出一个可靠的接收器可以参考 <a href="http://spark.apache.org/docs/latest/streaming-custom-receivers.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/streaming-custom-receivers.html</a></p>
<h3 id="DStreams的Transformations操作">DStreams的Transformations操作</h3><p>DStream的Transformations操作跟RDD的Transformations操作类似，</p>
<table>
<thead>
<tr>
<th style="text-align:center">Transformation</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">map(func)</td>
<td style="text-align:center">根据func函数生成一个新的DStream</td>
</tr>
<tr>
<td style="text-align:center">flatMap(func)</td>
<td style="text-align:center">跟map方法类似，但是每一项可以返回多个值。func函数的返回值是一个集合</td>
</tr>
<tr>
<td style="text-align:center">filter(func)</td>
<td style="text-align:center">根据func函数返回true的数据集</td>
</tr>
<tr>
<td style="text-align:center">repartition(numPartitions)</td>
<td style="text-align:center">重新给 DStream 分区</td>
</tr>
<tr>
<td style="text-align:center">union(otherStream)</td>
<td style="text-align:center">取2个DStream的并集，得到一个新的DStream</td>
</tr>
<tr>
<td style="text-align:center">count()</td>
<td style="text-align:center">返回一个新的只有一个元素的DStream，这个元素就是DStream中的所有RDD的个数</td>
</tr>
<tr>
<td style="text-align:center">reduce(func)</td>
<td style="text-align:center">返回一个新的只有一个元素的DStream，这个元素就是DStream中的所有RDD通过func函数聚合得到的结果</td>
</tr>
<tr>
<td style="text-align:center">countByValue()</td>
<td style="text-align:center">如果DStream的类型为K，那么返回一个新的DStream，这个新的DStream中的元素类型是(K, Long)，K是原先DStream的值，Long表示这个Key有多少次</td>
</tr>
<tr>
<td style="text-align:center">reduceByKey(func, [numTasks])</td>
<td style="text-align:center">本文的例子使用过这个方法，对于是键值对(K,V)的DStream，返回一个新的DStream以K为键，各个value使用func函数操作得到的聚合结果为value</td>
</tr>
<tr>
<td style="text-align:center">join(otherStream, [numTasks])</td>
<td style="text-align:center">基于(K, V)键值对的DStream，如果对(K, W)的键值对DStream使用join操作，可以产生(K, (V, W))键值对的DStream</td>
</tr>
<tr>
<td style="text-align:center">cogroup(otherStream, [numTasks])</td>
<td style="text-align:center">跟join方法类似，不过是基于(K, V)的DStream，cogroup基于(K, W)的DStream，产生(K, (Seq[V], Seq[W]))的DStream</td>
</tr>
<tr>
<td style="text-align:center">transform(func)</td>
<td style="text-align:center">基于DStream中的每个RDD调用func函数，func函数的参数是个RDD，返回值也是个RDD</td>
</tr>
<tr>
<td style="text-align:center">updateStateByKey(func)</td>
<td style="text-align:center">对于每个key都会调用func函数处理先前的状态和所有新的状态。比如就可以用来做累加，这个方法跟reduceByKey类似，但比它更加灵活</td>
</tr>
</tbody>
</table>
<h4 id="UpdateStateByKey操作">UpdateStateByKey操作</h4><p>使用UpdateStateByKey方法需要做以下两步：</p>
<p>1.定义状态：状态可以是任意的数据类型<br>2.定义状态更新函数：这个函数需要根据输入流把先前的状态和所有新的状态</p>
<p>不管有没有新数据进来，在每个批次中，Spark都会对所有存在的key调用func方法，如果func函数返回None，那么key-value键值对不会被处理。</p>
<p>以一个例子来讲解updateStateByKey方法，这个例子会统计每个单词的个数在一个文本输入流里：</p>
<p>runningCount是一个状态并且是Int类型，所以这个状态的类型是Int，runningCount是先前的状态，newValues是所有新的状态，是一个集合，函数如下：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span><span class="params">(newValues: Seq[Int], runningCount: Option[Int])</span>:</span> Option[Int] = {
    val newCount = ...  // add the new values <span class="keyword">with</span> the previous running count to get the new count
    Some(newCount)
}
</code></pre><p>updateStateByKey方法的调用：</p>
<pre><code>val runningCounts = pairs.updateStateByKey[<span class="link_label">Int</span>](<span class="link_url">updateFunction _</span>)
</code></pre><h4 id="Transform操作">Transform操作</h4><p>Transform操作针对的是RDD-RDD的操作，所以可以用来处理那些没有在DStream API中暴露的处理任意的RDD操作。比如在DStream中的每次批次没有join rdd的API，所以可以使用transform操作：</p>
<pre><code><span class="variable"><span class="keyword">val</span> spamInfoRDD</span> = ssc.sparkContext.newAPIHadoopRDD(...) <span class="comment">// RDD containing spam information</span>

<span class="variable"><span class="keyword">val</span> cleanedDStream</span> = wordCounts.transform(rdd =&gt; {
  rdd.join(spamInfoRDD).filter(...) <span class="comment">// join data stream with spam information to do data cleaning</span>
  ...
})
</code></pre><h4 id="Window操作">Window操作</h4><p>window操作效果图如下图所示，把几个批次的DStream合并成一个DStream：</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream-window.png" alt=""></p>
<p>每个window操作都需要2个参数：</p>
<p>1.window length。每个window对应的批次数(上图中是3，time1-time3是一个window, time3-time5也是一个window)<br>2.sliding interval。每个window之间的间隔时间，上图下方的window1，window3，window5的间隔。上图这个值为2</p>
<p>这两个参数必须是批次间隔的倍数。上个批次间隔值为1。</p>
<p>以1个例子来讲解window操作，基于本文一开始的那个例子，生成最后30秒的数据，每10秒为单位，这里就需要使用reduceByKeyAndWindow方法：</p>
<pre><code>val windowedWordCounts = pairs.<span class="function"><span class="title">reduceByKeyAndWindow</span><span class="params">((a:Int,b:Int)</span></span> =&gt; (<span class="tag">a</span> + b), <span class="function"><span class="title">Seconds</span><span class="params">(<span class="number">30</span>)</span></span>, <span class="function"><span class="title">Seconds</span><span class="params">(<span class="number">10</span>)</span></span>)
</code></pre><p>其他的一些window操作：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Transformation</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">window(windowLength, slideInterval)</td>
<td style="text-align:center">根据window操作的2个参数得到新的DStream</td>
</tr>
<tr>
<td style="text-align:center">countByWindow(windowLength, slideInterval)</td>
<td style="text-align:center">基于window操作的count操作</td>
</tr>
<tr>
<td style="text-align:center">reduceByWindow(func, windowLength, slideInterval)</td>
<td style="text-align:center">基于window操作的reduce操作</td>
</tr>
<tr>
<td style="text-align:center">reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</td>
<td style="text-align:center">基于window操作的reduceByKey操作</td>
</tr>
<tr>
<td style="text-align:center">reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</td>
<td style="text-align:center">跟reduceByKeyAndWindow方法类似，更有效率，invFunc方法跟func方法的参数返回值一样，表示从window离开的数据</td>
</tr>
<tr>
<td style="text-align:center">countByValueAndWindow(windowLength, slideInterval, [numTasks])</td>
<td style="text-align:center">基于window操作的countByValue操作</td>
</tr>
</tbody>
</table>
<h4 id="Join操作">Join操作</h4><p>DStream可以很容易地join其他DStream：</p>
<pre><code><span class="label">val</span> <span class="keyword">stream1: </span>DStream[<span class="keyword">String, </span><span class="keyword">String] </span>= ...
<span class="label">val</span> <span class="keyword">stream2: </span>DStream[<span class="keyword">String, </span><span class="keyword">String] </span>= ...
<span class="label">val</span> joinedStream = <span class="keyword">stream1.join(stream2)</span>
</code></pre><p>还可以使用leftOuterJoin，rightOuterJoin，fullOuterJoin等。同样地，也可以在window操作后的DStream中使用join：</p>
<pre><code>val windowedStream1 = stream1.<span class="function"><span class="title">window</span><span class="params">(Seconds(<span class="number">20</span>)</span></span>)
val windowedStream2 = stream2.<span class="function"><span class="title">window</span><span class="params">(Minutes(<span class="number">1</span>)</span></span>)
val joinedStream = windowedStream1.<span class="function"><span class="title">join</span><span class="params">(windowedStream2)</span></span>
</code></pre><p>基于rdd的join：</p>
<pre><code><span class="variable"><span class="keyword">val</span> dataset</span>: RDD[String, String] = ...
<span class="variable"><span class="keyword">val</span> windowedStream</span> = stream.window(Seconds(<span class="number">20</span>))...
<span class="variable"><span class="keyword">val</span> joinedStream</span> = windowedStream.transform { rdd =&gt; rdd.join(dataset) }
</code></pre><h3 id="DStream的输出操作">DStream的输出操作</h3><p>输出操作允许DStream中的数据输出到外部系统，比如像数据库、文件系统等。</p>
<table>
<thead>
<tr>
<th style="text-align:center">输出操作</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">print()</td>
<td style="text-align:center">打印出DStream中每个批次的前10条数据</td>
</tr>
<tr>
<td style="text-align:center">saveAsTextFiles(prefix, [suffix])</td>
<td style="text-align:center">把DStream中的数据保存到文本文件里。每次批次的文件名根据参数prefix和suffix生成：”prefix-TIME_IN_MS[.suffix]”</td>
</tr>
<tr>
<td style="text-align:center">saveAsObjectFiles(prefix, [suffix])</td>
<td style="text-align:center">把DStream中的数据按照Java序列化的方式保存Sequence文件里，文件名规则跟saveAsTextFiles方法一样</td>
</tr>
<tr>
<td style="text-align:center">saveAsHadoopFiles(prefix, [suffix])</td>
<td style="text-align:center">把DStream中的数据保存到Hadoop文件里，文件名规则跟saveAsTextFiles方法一样</td>
</tr>
<tr>
<td style="text-align:center">foreachRDD(func)</td>
<td style="text-align:center">遍历DStream中的每段RDD，遍历的过程中可以将RDD中的数据保存到外部系统中</td>
</tr>
</tbody>
</table>
<h4 id="foreachRDD中的设计模式">foreachRDD中的设计模式</h4><p>foreachRDD方法会遍历DStream中的每段RDD，遍历的过程中可以将RDD中的数据保存到外部系统中。这个方法很实用，所以理解foreachRDD方法显得很重要。</p>
<p>将数据写到外部系统通常都需要一个connection对象，所以很多时候都会不经意地创建这个connection对象：</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  val connection = createNewConnection()  <span class="comment">// executed at the driver</span>
  rdd.<span class="keyword">foreach</span> { record =&gt;
    connection.send(record) <span class="comment">// executed at the worker</span>
  }
}
</code></pre><p>这种写法是不正确的。这里connection需要被序列化并且发送到worker，而且connection对象会跨机器传递，会发生序列化错误(connection对象是不可序列化的)，初始化错误(connection对象需要在worker中初始化)。这个错误的解决方案就是在worker中创建connection对象。</p>
<p>为每条记录创建connection也是一个很常见的错误：</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  rdd.foreach { record =&gt;
    <span class="keyword">val</span> connection = createNewConnection<span class="literal">()</span>
    connection.send(record)
    connection.close<span class="literal">()</span>
  }
}
</code></pre><p>因为创建connection对象是一种很耗资源，很耗时间的操作。对于每条数据都创建一个connection代驾更大。所有可以使用rdd.foreachPartition方法，这个方法会创建单一的connection并且在一个RDD分区中所有数据都使用这个connection：</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  rdd.foreachPartition { partitionOfRecords =&gt;
    <span class="keyword">val</span> connection = createNewConnection<span class="literal">()</span>
    partitionOfRecords.foreach(record =&gt; connection.send(record))
    connection.close<span class="literal">()</span>
  }
}
</code></pre><p>一种更好的方式就是使用ConnectionPool，ConnectionPool可以重用connection对象在多个批次和RDD中。</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  rdd.foreachPartition { partitionOfRecords =&gt;
    <span class="comment">// ConnectionPool is a static, lazily initialized pool of connections</span>
    val connection = ConnectionPool.getConnection()
    partitionOfRecords.<span class="keyword">foreach</span>(record =&gt; connection.send(record))
    ConnectionPool.returnConnection(connection)  <span class="comment">// return to the pool for future reuse</span>
  }
}
</code></pre><p>其他需要注意的点：</p>
<p>1.DStream的输出操作也是延迟执行的，就像RDD的action操作一样。RDD的action操作在DStream的输出操作内部执行的话会强制Spark Streaming执行。如果程序里没有任何输出操作，或者有比如像dstream.foreachRDD操作一样内部没有rdd的action操作的话，这样就不会执行任意操作，会被Spark忽略。<br>2.默认情况下，在一个时间点下，只有一个输出操作被执行。它们是根据程序里的编写顺序执行的。</p>
<h3 id="DataFrame_and_SQL_Operations">DataFrame and SQL Operations</h3><p>在Spark Streaming中可以使用DataFrames and SQL操作。</p>
<pre><code><span class="comment">/** DataFrame operations inside your streaming program */</span>

<span class="variable"><span class="keyword">val</span> words</span>: DStream[String] = ...

words.foreachRDD { rdd =&gt;

  <span class="comment">// Get the singleton instance of SQLContext</span>
  <span class="variable"><span class="keyword">val</span> sqlContext</span> = SQLContext.getOrCreate(rdd.sparkContext)
  <span class="keyword">import</span> sqlContext.implicits._

  <span class="comment">// Convert RDD[String] to DataFrame</span>
  <span class="variable"><span class="keyword">val</span> wordsDataFrame</span> = rdd.toDF(<span class="string">"word"</span>)

  <span class="comment">// Register as table</span>
  wordsDataFrame.registerTempTable(<span class="string">"words"</span>)

  <span class="comment">// Do word count on DataFrame using SQL and print it</span>
  <span class="variable"><span class="keyword">val</span> wordCountsDataFrame</span> = 
    sqlContext.sql(<span class="string">"select word, count(*) as total from words group by word"</span>)
  wordCountsDataFrame.show()
}
</code></pre><h3 id="Caching_/_Persistence">Caching / Persistence</h3><p>跟RDD类似，DStream也允许将数据保存到内存中，使用persist方法可以做到这一点。</p>
<p>但是基于window和state的操作，reduceByWindow,reduceByKeyAndWindow,updateStateByKey它们就是隐式的保存了，系统已经帮它自动保存了。</p>
<p>从网络接收的数据(比如Kafka, Flume, sockets等)，默认是保存在两个节点来实现容错性，以序列化的方式保存在内存当中。</p>
<h3 id="Checkpointing">Checkpointing</h3><p>一个Spark Streaming程序必须是全天工作的，所以如果万一系统挂掉了或者JVM挂掉之后是要有容错性的。Spark Streaming需在容错存储系统做checkpoint，这样才能够处理错误信息。有两种类型的数据需要做checkpoint：</p>
<p>1.metadata checkpointing：元数据检查点。主要包括3个元数据：<br>配置：创建streaming程序的的配置信息<br>DStream操作：streaming程序中DStream的操作集合<br>未完成的批次：在队列中未完成的批次<br>2.data checkpointing：数据检查点。保存已经生成的RDD数据。在一些有状态的transformation操作中，一些RDD数据会依赖之前批次的RDD数据，随时时间的推移，这种依赖情况就会越发严重。为了解决这个问题，需要保存这些有依赖关系的RDD数据到存储系统中(比如HDFS)来剪断这种依赖关系</p>
<p>什么时候需要启用checkpoint？</p>
<p>满足以下2个条件中的任意1个即可启用checkpoint:</p>
<p>1.使用了有状态的transformation。比如使用了updateStateByKey或reduceByKeyAndWindow方法后，就需要启用checkpoint<br>2.恢复挂掉的程序。可以根据metadata数据恢复程序</p>
<p>一些比较简单的streaming程序没有用到有状态的transformation，并且也可以接受程序挂掉之后丢失部分数据，那么就没有必要启用checkpoint。</p>
<p>如何配置checkpoint？</p>
<p>checkpoint的配置需要设置一个目录，使用streamingContext.checkpoint(checkpointDirectory)方法。</p>
<pre><code>// Function to <span class="operator"><span class="keyword">create</span> <span class="keyword">and</span> setup a <span class="keyword">new</span> StreamingContext
<span class="keyword">def</span> functionToCreateContext(): StreamingContext = {
    val ssc = <span class="keyword">new</span> StreamingContext(...)   // <span class="keyword">new</span> <span class="keyword">context</span>
    val <span class="keyword">lines</span> = ssc.socketTextStream(...) // <span class="keyword">create</span> DStreams
    ...
    ssc.checkpoint(checkpointDirectory)   // <span class="keyword">set</span> checkpoint <span class="keyword">directory</span>
    ssc
}

// <span class="keyword">Get</span> StreamingContext <span class="keyword">from</span> checkpoint <span class="keyword">data</span> <span class="keyword">or</span> <span class="keyword">create</span> a <span class="keyword">new</span> one
val <span class="keyword">context</span> = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _)

// <span class="keyword">Do</span> additional setup <span class="keyword">on</span> <span class="keyword">context</span> that needs <span class="keyword">to</span> be done,
// irrespective <span class="keyword">of</span> whether it <span class="keyword">is</span> being started <span class="keyword">or</span> restarted
<span class="keyword">context</span>. ...

// <span class="keyword">Start</span> the <span class="keyword">context</span>
<span class="keyword">context</span>.<span class="keyword">start</span>()
<span class="keyword">context</span>.awaitTermination()</span>
</code></pre><p>因为检查操作会导致保存到hdfs上的开销，所以设置这个时间间隔，要很慎重。对于小批次的数据，比如一秒的，检查操作会大大降低吞吐量。但是检查的间隔太长，会导致任务变大。通常来说，5-10秒的检查间隔时间是比较合适的。</p>
]]></content>
    <summary type="html">
    <![CDATA[Spark Streaming是Spark核心API的扩展，用于处理实时数据流。Spark Streaming处理的数据源可以是Kafka，Flume，Twitter，ZeroMQ，Kinesis或者Tcp Sockets ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/tags/spark/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark编程指南笔记]]></title>
    <link href="http://fangjian0423.github.io/2016/01/27/spark-programming-guide/"/>
    <id>http://fangjian0423.github.io/2016/01/27/spark-programming-guide/</id>
    <published>2016-01-26T16:22:21.000Z</published>
    <updated>2016-01-26T16:24:07.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Spark初始化">Spark初始化</h2><p>使用Spark编程第一件要做的事就是初始化SparkContext对象，SparkContext对象会告诉Spark如何使用Spark集群。</p>
<p>SparkContext会使用SparkConf中的一些配置信息，所以构造SparkContext对象之前需要构造一个SparkConf对象。</p>
<p>一个JVM上的SparkContext只有一个是激活的，如果要构造一个新的SparkContext，必须stop一个已经激活的SparkContext。</p>
<pre><code>val conf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(<span class="string">"local"</span>)</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(<span class="string">"Test"</span>)</span></span>
</code></pre><p>  val sc = new SparkContext(conf)</p>
<p>appName为Test，这个name会在cluster UI上展示，master是一个Spark，Mesos，YARN cluster URL或者local。 具体的值可以参考 <a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="external">master-url解释</a>。</p>
<h2 id="RDD(Resilient_Distributed_Datasets)">RDD(Resilient Distributed Datasets)</h2><p>Spark提出的最主要抽象概念是RDD(弹性分布式数据集)，它是一个有容错机制并且可以被并行操作的元素集合。</p>
<p>有两种方式可以创建RDD:</p>
<p>1.使用一个已存在的集合进行并行计算<br>2.使用外部数据集，比如共享的文件系统，HDFS，HBase以及任何支持Hadoop InputFormat的数据源</p>
<h3 id="并行集合">并行集合</h3><p>使用SparkContext的parallelize方法构造并行集合。</p>
<pre><code>val dataSet = <span class="function"><span class="title">Array</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span></span>
val rdd = sc.<span class="function"><span class="title">parallelize</span><span class="params">(dataSet)</span></span>
rdd.<span class="function"><span class="title">reduce</span><span class="params">(_ + _)</span></span> <span class="comment">// 15</span>
</code></pre><p>parallelize方法有一个参数slices，表示数据集切分的份数。Spark会在集群上为每一个分片起一个任务。如果不设置的话，Spark会根据集群的情况，自动设置slices的数字。</p>
<h3 id="外部数据集">外部数据集</h3><p>文本文件可以使用SparkContext的textFile方法构造RDD。这个方法接收一个URI参数(也可以包括本地的文件)，并且以每行的方式读取文件内容。</p>
<p>比如data.txt文件里一行一个数字，取所有数字的和：</p>
<pre><code>val rdd = sc.textFile(this.getClass.getResource(<span class="string">"/data.txt"</span>).<span class="built_in">toString</span>)

rdd.<span class="built_in">reduce</span> {
   (a, b) =&gt; (a.toInt + b.toInt).<span class="built_in">toString</span>
}

<span class="comment">// 另外一种方式</span>
rdd.<span class="built_in">map</span>(s =&gt; s.toInt).<span class="built_in">reduce</span>(<span class="number">_</span> + <span class="number">_</span>)
</code></pre><p>Spark中所有基于文件的输入方法，都支持目录，压缩文件，通配符读取文件。比如</p>
<pre><code>sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"/data/*.txt"</span>)</span></span>
sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"/data"</span>)</span></span>
</code></pre><p>textFile方法也可以传入第二个可选参数来控制文件的分片数量。默认情况下，Spark会为文件的每一个块（在HDFS中块的大小默认是64MB）创建一个分片。但是你也可以通过传入一个更大的值来要求Spark建立更多的分片。注意，分片的数量绝不能小于文件块的数量。</p>
<p>除了文本文件之外，Spark还支持其他格式的输入：</p>
<p>1.SparkContext的wholeTextFiles方法会读取一个包含很多小文件的目录，并以filename，content为键值对的方式返回结果。<br>2.对于SequenceFiles，可以使用SparkContext的sequenceFile[K, V]方法创建。像 IntWritable和Text一样，它们必须是 Hadoop 的 Writable 接口的子类。另外，对于几种通用 Writable 类型，Spark 允许你指定原生类型来替代。例如：sequencFile[Int, String] 将会自动读取 IntWritable 和 Texts。<br>3.对于其他类型的 Hadoop 输入格式，你可以使用 SparkContext.hadoopRDD 方法，它可以接收任意类型的 JobConf 和输入格式类，键类型和值类型。按照像 Hadoop 作业一样的方法设置输入源就可以了。<br>4.RDD.saveAsObjectFile 和 SparkContext.objectFile 提供了以 Java 序列化的简单方式来保存 RDD。虽然这种方式没有 Avro 高效，但也是一种简单的方式来保存任意的 RDD。</p>
<h2 id="RDD操作">RDD操作</h2><h3 id="RDD操作基础">RDD操作基础</h3><p>RDD支持两种类型的操作。</p>
<p>1.transformations。从一个数据集产生一个新的数据集。比如map方法，就可以根据旧的数据集产生新的数据集。<br>2.actions。在一个数据集中进行聚合操作，并且返回一个最终的结果。</p>
<p>Spark中所有的transformations操作都是lazy的，就是说它们并不会立刻真的计算出结果。相反，它们仅仅是记录下了转换操作的操作对象（比如：一个文件）。只有当一个启动操作被执行，要向驱动程序返回结果时，转化操作才会真的开始计算。这样的设计使得Spark运行更加高效——比如，我们会发觉由map操作产生的数据集将会在reduce操作中用到，之后仅仅是返回了reduce的最终的结果而不是map产生的庞大数据集。</p>
<p>在默认情况下，每一个由转化操作得到的RDD都会在每次执行启动操作时重新计算生成。但是，你也可以通过调用persist(或cache)方法来将RDD持久化到内存中，这样Spark就可以在下次使用这个数据集时快速获得。Spark同样提供了对将RDD持久化到硬盘上或在多个节点间复制的支持。</p>
<p>一个计算文件中每行的字符串个数和所有字符串个数的和例子：</p>
<pre><code>val rdd = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"data.txt"</span>)</span></span>
val lineLengths = rdd.<span class="function"><span class="title">map</span><span class="params">(s =&gt; s.length)</span></span>
val totalLength = lineLengths.<span class="function"><span class="title">reduce</span><span class="params">(_ + _)</span></span>
</code></pre><p>lineLengths对象是一个transformations结果，所以它不是马上就开始执行的，当运行lineLengths.reduce的时候lineLengths才会开始去计算。如果之后还会用到这个lineLengths。可以在reduce方法之前加上:</p>
<pre><code>lineLengths.<span class="function"><span class="title">persist</span><span class="params">()</span></span>
</code></pre><h3 id="使用函数">使用函数</h3><p>Spark很多方法都可以使用函数完成。</p>
<p>使用对象：</p>
<pre><code><span class="class"><span class="keyword">object</span> <span class="title">SparkFunction</span> {</span>
  <span class="function"><span class="keyword">def</span> <span class="title">strLength</span> =</span> (s: <span class="type">String</span>) =&gt; s.length
}

<span class="keyword">val</span> lineLengths = rdds.map(<span class="type">SparkFunction</span>.strLength)
lineLengths.reduce(_ + _)
</code></pre><p>使用类：</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">SparkCls</span> </span>{
  def <span class="func"><span class="keyword">func</span> = <span class="params">(s: String)</span></span> =&gt; s.length
  def buildRdd(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = rdd.<span class="built_in">map</span>(<span class="func"><span class="keyword">func</span>)
}
<span class="title">new</span> <span class="title">SparkCls</span><span class="params">()</span></span>.buildRdd(rdds).<span class="built_in">reduce</span>(<span class="number">_</span> + <span class="number">_</span>)
</code></pre><h3 id="闭包">闭包</h3><pre><code><span class="tag">var</span> counter = <span class="number">0</span>
<span class="tag">var</span> rdd = sc.<span class="function"><span class="title">parallelize</span><span class="params">(data)</span></span>

<span class="comment">// Wrong: Don't do this!!</span>
rdd.<span class="function"><span class="title">foreach</span><span class="params">(x =&gt; counter += x)</span></span>

<span class="function"><span class="title">println</span><span class="params">(<span class="string">"Counter value: "</span> + counter)</span></span>
</code></pre><p>上述代码如果在local模式并且在一个JVM的情况下使用是可以得到正确的值的。这是因为所有的RDD和变量counter都是同一块内存上。</p>
<p>然后在集群模式下，上述代码的结果可能就不会是我们想要的正确结果。集群模式下，Spark会在RDD分成多个任务，每个任务都会被对应的executor执行。在executor执行之前，Spark会计算每个闭包。上面这个例子foreach方法和counter就组成了一个闭包。这个闭包会被序列化并且发送给每个executor。在local模式下，因为只有一个executor，所以共享相同的闭包。然后在集群模式下，有多个executor，并且各个executor在不同的节点上都有自己的闭包的拷贝。</p>
<p>所以counter变量就已经不再是节点上的变量了。虽然counter变量在内存上依然存在，但是它对于executor已经不可见，executor只知道它是序列化后的闭包的一份拷贝。因此如果counter的操作都是在闭包下的话，counter的值还是为0。</p>
<p>Spark提供了一种Accumulator的概念用来处理集群模式下的变量更新问题。</p>
<p>另外一个要注意的是不要使用foreach或者map方法打印数据。在一台机器上，这个操作是没有问题的。但是如果在集群上，不一定会打印出全部的数据。可以使用collect方法将RDD放到调用节点上。所以rdd.collect().foreach(println)是可以打印出数据的，但是可能数据量过大，会导致OOM。所以最好的方式还是使用take方法：rdd.take(100).foreach(println)。</p>
<h3 id="使用键值对">使用键值对</h3><p>Spark也支持键值对的操作，这在分组和聚合操作时候用得到。当键值对中的键为自定义对象时，需要自定义该对象的equals()和hashCode()方法。</p>
<p>一个使用键值对的单词统计例子：</p>
<pre><code>// 使用map方法将单词文本转换成一个键值对，(word, num)。 num初始值为<span class="number">1</span>
val pairs = rdd.map(s =&gt; (s, <span class="number">1</span>))
val reduceRdd = pairs.reduceByKey(_ + _)
val <span class="literal">result</span> = reduceRdd.sortByKey().collect()
<span class="literal">result</span>.foreach(println)
</code></pre><h3 id="Spark内置的Transformations">Spark内置的Transformations</h3><table>
<thead>
<tr>
<th style="text-align:center">转换</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">map(func)</td>
<td style="text-align:center">根据func函数生成一个新的rdd数据集</td>
</tr>
<tr>
<td style="text-align:center">filter(func)</td>
<td style="text-align:center">根据func函数返回true的数据集</td>
</tr>
<tr>
<td style="text-align:center">flatMap(func)</td>
<td style="text-align:center">跟map方法类似，但是每一项可以返回多个值。func函数的返回值是一个集合</td>
</tr>
<tr>
<td style="text-align:center">mapPartitions(func)</td>
<td style="text-align:center">跟map方法类似，但是是在每个partition上运行的。func函数的参数是一个Iteraror，返回值也是一个Iterator。如果map方法需要创建一个额外的对象，使用mapPartitions方法比map方法高效得多</td>
</tr>
<tr>
<td style="text-align:center">mapPartitionsWithIndex(func)</td>
<td style="text-align:center">作用跟mapPartitions方法一样，只是func方法多了一个index参数。 func方法定义 (Int, Iterator[T]) =&gt; Iterator[U]</td>
</tr>
<tr>
<td style="text-align:center">sample(withReplacement, fraction, seed)</td>
<td style="text-align:center">根据 fraction 指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed 用于指定随机数生成器种子</td>
</tr>
<tr>
<td style="text-align:center">union(otherDataset)</td>
<td style="text-align:center">取2个rdd的并集，得到一个新的rdd</td>
</tr>
<tr>
<td style="text-align:center">intersection(otherDataset)</td>
<td style="text-align:center">取2个rdd的交集，得到一个新的rdd。这个新的rdd没有重复的数据</td>
</tr>
<tr>
<td style="text-align:center">distinct([numTasks])</td>
<td style="text-align:center">返回一个新的没有重复数据的数据集</td>
</tr>
<tr>
<td style="text-align:center">groupByKey([numTasks])</td>
<td style="text-align:center">将一个(K,V)的键值对RDD转换成一个(K, Iterable[V])的新的键值对RDD。注意点：如果group的目的是为了做聚合计算(比如总和或者平均值)，使用reduceByKey或者aggregateByKey性能更好。</td>
</tr>
<tr>
<td style="text-align:center">reduceByKey(func, [numTasks])</td>
<td style="text-align:center">跟groupByKey方法一样，也是操作(K, V)的键值对RDD。返回值同样是一个(K, V)的键值对RDD，func函数的定义：(V, V) =&gt; V，也就是每两个值的值</td>
</tr>
<tr>
<td style="text-align:center">aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</td>
<td style="text-align:center">跟reduceByKey作用类似，zeroValue参数表示初始值，这个初始值的类型可以跟rdd中的键值对的值的类型不同。seqOp参数是个函数，定义为(U, V) =&gt; U，U类型是初始化zeroValue的类型，V类型是一开始rdd的键值对的值的类型。这个函数表示用来与初始值zeroValue进行比较，取一个新的值，需要注意的是这个新的值会作为参数出现在下一次key相等的情况下。 combOp参数也是个函数，定义为(U, U) =&gt; U，U类型也是初始值的类型。这个函数相当于reduce方法中的函数，用来做聚合操作</td>
</tr>
<tr>
<td style="text-align:center">sortByKey([ascending], [numTasks])</td>
<td style="text-align:center">对一个(K, V)键值对的RDD进行排行，返回一个基于K排序的新的RDD</td>
</tr>
<tr>
<td style="text-align:center">join(otherDataset, [numTasks])</td>
<td style="text-align:center">基于(K, V)键值对的rdd，如果对(K, W)的键值对rdd使用join操作，可以产生(K, (V, W))键值对的rdd。类似数据库中的join操作，spark还提供leftOuterJoin, rightOuterJoin, fullOuterJoin方法</td>
</tr>
<tr>
<td style="text-align:center">cogroup(otherDataset, [numTasks])</td>
<td style="text-align:center">跟join方法类似，不过是基于(K, V)的rdd，cogroup基于(K, W)的rdd，产生(K, (Iterable[V], Iterable[W]))的rdd。这个方法也叫做groupWith</td>
</tr>
<tr>
<td style="text-align:center">cartesian(otherDataset)</td>
<td style="text-align:center">笛卡尔积。 有K的rdd与V的rdd进行笛卡尔积，会生成(K, V)的rdd</td>
</tr>
<tr>
<td style="text-align:center">pipe(command, [envVars])</td>
<td style="text-align:center">对rdd进行管道操作。 就像shell命令一样</td>
</tr>
<tr>
<td style="text-align:center">coalesce(numPartitions)</td>
<td style="text-align:center">减少 RDD 的分区数到指定值。在过滤大量数据之后，可以执行此操作</td>
</tr>
<tr>
<td style="text-align:center">repartition(numPartitions)</td>
<td style="text-align:center">重新给 RDD 分区</td>
</tr>
<tr>
<td style="text-align:center">repartitionAndSortWithinPartitions(partitioner)</td>
<td style="text-align:center">重新给 RDD 分区，并且每个分区内以记录的 key 排序</td>
</tr>
</tbody>
</table>
<p>以这个数据为例：</p>
<pre><code><span class="literal">i</span>
am
<span class="keyword">format</span>
let
<span class="keyword">us</span>
go
hoho
good
nice
<span class="keyword">format</span>
is
nice
haha
haha
haha
<span class="keyword">scala</span> is cool, nice
</code></pre><p>一些Transformations操作：</p>
<pre><code>rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; s<span class="built_in">.</span>length) <span class="comment">// 每一行文本转换成长度</span>
rdd<span class="built_in">.</span>filter(s =&gt; s<span class="built_in">.</span>length == <span class="number">1</span>) <span class="comment">// 取文本长度为1的数据</span>
rdd<span class="built_in">.</span>flatMap(s =&gt; s<span class="built_in">.</span>split(<span class="string">","</span>)) <span class="comment">// 把有 , 的字符串转换成多行</span>
rdd<span class="built_in">.</span>sample(<span class="literal">false</span>, <span class="number">1.0</span>)
rdd<span class="built_in">.</span>union(rdd2)
rdd<span class="built_in">.</span>intersection(rdd2)
rdd<span class="built_in">.</span>distinct(rdd2)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>groupByKey()<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    print(<span class="built_in">pair</span><span class="built_in">.</span>_1) ; print(<span class="string">" ** "</span>)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
  }
}
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>reduceByKey {
  (x, y) =&gt; x + y
}<span class="built_in">.</span>foreach {
  (<span class="built_in">pair</span>) =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>aggregateByKey(<span class="number">0</span>)(
  (a, b) =&gt; {
    math<span class="built_in">.</span><span class="keyword">max</span>(a, b)
  }, (a, b) =&gt; {
    a + b
  }
)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>sortByKey(<span class="literal">true</span>)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span><span class="keyword">join</span>(rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>charAt(<span class="number">0</span>)<span class="built_in">.</span>toUpper<span class="built_in">.</span>toString)))<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>cogroup(rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>charAt(<span class="number">0</span>)<span class="built_in">.</span>toUpper<span class="built_in">.</span>toString)))<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">"======"</span>)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_1<span class="built_in">.</span>toList<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_2<span class="built_in">.</span>toList<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
    println(<span class="string">"**"</span> * <span class="number">8</span>)
  }
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; s<span class="built_in">.</span>length)<span class="built_in">.</span>cartesian(rdd)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    println(<span class="built_in">pair</span><span class="built_in">.</span>_1)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2)
    println(<span class="string">"**"</span> * <span class="number">8</span>)
  }
}
</code></pre><h3 id="Spark内置的Actions">Spark内置的Actions</h3><table>
<thead>
<tr>
<th style="text-align:center">动作</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">reduce(func)</td>
<td style="text-align:center">聚合操作。之前很多例子都使用了reduce方法。这个功能必须可交换且可关联的，从而可以正确的被并行执行。</td>
</tr>
<tr>
<td style="text-align:center">collect()</td>
<td style="text-align:center">返回rdd中所有的元素，返回值类型是Array。这个方法经常用来取数据量比较小的集合</td>
</tr>
<tr>
<td style="text-align:center">count()</td>
<td style="text-align:center">rdd中的元素个数</td>
</tr>
<tr>
<td style="text-align:center">first()</td>
<td style="text-align:center">返回数据集的第一个元素</td>
</tr>
<tr>
<td style="text-align:center">take(n)</td>
<td style="text-align:center">返回数据集的前n个元素，返回值是个Array</td>
</tr>
<tr>
<td style="text-align:center">takeSample(withReplacement, num, [seed])</td>
<td style="text-align:center">返回一个数组，在数据集中随机采样 num 个元素组成，可以选择是否用随机数替换不足的部分，seed 用于指定的随机数生成器种子返回数据集的前n个元素，返回值是个Array</td>
</tr>
<tr>
<td style="text-align:center">takeOrdered(n, [ordering])</td>
<td style="text-align:center">返回自然顺序或者自定义顺序的前 n 个元素</td>
</tr>
<tr>
<td style="text-align:center">saveAsTextFile(path)</td>
<td style="text-align:center">把数据集中的元素写到文件里，可以写到本地文件系统上，hdfs上或者任意Hadoop支持的文件系统上。Spark会调用元素的toString方法将其转换成文本的一行</td>
</tr>
<tr>
<td style="text-align:center">saveAsSequenceFile(path)</td>
<td style="text-align:center">跟saveAsTextFile方法类似，但是是写成SequenceFile文件格式，也是支持写到本地文件系统上，hdfs上或者任意Hadoop支持的文件系统上。这个方法只能作用于键值对的RDD</td>
</tr>
<tr>
<td style="text-align:center">saveAsObjectFile(path)</td>
<td style="text-align:center">跟saveAsTextFile方法类似，是使用Java的序列化的方式保存文件</td>
</tr>
<tr>
<td style="text-align:center">countByKey()</td>
<td style="text-align:center">计算键值的数量。对键值对(K, V)的rdd数据集，返回(K, Int)的Map</td>
</tr>
<tr>
<td style="text-align:center">foreach(func)</td>
<td style="text-align:center">使用func遍历rdd数据集中的各个元素。这通常用于边缘效果，例如更新一个Accumulator，或者和外部存储系统进行交互</td>
</tr>
</tbody>
</table>
<p>一些Actions操作：</p>
<pre><code>rdd<span class="built_in">.</span>saveAsTextFile(<span class="string">"file:///tmp/data01"</span>)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>saveAsSequenceFile(<span class="string">"file:///tmp/data02"</span>)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>countByKey()<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}
</code></pre><h2 id="RDD的持久化(Persistence)">RDD的持久化(Persistence)</h2><p>Spark的一个重要功能就是在将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。当持久化一个RDD的时候，每个存储着这个RDD的分片节点都会计算然后保存到内存中以便下次再次使用。这使得接下来的计算过程速度能够加快（经常能加快超过十倍的速度）。缓存是加快迭代算法和快速交互过程速度的关键工具。</p>
<p>可以使用persist或者cache方法让rdd持久化。在第一次被计算产生之后，它就会始终停留在节点的内存中。Spark的缓存是具有容错性的——如果RDD的任意一个分片丢失了，Spark就会依照这个RDD产生的转化过程自动重算一遍。</p>
<p>另外，每个持久化后的RDD可以使用不用级别的存储级别。比如可以存在硬盘中，可以存在内存中，还可以将这个数据集在节点之间复制，或者使用 Tachyon 将它储存到堆外。这些存储级别都是通过向 persist() 传递一个 StorageLevel 对象（Scala, Java, Python）来设置的。</p>
<p>Spark的一些存储级别如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">存储级别</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">MEMORY_ONLY</td>
<td style="text-align:center">默认级别。将RDD作为反序列化的的对象存储在JVM中。如果不能被内存装下，一些分区将不会被缓存，并且在需要的时候被重新计算</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_AND_DISK</td>
<td style="text-align:center">默认级别。将RDD作为反序列化的的对象存储在JVM中。如果不能被内存装下，会存在硬盘上，并且在需要的时候被重新计算</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_ONLY_SER</td>
<td style="text-align:center">将RDD作为序列化的的对象进行存储（每一分区占用一个字节数组）。通常来说，这比将对象反序列化的空间利用率更高，尤其当使用fast serializer,但在读取时会比较占用CPU</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_AND_DISK_SER</td>
<td style="text-align:center">与MEMORY_ONLY_SER相似，但是把超出内存的分区将存储在硬盘上而不是在每次需要的时候重新计算</td>
</tr>
<tr>
<td style="text-align:center">DISK_ONLY</td>
<td style="text-align:center">只存储RDD分区到硬盘上</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_ONLY_2, MEMORY_AND_DISK_2 等</td>
<td style="text-align:center">与上述的存储级别一样，但是将每一个分区都复制到两个集群结点上</td>
</tr>
</tbody>
</table>
<p>存储级别的选择：</p>
<p>如果你的 RDD 可以很好的与默认的存储级别契合，就不需要做任何修改了。这已经是 CPU 使用效率最高的选项，它使得 RDD的操作尽可能的快。</p>
<p>如果不行，试着使用 MEMORY_ONLY_SER 并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</p>
<p>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</p>
<p>如果你想有快速故障恢复能力，使用复制存储级别。例如：用 Spark 来响应web应用的请求。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在 RDD 上持续的运行任务，而不需要等待丢失的分区被重新计算。</p>
<p>如果你想要定义你自己的存储级别，比如复制因子为3而不是2，可以使用 StorageLevel 单例对象的 apply()方法。</p>
<h2 id="共享变量">共享变量</h2><p>通常情况下，当一个函数在远程集群节点上通过Spark操作(比如map或者reduce)，Spark会对涉及到的变量的所有副本执行这个函数。这些变量都会被拷贝到每台机器上，而且这个过程不会被反馈到驱动程序。通常情况下，在任务之间读写共享变量是很低效的。但是Spark仍然提供了有限的两种共享变量类型用于常见的使用场景：broadcast variables 和 accumulators。</p>
<h3 id="broadcast_variables(广播变量)">broadcast variables(广播变量)</h3><p>广播变量允许程序员在每台机器上保持一个只读变量的缓存而不是将一个变量的拷贝传递给各个任务。这些变量是可以被使用的，比如，给每个节点传递一份大输入数据集的拷贝是很耗时的。Spark试图使用高效的广播算法来分布广播变量，以此来降低通信花销。可以通过SparkContext.broadcast(v)来从变量v创建一个广播变量。这个广播变量是v的一个包装，同时它的值可以调用value方法获得：</p>
<pre><code>val broadcastVar = sc.<span class="function"><span class="title">broadcast</span><span class="params">(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span></span>)

broadcastVar<span class="class">.value</span> <span class="comment">// Array(1, 2, 3)</span>
</code></pre><p>一个广播变量被创建之后，在所有函数中都应当使用它来代替原来的变量v，这样就可以包装v在节点之间只被传递一次。另外，v变量在被广播之后不应该再被修改，这样可以确保每一个节点上储存的广播变量的一致性。</p>
<h3 id="accumulators(累加器)">accumulators(累加器)</h3><p>累加器是在一个相关过程中只能被”累加”的变量，对这个变量的操作可以有效地被并行化。它们可以被用于实现计数器（就像在MapReduce过程中）或求和运算。Spark原生支持对数字类型的累加器，程序员也可以为其他新的类型添加支持。累加器被以一个名字创建之后，会在Spark的UI中显示出来。这有助于了解计算的累进过程（注意：目前Python中不支持这个特性）。</p>
<p>可以通过SparkContext.accumulator(v)来从变量v创建一个累加器。在集群中运行的任务随后可以使用add方法或+=操作符（在Scala和Python中）来向这个累加器中累加值。但是，他们不能读取累加器中的值。只有驱动程序可以读取累加器中的值，通过累加器的value方法。</p>
<p>以下的代码展示了向一个累加器中累加数组元素的过程：</p>
<pre><code>val accum = sc.<span class="function"><span class="title">accumulator</span><span class="params">(<span class="number">0</span>, <span class="string">"My Accumulator"</span>)</span></span>
sc.<span class="function"><span class="title">parallelize</span><span class="params">(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span></span>).<span class="function"><span class="title">foreach</span><span class="params">(x =&gt; accum += x)</span></span>
accum<span class="class">.value</span> <span class="comment">// 10</span>
</code></pre><p>这段代码利用了累加器对Int类型的内建支持，程序员可以通过继承 AccumulatorParam 类来创建自己想要的类型支持。AccumulatorParam 的接口提供了两个方法：zero 用于为你的数据类型提供零值；addInPlace 用于计算两个值得和。比如，假设我们有一个 Vector类表示数学中的向量，我们可以这样写：</p>
<pre><code>object VectorAccumulatorParam extends AccumulatorParam[Vector] {
  <span class="function"><span class="keyword">def</span> <span class="title">zero</span><span class="params">(initialValue: Vector)</span>:</span> Vector = {
    Vector.zeros(initialValue.size)
  }
  <span class="function"><span class="keyword">def</span> <span class="title">addInPlace</span><span class="params">(v1: Vector, v2: Vector)</span>:</span> Vector = {
    v1 += v2
  }
}

// Then, create an Accumulator of this type:
val vecAccum = sc.accumulator(new Vector(...))(VectorAccumulatorParam)
</code></pre><p>累加器的更新操作只会被运行一次，Spark 提供了保证，每个任务中对累加器的更新操作都只会被运行一次。比如，重启一个任务不会再次更新累加器。在转化过程中，用户应该留意每个任务的更新操作在任务或作业重新运算时是否被执行了超过一次。</p>
<p>累加器不会改变Spark的惰性求值模型。如果累加器在对RDD的操作中被更新了，它们的值只会在启动操作中作为 RDD 计算过程中的一部分被更新。所以，在一个懒惰的转化操作中调用累加器的更新，并没法保证会被及时运行。下面的代码段展示了这一点：</p>
<pre><code><span class="title">accum</span> = sc.accumulator(<span class="number">0</span>)
<span class="typedef"><span class="keyword">data</span>.map<span class="container">(<span class="title">lambda</span> <span class="title">x</span> =&gt; <span class="title">acc</span>.<span class="title">add</span>(<span class="title">x</span>)</span>; f<span class="container">(<span class="title">x</span>)</span>)</span>
</code></pre><h2 id="参考资料">参考资料</h2><p><a href="http://spark.apache.org/docs/latest/programming-guide.html/" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[Spark编程指南笔记，参考官方文档的编程指南，翻译再加上一些自己写的代码 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/tags/spark/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[记录Flume Channel队列满了之后发生的怪异问题]]></title>
    <link href="http://fangjian0423.github.io/2016/01/19/flume-channel-full-exception/"/>
    <id>http://fangjian0423.github.io/2016/01/19/flume-channel-full-exception/</id>
    <published>2016-01-19T12:07:35.000Z</published>
    <updated>2016-01-19T12:11:10.000Z</updated>
    <content type="html"><![CDATA[<p>Flume的这个问题纠结了2个月，因为之前实在太忙了，没有时间来研究这个问题产生的原理，今天终于研究出来了，找出了这个问题所在。</p>
<p>先来描述一下这个问题的现象：</p>
<p>Flume的Sink用的是Custom Sink，由于这个Custom Sink写的有一点小问题，比如batchSize是5000次，第4000条就会发生exception，这样每次都会写入4000条数据。Sink处理的时候都会发生异常，每次都会rollback，rollback方面的知识可以参考<a href="http://fangjian0423.github.io/2016/01/03/flume-transaction/">Flume Transaction介绍</a>。</p>
<p>这样造成的后果有2个：</p>
<p>1.Channel中的数据满了。会发生以下异常：</p>
<pre><code>Caused by: org.apache.flume.ChannelFullException: Space <span class="keyword">for</span> commit <span class="keyword">to</span> queue couldn<span class="attribute">'t</span> be acquired. Sinks are likely <span class="keyword">not</span> keeping up <span class="keyword">with</span> sources, <span class="keyword">or</span> the <span class="keyword">buffer</span> size <span class="keyword">is</span> too tight
</code></pre><p>2.Sink会一直写数据，造成数据量暴增。</p>
<p>3.如果用了interceptor，且修改了event中的数据，那么会重复处理这些修改完后的event数据。</p>
<p>前面2个很容易理解，Sink发生异常，transaction rollback，导致channel中的队列满了。</p>
<p>关键是第三点，很让人费解。</p>
<p>以一段伪需求和伪代码为例，TestInterceptor的intercept方法：</p>
<p>比如处理一段json：</p>
<pre><code>{"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">languages</span>": <span class="value">[<span class="string">"java"</span>, <span class="string">"scala"</span>, <span class="string">"javascript"</span>]</span>}
</code></pre><p>使用interceptor处理成:</p>
<pre><code>[{"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"java"</span></span>}, {"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"scala"</span></span>}, {"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"javascript"</span></span>}]
</code></pre><p>interceptor代码如下：</p>
<pre><code><span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span>(<span class="params">Event <span class="keyword">event</span></span>) </span>{
    Model model = <span class="keyword">null</span>;
    String jsonStr = <span class="keyword">new</span> String(<span class="keyword">event</span>.getBody(), <span class="string">"UTF-8"</span>);
    <span class="keyword">try</span> {
        model = parseJsonStr(jsonStr);
    } <span class="keyword">catch</span> (Exception e) {
        log.error(<span class="string">"convert json data error"</span>);
    }
    <span class="keyword">event</span>.setBody(model.getJsonString().getBytes());
    <span class="keyword">return</span> <span class="keyword">event</span>;
}
</code></pre><p>当Channel中的队列已经满了以后，上述代码会打印出convert json data error，而且jsonStr的内容居然是转换后的数据，这一点一开始让我十分费解，误以为transaction rollback之后会修改source中的数据。后来debug源码发现错误在Source中。</p>
<p>后来发现并不是这样的。</p>
<p>以KafkaSource为例，KafkaSource中有一个属性eventList，是个ArrayList。用来接收kafka consume的message。</p>
<p>直接说明KafkaSource的process方法源码：</p>
<pre><code><span class="keyword">public</span> Status process() <span class="keyword">throws</span> EventDeliveryException {

  <span class="built_in">byte</span>[] kafkaMessage;
  <span class="built_in">byte</span>[] kafkaKey;
  Event event;
  Map&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; headers;
  <span class="keyword">long</span> batchStartTime = System.currentTimeMillis();
  <span class="keyword">long</span> batchEndTime = System.currentTimeMillis() + timeUpperLimit;
  <span class="keyword">try</span> {

    <span class="comment">/** 这里读取kafka中的message **/</span>
    <span class="built_in">boolean</span> iterStatus = <span class="keyword">false</span>;
    <span class="keyword">while</span> (eventList.<span class="built_in">size</span>() &lt; batchUpperLimit &amp;&amp;
            System.currentTimeMillis() &lt; batchEndTime) {
      iterStatus = hasNext();
      <span class="keyword">if</span> (iterStatus) {
        <span class="comment">// get next message</span>
        MessageAndMetadata&lt;<span class="built_in">byte</span>[], <span class="built_in">byte</span>[]&gt; messageAndMetadata = it.next();
        kafkaMessage = messageAndMetadata.message();
        kafkaKey = messageAndMetadata.<span class="variable">key</span>();

        <span class="comment">// Add headers to event (topic, timestamp, and key)</span>
        headers = <span class="keyword">new</span> <span class="keyword">HashMap</span>&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;();
        headers.put(KafkaSourceConstants.TIMESTAMP,
                <span class="keyword">String</span>.valueOf(System.currentTimeMillis()));
        headers.put(KafkaSourceConstants.TOPIC, topic);
        headers.put(KafkaSourceConstants.KEY, <span class="keyword">new</span> <span class="keyword">String</span>(kafkaKey));
        <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
          <span class="built_in">log</span>.debug(<span class="string">"Message: {}"</span>, <span class="keyword">new</span> <span class="keyword">String</span>(kafkaMessage));
        }
        event = EventBuilder.withBody(kafkaMessage, headers);
        eventList.<span class="built_in">add</span>(event);
      }
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Waited: {} "</span>, System.currentTimeMillis() - batchStartTime);
        <span class="built_in">log</span>.debug(<span class="string">"Event #: {}"</span>, eventList.<span class="built_in">size</span>());
      }
    }
    <span class="comment">/** 这里读取kafka中的message **/</span>

    <span class="comment">// If we have events, send events to channel</span>
    <span class="comment">// clear the event list</span>
    <span class="comment">// and commit if Kafka doesn't auto-commit</span>
    <span class="keyword">if</span> (eventList.<span class="built_in">size</span>() &gt; <span class="number">0</span>) {
      <span class="comment">// 使用ChannelProcess将Source中读取的数据给各个Channel</span>
      <span class="comment">// 如果getChannelProcessor().processEventBatch(eventList);发生了异常，eventList不会被清空，而且processEventBatch方法会调用Interceptor处理event中的数据，event中的数据已经被转换。所以下一次会将转换后的event数据再次传给Interceptor。</span>
      getChannelProcessor().processEventBatch(eventList);
      eventList.<span class="built_in">clear</span>();
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Wrote {} events to channel"</span>, eventList.<span class="built_in">size</span>());
      }
      <span class="keyword">if</span> (!kafkaAutoCommitEnabled) {
        <span class="comment">// commit the read transactions to Kafka to avoid duplicates</span>
        consumer.commitOffsets();
      }
    }
    <span class="keyword">if</span> (!iterStatus) {
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Returning with backoff. No more data to read"</span>);
      }
      <span class="keyword">return</span> Status.BACKOFF;
    }
    <span class="keyword">return</span> Status.READY;
  } <span class="keyword">catch</span> (Exception e) {
    <span class="built_in">log</span>.error(<span class="string">"KafkaSource EXCEPTION, {}"</span>, e);
    <span class="keyword">return</span> Status.BACKOFF;
  }
}
</code></pre><p>上述代码已经加了备注，再重申一下：ChannelProcess的processEventBatch方法会调用Interceptor处理event中的数据。所以如果Channel中的队列满了，那么processEventBatch方法会发生异常，发生异常之后eventList中的没有进入channel的数据已经被Interceptor修改，且不会被清空。因此下次还是会使用这些数据，所以会发生convert json data error错误。</p>
<p>画了一个序列图如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/flume-channel-full01.png" alt=""></p>
<p>第6步添加event到channel中的时候，队列已满，所以会抛出异常。最终异常被KafkaSource捕捉，但是eventList内部的部分数据已经被interceptor修改过。</p>
<p>多个channel的影响：</p>
<p>如果有多个channel，这个问题也会影响。比如有2个channel，c1和c2。c1的sink没有问题，一直稳定执行，c2对应的sink是一个CustomSink，会有问题。这样c2中的队列迟早会爆满，爆满之后，ChannelProcess批量处理event的时候，由于c2的队列满了，所以Source中的eventList永远不会被清空，eventList永远不会被清空的话，所有的channel都会被影响到，这就好比水源被污染之后，所有的用水都会受到影响。</p>
<p>举个例子：source为s1，c1对应的sink是k1，c2对应的sink是k2。k1和k2的batchSize都是5000，k2处理第4000条数据的时候总会发生异常，进行回滚。k1很稳定。这样c2迟早会爆满，爆满之后s1的eventList一直不能clear，这样也会导致c1一直在处理，所以k1的数据量跟k2一样也会暴增。</p>
<p>要避免本文所说的这一系列情况，最好的做法就是sink必须要加上很好的异常处理机制，不是任何情况都可以rollback的，要根据需求做对应的处理。</p>
]]></content>
    <summary type="html">
    <![CDATA[记录Flume Channel队列满了之后发生的怪异问题。数据量暴增，Channel队列爆满 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Kafka介绍]]></title>
    <link href="http://fangjian0423.github.io/2016/01/13/kafka-intro/"/>
    <id>http://fangjian0423.github.io/2016/01/13/kafka-intro/</id>
    <published>2016-01-13T15:54:51.000Z</published>
    <updated>2016-01-13T15:55:05.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Kafka介绍">Kafka介绍</h2><p>Kafka是一个分布式的发布-订阅消息系统(Producer-Consumer)，是一种快速、可扩展的、分区的和可复制的日志服务。</p>
<p>kafka中的几个概念：</p>
<p>Topic：用来区别各种message。比如A系统的所有message的Topic为A，B系统的所有message的Topic为B。</p>
<p>Broker：已发布的消息保存在一组服务器中，这组服务器就被称为Broker或Kafka集群。</p>
<p>Producer：生产者，用于发布消息，发布消息到kafka broker。</p>
<p>Consumer：消费者，订阅消息，订阅kafka broker中的已经被发布的消息。</p>
<p>下图是几个概念的说明：</p>
<p>producer发布消息到kafka cluster(也就是kafka broker)，然后发布后的这些消息被consumer订阅。</p>
<p>从图中也可以看出来，kafka支持多个producer和多个consumer。</p>
<p><img src="http://kafka.apache.org/images/producer_consumer.png" alt=""></p>
<h2 id="Kafka存储机制">Kafka存储机制</h2><p>Partition：Kafka中每个Topic都会有一个或多个Partition，由于kafka将数据直接写到硬盘里，这里的Partition对应一个文件夹，文件夹下存储这个Partition的所有消息和索引。如果有2个Topic，分别有3个和4个Partition。那么总共有7个文件夹。Kafka内部会根据一个算法，根据消息得出一个值，然后根据这个值放入对应的partition目录中的段文件里。</p>
<p>比如在一台机器上创建partition为3，topic为test01和partition为4，topic为test02的2个topic。</p>
<p>创建完之后 /tmp/kafka-logs中就会有7个文件夹，分别是 </p>
<p>test01-0<br>test01-1<br>test01-2<br>test02-0<br>test02-1<br>test02-2<br>test02-3</p>
<p>Segment：组成Partiton的组件。一个Partition代表一个文件夹，而Segment则是这个文件夹下的各个文件。每个Segmenet文件有大小限制，在配置文件中用log.segment.bytes配置。</p>
<pre><code><span class="built_in">log</span>.<span class="keyword">segment</span>.<span class="keyword">bytes</span>=<span class="number">1073741824</span>
</code></pre><p>当文件的大小超过1073741824字节的时候，会创建第一个段文件。需要注意的是这里每个段文件中的消息数量不一定相等，因为虽然他们的字节数一样，但是每个消息的字节数是不一样的，所以每个段文件中的消息数量不一定相等。</p>
<p>每个段文件由2部分组成，分别是index file和log file，表示索引文件和日志(数据)文件。这2个文件一一对应。</p>
<p>第一个segment文件从0开始，后续每个segment文件名是上一个segment文件的最后一条message的offset值，数值最大为64位long大小，19位数字字符长度，没有数字用0填充。</p>
<p>下面是做的一个例子，partition和replication-factor都为1，每个segmenet文件的大小是5M。有500000条message，一共生成了4对文件，这里00000000000000137200.log文件表示是00000000000000000000.log中存储了137199个message，这个文件开始存储第137200个message。</p>
<p>00000000000000000000.index<br>00000000000000000000.log</p>
<p>00000000000000137200.index<br>00000000000000137200.log</p>
<p>00000000000000271600.index<br>00000000000000271600.log</p>
<p>00000000000000406000.index<br>00000000000000406000.log</p>
<p>offset：用来标识message在partition中的下标，用来定位message。</p>
<p>Kafka内部存储结构可以参考<a href="http://tech.meituan.com/kafka-fs-design-theory.html" target="_blank" rel="external">Kafka文件存储机制那些事</a>文章里的讲解。</p>
<p>一个kafka producer例子：</p>
<pre><code><span class="keyword">import</span> java.util.<span class="type">Properties</span>
<span class="keyword">import</span> kafka.producer.{<span class="type">KeyedMessage</span>, <span class="type">Producer</span>, <span class="type">ProducerConfig</span>}

<span class="class"><span class="keyword">object</span> <span class="title">TestProducer</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">App</span> {</span>

  <span class="keyword">val</span> events = <span class="number">500000</span>
  <span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>()
  <span class="keyword">val</span> brokers = <span class="string">"localhost:9092"</span>
  props.put(<span class="string">"metadata.broker.list"</span>, brokers)
  props.put(<span class="string">"serializer.class"</span>, <span class="string">"kafka.serializer.StringEncoder"</span>)
  props.put(<span class="string">"producer.type"</span>, <span class="string">"async"</span>)

  <span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">ProducerConfig</span>(props)

  <span class="keyword">val</span> topic = <span class="string">"format03"</span>

  <span class="keyword">val</span> producer = <span class="keyword">new</span> <span class="type">Producer</span>[<span class="type">String</span>, <span class="type">String</span>](config)

  <span class="keyword">for</span>(nEvents &lt;- <span class="type">Range</span>(<span class="number">0</span>, events)) {
    <span class="keyword">val</span> msg = <span class="string">"Message"</span> + nEvents
    <span class="keyword">val</span> data = <span class="keyword">new</span> <span class="type">KeyedMessage</span>[<span class="type">String</span>, <span class="type">String</span>](topic, msg)
    producer.send(data)
  }

  producer.close()

}
</code></pre><h2 id="参考资料">参考资料</h2><p><a href="http://tech.meituan.com/kafka-fs-design-theory.html" target="_blank" rel="external">Kafka文件存储机制那些事</a><br><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-1/" target="_blank" rel="external">Kafka剖析（一）：Kafka背景及架构介绍</a><br><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-2/" target="_blank" rel="external">Kafka设计解析（二）：Kafka High Availability （上）</a><br><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-3/" target="_blank" rel="external">Kafka设计解析（三）：Kafka High Availability （下）</a></p>
]]></content>
    <summary type="html">
    <![CDATA[Kafka是一个分布式的发布-订阅消息系统(Producer-Consumer)，是一种快速、可扩展的、分区的和可复制的日志服务。Kafka中有几个概念，分别是Topic，Broker，Producer，Consumer等 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="log" scheme="http://fangjian0423.github.io/tags/log/"/>
    
      <category term="kafka" scheme="http://fangjian0423.github.io/categories/kafka/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Flume Transaction介绍]]></title>
    <link href="http://fangjian0423.github.io/2016/01/03/flume-transaction/"/>
    <id>http://fangjian0423.github.io/2016/01/03/flume-transaction/</id>
    <published>2016-01-03T09:35:53.000Z</published>
    <updated>2016-01-11T14:19:42.000Z</updated>
    <content type="html"><![CDATA[<p>Flume中有一个Transaction的概念。本文仅分析Transaction的实现类MemoryTransaction的实现原理，JdbcTransaction的原理跟数据库中的Transaction类似。</p>
<p>Transaction接口定义如下：</p>
<pre><code><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">begin</span><span class="params">()</span></span>;

<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">commit</span><span class="params">()</span></span>;

<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">rollback</span><span class="params">()</span></span>;

<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;
</code></pre><p>Transaction跟数据库中的Transaction概念类似，都有begin，commit，rollback，close方法。</p>
<p>Flume中的Transaction是在Channel中使用的，主要用来处理Source数据进入Channel的过程和Channel中的数据被Sink处理的过程。下面会从这2个方面根据源码分析Transaction的原理。</p>
<p>在分析具体的事务操作之前，看一下MemoryTransaction中的各个方法实现原理。</p>
<p>首先先看一下事务的获取方法：</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function">Transaction <span class="title">getTransaction</span><span class="params">()</span> </span>{

    <span class="keyword">if</span> (!initialized) {
      <span class="keyword">synchronized</span> (<span class="keyword">this</span>) {
        <span class="keyword">if</span> (!initialized) {
          initialize();
          initialized = <span class="keyword">true</span>;
        }
      }
    }
    <span class="comment">// currentTransaction是一个ThreadLocal对象</span>
    BasicTransactionSemantics transaction = currentTransaction.get();
    <span class="comment">// 如果是第一次获取事务或者当前事务的已经close。那么会重新create一个新的事务</span>
    <span class="keyword">if</span> (transaction == <span class="keyword">null</span> || transaction.getState().equals(
            BasicTransactionSemantics.State.CLOSED)) {
      transaction = createTransaction();
      currentTransaction.set(transaction);
    }
    <span class="keyword">return</span> transaction;
}
</code></pre><p><strong>再重复一下，第一次拿事务或者事务关闭之后，才会重新去构造一个新的事务。各个线程之间的事务都是独立的</strong></p>
<p>MemoryTransaction是MemoryChannel中的一个内部类。</p>
<p>然后介绍一下MemoryTransaction和MemoryChannel中的几个重要属性。</p>
<p>MemoryTransaction中有2个阻塞队列，分别是putList和takeList。putList放Source进来的数据，Sink从MemoryChannel中的queue中拿数据，然后这个数据丢到takeList中。</p>
<p>MemoryChannel中有个阻塞队列queue。每次事务commit的时候都会把putList中的数据丢到queue中。</p>
<p>begin方法MemoryTransaction没做任何处理，就不分析了。</p>
<p>put方法：</p>
<pre><code>@<span class="function">Override
<span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doPut</span>(<span class="params">Event <span class="keyword">event</span></span>) throws InterruptedException </span>{
  channelCounter.incrementEventPutAttemptCount();
  <span class="keyword">int</span> eventByteSize = (<span class="keyword">int</span>)Math.ceil(estimateEventSize(<span class="keyword">event</span>)/byteCapacitySlotSize);

  <span class="keyword">if</span> (!putList.offer(<span class="keyword">event</span>)) {
    <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(
      <span class="string">"Put queue for MemoryTransaction of capacity "</span> +
        putList.size() + <span class="string">" full, consider committing more frequently, "</span> +
        <span class="string">"increasing capacity or increasing thread count"</span>);
  }
  putByteCounter += eventByteSize;
}
</code></pre><p>put方法把数据丢入putList中，这个也就是之前分析的putList这个属性的作用，putList放Source进来的数据。</p>
<p>commit方法的关键性代码：</p>
<pre><code>@Override
<span class="keyword">protected</span> <span class="keyword">void</span> doCommit() <span class="keyword">throws</span> InterruptedException {
  <span class="built_in">int</span> puts = putList.<span class="built_in">size</span>();
  <span class="built_in">int</span> takes = takeList.<span class="built_in">size</span>();
  <span class="keyword">synchronized</span>(queueLock) {
    <span class="keyword">if</span>(puts &gt; <span class="number">0</span> ) {
      <span class="comment">// 清空putList，丢到外部类MemoryChannel中的queue队列里</span>
      <span class="keyword">while</span>(!putList.isEmpty()) {
        <span class="comment">// MemoryChannel中的queue队列</span>
        <span class="keyword">if</span>(!queue.offer(putList.removeFirst())) {
          <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Queue add failed, this shouldn't be able to happen"</span>);
        }
      }
    }
    putList.<span class="built_in">clear</span>();
    takeList.<span class="built_in">clear</span>();
  }
}
</code></pre><p>rollback方法关键性代码：</p>
<pre><code>@Override
<span class="keyword">protected</span> <span class="keyword">void</span> doRollback() {
  <span class="built_in">int</span> takes = takeList.<span class="built_in">size</span>();
  <span class="keyword">synchronized</span>(queueLock) {
    Preconditions.checkState(queue.remainingCapacity() &gt;= takeList.<span class="built_in">size</span>(), <span class="string">"Not enough space in memory channel "</span> +
        <span class="string">"queue to rollback takes. This should never happen, please report"</span>);
    <span class="comment">// 把takeList中的数据放回到queue中</span>
    <span class="keyword">while</span>(!takeList.isEmpty()) {
      queue.addFirst(takeList.removeLast());
    }
    putList.<span class="built_in">clear</span>();
  }
}
</code></pre><p>发生异常后才会调用rollback方法。也就是说take方法被调用之后，由于take方法是从queue中拿数据，并且放到takeList里。所以回滚的时候需要把takeList中的数据还给queue。</p>
<p>MemoryTransaction的close方法只是把状态改成了CLOSED，其他没做什么，就不分析了。</p>
<p>MemoryTransaction的take方法：</p>
<p>take方法从queue中拉出数据，然后放到takeList中。</p>
<pre><code>@<span class="function">Override
<span class="keyword">protected</span> Event <span class="title">doTake</span>(<span class="params"></span>) throws InterruptedException </span>{
  channelCounter.incrementEventTakeAttemptCount();
  <span class="keyword">if</span>(takeList.remainingCapacity() == <span class="number">0</span>) {
    <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(<span class="string">"Take list for MemoryTransaction, capacity "</span> +
        takeList.size() + <span class="string">" full, consider committing more frequently, "</span> +
        <span class="string">"increasing capacity, or increasing thread count"</span>);
  }
  <span class="keyword">if</span>(!queueStored.tryAcquire(keepAlive, TimeUnit.SECONDS)) {
    <span class="keyword">return</span> <span class="keyword">null</span>;
  }
  Event <span class="keyword">event</span>;
  synchronized(queueLock) {
    <span class="keyword">event</span> = queue.poll();
  }
  Preconditions.checkNotNull(<span class="keyword">event</span>, <span class="string">"Queue.poll returned NULL despite semaphore "</span> +
      <span class="string">"signalling existence of entry"</span>);
  takeList.put(<span class="keyword">event</span>);

  <span class="keyword">int</span> eventByteSize = (<span class="keyword">int</span>)Math.ceil(estimateEventSize(<span class="keyword">event</span>)/byteCapacitySlotSize);
  takeByteCounter += eventByteSize;

  <span class="keyword">return</span> <span class="keyword">event</span>;
}
</code></pre><p>Source数据进入Channel过程中Transaction的处理过程：</p>
<p>ChannelProcessor处理这个过程：</p>
<pre><code><span class="keyword">for</span> (Channel reqChannel : reqChannelQueue.keySet()) {
  <span class="comment">// 获取事务</span>
  Transaction tx = reqChannel.getTransaction();
  Preconditions.checkNotNull(tx, <span class="string">"Transaction object must not be null"</span>);
  <span class="keyword">try</span> {
    <span class="comment">// 事务开始</span>
    tx.begin();
    <span class="comment">// 获取Source处理的一个批次中的所有Event</span>
    List&lt;Event&gt; batch = reqChannelQueue.get(reqChannel);

    <span class="keyword">for</span> (Event event : batch) {
      <span class="comment">// MemoryChannel的put方法会MemoryTransaction的put方法。</span>
      reqChannel.put(event);
    }
    <span class="comment">// 提交事务</span>
    tx.commit();
  } <span class="keyword">catch</span> (Throwable t) {
      <span class="comment">// 发生异常回滚事务</span>
    tx.rollback();
    <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) {
      LOG.<span class="keyword">error</span>(<span class="string">"Error while writing to required channel: "</span> +
          reqChannel, t);
      <span class="keyword">throw</span> (Error) t;
    } <span class="keyword">else</span> {
      <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(<span class="string">"Unable to put batch on required "</span> +
          <span class="string">"channel: "</span> + reqChannel, t);
    }
  } <span class="keyword">finally</span> {
    <span class="keyword">if</span> (tx != <span class="keyword">null</span>) {
      <span class="comment">// 最后结束事务</span>
      tx.close();
    }
  }
}
</code></pre><p>Channel中的数据被Sink处理的过程：</p>
<p>以hdfs sink为例讲解：</p>
<pre><code><span class="keyword">public</span> <span class="function">Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>{
    Channel channel = getChannel();
    Transaction transaction = channel.getTransaction();
    List&lt;BucketWriter&gt; writers = Lists.newArrayList();
    transaction.begin();
    <span class="keyword">try</span> {
      <span class="keyword">int</span> txnEventCount = <span class="number">0</span>;
      <span class="keyword">for</span> (txnEventCount = <span class="number">0</span>; txnEventCount &lt; batchSize; txnEventCount++) {
        Event event = channel.take();
        <span class="keyword">if</span> (event == <span class="keyword">null</span>) {
          <span class="keyword">break</span>;
        }

      ... 

      transaction.commit();

      <span class="keyword">if</span> (txnEventCount &lt; <span class="number">1</span>) {
        <span class="keyword">return</span> Status.BACKOFF;
      } <span class="keyword">else</span> {
        sinkCounter.addToEventDrainSuccessCount(txnEventCount);
        <span class="keyword">return</span> Status.READY;
      }
    } <span class="keyword">catch</span> (IOException eIO) {
      transaction.rollback();
      LOG.warn(<span class="string">"HDFS IO error"</span>, eIO);
      <span class="keyword">return</span> Status.BACKOFF;
    } <span class="keyword">catch</span> (Throwable th) {
      transaction.rollback();
      LOG.<span class="keyword">error</span>(<span class="string">"process failed"</span>, th);
      <span class="keyword">if</span> (th <span class="keyword">instanceof</span> Error) {
        <span class="keyword">throw</span> (Error) th;
      } <span class="keyword">else</span> {
        <span class="keyword">throw</span> <span class="keyword">new</span> EventDeliveryException(th);
      }
    } <span class="keyword">finally</span> {
      transaction.close();
    }
 }
</code></pre><p>也是一样的流程，begin，take，commit or rollback，close。</p>
<p>总结：</p>
<ol>
<li><p>MemoryTransaction是MemoryChannel中的一个内部类，内部有2个阻塞队列putList和takeList。MemoryChannel内部有个queue阻塞队列。</p>
</li>
<li><p>putList接收Source交给Channel的event数据，takeList保存Channel交给Sink的event数据。</p>
</li>
<li><p>如果是Source交给Channel任务完成，进行commit的时候。会把putList中的所有event放到MemoryChannel中的queue。</p>
</li>
<li><p>如果是Source交给Channel任务失败，进行rollback的时候。程序就不会继续走下去，比如KafkaSource需要commitOffsets，如果任务失败就不会commitOffsets。</p>
</li>
<li><p>如果是Sink处理完Channel带来的event，进行commit的时候。会清空takeList中的event数据，因为已经没consume。</p>
</li>
<li><p>如果是Sink处理Channel带来的event失败的话，进行rollback的时候。会把takeList中的event写回到queue中。</p>
</li>
</ol>
<p>缺点：</p>
<p>Flume的Transaction跟数据库的Transaction不一样。数据库中的事务回滚之后所有操作的数据都会进行处理。而Flume的却不能还原。比如HDFSSink写数据到HDFS的时候需要rollback，比如本来要写入10000条数据，但是写到5000条的时候rollback，那么已经写入的5000条数据不能回滚，而那10000条数据回到了阻塞队列里，下次再写入的时候还会重新写入这10000条数据。这样就多了5000条重复数据，这是flume设计上的缺陷。</p>
]]></content>
    <summary type="html">
    <![CDATA[Flume中有一个Transaction的概念。本文仅分析Transaction的实现类MemoryTransaction的实现原理，JdbcTransaction的原理跟数据库中的Transaction类似 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[2015总结]]></title>
    <link href="http://fangjian0423.github.io/2016/01/01/2015_end/"/>
    <id>http://fangjian0423.github.io/2016/01/01/2015_end/</id>
    <published>2015-12-31T16:22:35.000Z</published>
    <updated>2015-12-31T16:48:01.000Z</updated>
    <content type="html"><![CDATA[<h2 id="2015年总结">2015年总结</h2><p>又是一年年底，2015年年底了。总结，总结，总结，还是分3个部分。</p>
<h3 id="工作">工作</h3><p>今年2月份春节过完之后就换了工作，之后在这家公司工作到了现在。</p>
<p>来新公司主要的目的是为了做大数据的，但是一进来就是给别人擦屁股，改bug = =。 后来大数据的东西只做了一部分，大概3个月的时间，之后被拉过去做公众号相关的东西了。没办法，公司人手太少，自己负责的etl这部分还有很多不完善的地方，可惜没有时间继续做下去。</p>
<p>公司人少没时间做是一个原因，但是更重要的原因还是自己懒了。</p>
<h3 id="技术">技术</h3><p>2015年，对技术做以下总结：</p>
<ol>
<li>继续写博客，15年写了43篇博客。相比去年的34篇博客，今年写的更多了。但是这43篇博客的质量其实都很一般，没有去年写的springmvc源码分析好。 但是想要写出好的文章得花费很长的时间。争取之后的文章写得多又能写得好。</li>
<li>了解了大数据方面的知识。包括hadoop，hdfs，flume，spark，hbase，elasticsearch，sqoop，hive等方面的知识。由于自己在公司负责的是etl方面的内容，所以对flume了解的比较多，然后又用base和elasticsearch存储了一些东西，所以对这两块内容也比较了解。但是对这些东西，也只是停留在使用的基础上，没有深入了解到内部的结构，明年会深入了解这些内容的。</li>
<li>github贡献了几个开源项目。包括flume，一个es教程，waterfall等。flume的HBaseSink在stop的时候居然没有把serializer关闭掉。给flume提了个pull request，但是flume居然不接受在github提出的pull request，只能在apache jira上处理ticket，但是新建了一个ticket之后居然不能assign给任何人，所以也就没人处理了，有点尴尬。</li>
<li>玩了会grails。由于公司内部的后台系统是用grails搭建的，所以自然就得会grails，grails内部用groovy写的。用了之后发现grails的调试在intellij中特别的慢，而且只要进了一个闭包，调试就特别麻烦。grails项目大了之后启动也非常地慢，有时候还会莫名其妙地出现一些错误，重启一下就好了。综上原因，对grails不是非常喜欢。</li>
<li>spring-boot的使用。公司内部发现grails项目大了之后启动会非常慢，后来开始使用micro-service就行新项目的开发。spring-boot其实是各个框架的整合，包括hibernate，spring，springdata等。提供了一些封装好的方便的方法，但是发现使用了spring-boot之后有些它内部定义好的内容你不看文档是不会知道的，而且有的东西文档里也没有说，所以只能看源码。这个算是使用spring-boot的一个弊端吧。</li>
<li>scala的学习。今年把scala in action这本书看完了。这本书很一般，很多scala的东西感觉都没有讲清楚。自己也把spray-json(scala写的一个json库，很小)的源码看了一遍。其实感觉看源码学语法也是不错的一个方法。</li>
<li>可以勉强算一个全栈了。公司技术少，前端更是只有一个。所以今年做了一些前端的工作，感觉自己一个人能搞定一个公众号的前后端了。写前端的时候使用了angularjs。后来又了解了js的一些打包工具，grunt，gulp等。给公司做了《超级邀请》这个公众号的开发任务，这个公众号的前台页面和接口都是自己写的。</li>
<li>了解了java高级部分的一些知识。知道了java并发的一些内容，知道了内置锁，信号量，栅栏，闭锁等知识。java内存模型也了解了一点，知道了happens-before，java的内存通信是通过共享内存实现的。jvm的书买了，但是还没开始看，是个弱项。</li>
<li>代码质量。今年写的代码质量感觉还是很烂，比去年也就稍微好了一点，依旧很烂。希望16年能写出更好的代码，而不仅仅只是为了完成任务的代码。</li>
</ol>
<h3 id="生活">生活</h3><p>宅，宅，宅。</p>
<p>不过双12买了把ukulele。但是还没开始学 = =，尴尬。不过买了就不会浪费，之后会开始学。</p>
<p>玩了高达模型，跟同事学的。搭了2个MG，武者MK2和迅雷高达。1个bb版强袭高达，年底又买了个pg强袭。pg还没开始搭，搭完绝壁炫酷到爆炸。</p>
<p>附上自己搭的高达图片，就放一张吧。毕竟是写总结的，不是介绍高达的：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/gundam1.JPG" alt=""></p>
<h3 id="2016年计划">2016年计划</h3><p>去年定的2015年计划：</p>
<ol>
<li>今年真的没看书。 有的书看了一点点就没看了，java并发的书看了100多页就没下文了。。 要养成看书的习惯。 (☑。今年把这本书看完了，还看了scala in action。其实还远远不够)</li>
<li><p>技术方面的计划：<br>redis(×。看了elasticsearch)<br>docker(×，而是看了大数据)<br>github贡献开源框架(☑)<br>nio(×)<br>并发包(☑)<br>Mina(×)<br>Netty(×)<br>Python深入(×，深入学习了scala)<br>搜索相关的知识(×)<br>看源码的时候多想想作者的思路以及架构方面，不用特别在意细节(☑)</p>
</li>
<li><p>继续写博客(☑)</p>
</li>
<li>做让生活变得更有趣的事，比如guitar(☑。ukulele，高达)</li>
</ol>
<p>2015的计划虽然只完成了一半，但是由于工作中接触大数据。所以很多内容都没看，转而去看大数据方面的知识了。所以总体完成度还是可以的，算80%。</p>
<p>2016年计划以及展望：</p>
<ol>
<li>继续看书，要看更多的书。 15年居然只看了两本书，对不起自己…. 16年要5本+。</li>
<li><p>技术方面<br>大数据的深入学习，不仅仅局限于会使用。包括spark，es，hbase，hadoop等。<br>分布式方面的学习<br>docker<br>scala的继续深入，要开始用scala写代码<br>netty<br>github继续贡献开源项目<br>jvm</p>
</li>
<li><p>继续写博客</p>
</li>
<li>继续玩高达</li>
<li>学会ukulele</li>
<li>希望能做一些逼格高一点的东西，而不仅仅是做一些功能性的东西</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[2015年年终总结]]>
    
    </summary>
    
      <category term="杂事" scheme="http://fangjian0423.github.io/tags/%E6%9D%82%E4%BA%8B/"/>
    
      <category term="总结" scheme="http://fangjian0423.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spray-json源码分析]]></title>
    <link href="http://fangjian0423.github.io/2015/12/23/scala-spray-json/"/>
    <id>http://fangjian0423.github.io/2015/12/23/scala-spray-json/</id>
    <published>2015-12-22T17:47:35.000Z</published>
    <updated>2015-12-23T03:31:30.000Z</updated>
    <content type="html"><![CDATA[<h2 id="spray-json介绍以及内部结构">spray-json介绍以及内部结构</h2><p><a href="https://github.com/spray/spray-json" target="_blank" rel="external">spray-json</a>是scala的一个轻量的，简洁的，简单的关于JSON实现。</p>
<p>同时也是<a href="http://spray.io/" target="_blank" rel="external">spray</a>项目的json模块。</p>
<p>本文分析spray-json的源码。</p>
<p>在分析spray-json的源码之前，我们先介绍一下spray-json的使用方法以及里面的几个概念。</p>
<p>首先是spray-json的一个使用例子，里面有各种黑魔法：</p>
<pre><code><span class="keyword">val</span> str = <span class="string">"""{
    "name": "Ed",
    "age": 24
}"""</span>

<span class="comment">// 黑魔法。不是String的parseJson方法，而是使用了隐式转换，隐式转换成PimpedString类。PimpedString里有parseJson方法，转换成JsValue对象</span>
<span class="keyword">val</span> jsonVal = str.parseJson <span class="comment">// jsonVal是个JsObject对象的实例</span>

<span class="comment">// jsonVal是个JsObject对象，也是个JsValue实例。JsValue对象都有compactPrint和prettyPrint方法</span>
println(jsonVal.compactPrint) <span class="comment">// 压缩打印</span>
println(jsonVal.prettyPrint) <span class="comment">// 格式化打印</span>

<span class="comment">// 手动构建一个JsObject</span>
<span class="keyword">val</span> jsonObj = <span class="type">JsObject</span>(
    (<span class="string">"name"</span>, <span class="type">JsString</span>(<span class="string">"format"</span>)), (<span class="string">"age"</span>, <span class="type">JsNumber</span>(<span class="number">99</span>))
)

println(jsonObj.compactPrint)
println(jsonObj.prettyPrint)

<span class="comment">// 黑方法，不是List的toJson方法，而是使用了隐式转换，隐式转换成PimpedAny类，PimpedAny类里有toList方法，转换成对应的类型</span>
<span class="keyword">val</span> jsonList = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).toJson

<span class="comment">// JsValue的toString方法引用了compactPrint方法</span>
println(jsonList)
</code></pre><p>然后是spray-json里的几个概念介绍：</p>
<p>1.JsValue: 抽象类。对原生json中的各种数据类型的抽象。它的实现类JsArray对应json里的数组；JsBoolean对应json里的布尔值；JsNumber对应json里的数值 …</p>
<p>2.JsonFormat: 一个trait，是json序列化和反序列话的抽象。继承JsonReader(JsValue转换成对象的抽象)和JsonWriter(对象转换成JsValue的抽象)。BasicFormats和CollectionFormats、AdditionalFormats等这些trait里都有各种JsonFormat的隐式转换。</p>
<p>3.RootJsonFormat。json文档的抽象，跟JsonFormat一样，只不过RootJsonFormat只支持JsArray和JsObject。因为这是一个json文档对象，只有一个json对象或者一个json数组才能称得上是一个json文档对象。</p>
<p>4.JsonPrinter: 一个trait。json打印成字符串的抽象。具体的实现特质有CompactPrinter(压缩后的字符串)和PrettyPrinter(格式化后的字符串)</p>
<p>5.DefaultJsonProtocol: 继承BasicFormats，混入StandardFormats、CollectionFormats、ProductFormats、AdditionalFormats的特质。我们需要转换一些基础类型或者集合类型的时候需要import这个trait。</p>
<h2 id="json_package对象">json package对象</h2><p>json package对象里定义了一些隐式转换方法和一些实用方法。</p>
<pre><code>package object json {

  // JsField。 一个二元元组，代表json中的一个项(key为String，<span class="keyword">value</span>为任意json类型)
  <span class="keyword">type</span> JsField = (String, JsValue)

  // 反序列化异常
  def deserializationError(msg: String, cause: Throwable = null, fieldNames: List[String] = Nil) = throw new DeserializationException(msg, cause, fieldNames)
  // 序列化异常
  def serializationError(msg: String) = throw new SerializationException(msg)
  // jsonReader方法，是个泛型。使用了隐式参数，返回值是这个隐式参数的引用。也就是说只要调用了jsonReader方法，那么就会自动去找对应泛型类型的实现
  def jsonReader[T](<span class="type">implicit</span> reader: JsonReader[T]) = reader
  // 跟jsonReader方法一个道理。只要调用了jsonWriter方法，那么就会自动去找对应泛型类型的实现
  def jsonWriter[T](<span class="type">implicit</span> writer: JsonWriter[T]) = writer 

  // 隐式转换方法。上面例子的toList使用了这个隐式转换
  <span class="type">implicit</span> def pimpAny[T](<span class="built_in">any</span>: T) = new PimpedAny(<span class="built_in">any</span>)
  // 隐式方法。上面例子的parseJson使用了这个隐式转换
  <span class="type">implicit</span> def pimpString(string: String) = new PimpedString(string)
}

package json {

  // 反序列异常类的定义，上面的deserializationError方法实例化了这个类
  <span class="keyword">case</span> <span class="keyword">class</span> DeserializationException(msg: String, cause: Throwable = null, fieldNames: List[String] = Nil) <span class="keyword">extends</span> RuntimeException(msg, cause)
  // 序列异常类的定义，上面的serializationError方法实例化了这个类
  <span class="keyword">class</span> SerializationException(msg: String) <span class="keyword">extends</span> RuntimeException(msg)

  // 上面的隐式方法pimpAny实例化了这个类。黑魔法toJson方法，不是List的toJson方法，而是List隐式转换成PimpedAny，然后调用PimpedAny的toJson方法。toJson方法的参数是个隐式参数，跟上面代码里的jsonWriter方法一样，会找对应泛型类型的JsonWriter实现类，然后调用JsonWriter的<span class="built_in">write</span>方法
  <span class="keyword">private</span>[json] <span class="keyword">class</span> PimpedAny[T](<span class="built_in">any</span>: T) {
    def toJson(<span class="type">implicit</span> writer: JsonWriter[T]): JsValue = writer.<span class="built_in">write</span>(<span class="built_in">any</span>)
  }

  // 上面的隐式方法pimpString实例化了这个类。黑魔法parseJson方法，不是String的parseJson方法，而是String隐式转换成PimpedString，然后调用PimpedString的parseJson方法
  <span class="keyword">private</span>[json] <span class="keyword">class</span> PimpedString(string: String) {
    @deprecated(<span class="string">"deprecated in favor of parseJson"</span>, <span class="string">"1.2.6"</span>)
    def asJson: JsValue = parseJson
    def parseJson: JsValue = JsonParser(string)
  }
}
</code></pre><h2 id="JsValue(原生json中的各种数据类型的抽象)">JsValue(原生json中的各种数据类型的抽象)</h2><p>JsValue是原生json中各种数据类型的抽象，是个抽象类，直接看JsValue的定义:</p>
<pre><code><span class="keyword">sealed</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">JsValue</span> {</span>
     <span class="comment">// 重载的toString方法引用了compactPrint方法，会打印出json的压缩格式</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span> =</span> compactPrint
  <span class="function"><span class="keyword">def</span> <span class="title">toString</span>(</span>printer: (<span class="type">JsValue</span> =&gt; <span class="type">String</span>)) = printer(<span class="keyword">this</span>)
  <span class="comment">// 压缩打印，使用了CompactPrinter。CompactPrinter是一个JsonPrinter的子类</span>
  <span class="function"><span class="keyword">def</span> <span class="title">compactPrint</span> =</span> <span class="type">CompactPrinter</span>(<span class="keyword">this</span>)
  <span class="comment">// 格式化打印，PrettyPrinter。PrettyPrinter也是一个JsonPrinter的子类</span>
  <span class="function"><span class="keyword">def</span> <span class="title">prettyPrint</span> =</span> <span class="type">PrettyPrinter</span>(<span class="keyword">this</span>)
  <span class="comment">// 转换成对应的类。jsonReader方法在json package里定义，已经分析过。会找对应那个的JsonReader实现类。然后调用read方法</span>
  <span class="function"><span class="keyword">def</span> <span class="title">convertTo</span>[</span><span class="type">T</span> :<span class="type">JsonReader</span>]: <span class="type">T</span> = jsonReader[<span class="type">T</span>].read(<span class="keyword">this</span>)

  <span class="comment">// 转换成JsObject对象，除了JsObject对象重写了这个，返回了自身。其他类型的JsValue都会抛出DeserializationException异常</span>
  <span class="function"><span class="keyword">def</span> <span class="title">asJsObject</span>(</span>errorMsg: <span class="type">String</span> = <span class="string">"JSON object expected"</span>): <span class="type">JsObject</span> = deserializationError(errorMsg)

  <span class="function"><span class="keyword">def</span> <span class="title">asJsObject</span>:</span> <span class="type">JsObject</span> = asJsObject()

  <span class="annotation">@deprecated</span>(<span class="string">"Superceded by 'convertTo'"</span>, <span class="string">"1.1.0"</span>)
  <span class="function"><span class="keyword">def</span> <span class="title">fromJson</span>[</span><span class="type">T</span> :<span class="type">JsonReader</span>]: <span class="type">T</span> = convertTo
}
</code></pre><p>JsNumber是数值类型的抽象：</p>
<pre><code><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JsNumber</span>(</span>value: <span class="type">BigDecimal</span>) <span class="keyword">extends</span> <span class="type">JsValue</span>

<span class="comment">// JsNumber中定义了几个方便的构造方法</span>
<span class="class"><span class="keyword">object</span> <span class="title">JsNumber</span> {</span>
  <span class="keyword">val</span> zero: <span class="type">JsNumber</span> = apply(<span class="number">0</span>)
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Int</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Long</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Double</span>) = n <span class="keyword">match</span> {
    <span class="keyword">case</span> n <span class="keyword">if</span> n.isNaN      =&gt; <span class="type">JsNull</span>
    <span class="keyword">case</span> n <span class="keyword">if</span> n.isInfinity =&gt; <span class="type">JsNull</span>
    <span class="keyword">case</span> _                 =&gt; <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  }
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">BigInt</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">String</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Array</span>[<span class="type">Char</span>]) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
}
</code></pre><p>JsString是字符串类型的抽象：</p>
<pre><code><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JsString</span>(</span>value: <span class="type">String</span>) <span class="keyword">extends</span> <span class="type">JsValue</span>

<span class="comment">// JsString中定义了几个方便的方法</span>
<span class="class"><span class="keyword">object</span> <span class="title">JsString</span> {</span>
    <span class="keyword">val</span> empty = <span class="type">JsString</span>(<span class="string">""</span>)
    <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>value: <span class="type">Symbol</span>) = <span class="keyword">new</span> <span class="type">JsString</span>(value.name)
}
</code></pre><p>JsObject是对象类型的抽象：</p>
<pre><code><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JsObject</span>(</span>fields: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">JsValue</span>]) <span class="keyword">extends</span> <span class="type">JsValue</span> {
  <span class="comment">// 重写了asJsObject方法</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">asJsObject</span>(</span>errorMsg: <span class="type">String</span>) = <span class="keyword">this</span>
  <span class="comment">// 根据字段名获得对应的JsValue</span>
  <span class="function"><span class="keyword">def</span> <span class="title">getFields</span>(</span>fieldNames: <span class="type">String</span>*): immutable.<span class="type">Seq</span>[<span class="type">JsValue</span>] = fieldNames.flatMap(fields.get)(collection.breakOut)
}

<span class="class"><span class="keyword">object</span> <span class="title">JsObject</span> {</span>
  <span class="keyword">val</span> empty = <span class="type">JsObject</span>(<span class="type">Map</span>.empty[<span class="type">String</span>, <span class="type">JsValue</span>])
  <span class="comment">// 使用多个JsField构造JsObject。这里的JsField就是代表一个(String, JsValue)</span>
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>members: <span class="type">JsField</span>*) = <span class="keyword">new</span> <span class="type">JsObject</span>(<span class="type">Map</span>(members: _*))
  <span class="annotation">@deprecated</span>(<span class="string">"Use JsObject(JsValue*) instead"</span>, <span class="string">"1.3.0"</span>)
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>members: <span class="type">List</span>[<span class="type">JsField</span>]) = <span class="keyword">new</span> <span class="type">JsObject</span>(<span class="type">Map</span>(members: _*))
}
</code></pre><h2 id="JsonFormat(JsonWriter和JsonReader的子类)">JsonFormat(JsonWriter和JsonReader的子类)</h2><p>一个trait，是json序列化和反序列话的抽象。继承JsonReader(JsValue转换成对象的抽象)和JsonWriter(对象转换成JsValue的抽象)。</p>
<p>JsonReader的定义，把一个JsValue转换成对应的类型：</p>
<pre><code>trait JsonReader[T] {
  <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(json: JsValue)</span>:</span> T
}
</code></pre><p>JsonWriter的定义，把一个类型转换成JsValue：</p>
<pre><code>trait JsonWriter[T] {
  <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(obj: T)</span>:</span> JsValue
}
</code></pre><p>json package对象里的jsonReader和jsonWriter方法有个隐式参数，我们也分析过：只要调用了jsonReader(JsonWriter)方法，那么就会自动去找对应泛型类型的实现。</p>
<p>AdditionalFormats、BasicFormats、CollectionFormats、StandardFormats等都定义了各种JsonFormat。</p>
<p>比如Int类型就找IntJsonFormat，String类型就找StringJsonFormat …  这些基础类型的JsonFormat都定义在BasicFormats这个trait中。</p>
<p>我们就分析几个BasicFormats中定义的基础类型JsonFormat：</p>
<p>Int基本类型的JsonFormat：</p>
<pre><code><span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">IntJsonFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">JsonFormat</span>[</span><span class="type">Int</span>] {
    <span class="comment">// write方法继承自JsonWriter。 直接实例化一个JsNumber对象</span>
    <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>x: <span class="type">Int</span>) = <span class="type">JsNumber</span>(x)
    <span class="comment">// read方法继承自JsonReader。读取JsNumber中对应的值</span>
    <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value <span class="keyword">match</span> {
      <span class="keyword">case</span> <span class="type">JsNumber</span>(x) =&gt; x.intValue
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected Int as JsNumber, but got "</span> + x)
    }
}
</code></pre><p>String基本类型的JsonFormat：</p>
<pre><code><span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">StringJsonFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">JsonFormat</span>[</span><span class="type">String</span>] {
    <span class="comment">// write方法实例化一个JsString</span>
    <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>x: <span class="type">String</span>) = {
      require(x ne <span class="literal">null</span>)
      <span class="type">JsString</span>(x)
    }
    <span class="comment">// read方法读取JsString中对应的字符串</span>
    <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value <span class="keyword">match</span> {
      <span class="keyword">case</span> <span class="type">JsString</span>(x) =&gt; x
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected String as JsString, but got "</span> + x)
    }
}
</code></pre><p>…..</p>
<p>CollectionFormats中定义了几个集合类的JsonFormat:</p>
<p>List类型的JsonFormat：</p>
<pre><code><span class="type">implicit</span> def listFormat[T :JsonFormat] = new RootJsonFormat[List[T]] {
    // 将List转换成JsArray对象。遍历list中的各个元素，对每个元素调用toJson方法。最后JsArray里的每个元素都是JsValue
    def <span class="built_in">write</span>(list: List[T]) = JsArray(list.map(_.toJson).toVector)
    // JsArray转换成List。对JsArray里的各个JsValue调用convertTo转换成对应的类型
    def read(<span class="keyword">value</span>: JsValue): List[T] = <span class="keyword">value</span> match {
      <span class="keyword">case</span> JsArray(elements) =&gt; elements.map(_.convertTo[T])(collection.breakOut)
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected List as JsArray, but got "</span> + x)
    }
}
</code></pre><p>Map类型的JsonFormat：</p>
<pre><code><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">mapFormat</span>[</span><span class="type">K</span> :<span class="type">JsonFormat</span>, <span class="type">V</span> :<span class="type">JsonFormat</span>] = <span class="keyword">new</span> <span class="type">RootJsonFormat</span>[<span class="type">Map</span>[<span class="type">K</span>, <span class="type">V</span>]] {
    <span class="comment">// 遍历Map中的每一个二元元组。如果元组的第一项不是String，直接抛出SerializationException异常。否则构造key为元组第一项字符串，value为元组第二项的JsVaue对象。</span>
    <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>m: <span class="type">Map</span>[<span class="type">K</span>, <span class="type">V</span>]) = <span class="type">JsObject</span> {
      m.map { field =&gt;
        field._1.toJson <span class="keyword">match</span> {
          <span class="keyword">case</span> <span class="type">JsString</span>(x) =&gt; x -&gt; field._2.toJson
          <span class="keyword">case</span> x =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SerializationException</span>(<span class="string">"Map key must be formatted as JsString, not '"</span> + x + <span class="string">"'"</span>)
        }
      }
    }
    <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value <span class="keyword">match</span> {
      <span class="keyword">case</span> x: <span class="type">JsObject</span> =&gt; x.fields.map { field =&gt;
        (<span class="type">JsString</span>(field._1).convertTo[<span class="type">K</span>], field._2.convertTo[<span class="type">V</span>])
      } (collection.breakOut)
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected Map as JsObject, but got "</span> + x)
    }
}
</code></pre><p>AdditionalFormats中定义了一些helper和额外的JsonFormat：</p>
<pre><code><span class="comment">// JsValue的JsonFormat，JsValue调用convertTo或者toJson返回的就是自身</span>
<span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">JsValueFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">JsonFormat</span>[</span><span class="type">JsValue</span>] {
  <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>value: <span class="type">JsValue</span>) = value
  <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value
}


<span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">RootJsObjectFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">RootJsonFormat</span>[</span><span class="type">JsObject</span>] {
  <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>value: <span class="type">JsObject</span>) = value
  <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value.asJsObject
}
</code></pre><p>StandardFormats中定义了Option、Either的JsonFormat，1元-7元元组的JsonFormat。</p>
<pre><code><span class="type">implicit</span> def tuple1Format[A :JF] = new JF[Tuple1[A]] {
  def <span class="built_in">write</span>(t: Tuple1[A]) = t._1.toJson
  def read(<span class="keyword">value</span>: JsValue) = Tuple1(<span class="keyword">value</span>.convertTo[A])
}

<span class="type">implicit</span> def tuple2Format[A :JF, B :JF] = new RootJsonFormat[(A, B)] {
  def <span class="built_in">write</span>(t: (A, B)) = JsArray(t._1.toJson, t._2.toJson)
  def read(<span class="keyword">value</span>: JsValue) = <span class="keyword">value</span> match {
    <span class="keyword">case</span> JsArray(Seq(a, b)) =&gt; (a.convertTo[A], b.convertTo[B])
    <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected Tuple2 as JsArray, but got "</span> + x)
  }
}
</code></pre><h2 id="JsonPrinter(将JsValue打印成原生json字符串)">JsonPrinter(将JsValue打印成原生json字符串)</h2><p>JsonPrinter继承一个函数对象，这个函数的输入是个JsValue，输出是String：</p>
<pre><code><span class="class"><span class="keyword">trait</span> <span class="title">JsonPrinter</span> <span class="keyword"><span class="keyword">extends</span></span> (</span><span class="type">JsValue</span> =&gt; <span class="type">String</span>)
</code></pre><p>内部定义了一个抽象方法：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">print</span><span class="params">(<span class="symbol">x:</span> <span class="constant">JsValue</span>, <span class="symbol">sb:</span> <span class="constant">JStringBuilder</span>)</span></span>
</code></pre><p>CompactPrinter继承JsonPrinter，压缩打印：</p>
<p>实现的print方法：</p>
<pre><code>def <span class="built_in">print</span>(x: JsValue, sb: StringBuilder) {
    x match {
      <span class="keyword">case</span> JsObject(x) =&gt; <span class="built_in">print</span>Object(x, sb)
      <span class="keyword">case</span> JsArray(x)  =&gt; <span class="built_in">print</span>Array(x, sb)
      <span class="keyword">case</span> _ =&gt; <span class="built_in">print</span>Leaf(x, sb)
    }
}
</code></pre><p>PrettyPrinter也继承JsonPrinter，格式化打印：</p>
<p>实现的print方法：</p>
<pre><code><span class="keyword">def</span> print(<span class="string">x:</span> JsValue, <span class="string">sb:</span> StringBuilder) {
  print(x, sb, <span class="number">0</span>)
}

  <span class="comment">// indent参数是格式化打印的关键</span>
<span class="keyword">protected</span> <span class="keyword">def</span> print(<span class="string">x:</span> JsValue, <span class="string">sb:</span> StringBuilder, <span class="string">indent:</span> Int) {
  x match {
    <span class="keyword">case</span> JsObject(x) =&gt; printObject(x, sb, indent)
    <span class="keyword">case</span> JsArray(x)  =&gt; printArray(x, sb, indent)
    <span class="keyword">case</span> _ =&gt; printLeaf(x, sb)
  }
}
</code></pre><h2 id="DefaultJsonProtocol(整合了多个JsonFormat)">DefaultJsonProtocol(整合了多个JsonFormat)</h2><p>直接看源码：</p>
<pre><code><span class="class"><span class="keyword">trait</span> <span class="title">DefaultJsonProtocol</span>
</span>    <span class="keyword">extends</span> <span class="type">BasicFormats</span>
    <span class="keyword">with</span> <span class="type">StandardFormats</span>
    <span class="keyword">with</span> <span class="type">CollectionFormats</span>
    <span class="keyword">with</span> <span class="type">ProductFormats</span>
    <span class="keyword">with</span> <span class="type">AdditionalFormats</span>

<span class="class"><span class="keyword">object</span> <span class="title">DefaultJsonProtocol</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">DefaultJsonProtocol</span></span>
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[spray-json是scala的一个轻量的，简洁的，简单的关于JSON实现。同时也是spray项目的json模块，本文分析spray-json的源码 ...]]>
    
    </summary>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/tags/jvm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala 隐式转换和隐式参数]]></title>
    <link href="http://fangjian0423.github.io/2015/12/20/scala-implicit/"/>
    <id>http://fangjian0423.github.io/2015/12/20/scala-implicit/</id>
    <published>2015-12-20T07:38:22.000Z</published>
    <updated>2015-12-20T17:11:57.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>Scala的implicit功能很强大，可以自动地给对象”添加一个属性”。 这里打上引号的原因是Scala内部进行编译的时候会自动加上隐式转换函数。</p>
<p>很多Scala开源框架内部都大量使用了implicit。因为implicit真的很强大，写得好的implicit可以让代码更优雅。但个人感觉implicit也有一些缺点，比如使用了implicit之后，看源码或者使用一些library的时候无法下手，因为你根本不知道作者哪里写了implicit。这个也会对初学者造成一些困扰。</p>
<p>比如Scala中Option就有一个implicit可以将Option转换成Iterable：</p>
<pre><code>val <span class="built_in">list</span> = List(<span class="number">1</span>, <span class="number">2</span>)
val <span class="built_in">map</span> = Map(<span class="number">1</span> -&gt; <span class="number">11</span>, <span class="number">2</span> -&gt; <span class="number">22</span>, <span class="number">3</span> -&gt; <span class="number">33</span>)

val newList = <span class="built_in">list</span>.flatMap {
    num =&gt; <span class="built_in">map</span>.get(num) <span class="comment">// map.get方法返回的是Option，可以被隐式转换成Iterable</span>
} 
</code></pre><p>以下是implicit的一个小例子。</p>
<p>比如以下一个例子，定义一个Int类型的变量num，但是赋值给了一个Double类型的数值。这时候就会编译错误：</p>
<pre><code><span class="variable"><span class="keyword">val</span> num</span>: <span class="typename">Int</span> = <span class="number">3.5</span> <span class="comment">// Compile Error</span>
</code></pre><p>但是我们加了一个隐式转换之后，就没问题了:</p>
<pre><code><span class="type">implicit</span> def double2Int(d: <span class="type">Double</span>) = d.toInt

val num: <span class="built_in">Int</span> = <span class="number">3.5</span> // <span class="number">3</span>， 这段代码会被编译成 val num: <span class="built_in">Int</span> = double2Int(<span class="number">3.5</span>)
</code></pre><h2 id="隐式转换规则">隐式转换规则</h2><h3 id="标记规则(Marking_Rule)">标记规则(Marking Rule)</h3><p>任何变量，函数或者对象都可以用implicit这个关键字进行标记，表示可以进行隐式转换。</p>
<pre><code><span class="type">implicit</span> def intToString(x: <span class="built_in">Int</span>) = x.toString
</code></pre><p>编译器可能会将x + y 转换成 convert(x) + y 如果convert被标记成implicit。</p>
<h3 id="作用域规则(Scope_Rule)">作用域规则(Scope Rule)</h3><p>在一个作用域内，一个隐式转换必须是一个唯一的标识。</p>
<p>比如说MyUtils这个object里有很多隐式转换。x + y 不会使用MyUtils里的隐式转换。 除非import进来。 import MyUtils._</p>
<p>Scala编译器还能在companion class中去找companion object中定义的隐式转换。</p>
<pre><code><span class="keyword">object</span> Player {
    implicit def getClub(player: Player): Club = Club(player.clubName)
}

<span class="class"><span class="keyword">class</span> <span class="title">Player</span></span>(<span class="variable"><span class="keyword">val</span> name</span>: String, <span class="variable"><span class="keyword">val</span> age</span>: <span class="typename">Int</span>, <span class="variable"><span class="keyword">val</span> clubName</span>: String) {

}

<span class="variable"><span class="keyword">val</span> p</span> = new Player(<span class="string">"costa"</span>, <span class="number">27</span>, <span class="string">"Chelsea"</span>)

println(p.welcome) <span class="comment">// Chelsea welcome you here!</span>
println(p.playerNum) <span class="comment">// 21</span>
</code></pre><h3 id="一次编译只隐式转换一次(One-at-a-time_Rule)">一次编译只隐式转换一次(One-at-a-time Rule)</h3><p>Scala不会把 x + y 转换成 convert1(convert2(x)) + y</p>
<h2 id="隐式转换类型">隐式转换类型</h2><h3 id="隐式转换成正确的类型">隐式转换成正确的类型</h3><p>这种类型是Scala编译器对隐式转换的第一选择。 比如说编译器看到一个类型的X的数据，但是需要一个类型为Y的数据，那么就会去找把X类型转换成Y类型的隐式转换。</p>
<p>本文一开始的double2Int方法就是这种类型的隐式转换。</p>
<pre><code><span class="type">implicit</span> def double2Int(d: <span class="type">Double</span>) = d.toInt

val num: <span class="built_in">Int</span> = <span class="number">3.5</span> // <span class="number">3</span>
</code></pre><p>当编译器发现变量num是个Int类型，并且用Double类型给它赋值的时候，会报错。 但是在报错之前，编译器会查找Double =&gt; Int的隐式转换。然后发现了double2Int这个隐式转换函数。于是就使用了隐式转换。</p>
<h3 id="方法调用的隐式转换">方法调用的隐式转换</h3><p>比如这段代码  obj.doSomeThing。 比如obj对象没有doSomeThing这个方法，编译器会会去查找拥有doSomeThing方法的类型，并且看obj类型是否有隐式转换成有doSomeThing类型的函数。有的话就是将obj对象隐式转换成拥有doSomeThing方法的对象。</p>
<p>以下是一个例子：</p>
<pre><code><span class="keyword">case</span> <span class="keyword">class</span> Person(<span class="keyword">name</span>: String, age: <span class="built_in">Int</span>) {
    def +(num: <span class="built_in">Int</span>) = age + num
    def +(p: Person) = age + p.age
  }

val person = Person(<span class="string">"format"</span>, <span class="number">99</span>)
println(person + <span class="number">1</span>) // <span class="number">100</span>
//  println(<span class="number">1</span> + person)  报错，因为<span class="built_in">Int</span>的+方法没有有Person参数的重载方法

<span class="type">implicit</span> def personAddAge(x: <span class="built_in">Int</span>) = Person(<span class="string">"unknown"</span>, x)

println(<span class="number">1</span> + person) // <span class="number">100</span>
</code></pre><p>有了隐式转换方法之后，编译器检查 1 + person 表达式，发现Int的+方法没有有Person参数的重载方法。在放弃之前查看是否有将Int类型的对象转换成以Person为参数的+方法的隐式转换函数，于是找到了，然后就进行了隐式转换。</p>
<p>Scala的Predef中也使用了方法调用的隐式转换。</p>
<pre><code>Map(<span class="number">1</span> -&gt; <span class="number">11</span>, <span class="number">2</span> -&gt; <span class="number">22</span>)
</code></pre><p>上面这段Map中的参数是个二元元组。 Int没有 -&gt; 方法。 但是在Predef中定义了：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> ArrowAssoc[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    @inline def -&gt; [B](y: B): Tuple2[A, B] = Tuple2(self, y)
    def →[B](y: B): Tuple2[A, B] = -&gt;(y)
}
</code></pre><h3 id="隐式参数">隐式参数</h3><p>隐式参数的意义是当方法需要多个参数的时候，可以定义一些隐式参数，这些隐式参数可以被自动加到方法填充的参数里，而不必手填充。</p>
<pre><code>def implicitParamFunc(<span class="keyword">name</span>: String)(<span class="type">implicit</span> tiger: Tiger, lion: Lion): <span class="keyword">Unit</span> = {
    println(<span class="keyword">name</span> + <span class="string">" have a tiget and a lion, their names are: "</span> + tiger.<span class="keyword">name</span> + <span class="string">", "</span> + lion.<span class="keyword">name</span>)
}

object Zoo {
    <span class="type">implicit</span> val tiger = Tiger(<span class="string">"tiger1"</span>)
    <span class="type">implicit</span> val lion = Lion(<span class="string">"lion1"</span>)
}

<span class="keyword">import</span> Zoo._

implicitParamFunc(<span class="string">"format"</span>)
</code></pre><p>上面这个代码中implicitParamFunc中的第二个参数定义成了隐式参数。</p>
<p>然后在Zoo对象里定义了两个隐式变量，import进来之后，调用implicitParamFunc方法的时候这两个变量被自动填充到了参数里。</p>
<p>这里需要注意的是不仅仅方法中的参数需要被定义成隐式参数，对应的隐式参数的变量也需要被定义成隐式变量。</p>
<h2 id="其他">其他</h2><p>对象中的隐式转换可以只import自己需要的。</p>
<pre><code>object MyUtils {
    <span class="type">implicit</span> def a ...
    <span class="type">implicit</span> def b ...
}

<span class="keyword">import</span> MyUtils.a
</code></pre><p>隐式转换修饰符implicit可以修饰class，method，变量，object。</p>
<p>修饰方法和变量的隐式转换本文已经介绍过，就不继续说了。</p>
<p>修饰class的隐式转换，它的作用跟修饰method的隐式转换类似：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMarker(val start: <span class="built_in">Int</span>) {
    def --&gt;(<span class="keyword">end</span>: <span class="built_in">Int</span>) = start to <span class="keyword">end</span>
}

<span class="number">1</span> --&gt; <span class="number">10</span> // <span class="built_in">Range</span>(<span class="number">1</span>, <span class="number">10</span>)
</code></pre><p>上段代码可以改造成使用Value Class完成类的隐式转换：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMaker(start: <span class="built_in">Int</span>) <span class="keyword">extends</span> AnyVal {
    def --&gt;(<span class="keyword">end</span>: <span class="built_in">Int</span>) = start to <span class="keyword">end</span>
}
</code></pre><p>修饰object的隐式转换：</p>
<pre><code>trait Calculate[T] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: T, y: T)</span>:</span> T
}

implicit object IntCal extends Calculate[Int] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: Int, y: Int)</span>:</span> Int = x + y
}

implicit object ListCal extends Calculate[List[Int]] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: List[Int], y: List[Int])</span>:</span> List[Int] = x ::: y
}

<span class="function"><span class="keyword">def</span> <span class="title">implicitObjMethod</span>[<span class="title">T</span>]<span class="params">(x: T, y: T)</span><span class="params">(implicit cal: Calculate[T])</span>:</span> Unit = {
    println(x + <span class="string">" + "</span> + y + <span class="string">" = "</span> + cal.add(x, y))
}

implicitObjMethod(<span class="number">1</span>, <span class="number">2</span>) // <span class="number">1</span> + <span class="number">2</span> = <span class="number">3</span>
implicitObjMethod(List(<span class="number">1</span>, <span class="number">2</span>), List(<span class="number">3</span>, <span class="number">4</span>)) // List(<span class="number">1</span>, <span class="number">2</span>) + List(<span class="number">3</span>, <span class="number">4</span>) = List(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Scala的implicit功能很强大，可以自动地给对象"添加一个属性"。 这里打上引号的原因是Scala内部进行编译的时候会自动加上隐式转换函数 ...]]>
    
    </summary>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/tags/jvm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[elasticsearch查询模板]]></title>
    <link href="http://fangjian0423.github.io/2015/11/07/elasticsearch-search-template/"/>
    <id>http://fangjian0423.github.io/2015/11/07/elasticsearch-search-template/</id>
    <published>2015-11-07T08:04:25.000Z</published>
    <updated>2015-12-20T17:06:16.000Z</updated>
    <content type="html"><![CDATA[<p>最近在公司又用到了elasticsearch，也用到了查询模板，顺便写篇文章记录一下查询模板的使用。</p>
<p>以1个需求为例讲解es模板的使用：</p>
<p><strong>页面上某个按钮在一段时间内的点击次数统计，并且可以以小时，天，月为单位进行汇总，并且需要去重。</strong></p>
<p>创建索引，只定义3个字段，user_id, user_name和create_time:</p>
<pre><code>-POST /<span class="variable">$ES</span>/event_index

{
  <span class="string">"mappings"</span>: {
    <span class="string">"event"</span>: {
      <span class="string">"_ttl"</span>: {
        <span class="string">"enabled"</span>: false
      },
      <span class="string">"_timestamp"</span>: {
        <span class="string">"enabled"</span>: true,
        <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
      },
      <span class="string">"properties"</span>: {
        <span class="string">"user_id"</span>: {
          <span class="string">"type"</span>: <span class="string">"string"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>,
          <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>
        },
        <span class="string">"create_time"</span>: {
          <span class="string">"type"</span>: <span class="string">"date"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>,
          <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>,
          <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
        },
        <span class="string">"user_name"</span>: {
          <span class="string">"type"</span>: <span class="string">"string"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>
        }
      }
    }
  }
}
</code></pre><p>定义对应的查询模板，模板名字stats，使用了Cardinality和DateHistogram这两个Aggregation<br>，其中Date Histogram嵌套在Cardinality里。在定义模板的时候，{ { } } 的表示是个参数，需要调用模板的时候传递进来:</p>
<pre><code>  -POST /<span class="variable">$ES</span>/_search/template/stats
{
    <span class="string">"template"</span>: {
        <span class="string">"query"</span>: {
            <span class="string">"bool"</span>: {
                <span class="string">"must"</span>: [
                    {
                        <span class="string">"range"</span>: {
                            <span class="string">"create_time"</span>: {
                                <span class="string">"gte"</span>: <span class="string">""</span>,
                                <span class="string">"lte"</span>: <span class="string">""</span>
                            }
                        }
                    }
                ]
            }
        },
        <span class="string">"size"</span>: <span class="number">0</span>,
        <span class="string">"aggs"</span>: {
            <span class="string">"stats_data"</span>: {
                <span class="string">"date_histogram"</span>: {
                    <span class="string">"field"</span>: <span class="string">"create_time"</span>,
                    <span class="string">"interval"</span>: <span class="string">""</span>
                },
                <span class="string">"aggs"</span>: {
                    <span class="string">"time"</span>: {
                        <span class="string">"cardinality"</span>: {
                            <span class="string">"field"</span>: <span class="string">"user_id"</span>
                        }
                    }
                }
            }
        }
    }
}
</code></pre><p>Cardinality Aggregation的作用就是类似sql中的distinct，去重。</p>
<p>Date Histogram Aggregation的作用是根据时间进行统计。内部有个interval属性表面统计的范畴。</p>
<p>下面加几条数据到event_index里：</p>
<pre><code>-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 12:00:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"2"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format2"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:30:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"3"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format3"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:30:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:50:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:55:00"</span>
}
</code></pre><p>11-07 12-13点有1条数据，1个用户<br>11-07 13-14点有4条数据，3个用户</p>
<p>使用模板查询：</p>
<pre><code>curl -XGET <span class="string">"<span class="variable">$ES</span>/event_index/_search/template"</span> -d'{
  <span class="string">"template"</span>: { <span class="string">"id"</span>: <span class="string">"stats"</span> }, 
  <span class="string">"params"</span>: { <span class="string">"earliest"</span>: <span class="string">"2015-11-07 00:00:00"</span>, <span class="string">"latest"</span>: <span class="string">"2015-11-07 23:59:59"</span>, <span class="string">"interval"</span>: <span class="string">"hour"</span> }
}'    
</code></pre><p>结果：</p>
<pre><code>{
    "<span class="attribute">took</span>": <span class="value"><span class="number">3</span></span>,
    "<span class="attribute">timed_out</span>": <span class="value"><span class="literal">false</span></span>,
    "<span class="attribute">_shards</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">successful</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">failed</span>": <span class="value"><span class="number">0</span>
    </span>}</span>,
    "<span class="attribute">hits</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">max_score</span>": <span class="value"><span class="number">0</span></span>,
        "<span class="attribute">hits</span>": <span class="value">[]
    </span>}</span>,
    "<span class="attribute">aggregations</span>": <span class="value">{
        "<span class="attribute">stats_data</span>": <span class="value">{
            "<span class="attribute">buckets</span>": <span class="value">[
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 12:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446897600000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">1</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">1</span>
                    </span>}
                </span>},
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 13:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446901200000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">4</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">3</span>
                    </span>}
                </span>}
            ]
        </span>}
    </span>}
</span>}
</code></pre><p>12点-13点的只有1条数据，1个用户。13-14点的有4条数据，3个用户。</p>
<p>以天(day)统计：</p>
<pre><code>curl -XGET <span class="string">"<span class="variable">$ES</span>/event_index/_search/template"</span> -d'{
  <span class="string">"template"</span>: { <span class="string">"id"</span>: <span class="string">"stats"</span> }, 
  <span class="string">"params"</span>: { <span class="string">"earliest"</span>: <span class="string">"2015-11-07 00:00:00"</span>, <span class="string">"latest"</span>: <span class="string">"2015-11-07 23:59:59"</span>, <span class="string">"interval"</span>: <span class="string">"day"</span> }
}'    
</code></pre><p>结果：</p>
<pre><code>{
    "<span class="attribute">took</span>": <span class="value"><span class="number">4</span></span>,
    "<span class="attribute">timed_out</span>": <span class="value"><span class="literal">false</span></span>,
    "<span class="attribute">_shards</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">successful</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">failed</span>": <span class="value"><span class="number">0</span>
    </span>}</span>,
    "<span class="attribute">hits</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">max_score</span>": <span class="value"><span class="number">0</span></span>,
        "<span class="attribute">hits</span>": <span class="value">[]
    </span>}</span>,
    "<span class="attribute">aggregations</span>": <span class="value">{
        "<span class="attribute">stats_data</span>": <span class="value">{
            "<span class="attribute">buckets</span>": <span class="value">[
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 00:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446854400000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">5</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">3</span>
                    </span>}
                </span>}
            ]
        </span>}
    </span>}
</span>}
</code></pre><p>11-07这一天有5条数据，3个用户。</p>
<p>本文只是简单说明了es查询模板的使用，也简单使用了2个aggregation。更多内容可以去官网查看相关资料。</p>
]]></content>
    <summary type="html">
    <![CDATA[近在公司又用到了elasticsearch，也用到了查询模板，顺便写篇文章记录一下查询模板的使用 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="elasticsearch" scheme="http://fangjian0423.github.io/tags/elasticsearch/"/>
    
      <category term="elasticsearch" scheme="http://fangjian0423.github.io/categories/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[html页面左右滑动菜单效果的实现]]></title>
    <link href="http://fangjian0423.github.io/2015/10/29/html-left-right-menu/"/>
    <id>http://fangjian0423.github.io/2015/10/29/html-left-right-menu/</id>
    <published>2015-10-29T13:15:13.000Z</published>
    <updated>2015-12-20T17:06:35.000Z</updated>
    <content type="html"><![CDATA[<h2 id="正文">正文</h2><p>最近要实现一个微信端页面左边弹出菜单的实现，效果如下：</p>
<iframe src="//jsfiddle.net/format/t0xda6zv/embedded/result,js,html,css" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
<p>绿色部分是左菜单内容，高度填充满整个页面。且有滚动条，并且滚动条内容随着滚动条的滚动不会影响正文的滚动，正文内容的滚动不会影响左菜单的滚动。</p>
<p>下面说下自己实现这种效果的思路。</p>
<p>1.首先由于左右两边的滚动不影响双方，这就需要将菜单和内容的position设置为绝对定位，设置都需要滚动条: overflow: auto;  。</p>
<p>2.菜单的内容会覆盖正文的内容：所以菜单的z-index比正文要大。</p>
<p>3.给左边的菜单加点width动画，需要显示的时候设置width，需要隐藏的时候width设置为0即可。</p>
<p>如果不想把菜单的内容覆盖在正文内容上面，而是正文内容向右偏移菜单的宽度：</p>
<iframe src="//jsfiddle.net/format/of445qxn/embedded/result,js,html,css" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
<p>这个效果与上一个效果一样，唯一的区别就是正文的z-index比菜单大，而且正文需要配置一个背景色。最后切换菜单的时候正文内容加点向右便宜的动画即可。</p>
<p>刚换了next主题…. 发现这个主题右边的sidebar也是这样的效果实现  →_→ 。 囧 。</p>
<h2 id="基础小知识">基础小知识</h2><p>上面第二个例子中内容的z-index比菜单的z-index要大，而且正文需要配置一个背景色。为什么正文需要配置一个背景色呢？</p>
<p><strong>因为html中的元素没指定背景色的话，那说明这个元素的背景色是透明的</strong></p>
<p>比如下面这个效果，上面的内容没设置背景色，所以是透明的，虽然它的z-index比上面的块要大，但是还是显示了。</p>
<iframe src="//jsfiddle.net/format/yxseu05z/embedded/result,html" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
<p>给上面的内容设置黄色的背景色就可以隐藏下面的内容的。</p>
<iframe src="//jsfiddle.net/format/kec1up6t/embedded/result,html" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
]]></content>
    <summary type="html">
    <![CDATA[最近要实现一个微信端页面左边弹出菜单的实现，效果如下 ...]]>
    
    </summary>
    
      <category term="css" scheme="http://fangjian0423.github.io/tags/css/"/>
    
      <category term="html" scheme="http://fangjian0423.github.io/tags/html/"/>
    
      <category term="css" scheme="http://fangjian0423.github.io/categories/css/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[最近前端的一点总结]]></title>
    <link href="http://fangjian0423.github.io/2015/10/24/front-end/"/>
    <id>http://fangjian0423.github.io/2015/10/24/front-end/</id>
    <published>2015-10-24T12:15:13.000Z</published>
    <updated>2015-12-20T17:01:03.000Z</updated>
    <content type="html"><![CDATA[<p>这段时间被拉去当做前端了，做了快1个多月了，也好久没写博客了。 做了两个项目的前端，这周是第二个项目的启动，而且这周简直是灾难的一周，基本上都是23点后下班的，有两天还是凌晨2点。悲剧。</p>
<p>做前端经验不是很多，这个月还是学到了一些前端的自己没掌握的知识。做个总结吧。</p>
<p>1.box-sizing属性。</p>
<p>box-sizing是css3引入的。有两个值，分别content-box和border-box。 默认为content-box。这个属性的作用是这样的：</p>
<p>比如我们定义一个div，宽度和高度都是50px。padding 5px， border: 5px。 那么这个div实际的宽度和高度是：</p>
<pre><code><span class="number">50</span> + <span class="number">2</span> * <span class="number">5</span> + <span class="number">2</span> * <span class="number">5</span> = <span class="number">70</span>。
</code></pre><p>如果使用border-box的话，那么这个div的宽度还是50。 因为border-box会把padding和border都一起算到宽度和高度里面。所以使用border-box后div的高度和宽度为还是50。但是实际上真正显示内容的高度和宽度是 50 - 5 <em> 2 - 5 </em> 2 = 30。</p>
<p>直接来点实际的代码，使用content-box，也就是默认情况：</p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"background-color: blue; width: 100px; height: 100px;"</span>&gt;
    &lt;<span class="keyword">div</span> style=<span class="string">"background-color: yellow; width: 50px; height: 50px; padding: 5px; border: 5px solid red; box-sizing: content-box;"</span>&gt;&lt;/<span class="keyword">div</span>&gt;
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/content-box1.png" alt=""><br><img src="http://7x2wh6.com1.z0.glb.clouddn.com/content-box2.png" alt=""></p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"background-color: blue; width: 100px; height: 100px;"</span>&gt;
    &lt;<span class="keyword">div</span> style=<span class="string">"background-color: yellow; width: 50px; height: 50px; padding: 5px; border: 5px solid red; box-sizing: border-box;"</span>&gt;&lt;/<span class="keyword">div</span>&gt;
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png" alt=""><br><img src="http://7x2wh6.com1.z0.glb.clouddn.com/border-box2.png" alt=""></p>
<p>Bootstrap3中也大量使用了border-box。</p>
<p>2.white-space属性。</p>
<p>使用white-space是为了让多张图片在同一行展示，不会换行。</p>
<pre><code>&lt;<span class="tag">div</span> style=<span class="string">"width: 200px;"</span>&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
&lt;/div&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/white-space1.png" alt=""></p>
<p>将外层的div改为：</p>
<pre><code>&lt;<span class="tag">div</span> style=<span class="string">"width: 200px; white-space: nowrap; overflow: scroll;"</span>&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/white-space2.png" alt=""></p>
<p>white-space设置为nowrap后，文本内容不会换行直到遇到br标签为止。</p>
<p>3.CSS3的动画。</p>
<p>一开始没有使用CSS3的动画，使用了JQuery的animate，结果发现页面动画有点卡，后来改成了CSS3的动画。</p>
<p>因为都是一些比较简单的动画，所以只能写下简单的动画属性了。</p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"width: 50px; height: 50px; background-color: yellow; transition: width .2s;"</span>&gt;
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p>div的点击事件：</p>
<pre><code>$(<span class="string">"div"</span>).click(<span class="keyword">function</span>() {
    var <span class="variable">$div</span> = $(this);
    if(<span class="variable">$div</span>.width() == <span class="number">50</span>) {
      <span class="variable">$div</span>.width(<span class="string">"100"</span>);  
    } else {
      <span class="variable">$div</span>.width(<span class="string">"50"</span>);  
    }
});
</code></pre><p>动画还有延迟效果，这里就不举例了。</p>
<p>4.垂直居中</p>
<p>高度固定的垂直居中，设置line-height为容器高度，text-align为center即可：</p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"width: 100px; height: 100px; background-color: yellow; line-height: 100px; text-align: center;"</span>&gt;
    我居中了
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p>高度不固定的垂直居中：</p>
<pre><code>html, body {
  height: 100%;
}
body {
  display: -webkit-box;
  display: -webkit-flex;

  display: -moz-box;
  display: -moz-flex;

  display: -ms-flexbox;

  display: flex;
<span class="comment">
  /* 水平居中*/</span>
  -<span class="ruby">webkit-box-<span class="symbol">align:</span> center;
</span>  -<span class="ruby">moz-box-<span class="symbol">align:</span> center;
</span>  -<span class="ruby">ms-flex-<span class="symbol">pack:</span>center;<span class="regexp">/* IE 10 */</span>
</span>
  -<span class="ruby">webkit-justify-<span class="symbol">content:</span> center;
</span>  -<span class="ruby">moz-justify-<span class="symbol">content:</span> center;
</span>  justify-content: center;/* IE 11+,Firefox 22+,Chrome 29+,Opera 12.1*/
<span class="comment">
  /* 垂直居中 */</span>
  -<span class="ruby">webkit-box-<span class="symbol">pack:</span> center;
</span>  -<span class="ruby">moz-box-<span class="symbol">pack:</span> center;
</span>  -<span class="ruby">ms-flex-<span class="symbol">align:</span>center;<span class="regexp">/* IE 10 */</span>
</span>
  -<span class="ruby">webkit-align-<span class="symbol">items:</span> center;
</span>  -<span class="ruby">moz-align-<span class="symbol">items:</span> center;
</span>  align-items: center;
}

...
&lt;body&gt;
    垂直居中
&lt;/body&gt;
...
</code></pre><p>还有其他的一些比如绝对定位，固定定位，相对定位，overflow等问题就不一一举例了。 估计之后还是得做前端的一些工作，到时候用到了一些新内容的话还会继续更新前端相关的博客的。</p>
]]></content>
    <summary type="html">
    <![CDATA[这段时间被拉去当做前端了，做了快1个多月了，也好久没写博客了。 做了两个项目的前端，这周是第二个项目的启动 ...]]>
    
    </summary>
    
      <category term="css" scheme="http://fangjian0423.github.io/tags/css/"/>
    
      <category term="html" scheme="http://fangjian0423.github.io/tags/html/"/>
    
      <category term="css" scheme="http://fangjian0423.github.io/categories/css/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala的Predef介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/10/07/scala-predef/"/>
    <id>http://fangjian0423.github.io/2015/10/07/scala-predef/</id>
    <published>2015-10-06T16:37:20.000Z</published>
    <updated>2015-12-20T17:03:03.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>Scala每个程序都会自动import以下3个包，分别是 java.lang.<em>  ,  scala.</em> 和 Predef._ 。跟Java程序都会自动import java.lang一样。</p>
<p>Predef是一个单例对象，所以我们import进来之后，可以直接使用Predef中定义的方法。</p>
<h2 id="Predef中定义的方法和属性">Predef中定义的方法和属性</h2><h3 id="常用方法和类">常用方法和类</h3><pre><code><span class="function"><span class="keyword">def</span> <span class="title">classOf</span>[</span><span class="type">T</span>]: <span class="type">Class</span>[<span class="type">T</span>] = <span class="literal">null</span> <span class="comment">// This is a stub method. The actual implementation is filled in by the compiler.</span>

<span class="class"><span class="keyword">type</span> <span class="title">String</span>        =</span> java.lang.<span class="type">String</span>  <span class="comment">// scala中的String使用jdk中的String</span>
<span class="class"><span class="keyword">type</span> <span class="title">Class</span>[</span><span class="type">T</span>]      = java.lang.<span class="type">Class</span>[<span class="type">T</span>] <span class="comment">// scala中的Class使用jdk中的Class</span>

<span class="class"><span class="keyword">type</span> <span class="title">Function</span>[</span>-<span class="type">A</span>, +<span class="type">B</span>] = <span class="type">Function1</span>[<span class="type">A</span>, <span class="type">B</span>] <span class="comment">// Function1取别名Function</span>

<span class="class"><span class="keyword">type</span> <span class="title">Map</span>[</span><span class="type">A</span>, +<span class="type">B</span>] = immutable.<span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>] <span class="comment">// Map类型默认使用immutable包下的Map</span>
<span class="class"><span class="keyword">type</span> <span class="title">Set</span>[</span><span class="type">A</span>]     = immutable.<span class="type">Set</span>[<span class="type">A</span>] <span class="comment">// Set类型默认使用immutable包下的Set</span>
<span class="keyword">val</span> <span class="type">Map</span>         = immutable.<span class="type">Map</span> <span class="comment">// Map对象默认使用immutable包下的Map对象(下面测试用到)</span>
<span class="keyword">val</span> <span class="type">Set</span>         = immutable.<span class="type">Set</span> <span class="comment">// Set对象默认使用immutable包下的Set对象(下面测试用到)</span>
</code></pre><p>一些测试：</p>
<pre><code>// 默认的Map和<span class="operator"><span class="keyword">Set</span>都是immutable包下的，这里的<span class="keyword">Set</span>和<span class="keyword">Map</span>都是在Predef中定义的一个变量。
<span class="keyword">Map</span>(<span class="number">1</span> -&gt; <span class="number">1</span>, <span class="number">2</span> -&gt; <span class="number">2</span>) // scala.collection.immutable.<span class="keyword">Map</span>[<span class="built_in">Int</span>,<span class="built_in">Int</span>] = <span class="keyword">Map</span>(<span class="number">1</span> -&gt; <span class="number">1</span>, <span class="number">2</span> -&gt; <span class="number">2</span>)
<span class="keyword">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>) // scala.collection.immutable.<span class="keyword">Set</span>[<span class="built_in">Int</span>] = <span class="keyword">Set</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span>
</code></pre><h3 id="打印方法">打印方法</h3><pre><code>def <span class="function"><span class="title">print</span><span class="params">(x: Any)</span></span> = Console.<span class="function"><span class="title">print</span><span class="params">(x)</span></span>
def <span class="function"><span class="title">println</span><span class="params">()</span></span> = Console.<span class="function"><span class="title">println</span><span class="params">()</span></span>
def <span class="function"><span class="title">println</span><span class="params">(x: Any)</span></span> = Console.<span class="function"><span class="title">println</span><span class="params">(x)</span></span>
def <span class="function"><span class="title">printf</span><span class="params">(text: String, xs: Any*)</span></span> = Console.<span class="function"><span class="title">print</span><span class="params">(text.format(xs: _*)</span></span>)
</code></pre><p>因此，平时我们使用的println，print，printf这些打印方法都是在Predef中定义的，</p>
<h3 id="一些调试和错误方法">一些调试和错误方法</h3><pre><code><span class="comment">// 过期方法，抛出带有message消息的RuntimeException</span>
<span class="annotation">@deprecated</span>(<span class="string">"Use `sys.error(message)` instead"</span>, <span class="string">"2.9.0"</span>)
<span class="keyword">def</span> error(<span class="string">message:</span> String): Nothing = sys.error(message)

<span class="comment">// 断言。 参数是一个Boolean类型，失败抛出java.lang.AssertionError异常</span>
<span class="annotation">@elidable</span>(ASSERTION)
<span class="keyword">def</span> <span class="keyword">assert</span>(<span class="string">assertion:</span> Boolean) {
<span class="keyword">if</span> (!assertion)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assertion failed"</span>)
}

<span class="comment">// 跟上一个方法类似，多了一个message参数。抛出的异常就打印出这个message参数</span>
<span class="annotation">@elidable</span>(ASSERTION) <span class="annotation">@inline</span>
<span class="keyword">final</span> <span class="keyword">def</span> <span class="keyword">assert</span>(<span class="string">assertion:</span> Boolean, <span class="string">message:</span> =&gt; Any) {
<span class="keyword">if</span> (!assertion)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assertion failed: "</span>+ message)
}

<span class="comment">// 跟assert类似，唯一的区别是assume支持静态经验(static checker)</span>
<span class="annotation">@elidable</span>(ASSERTION)
<span class="keyword">def</span> assume(<span class="string">assumption:</span> Boolean) {
<span class="keyword">if</span> (!assumption)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assumption failed"</span>)
}

<span class="comment">// 跟assume一样，多了个message参数</span>
<span class="annotation">@elidable</span>(ASSERTION) <span class="annotation">@inline</span>
<span class="keyword">final</span> <span class="keyword">def</span> assume(<span class="string">assumption:</span> Boolean, <span class="string">message:</span> =&gt; Any) {
<span class="keyword">if</span> (!assumption)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assumption failed: "</span>+ message)
}

<span class="comment">// 跟assert类似，只不过抛出的是IllegalArgumentException异常</span>
<span class="keyword">def</span> require(<span class="string">requirement:</span> Boolean) {
<span class="keyword">if</span> (!requirement)
  <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"requirement failed"</span>)
}

<span class="comment">// 多个参数，作用如上一样</span>
<span class="annotation">@inline</span> <span class="keyword">final</span> <span class="keyword">def</span> require(<span class="string">requirement:</span> Boolean, <span class="string">message:</span> =&gt; Any) {
<span class="keyword">if</span> (!requirement)
  <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"requirement failed: "</span>+ message)
}
</code></pre><p>调试和错误方法测试：</p>
<pre><code><span class="function"><span class="title">assert</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>)</span></span> <span class="comment">// java.lang.AssertionError: assertion failed</span>
<span class="function"><span class="title">assert</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>, <span class="string">"test"</span>)</span></span> <span class="comment">// java.lang.AssertionError: assertion failed: test</span>
<span class="function"><span class="title">assume</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>)</span></span> <span class="comment">// java.lang.AssertionError: assumption failed</span>
<span class="function"><span class="title">assume</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>, <span class="string">"test"</span>)</span></span> <span class="comment">// java.lang.AssertionError: assumption failed: test</span>
<span class="function"><span class="title">require</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>)</span></span> <span class="comment">// java.lang.IllegalArgumentException: requirement failed</span>
<span class="function"><span class="title">require</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>, <span class="string">"test"</span>)</span></span> <span class="comment">// java.lang.IllegalArgumentException: requirement failed: test</span>
</code></pre><h3 id="一个特殊的属性">一个特殊的属性</h3><p>Predef中有个 ??? 属性，抛出一个NotImplementedError：</p>
<pre><code><span class="function"><span class="keyword">def</span> ??? :</span> Nothing = throw new NotImplementedError
</code></pre><p>比如定义一些方法的时候，这个方法还没有实现，这个时候可以使用 ???， 而非TODO：</p>
<pre><code><span class="keyword">def</span> todoMethod(x: <span class="keyword">Int</span>): <span class="keyword">Int</span> = ???

todoMethod(<span class="number">2</span>) <span class="comment">// scala.NotImplementedError: an implementation is missing</span>
</code></pre><h2 id="Predef还有大量的隐式转换和隐式转换类">Predef还有大量的隐式转换和隐式转换类</h2><p>再讲Predef中的隐式转换和隐式转换类之前，先介绍一下这2个概念。</p>
<h3 id="隐式转换">隐式转换</h3><p>隐式转换的意思是一个方法中有一个类型的参数，并返回另外一个类型的返回值。比如一个Double类型的方法返回一个Int类型的返回值。</p>
<pre><code>def <span class="function"><span class="title">double2Int</span><span class="params">(d: Double)</span></span> = d<span class="class">.toInt</span>

<span class="function"><span class="title">double2Int</span><span class="params">(<span class="number">2.4</span>)</span></span> <span class="comment">// 2</span>

val <span class="tag">a</span>: Int = <span class="number">2.3</span> <span class="comment">// 报错</span>
</code></pre><p>重新定义double2Int，使其支持隐式转换：</p>
<pre><code><span class="type">implicit</span> def double2Int(d: <span class="type">Double</span>) = d.toInt

val a: <span class="built_in">Int</span> = <span class="number">2.3</span> // a: <span class="built_in">Int</span> = <span class="number">2</span>
</code></pre><h3 id="隐式转换类">隐式转换类</h3><p>当需要给Int类型添加一个 –&gt; 方法的时候，需要使用到隐式转换类。因为隐式转换只支持1个参数，所以只能通过隐式转换类完成。</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMaker(left: <span class="built_in">Int</span>) {
    def --&gt;(right: <span class="built_in">Int</span>) = left to right
}

val <span class="built_in">range</span> = <span class="number">1</span> --&gt; <span class="number">10</span> // <span class="built_in">Range</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)
</code></pre><p>隐式转换类有个弊端，那就是每次都会创建一个类的实例，有时候这完全没有必要。</p>
<p>scala提供了一个将隐式转换类转换成value class的方法，只需要继承AnyVal即可，还需要注意参数需要加上val标识符。</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMaker(val left: <span class="built_in">Int</span>) <span class="keyword">extends</span> AnyVal {
    def --&gt;(right: <span class="built_in">Int</span>) = left to right
}
</code></pre><h3 id="Predef中的隐式转换和隐式转换类">Predef中的隐式转换和隐式转换类</h3><p>-&gt; 方法：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> ArrowAssoc[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    @inline def -&gt; [B](y: B): Tuple2[A, B] = Tuple2(self, y)
    def →[B](y: B): Tuple2[A, B] = -&gt;(y)
}
</code></pre><p>生成一个二元元组，Tuple2。</p>
<pre><code><span class="number">1</span> → <span class="number">2</span>  <span class="comment">// (Int, Int) = (1, 2)</span>
<span class="number">1</span> -&gt; <span class="number">2</span> <span class="comment">// (Int, Int) = (1, 2)</span>
</code></pre><p>ensuring 方法：</p>
<pre><code>implicit final <span class="class"><span class="keyword">class</span> <span class="title">Ensuring</span>[<span class="title">A</span>](<span class="title">private</span> <span class="title">val</span> <span class="title">self</span>: <span class="title">A</span>) <span class="title">extends</span> <span class="title">AnyVal</span> {</span>
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">Boolean</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond)</span>;</span> <span class="keyword">self</span> }
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">Boolean</span>, <span class="symbol">msg:</span> =&gt; <span class="constant">Any</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond, msg)</span>;</span> <span class="keyword">self</span> }
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">A</span> =&gt; <span class="constant">Boolean</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond(<span class="keyword">self</span>)</span>);</span> <span class="keyword">self</span> }
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">A</span> =&gt; <span class="constant">Boolean</span>, <span class="symbol">msg:</span> =&gt; <span class="constant">Any</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond(<span class="keyword">self</span>)</span>, <span class="title">msg</span>);</span> <span class="keyword">self</span> }
}
</code></pre><p>ensuring内部调用assert方法。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">doublePositive</span><span class="params">(n: Int)</span>:</span> Int = {
    n * <span class="number">2</span>
} ensuring(n =&gt; n &gt;= <span class="number">0</span> &amp;&amp; n % <span class="number">2</span> == <span class="number">0</span>)

doublelPositive(<span class="number">1</span>) // <span class="number">2</span>
doublelPositive(-<span class="number">1</span>) // java.lang.AssertionError: assertion failed
</code></pre><p>formatted 方法：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> StringFormat[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    @inline def <span class="keyword">formatted</span>(fmtstr: String): String = fmtstr <span class="keyword">format</span> self
}
</code></pre><p>格式化字符串，使用java.lang.String.format方法。</p>
<pre><code><span class="string">"Format"</span> formatted <span class="string">"%s, let's go"</span> <span class="comment">// Format, let's go</span>
</code></pre><p>+ 方法：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> any2stringadd[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    def +(other: String): String = String.valueOf(self) + other
}
</code></pre><p>case class使用+方法：</p>
<pre><code>case class <span class="function"><span class="title">Student</span><span class="params">(name: String, age: Int)</span></span>
<span class="function"><span class="title">Student</span><span class="params">(<span class="string">"format"</span>, <span class="number">11</span>)</span></span> + <span class="string">" 22"</span> <span class="comment">// Student(format,11) 22</span>
</code></pre><p>StringOps类：</p>
<pre><code>@inline <span class="type">implicit</span> def augmentString(x: String): StringOps = new StringOps(x)
@inline <span class="type">implicit</span> def unaugmentString(x: StringOps): String = x.repr
</code></pre><p>StringOps提供了丰富的原生String没提供的方法：</p>
<pre><code><span class="string">"format"</span>.length <span class="comment">// 6 原生String是没有提供length方法的</span>
<span class="string">"format"</span>.foreach(println(_))
<span class="string">"format"</span>.stripPrefix(<span class="string">"for"</span>) <span class="comment">// mat</span>
<span class="string">"format"</span>.slice(<span class="number">1</span>, <span class="number">3</span>) <span class="comment">// or</span>
<span class="string">"format"</span> * <span class="number">5</span> <span class="comment">// formatformatformatformatformat</span>
<span class="string">"true"</span>.toBoolean <span class="comment">// true</span>
</code></pre><p>ArrayOps类：</p>
<pre><code><span class="type">implicit</span> def genericArrayOps[T](xs: Array[T]): ArrayOps[T] = (xs match {
    <span class="keyword">case</span> x: Array[AnyRef]  =&gt; refArrayOps[AnyRef](x)
    <span class="keyword">case</span> x: Array[Boolean] =&gt; booleanArrayOps(x)
    <span class="keyword">case</span> x: Array[Byte]    =&gt; byteArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="built_in">Char</span>]    =&gt; charArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="type">Double</span>]  =&gt; doubleArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="built_in">Float</span>]   =&gt; floatArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="built_in">Int</span>]     =&gt; intArrayOps(x)
    <span class="keyword">case</span> x: Array[Long]    =&gt; longArrayOps(x)
    <span class="keyword">case</span> x: Array[Short]   =&gt; shortArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="keyword">Unit</span>]    =&gt; unitArrayOps(x)
    <span class="keyword">case</span> null              =&gt; null
}).asInstanceOf[ArrayOps[T]]


  <span class="type">implicit</span> def booleanArrayOps(xs: Array[Boolean]): ArrayOps[Boolean] = new ArrayOps.ofBoolean(xs)
  <span class="type">implicit</span> def byteArrayOps(xs: Array[Byte]): ArrayOps[Byte]          = new ArrayOps.ofByte(xs)
  <span class="type">implicit</span> def charArrayOps(xs: Array[<span class="built_in">Char</span>]): ArrayOps[<span class="built_in">Char</span>]          = new ArrayOps.ofChar(xs)
  <span class="type">implicit</span> def doubleArrayOps(xs: Array[<span class="type">Double</span>]): ArrayOps[<span class="type">Double</span>]    = new ArrayOps.ofDouble(xs)
  <span class="type">implicit</span> def floatArrayOps(xs: Array[<span class="built_in">Float</span>]): ArrayOps[<span class="built_in">Float</span>]       = new ArrayOps.ofFloat(xs)
  <span class="type">implicit</span> def intArrayOps(xs: Array[<span class="built_in">Int</span>]): ArrayOps[<span class="built_in">Int</span>]             = new ArrayOps.ofInt(xs)
  <span class="type">implicit</span> def longArrayOps(xs: Array[Long]): ArrayOps[Long]          = new ArrayOps.ofLong(xs)
  <span class="type">implicit</span> def refArrayOps[T &lt;: AnyRef](xs: Array[T]): ArrayOps[T]    = new ArrayOps.ofRef[T](xs)
  <span class="type">implicit</span> def shortArrayOps(xs: Array[Short]): ArrayOps[Short]       = new ArrayOps.ofShort(xs)
  <span class="type">implicit</span> def unitArrayOps(xs: Array[<span class="keyword">Unit</span>]): ArrayOps[<span class="keyword">Unit</span>]          = new ArrayOps.ofUnit(xs)
</code></pre><p>ArrayOps提供了丰富的原生数组没提供的方法：</p>
<pre><code>Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>) :+ <span class="number">4</span> <span class="comment">// Array(1, 2, 3, 4)</span>
</code></pre><p>Scala程序员可以较少关心装箱和拆箱操作，这也是由于Predef对象里定义了Scala值类型与java基本类型直接的隐式转换：</p>
<pre><code><span class="type">implicit</span> def byte2Byte(x: Byte)           = java.lang.Byte.valueOf(x)
<span class="type">implicit</span> def short2Short(x: Short)        = java.lang.Short.valueOf(x)
<span class="type">implicit</span> def char2Character(x: <span class="built_in">Char</span>)      = java.lang.<span class="type">Character</span>.valueOf(x)
<span class="type">implicit</span> def int2Integer(x: <span class="built_in">Int</span>)          = java.lang.<span class="type">Integer</span>.valueOf(x)
<span class="type">implicit</span> def long2Long(x: Long)           = java.lang.Long.valueOf(x)
<span class="type">implicit</span> def float2Float(x: <span class="built_in">Float</span>)        = java.lang.<span class="built_in">Float</span>.valueOf(x)
<span class="type">implicit</span> def double2Double(x: <span class="type">Double</span>)     = java.lang.<span class="type">Double</span>.valueOf(x)
<span class="type">implicit</span> def boolean2Boolean(x: Boolean)  = java.lang.Boolean.valueOf(x)

<span class="type">implicit</span> def Byte2byte(x: java.lang.Byte): Byte             = x.byteValue
<span class="type">implicit</span> def Short2short(x: java.lang.Short): Short         = x.shortValue
<span class="type">implicit</span> def Character2char(x: java.lang.<span class="type">Character</span>): <span class="built_in">Char</span>   = x.charValue
<span class="type">implicit</span> def Integer2int(x: java.lang.<span class="type">Integer</span>): <span class="built_in">Int</span>         = x.intValue
<span class="type">implicit</span> def Long2long(x: java.lang.Long): Long             = x.longValue
<span class="type">implicit</span> def Float2float(x: java.lang.<span class="built_in">Float</span>): <span class="built_in">Float</span>         = x.floatValue
<span class="type">implicit</span> def Double2double(x: java.lang.<span class="type">Double</span>): <span class="type">Double</span>     = x.doubleValue
<span class="type">implicit</span> def Boolean2boolean(x: java.lang.Boolean): Boolean = x.booleanValue
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Scala每个程序都会自动import以下3个包，分别是 java.lang._  ,  scala._ 和 Predef._ 。跟Java程序都会自动import java.lang一样 ...]]>
    
    </summary>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/tags/jvm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spring Boot 部分特性记录]]></title>
    <link href="http://fangjian0423.github.io/2015/09/21/springboot-intro/"/>
    <id>http://fangjian0423.github.io/2015/09/21/springboot-intro/</id>
    <published>2015-09-20T16:21:49.000Z</published>
    <updated>2015-12-20T17:07:24.000Z</updated>
    <content type="html"><![CDATA[<p>SpringBoot是Java的一个micro-service框架。它设计的目的是简化Spring应用的初始搭建以及开发过程。使用SpringBoot可以避免大量的xml配置文件，它内部使用很多约定的方式。</p>
<p>以一个最简单的MVC例子来说，使用SpringBoot进行开发的话定义好对应的Controller，Repository和Entity之后，加上各自的Annotation即可。</p>
<p>Repository框架可以选择Spring Data或者Hibernate，可通过自由配置。</p>
<p>视图框架也可通过配置选择freemarker或者velocity等视图框架。</p>
<p>下面，介绍一下SpringBoot的一些功能。</p>
<h2 id="SpringBoot框架的启动">SpringBoot框架的启动</h2><p>SpringBoot使用SpringApplication这个类进行项目的启动。一般都会这么写：</p>
<pre><code><span class="annotation">@SpringBootApplication</span>
<span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> {</span>
    <span class="keyword">public</span> <span class="keyword">static</span> <span class="typename">void</span> main(String[] args) {
        SpringApplication.run(Application.<span class="keyword">class</span>, args);
    }
}
</code></pre><p>使用SpringBootApplication注解相当于使用了3个注解，分别是@ComponentScan，@Configuration，@EnableAutoConfiguration。</p>
<p>这里需要注意的是Application这个类所有的package位置。</p>
<p>比如你的项目的包路径是 me.format.project1，对应的controller和repository包是 me.format.project1.controller和me.format.project1.repository。 那么这个Application需要在的包路径为 me.format.project1。 因为SpringBootApplication注解内部是使用ComponentScan注解，这个注解会扫描Application包所在的路径下的各个bean。</p>
<h2 id="Profile的使用">Profile的使用</h2><p>可以在springboot项目中加入配置文件application.yml。</p>
<p>yaml中可以定义多个profile，也可以指定激活的profile：</p>
<pre><code><span class="attribute">spring</span>:
  profiles.<span class="attribute">active</span>: dev

---
<span class="attribute">spring</span>:
  <span class="attribute">profiles</span>: dev
<span class="attribute">myconfig</span>:
  <span class="attribute">config1</span>: dev-enviroment

---
<span class="attribute">spring</span>:
  <span class="attribute">profiles</span>: test
<span class="attribute">myconfig</span>:
  <span class="attribute">config1</span>: test-enviroment

---
<span class="attribute">spring</span>:
  <span class="attribute">profiles</span>: prod
<span class="attribute">myconfig</span>:
  <span class="attribute">config1</span>: prod-envioment
</code></pre><p>也可以在运行的执行指定profile：</p>
<pre><code>java -Dspring<span class="class">.profiles</span><span class="class">.active</span>=<span class="string">"prod"</span> -jar yourjar.jar
</code></pre><p>还可以使用Profile注解，MyConfig只会在prod这个profile下才会生效，其他profile不会生效：</p>
<pre><code><span class="variable">@Profile</span>(<span class="string">"prod"</span>)
<span class="variable">@Component</span>
public class MyConfig {
    ....
}
</code></pre><h2 id="自定义的一些Conveter，Interceptor">自定义的一些Conveter，Interceptor</h2><p>如果想配置springmvc的HandlerInterceptorAdapter或者HttpMessageConverter。 只需要定义自己的interceptor或者converter，然后加上Component注解。这样SpringBoot会自动处理这些类，不用自己在配置文件里指定对应的内容。 这个也是相当方便的。</p>
<pre><code><span class="annotation">@Component</span>
public <span class="class"><span class="keyword">class</span> <span class="title">AuthInterceptor</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">HandlerInterceptorAdapter</span> {</span>
    ...
}

<span class="annotation">@Component</span>
public <span class="class"><span class="keyword">class</span> <span class="title">MyConverter</span> <span class="title">implements</span> <span class="title">HttpMessageConverter&lt;MyObj&gt;</span> {</span> 
    ...
}
</code></pre><h2 id="模板的使用">模板的使用</h2><p>比如使用freemarker的时候，加入以下依赖：</p>
<pre><code><span class="tag">&lt;<span class="title">dependency</span>&gt;</span>
    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span>
    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spring-boot-starter-freemarker<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span>
<span class="tag">&lt;/<span class="title">dependency</span>&gt;</span>
</code></pre><p>然后在resources目录下建立一个templates目录即可，视图将会从这个templates位置开始找。</p>
<h2 id="其他">其他</h2><p>关于其他的特性可以参考官方文档：</p>
<p><a href="http://docs.spring.io/spring-boot/docs/current/reference/html/" target="_blank" rel="external">http://docs.spring.io/spring-boot/docs/current/reference/html/</a></p>
<p>springboot还提供了一系列sample供参考：</p>
<p><a href="https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples" target="_blank" rel="external">https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples</a></p>
]]></content>
    <summary type="html">
    <![CDATA[SpringBoot是Java的一个micro-service框架。它设计的目的是简化Spring应用的初始搭建以及开发过程。使用SpringBoot可以避免大量的xml配置文件，它内部使用很多约定的方式 ...]]>
    
    </summary>
    
      <category term="microservice" scheme="http://fangjian0423.github.io/tags/microservice/"/>
    
      <category term="springboot" scheme="http://fangjian0423.github.io/tags/springboot/"/>
    
      <category term="springboot" scheme="http://fangjian0423.github.io/categories/springboot/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala持久层框架Slick介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/08/18/slick-intro/"/>
    <id>http://fangjian0423.github.io/2015/08/18/slick-intro/</id>
    <published>2015-08-18T15:22:33.000Z</published>
    <updated>2015-12-20T17:05:53.000Z</updated>
    <content type="html"><![CDATA[<h2 id="FRM介绍">FRM介绍</h2><p>最近看到了一个FRM的框架Slick。 FRM的意思是Functional Relational Mapping， 一种基于函数式的ORM。</p>
<p>举一个最简单的例子：</p>
<pre><code>val queryResult = db.query(queryStr)

queryResult.onSuccess { <span class="literal">result</span> =&gt;
    <span class="literal">result</span>.doSomething ...
}
</code></pre><p>数据库db查询一条sql语句。查询成功的时候使用闭包完成处理。 看到这段代码的第一反应就是js的ajax处理，代码几乎是一样的，也发现之前在学校里写nodejs的时候查询db也是这样的语法。</p>
<pre><code><span class="keyword">var</span> jqxhr = $.ajax( {
    url: 'url',
    <span class="keyword">method</span>: '<span class="type">GET</span>',
    data: [user: 'format']
});

jqxhr.success(function() {
    ...
});
</code></pre><p>FRM相比ORM最明显的优势就是FRM基于多线程的Future的数据查询，而ORM是单线程的线性执行。</p>
<p>FRM构造sql查询也是相当简单的：</p>
<pre><code><span class="comment">// 构造查询</span>
val newQuery = students.<span class="function"><span class="title">filter</span><span class="params">(_.age &gt; <span class="number">24</span>)</span></span>.<span class="function"><span class="title">sortBy</span><span class="params">(_.name)</span></span>
<span class="comment">// 执行查询</span>
db.<span class="function"><span class="title">run</span><span class="params">(newQuery)</span></span>
</code></pre><p>FRM其他的优势可以参考<a href="http://slick.typesafe.com/doc/3.0.1/introduction.html#functional-relational-mapping" target="_blank" rel="external">官方文档</a>。</p>
<h2 id="Slick实例">Slick实例</h2><p>下面以一个Students和Classrooms的实例来说明一下Slick的使用。</p>
<p>首先是创建对应的domain，学生与教室的关系是1对多。</p>
<p>Students domain(使用Option类型说明该列是可为空的)：</p>
<pre><code>class Student(tag: Tag) extends Table[<span class="link_label">(Int, String, Int, Int, Option[Date</span>])](tag, "Students") {

  def id: Rep[<span class="link_label">Int</span>] = column[<span class="link_label">Int</span>](<span class="link_url">"id", O.PrimaryKey, O.AutoInc</span>)
  def name: Rep[<span class="link_label">String</span>] = column[<span class="link_label">String</span>](<span class="link_url">"name"</span>)
  def age: Rep[<span class="link_label">Int</span>] = column[<span class="link_label">Int</span>](<span class="link_url">"age"</span>)
  def birthDate: Rep[<span class="link_label">Option[Date</span>]] = column[<span class="link_label">Option[Date</span>]]("birth_date")
  def classroomId = column[<span class="link_label">Int</span>](<span class="link_url">"classroom_id"</span>)

   def * : ProvenShape[(Int, String, Int, Int, Option[Date])] = (id, name, age, classroomId, birthDate)

  def classroom: ForeignKeyQuery[Classroom, (Int, String)] = foreignKey("FK<span class="emphasis">_CLASSROOM", classroomId, TableQuery[Classroom])(_</span>.id)
}
</code></pre><p>Classrooms domain：</p>
<pre><code>class Classroom(tag: Tag) extends Table[<span class="link_label">(Int, String)</span>](<span class="link_url">tag, "Classrooms"</span>) {
  def id = column[<span class="link_label">Int</span>](<span class="link_url">"id", O.PrimaryKey, O.AutoInc</span>)
  def name = column[<span class="link_label">String</span>](<span class="link_url">"name"</span>)

  def * = (id, name)
}
</code></pre><p>各个db操作，schema创建，sql插入，sql查询等操作如下，加了几句备注，具体的代码就不分析了：</p>
<pre><code><span class="keyword">object</span> <span class="type">SampleSlickDemo</span> extends <span class="type">App</span> {

  val db = <span class="type">Database</span>.forConfig(<span class="string">"h2mem1"</span>)

  <span class="keyword">try</span> {

    val classrooms = <span class="type">TableQuery</span>[<span class="type">Classroom</span>]
    val students = <span class="type">TableQuery</span>[<span class="type">Student</span>]


    val setupAction: <span class="type">DBIO</span>[<span class="type">Unit</span>] = <span class="type">DBIO</span>.<span class="type">seq</span>(
      // create student <span class="keyword">and</span> classroom table <span class="keyword">in</span> database
      (classrooms.schema ++ students.schema).create,
      // insert some rows <span class="keyword">in</span> classroom
      classrooms += (<span class="number">1</span>, <span class="string">"classroom1"</span>),
      classrooms += (<span class="number">2</span>, <span class="string">"classroom2"</span>),
      classrooms += (<span class="number">3</span>, <span class="string">"classroom2"</span>)
    )

    val setupFuture = db.run(setupAction)

    val f = setupFuture.flatMap { _ =&gt;

      val insertAction: <span class="type">DBIO</span>[<span class="type">Option</span>[<span class="type">Int</span>]] = students ++= <span class="type">Seq</span> (
        (<span class="number">1</span>, <span class="string">"format1"</span>, <span class="number">11</span>, <span class="number">1</span>, new <span class="type">Date</span>(<span class="type">System</span>.currentTimeMillis())),
        (<span class="number">2</span>, <span class="string">"format2"</span>, <span class="number">22</span>, <span class="number">2</span>, new <span class="type">Date</span>((<span class="type">System</span>.currentTimeMillis()))),
        (<span class="number">3</span>, <span class="string">"format3"</span>, <span class="number">33</span>, <span class="number">3</span>, new <span class="type">Date</span>((<span class="type">System</span>.currentTimeMillis())))
      )

      val insertAndPrintAction = insertAction.map { studentResult =&gt;
        studentResult.foreach { numRows =&gt;
          println(<span class="string">s"inserted $numRows students"</span>)
        }
      }

      db.run(insertAndPrintAction)
    }.flatMap { _ =&gt;

      // print <span class="type">All</span> <span class="type">Classrooms</span>
      db.run(classrooms.<span class="literal">result</span>).map { classroom =&gt;
        classroom.foreach(println);
      }

      // print <span class="type">All</span> <span class="type">Students</span>
      db.run(students.<span class="literal">result</span>).map { studnet =&gt;
        studnet.foreach(println);
      }

      // condition search
      val studentQuery = students.filter(_.age &gt; <span class="number">20</span>).sortBy(_.name)
      db.run(studentQuery.<span class="literal">result</span>).map { student =&gt;
        student.foreach(println)
      }
    }
    <span class="type">Await</span>.<span class="literal">result</span>(f, <span class="type">Duration</span>.<span class="type">Inf</span>)
  } <span class="keyword">finally</span> db.close()
}
</code></pre><h2 id="数据库配置">数据库配置</h2><p>在配置文件application.conf里配置数据库配置信息：</p>
<pre><code><span class="title">h2mem1</span> = {
  <span class="title">url</span> = <span class="string">"jdbc:h2:mem:test1"</span>
  driver = org.h2.Driver
  connectionPool = disabled
  keepAliveConnection = <span class="built_in">true</span>
}
</code></pre><p>然后就可使用Database初始化数据库，参数就是配置文件里对应的数据库name：</p>
<pre><code>val db = Database.<span class="function"><span class="title">forConfig</span><span class="params">(<span class="string">"h2mem1"</span>)</span></span>
</code></pre><h2 id="DBIOAction介绍">DBIOAction介绍</h2><p>DBIOAction就是数据库的一个操作，比如Insert，Update，Delete，Query等操作。</p>
<p>可以使用上面分析的数据库配置变量db进行操作。</p>
<p>db有个run方法使用DBIOAction作为参数，返回Future类型的返回值。</p>
<p>DBIO是一个单例对象，它的seq方法可以传入多个DBIOAction，然后返回一个新的DBIOAction。 += 方法返回的也是DBIOAction。</p>
<pre><code>val setupAction: DBIO[Unit] = DBIO.se<span class="string">q(
  (classrooms.schema ++ students.schema)</span>.create,
  classrooms += (<span class="number">1</span>, <span class="string">"classroom1"</span>),
  classrooms += (<span class="number">2</span>, <span class="string">"classroom2"</span>),
  classrooms += (<span class="number">3</span>, <span class="string">"classroom2"</span>)
)
</code></pre><p>++=方法跟+=方法一样会返回DBIOAction，只不过它的参数是个Iterable：</p>
<pre><code>val insertAction: DBIO[Option[Int]] = students ++= Seq (
    (<span class="number">1</span>, <span class="string">"format1"</span>, <span class="number">11</span>, <span class="number">1</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),
    (<span class="number">2</span>, <span class="string">"format2"</span>, <span class="number">22</span>, <span class="number">2</span>, <span class="keyword">new</span> Date((System.currentTimeMillis()))),
    (<span class="number">3</span>, <span class="string">"format3"</span>, <span class="number">33</span>, <span class="number">3</span>, <span class="keyword">new</span> Date((System.currentTimeMillis())))
  )
</code></pre><p>DBIOAction提供许多好用的方法：</p>
<p>map方法：参数是个函数，这个函数可以返回任意类型的值，返回是个DBIOAction。 所以可以使用map关联起来多个DBIOAction。</p>
<p>flatMap方法：参数是个函数，这个函数的返回值必须是个DBIOAction，返回值是个DBIOAction。作用跟map类似，只不过函数参数的返回值不一样。</p>
<p>filter方法：参数是个函数，这个函数的返回值必须是个Boolean，返回值是个DBIOAction。过滤作用。</p>
<p>andThen方法：参数是个DBIOAction，返回值是个DBIOAction。在Action完成后执行另外一个Action。</p>
<h2 id="增删改查操作">增删改查操作</h2><h3 id="查询">查询</h3><p>Slick的查询可以直接通过TableQuery操作，使用TableQuery提供的filter可以实现过滤操作，使用drop和take完成分页操作，使用sortBy完成排序操作。</p>
<pre><code>students.<span class="function"><span class="title">filter</span><span class="params">(_.classroomId === <span class="number">1</span>)</span></span>
students.<span class="function"><span class="title">drop</span><span class="params">(<span class="number">1</span>)</span></span>.<span class="function"><span class="title">take</span><span class="params">(<span class="number">2</span>)</span></span>
students.<span class="function"><span class="title">sortBy</span><span class="params">(_.age.desc)</span></span>
</code></pre><p>可以使用map方法找出需要的列。 </p>
<p>多列：</p>
<pre><code><span class="component">students.map { student =&gt;
  (student<span class="string">.name</span>, student<span class="string">.age)</span>
}</span>
</code></pre><p>一列：</p>
<pre><code>students.<span class="function"><span class="title">map</span><span class="params">(_.name)</span></span>
</code></pre><p>Join方法：</p>
<p>cross join操作：</p>
<pre><code>val crossJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students <span class="built_in">join</span> classrooms
} yield (s.<span class="built_in">name</span>, c.<span class="built_in">name</span>)
</code></pre><p>inner Join操作：</p>
<pre><code>val innerJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students join classrooms <span class="function_start"><span class="keyword">on</span></span> (_.classroomId === _.<span class="property">id</span>)
} yield (s.<span class="property">name</span>, c.<span class="property">name</span>)
</code></pre><p>另外一个inner join：</p>
<pre><code>val innerJoin = <span class="keyword">for</span> {
  s &lt;- students
  c &lt;- classrooms <span class="keyword">if</span> c.<span class="property">id</span> === s.classroomId
} yield (s.<span class="property">name</span>, c.<span class="property">name</span>)
</code></pre><p>left join操作：</p>
<pre><code>val leftJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students joinLeft classrooms <span class="function_start"><span class="keyword">on</span></span> (_.classroomId === _.<span class="property">id</span>)
} yield (s.<span class="property">name</span>, c.map(_.<span class="property">name</span>))
</code></pre><p>right join操作：</p>
<pre><code>val rightJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students joinRight classrooms <span class="function_start"><span class="keyword">on</span></span> (_.classroomId === _.<span class="property">id</span>)
} yield (s.map(_.<span class="property">name</span>), c.<span class="property">name</span>)
</code></pre><h3 id="新增">新增</h3><p>所有列都有值：</p>
<pre><code>val insertAction = DBIO.seq(
  students += (<span class="number">4</span>, <span class="string">"format4"</span>, <span class="number">44</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),
  students += (<span class="number">5</span>, <span class="string">"format5"</span>, <span class="number">55</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),

  students ++= Seq (
    (<span class="number">6</span>, <span class="string">"format6"</span>, <span class="number">66</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),
    (<span class="number">7</span>, <span class="string">"format7"</span>, <span class="number">77</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis()))
  )
)
</code></pre><p>部分列有值：</p>
<pre><code>students.<span class="built_in">map</span>(s =&gt; (s.name, s.age, s.classroomId)) += (<span class="string">"format8"</span>, <span class="number">88</span>, <span class="number">3</span>)
</code></pre><h3 id="删除">删除</h3><p>删除classroomId为3的所有数据：</p>
<pre><code><span class="variable"><span class="keyword">val</span> q</span> = students.filter(_.classroomId === <span class="number">3</span>)
<span class="variable"><span class="keyword">val</span> affectedRowsCountFuture</span> = db.run(q.delete)
affectedRowsCountFuture.map { rows =&gt;
  println(rows)
}
</code></pre><h3 id="修改">修改</h3><p>修改单列：</p>
<pre><code>val <span class="tag">q</span> = students.<span class="function"><span class="title">filter</span><span class="params">(_.id === <span class="number">2</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(_.name)</span></span>
val updateSql = <span class="tag">q</span>.<span class="function"><span class="title">update</span><span class="params">(<span class="string">"format2222"</span>)</span></span>
db.<span class="function"><span class="title">run</span><span class="params">(updateSql)</span></span>
</code></pre><p>修改多列：</p>
<pre><code>val <span class="tag">q</span> = students.<span class="function"><span class="title">filter</span><span class="params">(_.id === <span class="number">2</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(s =&gt; (s.name, s.age)</span></span>)
val updateSql = <span class="tag">q</span>.<span class="function"><span class="title">update</span><span class="params">((<span class="string">"format2222"</span>, <span class="number">222</span>)</span></span>)
db.<span class="function"><span class="title">run</span><span class="params">(updateSql)</span></span>
</code></pre><h2 id="CaseClass的使用">CaseClass的使用</h2><p>之前的例子都是使用Tuple构造domain。 还有一种更方便的方式，那就是使用CaseClass。</p>
<pre><code><span class="keyword">case</span> <span class="keyword">class</span> People(id: Long, <span class="keyword">name</span>: String, age: <span class="built_in">Int</span>)
</code></pre><p>例子：</p>
<pre><code>private class PeopleTable(tag: Tag) extends Table[<span class="link_label">People</span>](<span class="link_url">tag, "people"</span>) {
  val id = column[<span class="link_label">Long</span>](<span class="link_url">"id", O.PrimaryKey, O.AutoInc</span>)
  val name = column[<span class="link_label">String</span>](<span class="link_url">"name"</span>)
  val age = column[<span class="link_label">Int</span>](<span class="link_url">"age"</span>)

  def * = (id, name, age) <span class="xml"><span class="tag">&lt;&gt;</span></span> ((People.apply _).tupled, People.unapply)
}

val db = Database.forConfig("h2mem1")

try {

  val people = TableQuery[PeopleTable]

  val setupAction = DBIO.seq(
<span class="code">      people.schema.create,</span>
<span class="code">      people += People(1, "format1", 11)</span>
  )

  val setupFuture = db.run(setupAction);

  val f = setupFuture.flatMap { _ =&gt;
<span class="code">    db.run(people.result).map { p =&gt;</span>
<span class="code">      p.foreach(println)</span>
<span class="code">    }</span>
  }

  Await.result(f, Duration.Inf)

} finally db.close()
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[近看到了一个FRM的框架Slick。 FRM的意思是Functional Relational Mapping， 一种基于函数式的ORM ...]]>
    
    </summary>
    
      <category term="frm" scheme="http://fangjian0423.github.io/tags/frm/"/>
    
      <category term="orm" scheme="http://fangjian0423.github.io/tags/orm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala并发的Future]]></title>
    <link href="http://fangjian0423.github.io/2015/08/14/scala-future/"/>
    <id>http://fangjian0423.github.io/2015/08/14/scala-future/</id>
    <published>2015-08-14T12:22:33.000Z</published>
    <updated>2015-12-20T16:55:17.000Z</updated>
    <content type="html"><![CDATA[<p>Future这个概念其实java里也已经有了， 表示一个未来的意思。 某线程执行一项操作，这个操作有延迟的话，Future会提供一系列方法来处理这个线程过程，可取消，可操作完成后执行其他操作等等。</p>
<p>使用Future是非阻塞的，在Future中可以使用回调函数可以避免阻塞操作。 Scala在Future中提供了flatMap，foreach，filter等方法。</p>
<h2 id="Future概念">Future概念</h2><p>Future的状态：</p>
<p>1.未完成：线程操作还未结束<br>2.已完成：操作操作完成，并且有返回值或者有异常。 当一个Future完成的时候，它就变成了一个不可变对象，永远不会被重写</p>
<p>构造Future最简单的方法是使用Future这个object提供的apply方法：</p>
<pre><code><span class="tag">Future</span> {
    ...
}
</code></pre><p>来看一个最简单的Future操作， 计算和：</p>
<pre><code>import <span class="keyword">scala</span>.concurrent.ExecutionContext.Implicits.<span class="keyword">global</span>
import <span class="keyword">scala</span>.concurrent.duration.Duration
import <span class="keyword">scala</span>.concurrent.{Await, Future}

val sumFuture = Future[Int] {
  <span class="keyword">var</span> <span class="keyword">sum</span> = 0
  <span class="keyword">for</span>(i &lt;- <span class="keyword">Range</span>(1,100000)) <span class="keyword">sum</span> = <span class="keyword">sum</span> + <span class="literal">i</span>
  <span class="literal">sum</span>
}

sumFuture.onSuccess {
  case <span class="keyword">sum</span> =&gt; println(<span class="keyword">sum</span>)
}

Await.result(sumFuture, Duration.<span class="keyword">Inf</span>)
</code></pre><p>Await的作用是阻断Future等待Future的执行结果。 import scala.concurrent.ExecutionContext.Implicits.global 这个global表示一个ExecutionContext，类似线程池，所有线程的提交都得交给ExecutionContext。如果没有import这个global对象，那么执行的时候会报错。</p>
<p>文本文件中找关键字的例子，读io文件的时候如果文件很大，肯定会阻塞。使用Future完成，可以更有效率，等关键字索引找到的时候再去拿数据。这段时间完成可以去做其他事情：</p>
<pre><code><span class="variable"><span class="keyword">val</span> keywordIndex</span> = Future[<span class="typename">Int</span>] {
  <span class="variable"><span class="keyword">val</span> source</span> = scala.io.Source.fromFile(<span class="string">"intro.txx"</span>)
  source.toSeq.indexOfSlice(<span class="string">"format"</span>)
}
</code></pre><h2 id="回调">回调</h2><p>Future提供了3种Callback，分别是 onComplete，onFailure，onSuccess。</p>
<p>onComplete回调表示Future执行完毕了。需要1个Try[T] =&gt; U类型的参数，如果执行成功且没发生一次，那么匹配Success类型，否则匹配Failure类型。</p>
<p>onComplete例子：</p>
<pre><code>val calFuture = <span class="type">Future</span>[<span class="type">Int</span>] {
  val a = <span class="number">1</span> / <span class="number">1</span>
  a
}

calFuture.onComplete {
  <span class="keyword">case</span> <span class="type">Success</span>(<span class="literal">result</span>) =&gt; println(<span class="literal">result</span>)
  <span class="keyword">case</span> <span class="type">Failure</span>(e) =&gt; println(<span class="string">"error: "</span> + e.getMessage)
}
</code></pre><p>onFailure回调表示Future已经执行完成，但是出错了。</p>
<pre><code>val errorFuture = Future[Unit] {
  <span class="number">1</span> / <span class="number">0</span>
}

errorFuture.onFailure {
  <span class="keyword">case</span> e =&gt; println(e.getMessage)
}
</code></pre><p>onSuccess回调表示Future已经执行完成，而且执行成功了。</p>
<pre><code>val successFuture = Future[Int] {
  <span class="number">1000</span>
}

successFuture.onSuccess {
  <span class="keyword">case</span> <span class="built_in">num</span> =&gt; println(<span class="built_in">num</span>)
}
</code></pre><h2 id="Future的组合">Future的组合</h2><p>使用map方法可以组合Future：</p>
<pre><code>val firstValFuture = <span class="type">Future</span>[<span class="type">Int</span>] {
  <span class="number">1</span>
}

val secondFuture = firstValFuture.map { num =&gt;
  println(<span class="string">"firstFuture: "</span> + num)
  num + <span class="string">"111"</span>
}

secondFuture.onSuccess {
  <span class="keyword">case</span> <span class="literal">result</span> =&gt; println(<span class="literal">result</span>)
}
</code></pre><p>flapMap方法也可以组合Future，map方法和flatMap方法唯一的区别是flatMap内部需要返回Future，而map不是。</p>
<pre><code>val firstValFuture = <span class="type">Future</span>[<span class="type">Int</span>] {
  <span class="number">1</span>
}

val secondFuture = firstValFuture.flatMap { num =&gt;
  println(<span class="string">"firstFuture: "</span> + num)
  <span class="type">Future</span> {
    num + <span class="string">"111"</span>
  }
}

secondFuture.onSuccess {
  <span class="keyword">case</span> <span class="literal">result</span> =&gt; println(<span class="literal">result</span>)
}
</code></pre><p>其他资料参考<a href="http://docs.scala-lang.org/overviews/core/futures.html" target="_blank" rel="external">Scala的官方文档</a>即可。</p>
]]></content>
    <summary type="html">
    <![CDATA[Future这个概念其实java里也已经有了， 表示一个未来的意思。 某线程执行一项操作，这个操作有延迟的话，Future会提供一系列方法来处理这个线程过程，可取消，可操作完成...]]>
    
    </summary>
    
      <category term="concurrent" scheme="http://fangjian0423.github.io/tags/concurrent/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HBase介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/08/07/hbase-intro/"/>
    <id>http://fangjian0423.github.io/2015/08/07/hbase-intro/</id>
    <published>2015-08-06T16:22:33.000Z</published>
    <updated>2015-12-20T17:02:23.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>HBase在公司已经用过一段时间，在Flume中添加一个HBase sink将一些数据存储到HBase里。</p>
<p>当时HBase也没学，看了看几个例子，了解了它是基于列的表设计之后，就马上上手了，而且也把东西做出来了。 现在记录一下HBase的一些学习笔记。</p>
<h2 id="HBase简介">HBase简介</h2><p>HBase是什么？</p>
<p>HBase是运行在hadoop上的数据库，是一个分布式的，扩展性高的，存储大数据的数据库。</p>
<p>HBase也是开源的，非关系型数据库。基于Google的Bigtable设计。</p>
<p>什么时候需要使用HBase？</p>
<p>需要实时地读写大数据。HBase的目的就是管理亿级的数据。</p>
<h2 id="HBase基本概念">HBase基本概念</h2><p>HBase是基于列设计的，那什么是基于列呢？</p>
<p>首先看下关系型数据库的表结构，第一行是table的所有列，第二行开始就是各个列对应的值：</p>
<table>
<thead>
<tr>
<th style="text-align:center">id</th>
<th style="text-align:center">name</th>
<th style="text-align:center">age</th>
<th style="text-align:center">birth_date</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">format1</td>
<td style="text-align:center">11</td>
<td style="text-align:center">1980-01-01</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">format2</td>
<td style="text-align:center">22</td>
<td style="text-align:center">1985-01-01</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">format3</td>
<td style="text-align:center">33</td>
<td style="text-align:center">1990-01-01</td>
</tr>
</tbody>
</table>
<p>HBase的表结构是这样的：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Row Key</th>
<th style="text-align:center">Time Stamp</th>
<th style="text-align:center">ColumnFamily contents</th>
<th style="text-align:center">ColumnFamily names</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">‘me.format.hbase’</td>
<td style="text-align:center">t1</td>
<td style="text-align:center">contents:format = “format1”</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">‘me.format.hbase’</td>
<td style="text-align:center">t2</td>
<td style="text-align:center">contents:title = “title1”</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">‘me.format.hbase’</td>
<td style="text-align:center">t3</td>
<td style="text-align:center"></td>
<td style="text-align:center">names:gogogo = “data1”</td>
</tr>
</tbody>
</table>
<p>从上面这个HBase表的例子来说明HBase的存储结构。</p>
<p>Row Key：行的键值，其实就相当于这一行的标识符。上面的数据其实只有1行，因为他们的标识符是一样的。</p>
<p>TimeStamp：时间戳，创建数据的时间戳，hbase默认会自动生成</p>
<p>ColumnFamily：列的前缀，一列可以存储多条数据，具体存储什么类型的数据还需要另外一个标示符qualify，上面那个例子中，contents和names就是两个Column Family</p>
<p>ColumnFamily qualify：列前缀后的标识符，一个ColumnFamily可以有多个qualify。上面那个例子中format和title就是contents这个ColumnFamily的qualify。gogogo是names这个ColumnFamily的qualify</p>
<h2 id="HBase的启动">HBase的启动</h2><p>HBase下载完之后解压，解压后使用以下命令启动hbase：</p>
<pre><code>$ ./bin/<span class="literal">start</span>-hbase.sh
</code></pre><p>启动之前注意，机器要装好jdk，并且启动hadoop。因为hbase底层数据是存储在hdfs上的。</p>
<h2 id="HBase的基本操作">HBase的基本操作</h2><h3 id="表的创建">表的创建</h3><p>创建一个表名位tableName，ColumnFamily有contents和names的表，qualify不需要声明，每次添加数据随意指定qualify即可：</p>
<pre><code><span class="built_in">create</span> <span class="string">'tableName'</span>,[<span class="string">'contents'</span>, <span class="string">'names'</span>]
</code></pre><h3 id="表的删除">表的删除</h3><p>删除表的所有数据：</p>
<pre><code>truncate <span class="built_in">table</span> <span class="built_in">table</span>Name
</code></pre><p>删除表，删除之前需要先disable表，然后才可删除：</p>
<pre><code><span class="built_in">disable</span> <span class="string">'tableName'</span>
drop <span class="string">'tableName'</span>
</code></pre><h3 id="数据查询">数据查询</h3><p>查询tableName表数据：</p>
<pre><code><span class="built_in">scan</span> tableName
</code></pre><p>返回：</p>
<pre><code>ROW                                         COLUMN+CELL
 me.<span class="keyword">format</span>.hbase                            column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438875060466</span>, <span class="keyword">value</span>=format1
</code></pre><h3 id="数据删除">数据删除</h3><p>比如，表tableName里有如下数据：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875566613</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=names:gogogo, timestamp=<span class="number">1438875597592</span>, value=data1
</code></pre><p>进行删除操作，删除ColumnFamily qulify为contents:format的数据：</p>
<pre><code>delete <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:format'
</code></pre><h3 id="数据修改">数据修改</h3><p>HBase没有直接的update操作，只有put操作，put操作如果对应的地方有值，会覆盖：</p>
<pre><code>put <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:format', <span class="symbol">'format111'</span>
</code></pre><h3 id="添加数据">添加数据</h3><p>在tableName表里插入一个Row Key为me.format.hbase, ColumnFamily为contents，qualify为format，值的format1的数据：</p>
<pre><code>put <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:format', <span class="symbol">'format1'</span>
</code></pre><h3 id="计数器">计数器</h3><p>HBase提供了一种计数器的概念，每次可以对某个值进行incr操作：</p>
<pre><code>incr <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">1</span>
</code></pre><p>查询数据：</p>
<pre><code>me.format.hbase                            column=contents:num, timestamp=1438875837362, value=<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>01
</code></pre><p>可以使用get_counter命令获得计数器的值：</p>
<pre><code>get_counter <span class="symbol">'tableName'</span>,<span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">0</span>
</code></pre><p>返回：</p>
<pre><code>COUNTER VALUE = <span class="number">1</span>
</code></pre><p>再次修改：</p>
<pre><code>incr <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">100</span>

get_counter <span class="symbol">'tableName'</span>,<span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">0</span>

<span class="type">COUNTER</span> <span class="type">VALUE</span> = <span class="number">101</span>

incr <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', -<span class="number">102</span>

get_counter <span class="symbol">'tableName'</span>,<span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">0</span>

<span class="type">COUNTER</span> <span class="type">VALUE</span> = -<span class="number">1</span>
</code></pre><h3 id="带条件的数据查询">带条件的数据查询</h3><p>scan查询可以带几个参数。</p>
<p>COLUMNS： ColumnFamily和qualify的值<br>LIMIT：展示的个数<br>FILTER：过滤条件</p>
<p>比如有以下数据：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875707700</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:num, timestamp=<span class="number">1438876106259</span>, value=\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=names:gogogo, timestamp=<span class="number">1438875597592</span>, value=data1
 me<span class="class">.format</span><span class="class">.hbase1</span>                           column=contents:format, timestamp=<span class="number">1438877417358</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase2</span>                           column=contents:format, timestamp=<span class="number">1438877422756</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase3</span>                           column=contents:format, timestamp=<span class="number">1438877427312</span>, value=format1
</code></pre><p>查询ColumnFamily，qualify为contents:format的数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents:format"</span>, LIMIT =&gt; <span class="number">10</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me.<span class="keyword">format</span>.hbase                            column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438875707700</span>, <span class="keyword">value</span>=format1
 me.<span class="keyword">format</span>.hbase1                           column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438877417358</span>, <span class="keyword">value</span>=format1
 me.<span class="keyword">format</span>.hbase2                           column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438877422756</span>, <span class="keyword">value</span>=format1
 me.<span class="keyword">format</span>.hbase3                           column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438877427312</span>, <span class="keyword">value</span>=format1
</code></pre><p>查询ColumnFamily未contents的数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents"</span>, LIMIT =&gt; <span class="number">10</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875707700</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:num, timestamp=<span class="number">1438876106259</span>, value=\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase1</span>                           column=contents:format, timestamp=<span class="number">1438877417358</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase2</span>                           column=contents:format, timestamp=<span class="number">1438877422756</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase3</span>                           column=contents:format, timestamp=<span class="number">1438877427312</span>, value=format1
</code></pre><p>查询ColumnFamily未contents的数据，并只展示2行数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents"</span>, LIMIT =&gt; <span class="number">2</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875707700</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:num, timestamp=<span class="number">1438876106259</span>, value=\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase1</span>                           column=contents:format, timestamp=<span class="number">1438877417358</span>, value=format1
</code></pre><p>查询ColumnFamily未contents的数据，并只展示2行数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents"</span>, FILTER =&gt; <span class="string">"ValueFilter( =, 'binaryprefix:title' )"</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
</code></pre><p>scan命令具体其他的参数就不一一列举了，可查询文档解决。</p>
]]></content>
    <summary type="html">
    <![CDATA[HBase在公司已经用过一段时间，在Flume中添加一个HBase sink将一些数据存储到HBase里 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="hbase" scheme="http://fangjian0423.github.io/tags/hbase/"/>
    
      <category term="hbase" scheme="http://fangjian0423.github.io/categories/hbase/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hive介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/07/31/hive-intro/"/>
    <id>http://fangjian0423.github.io/2015/07/31/hive-intro/</id>
    <published>2015-07-31T05:32:33.000Z</published>
    <updated>2015-12-20T17:04:55.000Z</updated>
    <content type="html"><![CDATA[<p>Hive是基于Hadoop的一个数据仓库工具，使用它可以查询和管理分布式存储系统上的大数据集。</p>
<p>Hive提供了一种叫做HiveQL的类似SQL查询语言用来查询数据，HiveQL也允许熟悉MapReduce开发者开发自定义的mapper和reducer来处理内建的mapper和reducer无法完成的复杂的分析工作。</p>
<p>Hive的工作模式是提交一个任务，等到任务结束时被通知，而不是实时查询。</p>
<h2 id="Hive安装">Hive安装</h2><p>直接去<a href="https://hive.apache.org/" target="_blank" rel="external">Hive官网</a>下载最新的文件，解压。</p>
<p>运行bin目录里的hive文件，运行之前先启动hadoop，运行的时候可能会出现：</p>
<pre><code>Missing Hive <span class="keyword">CLI</span> Jar ....
</code></pre><p>将hive解压出来的lib目录里的jline<em>.jar拷贝到$HADOOP/share/hadoop/yarn/lib里，同时将$HADOOP/share/hadoop/yarn/lib里的jline</em>.jar删除，重启hadoop。</p>
<p>再次运行，可能还会出现</p>
<pre><code>The reported blocks <span class="number">2662</span> has reached <span class="operator">the</span> threshold <span class="number">0.9990</span> <span class="operator">of</span> total blocks <span class="number">2662.</span> The <span class="built_in">number</span> <span class="operator">of</span> live datanodes <span class="number">1</span> has reached <span class="operator">the</span> minimum <span class="built_in">number</span> <span class="number">0.</span> In safe mode extension. Safe mode will be turned off automatically <span class="operator">in</span> <span class="number">0</span> <span class="built_in">seconds</span>. ...
</code></pre><p>类似的问题，关闭hdfs的安全模式即可：</p>
<pre><code><span class="title">hadoop</span> dfsadmin -safemode leave
</code></pre><h2 id="基本命令">基本命令</h2><p>HiveQL就是模仿sql而创建的，以SQL的角度来介绍HiveQL。</p>
<h3 id="DDL操作">DDL操作</h3><p>创建表：</p>
<pre><code>hive&gt; create table users(age <span class="built_in">INT</span>, name <span class="built_in">STRING</span>)<span class="comment">;</span>
</code></pre><p>查看所有的表：</p>
<pre><code>hive&gt; <span class="built_in">show</span> <span class="built_in">tables</span>;
</code></pre><p>查看以CLIENT开头的表：</p>
<pre><code>hive&gt; <span class="built_in">show</span> <span class="built_in">tables</span> 'CLIENT.*';
</code></pre><p>表加列：</p>
<pre><code>hive&gt; alter table users add columns<span class="list">(<span class="keyword">gender</span> BOOLEAN)</span><span class="comment">;</span>
</code></pre><p>改表名字：</p>
<pre><code>hive&gt; alter <span class="built_in">table</span> users rename <span class="keyword">to</span> <span class="keyword">user</span>;
</code></pre><p>删除表：</p>
<pre><code>hive&gt; drop <span class="built_in">table</span> <span class="keyword">user</span>;
</code></pre><p>查看表的具体信息：</p>
<pre><code>hive&gt; descibe user<span class="comment">;</span>
hive&gt; desc user<span class="comment">;</span>
</code></pre><h3 id="DML操作">DML操作</h3><p>hive创建表的时候可以指定分隔符，由于hive操作的是hdfs，数据最终会存储在hdfs上，所以hdfs上的内容肯定是以某种分隔符分开各个列的。 hive默认的列分隔符是 <strong>^A</strong> 。 我们可以自定义自己的分隔符，在创建表的时候指定分隔符即可。</p>
<p>本地导入数据到hive：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table users;
</code></pre><p>users表的结构只有2列，name和age，而且使用默认的分隔符。</p>
<p>比如本地文件的内容是这样的：</p>
<pre><code><span class="xml">format1</span><span class="keyword">^A11</span><span class="xml">
format2</span><span class="keyword">^A22</span><span class="xml"></span>
</code></pre><p>导入之后进行查询：</p>
<pre><code>hive&gt; <span class="keyword">select</span> * <span class="keyword">from</span> users;
</code></pre><p>显示结果：</p>
<pre><code>OK
format1    <span class="number">11</span>
format2    <span class="number">22</span>
Time taken: <span class="number">0.362</span> seconds, Fetched: <span class="number">2</span> row(s)
</code></pre><p>在创建表的时候可以指定列分隔符和数组分隔符：</p>
<pre><code>hive&gt; create table users(name <span class="built_in">string</span>, age int)
 &gt; ROW FORMAT DELIMITED
 &gt; FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">'\t'</span>
 &gt; COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">','</span>;
</code></pre><p>导入数据还有几个参数：</p>
<p>local参数意味着从本地加载文件，如果没有local参数，那表示从hdfs加载文件。</p>
<p>关键字overwrite意味着当前表中已经存在的数据将会被删除掉，没有overwrite关键字，表示数据是追加，追加到原先数据集里面。</p>
<p>插入数据，插入数据后会起一个map reduce job去跑插入的数据：</p>
<pre><code>hive&gt; <span class="function">insert <span class="keyword">into</span> table users <span class="title">values</span>(<span class="params"><span class="string">'formatgogo'</span>, <span class="number">222</span></span>)</span>;
</code></pre><p>带条件的查询数据：</p>
<pre><code>hive&gt; <span class="keyword">select</span> * <span class="keyword">from</span> users <span class="keyword">where</span> age = <span class="number">11</span>;
</code></pre><p>group by查询：</p>
<pre><code>hive&gt; <span class="keyword">select</span> age, <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> users <span class="built_in">group</span> <span class="keyword">by</span> age;
</code></pre><p>partition的使用，以部门表为例，用type进行partition：</p>
<pre><code>hive&gt; create table dept<span class="list">(<span class="keyword">name</span> STRING)</span> partitioned by <span class="list">(<span class="keyword">type</span> INT)</span><span class="comment">;</span>
</code></pre><p>以2个文件为例，dept1.txt：</p>
<pre><code><span class="label">dept1</span>^<span class="literal">A1</span>
<span class="label">dept11</span>^<span class="literal">A1</span>
<span class="label">dept111</span>^<span class="literal">A1</span>
<span class="label">dept1111</span>^<span class="literal">A1</span>
<span class="label">dept11111</span>^<span class="literal">A1</span>
<span class="label">dept111111</span>^<span class="literal">A1</span>
</code></pre><p>dept2.txt</p>
<pre><code><span class="label">dept2</span>^<span class="literal">A2</span>
<span class="label">dept22</span>^<span class="literal">A2</span>
<span class="label">dept222</span>^<span class="literal">A2</span>
<span class="label">dept2222</span>^<span class="literal">A2</span>
<span class="label">dept22222</span>^<span class="literal">A2</span>
<span class="label">dept222222</span>^<span class="literal">A2</span>
</code></pre><p>使用partition之后，导入数据的时候需要指定对应的partition：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'$PATH/dept1.txt'</span> overwrite <span class="keyword">into</span> table dept partition(<span class="keyword">type</span>=<span class="number">1</span>);
hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'$PATH/dept2.txt'</span> overwrite <span class="keyword">into</span> table dept partition(<span class="keyword">type</span>=<span class="number">2</span>);

hive&gt; <span class="keyword">select</span> * from dept;
</code></pre><p>结果：</p>
<pre><code>OK
dept1    <span class="number">1</span>
dept11    <span class="number">1</span>
dept111    <span class="number">1</span>
dept1111    <span class="number">1</span>
dept11111    <span class="number">1</span>
dept111111    <span class="number">1</span>
dept2    <span class="number">2</span>
dept22    <span class="number">2</span>
dept222    <span class="number">2</span>
dept2222    <span class="number">2</span>
dept22222    <span class="number">2</span>
dept222222    <span class="number">2</span>
Time taken: <span class="number">0.114</span> seconds, Fetched: <span class="number">12</span> row(s)
</code></pre><p>insert可以将数据导出到指定目录，将users表导入到本地文件。 去掉local关键字表示导出到hdfs目录：</p>
<pre><code>hive&gt; insert overwrite <span class="keyword">local</span> directory <span class="string">'localFileName'</span> <span class="keyword">select</span> * from users<span class="comment">;</span>
</code></pre><h3 id="hive存储在hdfs的位置">hive存储在hdfs的位置</h3><p>进入hive控制台之后，可以使用：</p>
<pre><code><span class="tag">hive</span>&gt; <span class="tag">set</span> <span class="tag">hive</span><span class="class">.metastore</span><span class="class">.warehouse</span><span class="class">.dir</span>;
</code></pre><p>查看hive存储在hdfs的位置，默认是存在 /user/hive/warehouse 目录。</p>
<p>之前的users表，会存储在/user/hive/warehouse/user目录里。</p>
<p>有partition的表会存在的不同位置，比如之前的dept表的type为1和2的分别存储在 /user/hive/warehouse/dept/type=1 和 /user/hive/warehouse/dept/type=2。</p>
<h2 id="数据类型">数据类型</h2><p>hive中的数据类型分2种，简单类型和复杂类型。</p>
<p>简单类型有以下几种：TINYINT, SMALLINT, INT, BIGINT, BOOLEAN, FLOAT, DOUBLE, STRING。</p>
<p>复杂类型有以下几种：Structs(结构体，学过C都知道)，MAPS(key-value键值对)，Arrays(数组类型，数组内的元素类型都必须一致)</p>
<p>简单类型就不分析了，来看一下复杂类型的使用：</p>
<h3 id="Structs">Structs</h3><pre><code><span class="label">hive</span>&gt; create table employee(id INT, <span class="preprocessor">info</span> <span class="keyword">struct&lt;name:STRING, </span>age:INT&gt;)
      &gt; ROW FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY </span><span class="string">','</span> 
      &gt; COLLECTION <span class="keyword">ITEMS </span>TERMINATED <span class="keyword">BY </span><span class="string">':'</span><span class="comment">; </span>
</code></pre><p>要导入的数据：</p>
<pre><code><span class="number">1</span>,format:<span class="number">11</span>
<span class="number">2</span>,fj:<span class="number">22</span>
<span class="number">3</span>,formatfj:<span class="number">33</span>
</code></pre><p>导入数据：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table employee;
</code></pre><p>查询：</p>
<pre><code>select * from employee;

OK
<span class="number">1</span>    {<span class="string">"name"</span>:<span class="string">"format"</span>,<span class="string">"age"</span>:<span class="number">11</span>}
<span class="number">2</span>    {<span class="string">"name"</span>:<span class="string">"fj"</span>,<span class="string">"age"</span>:<span class="number">22</span>}
<span class="number">3</span>    {<span class="string">"name"</span>:<span class="string">"formatfj"</span>,<span class="string">"age"</span>:<span class="number">33</span>}
Time taken: <span class="number">0.042</span> seconds, Fetched: <span class="number">3</span> row(s)

hive&gt; select info.name from employee;

OK
format
fj
formatfj
Time taken: <span class="number">0.061</span> seconds, Fetched: <span class="number">3</span> row(s)
</code></pre><h3 id="Maps">Maps</h3><pre><code>hive&gt; create table lessons(id <span class="built_in">string</span>, score <span class="built_in">map</span>&lt;<span class="built_in">string</span>, int&gt;)
    &gt; ROW FORMAT DELIMITED
    &gt; FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">'\t'</span>
    &gt; COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">','</span>
    &gt; <span class="built_in">MAP</span> KEYS TERMINATED <span class="keyword">BY</span> <span class="string">':'</span>;
</code></pre><p>要导入的数据：</p>
<pre><code><span class="number">1</span>       chinese:<span class="number">80</span>,english:<span class="number">60</span>,math:<span class="number">70</span>  
<span class="number">2</span>       computer:<span class="number">60</span>,chemistry:<span class="number">80</span>   
</code></pre><p>导入数据：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table lessons;
</code></pre><p>查询：</p>
<pre><code>hive&gt; <span class="keyword">select</span> score[<span class="string">'chinese'</span>] <span class="keyword">from</span> lessions;

OK
<span class="number">80</span>
NULL
Time taken: <span class="number">0.05</span> seconds, Fetched: <span class="number">2</span> row(s)

hive&gt; <span class="keyword">select</span> * <span class="keyword">from</span> lessons;
OK
<span class="number">1</span>    {<span class="string">"chinese"</span>:<span class="number">80</span>,<span class="string">"english"</span>:<span class="number">60</span>,<span class="string">"math"</span>:<span class="number">70</span>}
<span class="number">2</span>    {<span class="string">"computer"</span>:<span class="number">60</span>,<span class="string">"chemistry"</span>:<span class="number">80</span>}
Time taken: <span class="number">0.032</span> seconds, Fetched: <span class="number">2</span> row(s)
</code></pre><h3 id="Arrays">Arrays</h3><pre><code>hive&gt; create table student(name <span class="built_in">string</span>, hobby_list <span class="built_in">array</span>&lt;<span class="built_in">STRING</span>&gt;)
    &gt; ROW FORMAT DELIMITED
    &gt; FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">','</span>
    &gt; COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">':'</span>;
</code></pre><p>要导入的数据：</p>
<pre><code><span class="tag">format</span>,<span class="tag">basketball</span><span class="pseudo">:football</span><span class="pseudo">:swimming</span>
<span class="tag">fj</span>,<span class="tag">coding</span><span class="pseudo">:running</span>
</code></pre><p>导入数据：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table student;
</code></pre><p>查询(数组下标没对应的值的话返回NULL)：</p>
<pre><code>hive&gt; select * from student;

OK
format    [<span class="string">"basketball"</span>,<span class="string">"football"</span>,<span class="string">"swimming"</span>]
fj    [<span class="string">"coding"</span>,<span class="string">"running"</span>]
Time taken: <span class="number">0.041</span> seconds, Fetched: <span class="number">2</span> row(s)

hive&gt; select hobby_list[<span class="number">2</span>] from student;

OK
swimming
<span class="literal">NULL</span>
Time taken: <span class="number">0.07</span> seconds, Fetched: <span class="number">2</span> row(s)
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Hive是基于Hadoop的一个数据仓库工具，使用它可以查询和管理分布式存储系统上的大数据集 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="hive" scheme="http://fangjian0423.github.io/tags/hive/"/>
    
      <category term="hive" scheme="http://fangjian0423.github.io/categories/hive/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[google guava类库介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/07/26/google-guava-intro/"/>
    <id>http://fangjian0423.github.io/2015/07/26/google-guava-intro/</id>
    <published>2015-07-26T08:32:33.000Z</published>
    <updated>2015-12-20T16:57:40.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Guava简介">Guava简介</h2><p><a href="https://mail.google.com/mail/u/0/" target="_blank" rel="external">Guava</a>是一个Google开发的基于java的扩展项目，提供了很多有用的工具类，可以让java代码更加优雅，更加简洁。</p>
<p>Guava包括诸多工具类，比如Collections，cache，concurrent，hash，reflect，annotations，eventbus等。</p>
<p>刚好在看flume源码的时候看到源码里面使用了很多guava提供的代码，于是记录学习一下这个类库。</p>
<h2 id="各个模块介绍">各个模块介绍</h2><p><a href="https://code.google.com/p/guava-libraries/wiki/GuavaExplained" target="_blank" rel="external">Guava的wiki</a>已经很明细地介绍了各个工具类的作用和说明。</p>
<p>简单翻译一下各个工具类的说明，有用到的需要了解详情的直接去官网看就可以了。</p>
<h3 id="Basic_utilties_基础工具类">Basic utilties 基础工具类</h3><p>基础工具类的作用是写java语言写的更轻松。它包括了5个子模块：</p>
<p>1.<a href="https://code.google.com/p/guava-libraries/wiki/UsingAndAvoidingNullExplained" target="_blank" rel="external">使用和避免null</a>，null值是有歧义的，也会引起错误。有时候它会让人很不舒服，<br>2.<a href="https://code.google.com/p/guava-libraries/wiki/PreconditionsExplained" target="_blank" rel="external">前置条件</a>,让方法中的条件检查更简单<br>3.<a href="https://code.google.com/p/guava-libraries/wiki/CommonObjectUtilitiesExplained" target="_blank" rel="external">公用的object方法</a>，简化object对象的hashCode和toString<br>4.<a href="https://code.google.com/p/guava-libraries/wiki/OrderingExplained" target="_blank" rel="external">排序</a>，Guava提供了强大的fluent Comparator<br>5.<a href="https://code.google.com/p/guava-libraries/wiki/ThrowablesExplained" target="_blank" rel="external">Throwables</a>，简化了异常和错误的传播与检查</p>
<h3 id="Collections_集合">Collections 集合</h3><p>Guava扩展了jdk提供的集合机制</p>
<p>1.<a href="https://code.google.com/p/guava-libraries/wiki/ImmutableCollectionsExplained" target="_blank" rel="external">不可变集合</a>用不变的集合进行防御性编程和性能提升<br>2.<a href="https://code.google.com/p/guava-libraries/wiki/NewCollectionTypesExplained" target="_blank" rel="external">新集合类型</a>multisets，multimaps，tables，bidirectional map等<br>3.<a href="https://code.google.com/p/guava-libraries/wiki/CollectionUtilitiesExplained" target="_blank" rel="external">强大的集合工具类</a>提供了jdk中没有的集合工具类<br>4.<a href="https://code.google.com/p/guava-libraries/wiki/CollectionHelpersExplained" target="_blank" rel="external">扩展工具类</a>让实现和扩展集合类变得更容易，比如创建Collection的装饰器，或实现迭代器</p>
<h3 id="Caches_缓存">Caches 缓存</h3><p>本地缓存实现，支持多种缓存过期策略</p>
<h3 id="函数式风格">函数式风格</h3><p>Guava的函数式支持可以显著简化代码，但请谨慎使用它</p>
<h3 id="并发">并发</h3><p>1.<a href="https://code.google.com/p/guava-libraries/wiki/ListenableFutureExplained" target="_blank" rel="external">ListenableFuture</a>：完成后触发回调的Future<br>2.<a href="https://code.google.com/p/guava-libraries/wiki/ServiceExplained" target="_blank" rel="external">Service框架</a>：抽象可开启和关闭的服务，帮助你维护服务的状态逻辑</p>
<h3 id="字符串处理">字符串处理</h3><p>非常有用的字符串工具，包括分割、连接、填充等操作</p>
<h3 id="原生类型">原生类型</h3><p>扩展 JDK 未提供的原生类型（如int、char）操作， 包括某些类型的无符号形式</p>
<h3 id="区间">区间</h3><p>可比较类型的区间API，包括连续和离散类型</p>
<h3 id="IO">IO</h3><p>简化I/O尤其是I/O流和文件的操作，针对Java5和6版本</p>
<h3 id="散列">散列</h3><p>提供比Object.hashCode()更复杂的散列实现，并提供布鲁姆过滤器的实现</p>
<h3 id="事件总线">事件总线</h3><p>发布-订阅模式的组件通信，但组件不需要显式地注册到其他组件中</p>
<h3 id="数学运算">数学运算</h3><p>优化的、充分测试的数学工具类</p>
<h3 id="反射">反射</h3><p>Guava的Java反射机制工具类</p>
]]></content>
    <summary type="html">
    <![CDATA[Guava是一个Google开发的基于java的扩展项目，提供了很多有用的工具类，可以让java代码更加优雅，更加简洁 ...]]>
    
    </summary>
    
      <category term="guava" scheme="http://fangjian0423.github.io/tags/guava/"/>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[java内置的线程池笔记]]></title>
    <link href="http://fangjian0423.github.io/2015/07/24/java-poolthread/"/>
    <id>http://fangjian0423.github.io/2015/07/24/java-poolthread/</id>
    <published>2015-07-24T12:32:33.000Z</published>
    <updated>2015-12-20T17:10:06.000Z</updated>
    <content type="html"><![CDATA[<p>目前的工作是接触大数据相关的内容，自己也缺少高并发的知识，刚好前几天看了flume的源码，里面也用到了各种线程池内容，刚好学习一下，做个笔记。</p>
<p>写这篇博客的时候又刚好想起了当时自己实习的时候遇到的一个问题。1000个爬虫任务使用了多线程的处理方式，比如开5个线程处理这1000个任务，每个线程分200个任务，然后各个线程处理那200个爬虫任务→_→，太笨了。其实更合理的方法是使用阻塞队列+线程池的方法。</p>
<h2 id="ExecutorService">ExecutorService</h2><p>ExecutorService就是线程池的概念，ExecutorService的初始化可以使用Executors类的静态方法。</p>
<p>Executors提供了很多方法用来初始化ExecutorService，可以初始化指定数目个线程的线城市或者单个线程的线程池。</p>
<p>比如构造一个10个线程的线程池，使用了guava的ThreadFactoryBuilder，guava的ThreadFactoryBuilder可以传入一个namFormat参数用户来表示线程的name，它内部会使用数字增量表示%d，比如一下的nameFormat，10个线程，名字分别是thread-call-runner-1，thread-call-runner-2 … thread-call-runner-10:</p>
<pre><code>Executors.newFixedThreadPool(<span class="number">10</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
</code></pre><p>ExecutorService线程池使用线程执行任务例子：</p>
<p>1.阻塞队列里有10个元素，初始化带有2个线程的线程池，跑2个线程分别去阻塞队列里取数据执行。</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test01</span><span class="params">()</span> throws Exception </span>{
    ExecutorService es = Executors.newFixedThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
    final LinkedBlockingDeque&lt;String&gt; <span class="built_in">deque</span> = <span class="keyword">new</span> LinkedBlockingDeque&lt;String&gt;();
    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10</span>; i ++) {
        <span class="built_in">deque</span>.add(i + <span class="string">""</span>);
    }
    es.submit(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            <span class="keyword">while</span>(!<span class="built_in">deque</span>.isEmpty()) {
                System.out.println(<span class="built_in">deque</span>.poll() + <span class="string">"-"</span> + Thread.currentThread().getName());
            }
        }
    });
    es.submit(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            <span class="keyword">while</span>(!<span class="built_in">deque</span>.isEmpty()) {
                System.out.println(<span class="built_in">deque</span>.poll() + <span class="string">"-"</span> + Thread.currentThread().getName());
            }
        }
    });
    Thread.sleep(<span class="number">10000l</span>);
}
</code></pre><p>打印，2个线程都会执行：</p>
<pre><code><span class="number">1</span>-thread-call-runner-<span class="number">0</span>
<span class="number">2</span>-thread-call-runner-<span class="number">0</span>
<span class="number">3</span>-thread-call-runner-<span class="number">1</span>
<span class="number">4</span>-thread-call-runner-<span class="number">0</span>
<span class="number">5</span>-thread-call-runner-<span class="number">0</span>
<span class="number">6</span>-thread-call-runner-<span class="number">1</span>
<span class="number">7</span>-thread-call-runner-<span class="number">0</span>
<span class="number">8</span>-thread-call-runner-<span class="number">1</span>
<span class="number">9</span>-thread-call-runner-<span class="number">0</span>
<span class="number">10</span>-thread-call-runner-<span class="number">0</span>
</code></pre><p>2.执行Callable线程，Callable线程和Runnable线程的区别就是Callable的线程会有返回值，这个返回值是Future，未来的意思，而且这Future是个接口，提供了几个实用的方法，比如cancel, idDone, isCancelled, get等方法。</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test02() throws <span class="type">Exception</span> {
    <span class="type">ExecutorService</span> es = <span class="type">Executors</span>.newFixedThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
    final <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt; deque = new <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt;();
    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">500</span>; i ++) {
        deque.add(i + <span class="string">""</span>);
    }
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = es.submit(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="keyword">while</span> (!deque.isEmpty()) {
                <span class="type">System</span>.<span class="keyword">out</span>.println(deque.poll() + <span class="string">"-"</span> + <span class="type">Thread</span>.currentThread().getName());
            }
            <span class="keyword">return</span> <span class="string">"done"</span>;
        }
    });
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.isDone());
    // get方法会阻塞
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.get());
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec next"</span>);
}
</code></pre><p>打印：</p>
<pre><code>先打印出几百个 数字-thread-call-runner-<span class="number">0</span>
然后打印出 isDone的结果， 是<span class="literal">false</span>
<span class="type">Future</span>的get是得到<span class="type">Callable</span>线程执行完毕后的结果，该方法会阻塞，直到该<span class="type">Future</span>对应的线程全部执行完才会继续执行下去。这个例子<span class="type">Callable</span>线程执行完返回done，所以get方法也是返回done

get方法还有个重载的方法，带有<span class="number">2</span>个参数，第一个参数是一个long类型的数字，第二个参数是时间单位。<span class="literal">result</span>.get(<span class="number">1</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>) 就表示<span class="number">1</span>毫秒，等待这个<span class="type">Future</span>的时间为<span class="number">1</span>毫秒，如果时间<span class="number">1</span>毫秒，那么这个get方法的调用会抛出java.util.concurrent.<span class="type">TimeoutException</span>异常，并且线程内部的执行也会停止。注意，但是如果我们catch这个<span class="type">TimeoutException</span>的话，那么线程里的代码还是会被执行完毕。

<span class="keyword">try</span> {
    // catch <span class="type">TimeoutException</span>的话线程里的代码还是会执行下去
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.get(<span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>));
} catch (<span class="type">TimeoutException</span> e) {
    e.printStackTrace();
}
</code></pre><p>3.Future的cancel方法的使用</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test03() throws <span class="type">Exception</span> {
    <span class="type">ExecutorService</span> es = <span class="type">Executors</span>.newFixedThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
    final <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt; deque = new <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt;();
    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">5000</span>; i ++) {
        deque.add(i + <span class="string">""</span>);
    }
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = es.submit(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="keyword">while</span> (!deque.isEmpty() &amp;&amp; !<span class="type">Thread</span>.currentThread().isInterrupted()) {
                <span class="type">System</span>.<span class="keyword">out</span>.println(deque.poll() + <span class="string">"-"</span> + <span class="type">Thread</span>.currentThread().getName());
            }
            <span class="keyword">return</span> <span class="string">"done"</span>;
        }
    });

    <span class="keyword">try</span> {
        <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.get(<span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>));
    } catch (<span class="type">TimeoutException</span> e) {
        <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"cancel result: "</span> + <span class="literal">result</span>.cancel(<span class="literal">true</span>));
        <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"is cancelled: "</span> + <span class="literal">result</span>.isCancelled());
    }
    <span class="type">Thread</span>.sleep(<span class="number">2000</span>l);
}
</code></pre><p>打印：</p>
<pre><code>先打印出几百个 数字-thread-call-runner-<span class="number">0</span>
然后打印出
cancel <span class="literal">result</span>: <span class="literal">true</span>
<span class="keyword">is</span> cancelled: <span class="literal">true</span>

cancel方法用来取消线程的继续执行，它有个boolean类型的返回值，表示是否cancel成功。这里我们使用了get方法，<span class="number">10</span>毫秒处理<span class="number">5000</span>条数据，报了<span class="type">TimeoutException</span>异常，catch之后对<span class="type">Future</span>进行了cancel调用。注意，我们在<span class="type">Callable</span>里执行的代码里加上了

!<span class="type">Thread</span>.currentThread().isInterrupted()

如果去掉了这个条件，那么还是会打印出<span class="number">5000</span>条处理数据。cancel方法底层会去interrupted对应的线程，所以才需要加上这个条件的判断。
</code></pre><h2 id="ScheduledExecutorService">ScheduledExecutorService</h2><p>ScheduledExecutorService接口是ExecutorService接口的子类。</p>
<p>看名字也知道，ScheduledExecutorService是基于调度的线程池。</p>
<p>1.ScheduledExecutorService的schedule方法例子：</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test04() throws <span class="type">Exception</span> {
    <span class="type">ScheduledExecutorService</span> ses = <span class="type">Executors</span>.newScheduledThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = ses.schedule(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec task"</span>);
            <span class="keyword">return</span> <span class="string">"ok"</span>;
        }
    }, <span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>);
    <span class="type">Thread</span>.sleep(<span class="number">15000</span>);
}
</code></pre><p>打印：</p>
<pre><code>执行<span class="number">10</span>秒后打印出  <span class="built_in">exec</span> task
</code></pre><p>2.cancel在schedule中的使用：</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test05() throws <span class="type">Exception</span> {
    <span class="type">ScheduledExecutorService</span> ses = <span class="type">Executors</span>.newScheduledThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = ses.schedule(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec task"</span>);
            <span class="keyword">try</span> {
                <span class="type">Thread</span>.sleep(<span class="number">5000</span>l);
            } catch (<span class="type">InterruptedException</span> e) {
                e.printStackTrace();
            }
            <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec done, take 5 seconds"</span>);
            <span class="keyword">return</span> <span class="string">"ok"</span>;
        }
    }, <span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>);
    <span class="type">Thread</span>.sleep(<span class="number">11000</span>);
    <span class="literal">result</span>.cancel(<span class="literal">true</span>);
    <span class="type">Thread</span>.sleep(<span class="number">10000</span>);
}
</code></pre><p>打印：</p>
<pre><code>先打印出exec task，然后抛出InterruptedException异常，异常被<span class="keyword">catch</span>。接着打印出exec done, take <span class="number">5</span> seconds
因为Callable线程是<span class="number">10</span>秒后执行的，线程会执行<span class="number">5</span>秒，在<span class="number">11</span>秒的时候会调用Future的cancel方法，会取消线程的时候，由于我们<span class="keyword">catch</span>了异常，所以线程会执行完毕。

注意一下，cancel方法有个boolean类型的参数mayInterruptIfRunning。上个例子中我们传入了<span class="literal">true</span>，所以会打断正在执行的线程，因此抛出了异常。如果我们传入<span class="literal">false</span>，线程正在执行，所以不会去打断它，因此会打印出exec task，然后再打印出exec done, take <span class="number">5</span> seconds，并且没有异常抛出。
</code></pre><p>3.scheduleWithFixedDelay方法，定时器，每隔多少时间执行</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test06</span><span class="params">()</span> throws Exception </span>{
    ScheduledExecutorService sec = Executors.newScheduledThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    sec.scheduleWithFixedDelay(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            System.out.println(<span class="string">"exec"</span>);
        }
    }, <span class="number">0</span>, <span class="number">5</span>, TimeUnit.SECONDS);
    Thread.sleep(<span class="number">16000l</span>);
}
</code></pre><p>打印：</p>
<pre><code>打印出<span class="number">4</span>次exec。
scheduleWithFixedDelay有<span class="number">4</span>个参数，第一个参数是个Runnable接口的实现类，第二个参数是首次执行线程的延迟时间，第三个参数是每隔多少时间再次执行线程时间，第四个参数是时间的单位。
如果Runnable中执行的代码发生了异常并且没有被<span class="keyword">catch</span>的话，那么发生异常之后，Runnable里的代码就不会再次执行。
</code></pre><p>4.scheduleAtFixedRate方法，scheduleAtFixedRate方法跟scheduleWithFixedDelay类似。唯一的区别是scheduleWithFixedDelay是在线程全部执行完毕之后开始计算时间的，而scheduleAtFixedRate是在线程开始执行的时候计算时间的，所以scheduleAtFixedRate有时会产生不是定时执行的感觉。</p>
<p>先看scheduleWithFixedDelay：</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test07</span><span class="params">()</span> throws Exception </span>{
    ScheduledExecutorService sec = Executors.newScheduledThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    sec.scheduleWithFixedDelay(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            System.out.println(<span class="string">"exec"</span>);
            <span class="keyword">try</span> {
                Thread.sleep(<span class="number">3000l</span>);
            } <span class="keyword">catch</span> (Exception e) {
                e.printStackTrace();
            }
        }
    }, <span class="number">0</span>, <span class="number">5</span>, TimeUnit.SECONDS);
    Thread.sleep(<span class="number">17000l</span>);
}
</code></pre><p>打印：</p>
<pre><code>执行<span class="number">3</span>次exec。Runnable每次执行<span class="number">3</span>秒。第一次是在<span class="number">0</span>秒执行，执行了<span class="number">3</span>秒，第二次是在<span class="number">8</span>秒，执行了<span class="number">3</span>秒。第三次是在<span class="number">16</span>秒执行
</code></pre><p>然后再看scheduleAtFixedRate：</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test09</span><span class="params">()</span> throws Exception </span>{
    ScheduledExecutorService sec = Executors.newScheduledThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    sec.scheduleAtFixedRate(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            System.out.println(<span class="string">"exec"</span>);
            <span class="keyword">try</span> {
                Thread.sleep(<span class="number">3000l</span>);
            } <span class="keyword">catch</span> (Exception e) {
                e.printStackTrace();
            }
        }
    }, <span class="number">0</span>, <span class="number">5</span>, TimeUnit.SECONDS);
    Thread.sleep(<span class="number">16000l</span>);
}
</code></pre><p>打印：</p>
<pre><code>执行了<span class="number">4</span>次exec，第一次在<span class="number">0</span>秒执行，第二次在<span class="number">5</span>秒，第三次是<span class="number">10</span>秒，第四次在<span class="number">15</span>秒执行
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[写这篇博客的时候又刚好想起了当时自己实习的时候遇到的一个问题。1000个爬虫任务使用了多线程的处理方式，比如开5个线程处理这1000个任务 ...]]>
    
    </summary>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="thread" scheme="http://fangjian0423.github.io/tags/thread/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
</feed>
