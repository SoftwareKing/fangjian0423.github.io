<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Format's Notes]]></title>
  <subtitle><![CDATA[吃饭睡觉撸代码]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://fangjian0423.github.io/"/>
  <updated>2016-03-29T16:35:05.000Z</updated>
  <id>http://fangjian0423.github.io/</id>
  
  <author>
    <name><![CDATA[Format]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[jdk HashSet, LinkedHashSet工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/29/jdk_hashset_linkedhashset/"/>
    <id>http://fangjian0423.github.io/2016/03/29/jdk_hashset_linkedhashset/</id>
    <published>2016-03-29T15:46:33.000Z</published>
    <updated>2016-03-29T16:35:05.000Z</updated>
    <content type="html"><![CDATA[<p>Set是一个没有包括重复数据的集合，跟List一样，他们都继承自Collection。</p>
<p>Java中的Set接口最主要的实现类就是HashSet和LinkedHashSet。</p>
<a id="more"></a>
<h2 id="HashSet原理分析">HashSet原理分析</h2><p>首先看下HashSet的属性。</p>
<p>HashSet内部有个HashMap属性和一个对象属性：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">HashMap</span>&lt;E,<span class="keyword">Object</span>&gt; <span class="built_in">map</span>;

<span class="comment">// HashSet内部使用HashMap进行处理，由于Set只需要键值对中的键，而不需要值，所有的值都用这个对象</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">Object</span> PRESENT = <span class="keyword">new</span> <span class="keyword">Object</span>();
</code></pre><p>HashSet的构造函数中也提供了HashMap的capacity，loadFactor这些参数。</p>
<h3 id="add方法">add方法</h3><p>调用HashMap的put操作完成Set的add操作。</p>
<pre><code><span class="keyword">public</span> <span class="built_in">boolean</span> <span class="built_in">add</span>(E e) {
    <span class="keyword">return</span> <span class="built_in">map</span>.put(e, PRESENT)==<span class="keyword">null</span>;  <span class="comment">// HashMap put成功返回true，否则false</span>
}
</code></pre><p>HashMap相关的put操作在之前的博客中已经介绍过了，这里就不分析了。</p>
<h3 id="boolean_remove(Object_o)">boolean remove(Object o)</h3><p>调用HashMap的remove操作完成。</p>
<pre><code><span class="keyword">public</span> <span class="built_in">boolean</span> remove(<span class="keyword">Object</span> o) {
    <span class="keyword">return</span> <span class="built_in">map</span>.remove(o)==PRESENT; <span class="comment">// 对应的节点移除成功返回true，否则false</span>
}
</code></pre><h3 id="一个HashSet例子">一个HashSet例子</h3><pre><code>Set&lt;<span class="keyword">String</span>&gt; <span class="built_in">set</span> = <span class="keyword">new</span> HashSet&lt;<span class="keyword">String</span>&gt;(<span class="number">5</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"java"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"golang"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"python"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"ruby"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"scala"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"c"</span>);

<span class="keyword">for</span>(<span class="keyword">String</span> <span class="built_in">str</span> : <span class="built_in">set</span>) {
    System.out.<span class="built_in">println</span>(<span class="built_in">str</span>);
}
</code></pre><p>这个例子中set中的HashMap内部结构如下图所示：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap03.jpg" alt=""></p>
<h3 id="HashSet总结">HashSet总结</h3><ol>
<li>HashSet内部使用HashMap，HashSet集合内部所有的操作基本上都是基于HashMap完成的</li>
<li>HashSet中的元素是无序的，这是因为它内部使用HashMap进行存储，而HashMap添加键值对的时候是根据hash函数得到数组的下标的</li>
</ol>
<h2 id="LinkedHashSet原理分析">LinkedHashSet原理分析</h2><p>LinkedHashSet继承自HashSet，它的构造函数会调用父类HashSet的构造函数：</p>
<pre><code><span class="function"><span class="keyword">public</span> <span class="title">LinkedHashSet</span><span class="params">(<span class="keyword">int</span> initialCapacity, <span class="keyword">float</span> loadFactor)</span> </span>{
    <span class="keyword">super</span>(initialCapacity, loadFactor, <span class="keyword">true</span>);
}

HashSet(<span class="keyword">int</span> initialCapacity, <span class="keyword">float</span> loadFactor, <span class="keyword">boolean</span> dummy) {
    <span class="comment">// map使用LinkedHashMap构造，LinkedHashMap是HashMap的子类，accessOrder为false，即使用插入顺序</span>
    map = <span class="keyword">new</span> LinkedHashMap&lt;&gt;(initialCapacity, loadFactor);
}
</code></pre><h3 id="一个LinkedHashSet例子">一个LinkedHashSet例子</h3><pre><code>Set&lt;<span class="keyword">String</span>&gt; <span class="built_in">set</span> = <span class="keyword">new</span> LinkedHashSet&lt;<span class="keyword">String</span>&gt;(<span class="number">5</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"java"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"golang"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"python"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"ruby"</span>);
<span class="built_in">set</span>.<span class="built_in">add</span>(<span class="string">"scala"</span>);
<span class="keyword">for</span>(<span class="keyword">String</span> <span class="built_in">str</span> : <span class="built_in">set</span>) {
    System.out.<span class="built_in">println</span>(<span class="built_in">str</span>);
}
</code></pre><p>这个例子中set中的LinkedHashMap内部结构如下图所示：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedhashmap02.jpg" alt=""></p>
<h3 id="LinkedHashSet总结">LinkedHashSet总结</h3><ol>
<li>LinkedHashSet继自HashSet，但是内部的map是使用LinkedHashMap构造的，并且accessOrder为false，使用查询顺序。所以LinkedHashSet遍历的顺序就是插入顺序。</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>Set是一个没有包括重复数据的集合，跟List一样，他们都继承自Collection。</p>
<p>Java中的Set接口最主要的实现类就是HashSet和LinkedHashSet。</p>]]>
    
    </summary>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="set" scheme="http://fangjian0423.github.io/tags/set/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk LinkedHashMap工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/29/jdk_linkedhashmap/"/>
    <id>http://fangjian0423.github.io/2016/03/29/jdk_linkedhashmap/</id>
    <published>2016-03-29T11:23:23.000Z</published>
    <updated>2016-03-29T16:35:14.000Z</updated>
    <content type="html"><![CDATA[<p>LinkedHashMap是一种会记录插入顺序的Map，内部维护着一个accessOrder属性，用于表示map数据的迭代顺序是基于访问顺序还是插入顺序。</p>
<a id="more"></a>
<h2 id="LinkedHashMap原理分析">LinkedHashMap原理分析</h2><p>首先是LinkedHashMap的定义：</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LinkedHashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;
    <span class="keyword">extends</span> <span class="title">HashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;
        <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;</span>
</code></pre><p>LinkedHashMap继承HashMap，实现Map接口，所以它的结构跟HashMap是一样的，使用链表法解决哈希冲突的哈希表，基本操作跟HashMap也是一样的，就是多了一点额外的步骤用于处理链表。</p>
<p>LinkedHashMap有个内部类Entry，这个Entry就是链表中的节点，继承自HashMap.Node，多出了2个属性before和after，所以LinkedHashMap内部链表的节点是双向的，代码如下：</p>
<pre><code>static <span class="class"><span class="keyword">class</span> <span class="title">Entry&lt;K</span>,<span class="title">V&gt;</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">HashMap</span>.<span class="title">Node&lt;K</span>,<span class="title">V&gt;</span> {</span>
    <span class="type">Entry</span>&lt;<span class="type">K</span>,<span class="type">V</span>&gt; before, after;
    <span class="type">Entry</span>(int hash, <span class="type">K</span> key, <span class="type">V</span> value, <span class="type">Node</span>&lt;<span class="type">K</span>,<span class="type">V</span>&gt; next) {
        <span class="keyword">super</span>(hash, key, value, next);
    }
}
</code></pre><p>另外LinkedHashMap还有两个重要的属性head，tail，这2个属性用于存储插入的节点，形成一个双向链表：</p>
<pre><code><span class="comment">// 首节点</span>
<span class="keyword">transient</span> LinkedHashMap.Entry&lt;K,V&gt; head;

<span class="comment">// 尾节点</span>
<span class="keyword">transient</span> LinkedHashMap.Entry&lt;K,V&gt; tail;
</code></pre><p>跟HashMap一样，下面这个例子对应的LinkedHashMap结构图示如下所示，accessOrder为false，使用插入顺序：</p>
<pre><code>Map&lt;String, Integer&gt; <span class="built_in">map</span> = <span class="keyword">new</span> LinkedHashMap&lt;String, Integer&gt;(<span class="number">5</span>);
<span class="built_in">map</span>.put(<span class="string">"java"</span>, <span class="number">1</span>);
<span class="built_in">map</span>.put(<span class="string">"golang"</span>, <span class="number">2</span>);
<span class="built_in">map</span>.put(<span class="string">"python"</span>, <span class="number">3</span>);
<span class="built_in">map</span>.put(<span class="string">"ruby"</span>, <span class="number">4</span>);
<span class="built_in">map</span>.put(<span class="string">"scala"</span>, <span class="number">5</span>);
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedhashmap02.jpg" alt=""></p>
<h3 id="put操作">put操作</h3><p>LinkedHashMap没有覆盖HashMap的put方法，所以put操作跟HashMap是一样的。但是它覆盖了newNode方法，也就是说构造新节点的时候，LinkedHashMap跟HashMap是不一样的：</p>
<pre><code><span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;K</span>,V&gt; newNode(int hash, K key, V value, <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;K</span>,V&gt; e) {
    // 使用Entry双向链表构造节点，而不是HashMap的<span class="keyword">Node</span><span class="identifier"></span><span class="title">单向链表
    LinkedHashMap</span>.Entry<span class="tag">&lt;K,V&gt;</span> p =
        new LinkedHashMap.Entry<span class="tag">&lt;K,V&gt;</span>(hash, key, value, e);
    linkNodeLast(p); // 更新双向链表，这一操作在HashMap里面是没有的
    return p;
}
</code></pre><p>另外，LinkedHashMap重写了afterNodeInsertion这个钩子方法，在put一个关键字不存在的节点之后会调用这个方法：</p>
<pre><code><span class="keyword">void</span> afterNodeInsertion(<span class="built_in">boolean</span> evict) { <span class="comment">// possibly remove eldest</span>
    LinkedHashMap.Entry&lt;K,V&gt; first;
    <span class="comment">// removeEldestEntry方法LinkedHashMap永远返回false，一些使用缓存策略的Map会覆盖这个方法，比如jackson的LRUMap，会移除最老的节点，也就是首节点</span>
    <span class="keyword">if</span> (evict &amp;&amp; (first = head) != <span class="keyword">null</span> &amp;&amp; removeEldestEntry(first)) {
        K <span class="variable">key</span> = first.<span class="variable">key</span>;
        removeNode(hash(<span class="variable">key</span>), <span class="variable">key</span>, <span class="keyword">null</span>, <span class="keyword">false</span>, <span class="keyword">true</span>);
    }
}
</code></pre><p>put操作如果关键字已经存在，会调用afterNodeAccess这个钩子方法：</p>
<pre><code><span class="label">void</span> afterNodeAccess(Node&lt;K,V&gt; e) { // <span class="keyword">move </span>node to last
    LinkedHashMap.Entry&lt;K,V&gt; last<span class="comment">;</span>
    <span class="preprocessor">if</span> (accessOrder &amp;&amp; (last = tail) != e) { // 如果使用访问顺序并且访问的不是尾节点
        LinkedHashMap.Entry&lt;K,V&gt; p =
            (LinkedHashMap.Entry&lt;K,V&gt;)e, <span class="keyword">b </span>= p.<span class="keyword">before, </span>a = p.after<span class="comment">;</span>
        p.after = null<span class="comment">;</span>
        <span class="preprocessor">if</span> (<span class="keyword">b </span>== null)
            head = a<span class="comment">;</span>
        <span class="preprocessor">else</span>
            <span class="keyword">b.after </span>= a<span class="comment">;</span>
        <span class="preprocessor">if</span> (a != null)
            a.<span class="keyword">before </span>= <span class="keyword">b;
</span>        <span class="preprocessor">else</span>
            last = <span class="keyword">b;
</span>        <span class="preprocessor">if</span> (last == null)
            head = p<span class="comment">;</span>
        <span class="preprocessor">else</span> {
            p.<span class="keyword">before </span>= last<span class="comment">;</span>
            last.after = p<span class="comment">;</span>
        }
        tail = p<span class="comment">;</span>
        ++modCount<span class="comment">;</span>
    }
}
</code></pre><h3 id="get操作">get操作</h3><p>LinkedHashMap复写了get方法：</p>
<pre><code><span class="keyword">public</span> V <span class="built_in">get</span>(<span class="keyword">Object</span> <span class="variable">key</span>) {
    Node&lt;K,V&gt; e;
    <span class="keyword">if</span> ((e = getNode(hash(<span class="variable">key</span>), <span class="variable">key</span>)) == <span class="keyword">null</span>)
        <span class="keyword">return</span> <span class="keyword">null</span>;
    <span class="keyword">if</span> (accessOrder) <span class="comment">// 使用访问顺序的话，调用afterNodeAccess方法</span>
        afterNodeAccess(e);
    <span class="keyword">return</span> e.value;
}
</code></pre><h3 id="remove操作">remove操作</h3><p>LinkedHashMap的remove方法没有复写HashMap的remove方法，但是同样实现了afterNodeRemoval这个钩子方法：</p>
<pre><code>// 更新双向链表
<span class="label">void</span> afterNodeRemoval(Node&lt;K,V&gt; e) { // unlink
    LinkedHashMap.Entry&lt;K,V&gt; p =
        (LinkedHashMap.Entry&lt;K,V&gt;)e, <span class="keyword">b </span>= p.<span class="keyword">before, </span>a = p.after<span class="comment">;</span>
    p.<span class="keyword">before </span>= p.after = null<span class="comment">;</span>
    <span class="preprocessor">if</span> (<span class="keyword">b </span>== null)
        head = a<span class="comment">;</span>
    <span class="preprocessor">else</span>
        <span class="keyword">b.after </span>= a<span class="comment">;</span>
    <span class="preprocessor">if</span> (a == null)
        tail = <span class="keyword">b;
</span>    <span class="preprocessor">else</span>
        a.<span class="keyword">before </span>= <span class="keyword">b;
</span>}
</code></pre><h3 id="accessOrder属性分析">accessOrder属性分析</h3><p>LinkedHashMap默认情况下，accessOrder属性为false，也就是使用插入顺序，这个插入顺序是根据LinkedHashMap内部的一个双向链表实现的。如果accessOrder为true，也就是使用访问顺序，那么afterNodeAccess这个钩子方法内部的逻辑会被执行，将会修改双向链表的结构，再来看一下这个方法的具体逻辑：</p>
<pre><code><span class="function"><span class="keyword">void</span> <span class="title">afterNodeAccess</span><span class="params">(Node&lt;K,V&gt; e)</span> </span>{ <span class="comment">// move node to last</span>
    LinkedHashMap.Entry&lt;K,V&gt; last;
    <span class="keyword">if</span> (accessOrder &amp;&amp; (last = tail) != e) { <span class="comment">// 使用访问顺序，把节点移动到双向链表的最后面，如果已经在最后面了，不需要进行移动</span>
        LinkedHashMap.Entry&lt;K,V&gt; p =
            (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after;
        p.after = <span class="keyword">null</span>;
        <span class="keyword">if</span> (b == <span class="keyword">null</span>)
            head = a; <span class="comment">// 特殊情况，处理头节点</span>
        <span class="keyword">else</span>
            b.after = a; <span class="comment">// 节点处理</span>
        <span class="keyword">if</span> (a != <span class="keyword">null</span>)
            a.before = b; <span class="comment">// 节点处理</span>
        <span class="keyword">else</span>
            last = b; <span class="comment">// 特殊情况，处理尾节点</span>
        <span class="keyword">if</span> (last == <span class="keyword">null</span>)
            head = p;
        <span class="keyword">else</span> {
            p.before = last; <span class="comment">// 尾节点处理</span>
            last.after = p;
        }
        tail = p;
        ++modCount;
    }
}
</code></pre><p>afterNodeAccess在使用get方法或者put方法遇到关键字已经存在的情况下，会被触发，一个例子如下：</p>
<pre><code>Map&lt;String, Integer&gt; <span class="built_in">map</span> = <span class="keyword">new</span> LinkedHashMap&lt;String, Integer&gt;(<span class="number">5</span>, <span class="number">0.75f</span>, <span class="literal">true</span>);
<span class="built_in">map</span>.put(<span class="string">"java"</span>, <span class="number">1</span>);
<span class="built_in">map</span>.put(<span class="string">"golang"</span>, <span class="number">2</span>);
<span class="built_in">map</span>.put(<span class="string">"python"</span>, <span class="number">3</span>);
<span class="built_in">map</span>.put(<span class="string">"ruby"</span>, <span class="number">4</span>);
<span class="built_in">map</span>.put(<span class="string">"scala"</span>, <span class="number">5</span>);
System.out.println(<span class="built_in">map</span>.get(<span class="string">"ruby"</span>));
</code></pre><p>上面这段代码，LinkedHashMap的accessOrder属性为true，使用访问顺序，最后调用了get方法，触发afterNodeAccess方法，修改双向链表，效果如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedhashmap03.jpg" alt=""></p>
<h2 id="注意点">注意点</h2><p>LinkedHashMap使用访问顺序并且进行遍历的时候，如果使用如下代码，会发生ConcurrentModificationException异常：</p>
<pre><code><span class="keyword">for</span>(<span class="keyword">String</span> <span class="built_in">str</span> : <span class="built_in">map</span>.keySet()) {
    System.out.<span class="built_in">println</span>(<span class="built_in">map</span>.<span class="built_in">get</span>(<span class="built_in">str</span>));
}
</code></pre><p>不应该这么使用，而是应该直接读取value：</p>
<pre><code><span class="function">for</span>(Integer it <span class="value">: map.<span class="function">values</span>()) {
    System.out.<span class="function">println</span>(it);</span>
}
</code></pre><p>具体可以参考<a href="http://stackoverflow.com/questions/16180568/concurrentmodificationexception-with-linkedhashmap" target="_blank" rel="external">stackoverflow上的这篇帖子</a>。</p>
<h2 id="总结">总结</h2><ol>
<li><p>LinkedHashMap也是一种使用拉链式哈希表的数据结构，除了哈希表，它内部还维护着一个双向链表，用于处理访问顺序和插入顺序的问题</p>
</li>
<li><p>LinkedHashMap继承自HashMap，大多数的方法都是跟HashMap一样的，不过覆盖了一些方法</p>
</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>LinkedHashMap是一种会记录插入顺序的Map，内部维护着一个accessOrder属性，用于表示map数据的迭代顺序是基于访问顺序还是插入顺序。</p>]]>
    
    </summary>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="map" scheme="http://fangjian0423.github.io/tags/map/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk HashMap工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/29/jdk_hashmap/"/>
    <id>http://fangjian0423.github.io/2016/03/29/jdk_hashmap/</id>
    <published>2016-03-28T17:49:58.000Z</published>
    <updated>2016-03-29T16:34:57.000Z</updated>
    <content type="html"><![CDATA[<p>Map是一个映射键和值的对象。类似于Python中的字典。</p>
<p>HashMap为什么会出现呢?</p>
<p>因为数组这种数据结构，虽然遍历简单，但是插入和删除操作复杂，需要移动数组内部的元素；链表这种数据结构，插入和删除操作简单，但是查找复杂，只能一个一个地遍历。</p>
<p>有没有一种新的数据结构，插入数据简单，同时查找也简单？ 这个时候就出现了哈希表这种数据结构。 这是一种折中的方式，插入没链表快，查询没数组快。</p>
<p>wiki上就是这么定义哈希表的：</p>
<p>散列表（Hash table，也叫哈希表），是根据关键字（Key value）而直接访问在内存存储位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录，这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做散列表。</p>
<a id="more"></a>
<p>有几个概念要解释一下：</p>
<ol>
<li>如果有1个关键字为k，它是通过一种函数f(k)得到散列表的地址，然后把值放到这个地址上。这个函数f就称为散列函数，也叫哈希函数。</li>
<li>对于不同的关键字，得到了同一地址，即k1 != k2，但是f(k1) = f(k2)。这种现象称为冲突，</li>
<li>若对于关键字集合中的任一个关键字，经散列函数映象到地址集合中任何一个地址的概率是相等的，则称此类散列函数为均匀散列函数</li>
</ol>
<p>散列函数有好几种实现，分别有直接定址法、随机数法、除留余数法等，在<a href="https://zh.wikipedia.org/wiki/%E5%93%88%E5%B8%8C%E8%A1%A8" target="_blank" rel="external">wiki散列表</a>上都有介绍。</p>
<p>散列表的冲突解决方法，也有好几种，有开放定址法、单独链表法、再散列等。</p>
<p>Java中的HashMap采用的冲突解决方法是使用单独链表法，如下图所示：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap01.png" alt=""></p>
<h2 id="HashMap原理分析">HashMap原理分析</h2><p>HashMap是jdk中Map接口的实现类之一，是一个散列表的实现。</p>
<p>HashMap中的key和value都可以为null，且它的方法都没有synchronized。 其他方法的实现大部分跟HashTable一致。HashTable的相关源码不在这里介绍，基本上跟HashTable一致。</p>
<p>HashMap有个内部静态类Node，这个Node就是为了解决冲突而设计的链表中的节点的概念。它有4个属性，hash表示哈希地址，key表示关键字，value表示值, next表示这个节点的下一个节点，是一个单项链表：</p>
<pre><code>static class <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;K</span>,V&gt; implements Map.Entry<span class="tag">&lt;K,V&gt;</span> {
    final int hash;
    final K key;
    V value;
    <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;K</span>,V&gt; next;

    <span class="keyword">Node</span><span class="identifier"></span><span class="title">(int</span> hash, K key, V value, <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;K</span>,V&gt; next) {
        this.hash = hash;
        this.key = key;
        this.value = value;
        this.next = next;
    }

    ...
}
</code></pre><h3 id="在分析HashMap源码之前，先看一个HashMap使用例子">在分析HashMap源码之前，先看一个HashMap使用例子</h3><pre><code>Map&lt;String, Integer&gt; <span class="built_in">map</span> = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;(<span class="number">5</span>);
<span class="built_in">map</span>.put(<span class="string">"java"</span>, <span class="number">1</span>);
<span class="built_in">map</span>.put(<span class="string">"golang"</span>, <span class="number">2</span>);
<span class="built_in">map</span>.put(<span class="string">"python"</span>, <span class="number">3</span>);
<span class="built_in">map</span>.put(<span class="string">"ruby"</span>, <span class="number">4</span>);
<span class="built_in">map</span>.put(<span class="string">"scala"</span>, <span class="number">5</span>);
</code></pre><p>上面这段代码执行之后会生成下面这张哈希表。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap05.jpg" alt=""></p>
<p>至于为什么会生成这样的哈希表，会在后面分析源码中讲解。</p>
<h3 id="HashMap的属性">HashMap的属性</h3><p>HashMap的几个重要的属性:</p>
<pre><code><span class="keyword">transient</span> Node&lt;K,V&gt;[] table; <span class="comment">// 哈希表数组</span>

<span class="keyword">transient</span> <span class="keyword">int</span> <span class="keyword">size</span>; <span class="comment">// 键值对个数</span>

<span class="keyword">int</span> threshold; <span class="comment">// 阀值。 值 = 容量 * 加载因子。默认值为12(16(默认容量) * 0.75(默认加载因子))。当哈希表中的键值对个数超过该值时，会进行扩容</span>

<span class="keyword">final</span> <span class="keyword">float</span> loadFactor; <span class="comment">// 加载因子，默认是0.75</span>
</code></pre><p>有2个重要的特性影响着HashMap的性能，分别是capacity(容量)和load factor(加载因子)。</p>
<p>其中capacity表示哈希表bucket的数量，HashMap的默认值是16。load factor加载因子表示当一个map填满了达到这个比例之后的bucket时候，和ArrayList一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程也叫做重哈希。默认的load factor为0.75 。</p>
<h3 id="HashMap的操作">HashMap的操作</h3><p>分析一下HashMapput键值对的过程，是如何找到bucket的，遇到哈希冲突的时候是如何使用链表法的。</p>
<h4 id="put操作">put操作</h4><pre><code><span class="keyword">public</span> V put(K <span class="variable">key</span>, V value) {
    <span class="comment">// 第一个参数就是关键字key的哈希值</span>
    <span class="keyword">return</span> putVal(hash(<span class="variable">key</span>), <span class="variable">key</span>, value, <span class="keyword">false</span>, <span class="keyword">true</span>);
}

<span class="keyword">final</span> V putVal(<span class="built_in">int</span> hash, K <span class="variable">key</span>, V value, <span class="built_in">boolean</span> onlyIfAbsent,
               <span class="built_in">boolean</span> evict) {
    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; <span class="built_in">int</span> n, i;
    <span class="keyword">if</span> ((tab = table) == <span class="keyword">null</span> || (n = tab.length) == <span class="number">0</span>)
        n = (tab = resize()).length; <span class="comment">// 哈希表是空的话，重新构建，进行扩容</span>
    <span class="keyword">if</span> ((p = tab[i = (n - <span class="number">1</span>) &amp; hash]) == <span class="keyword">null</span>)
        tab[i] = newNode(hash, <span class="variable">key</span>, value, <span class="keyword">null</span>); <span class="comment">// 没有hash冲突的话，直接在对应位置上构造一个新的节点即可</span>
    <span class="keyword">else</span> { <span class="comment">// 如果哈希表当前位置上已经有节点的话，说明有hash冲突</span>
        Node&lt;K,V&gt; e; K k;
        <span class="comment">// 关键字跟哈希表上的首个节点济宁比较</span>
        <span class="keyword">if</span> (p.hash == hash &amp;&amp;
            ((k = p.<span class="variable">key</span>) == <span class="variable">key</span> || (<span class="variable">key</span> != <span class="keyword">null</span> &amp;&amp; <span class="variable">key</span>.equals(k))))
            e = p;
        <span class="comment">// 如果使用的是红黑树，用红黑树的方式进行处理</span>
        <span class="keyword">else</span> <span class="keyword">if</span> (p <span class="keyword">instanceof</span> TreeNode)
            e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(<span class="keyword">this</span>, tab, hash, <span class="variable">key</span>, value);
        <span class="keyword">else</span> { <span class="comment">// 跟链表进行比较</span>
            <span class="keyword">for</span> (<span class="built_in">int</span> binCount = <span class="number">0</span>; ; ++binCount) {
                <span class="keyword">if</span> ((e = p.next) == <span class="keyword">null</span>) { <span class="comment">// 一直遍历链表，直到找到最后一个</span>
                    p.next = newNode(hash, <span class="variable">key</span>, value, <span class="keyword">null</span>); <span class="comment">// 构造链表上的新节点</span>
                    <span class="keyword">if</span> (binCount &gt;= TREEIFY_THRESHOLD - <span class="number">1</span>) <span class="comment">// -1 for 1st</span>
                        treeifyBin(tab, hash);
                    <span class="keyword">break</span>;
                }
                <span class="keyword">if</span> (e.hash == hash &amp;&amp;
                    ((k = e.<span class="variable">key</span>) == <span class="variable">key</span> || (<span class="variable">key</span> != <span class="keyword">null</span> &amp;&amp; <span class="variable">key</span>.equals(k))))
                    <span class="keyword">break</span>;
                p = e;
            }
        }
        <span class="keyword">if</span> (e != <span class="keyword">null</span>) { <span class="comment">// 如果找到了节点，说明关键字相同，进行覆盖操作，直接返回旧的关键字的值</span>
            V oldValue = e.value;
            <span class="keyword">if</span> (!onlyIfAbsent || oldValue == <span class="keyword">null</span>)
                e.value = value;
            afterNodeAccess(e);
            <span class="keyword">return</span> oldValue;
        }
    }
    ++modCount;
    <span class="keyword">if</span> (++<span class="built_in">size</span> &gt; threshold) <span class="comment">// 如果目前键值对个数已经超过阀值，重新构建</span>
        resize();
    afterNodeInsertion(evict); <span class="comment">// 节点插入以后的钩子方法</span>
    <span class="keyword">return</span> <span class="keyword">null</span>;
}
</code></pre><h4 id="get操作">get操作</h4><p>get操作关键点就是怎么在哈希表上取数据，理解了put操作之后，get方法很容易理解了：</p>
<pre><code><span class="keyword">public</span> V <span class="built_in">get</span>(<span class="keyword">Object</span> <span class="variable">key</span>) {
    Node&lt;K,V&gt; e;
    <span class="keyword">return</span> (e = getNode(hash(<span class="variable">key</span>), <span class="variable">key</span>)) == <span class="keyword">null</span> ? <span class="keyword">null</span> : e.value;
}
</code></pre><p>getNode方法就说明了如何取数据：</p>
<pre><code><span class="keyword">final</span> Node&lt;K,V&gt; getNode(<span class="built_in">int</span> hash, <span class="keyword">Object</span> <span class="variable">key</span>) {
    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; <span class="built_in">int</span> n; K k;
    <span class="keyword">if</span> ((tab = table) != <span class="keyword">null</span> &amp;&amp; (n = tab.length) &gt; <span class="number">0</span> &amp;&amp;
        (first = tab[(n - <span class="number">1</span>) &amp; hash]) != <span class="keyword">null</span>) { <span class="comment">// 如果哈希表容量为0或者关键字没有命中，直接返回null</span>
        <span class="keyword">if</span> (first.hash == hash &amp;&amp;  <span class="comment">// 关键字命中的话比较第一个节点</span>
            ((k = first.<span class="variable">key</span>) == <span class="variable">key</span> || (<span class="variable">key</span> != <span class="keyword">null</span> &amp;&amp; <span class="variable">key</span>.equals(k)))) 
            <span class="keyword">return</span> first;
        <span class="keyword">if</span> ((e = first.next) != <span class="keyword">null</span>) {
            <span class="keyword">if</span> (first <span class="keyword">instanceof</span> TreeNode) <span class="comment">// 以红黑树的方式查找</span>
                <span class="keyword">return</span> ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, <span class="variable">key</span>);
            do { <span class="comment">// 遍历链表查找</span>
                <span class="keyword">if</span> (e.hash == hash &amp;&amp;
                    ((k = e.<span class="variable">key</span>) == <span class="variable">key</span> || (<span class="variable">key</span> != <span class="keyword">null</span> &amp;&amp; <span class="variable">key</span>.equals(k))))
                    <span class="keyword">return</span> e;
            } <span class="keyword">while</span> ((e = e.next) != <span class="keyword">null</span>);
        }
    }
    <span class="keyword">return</span> <span class="keyword">null</span>;
}
</code></pre><h4 id="hash过程和resize过程分析">hash过程和resize过程分析</h4><p>hash过程在HashMap里就是一个hash方法：</p>
<pre><code><span class="keyword">static</span> <span class="keyword">final</span> <span class="built_in">int</span> hash(<span class="keyword">Object</span> <span class="variable">key</span>) {
    <span class="built_in">int</span> h;
    <span class="comment">// 使用hashCode的值和hashCode的值无符号右移16位做异或操作</span>
    <span class="keyword">return</span> (<span class="variable">key</span> == <span class="keyword">null</span>) ? <span class="number">0</span> : (h = <span class="variable">key</span>.hashCode()) ^ (h &gt;&gt;&gt; <span class="number">16</span>);
}
</code></pre><p>这段代码是什么意思呢？ 我们以文中的那个demo为例，说明”java”这个关键字是如何找到对应bucket的过程。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap06.jpg" alt=""></p>
<p>从上图可以看到，hash方法得到的hash值是根据关键字的hashCode的高16位和低16位进行异或操作得到的一个值。</p>
<p>这个值再与哈希表容量-1值进行与操作得到最终的bucket索引值。</p>
<pre><code><span class="list">(<span class="keyword">n</span> - <span class="number">1</span>)</span> &amp; hash
</code></pre><p>hashCode的高16位与低16位进行异或操作主要是设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)来做的。</p>
<p>如果链表的数量大了，HashMap会把哈希表转换成红黑树来进行处理，本文不讨论这部分内容。</p>
<p>现在回过头来看例子，为什么初始化了一个容量为5的HashMap，但是哈希表的容量为8，而且阀值为6？</p>
<p>因为HashMap的构造函数初始化threshold的时候调用了tableSizeFor方法，这个方法会把容量改成2的幂的整数，主要是为了哈希表散列更均匀。</p>
<pre><code><span class="comment">// 定位bucket索引的最后操作。如果n为奇数，n-1就是偶数，偶数的话转成二进制最后一位是0，相反如果是奇数，最后一位是1，这样产生的索引值将更均匀</span>
(n - <span class="number">1</span>) &amp; hash
</code></pre><p>tableSizeFor方法如下：</p>
<pre><code>this.threshold = tableSizeFor(initialCapacity);

<span class="comment">// 保证thresold为2的幂</span>
static final int tableSizeFor(int <span class="keyword">cap</span>) {
    int <span class="keyword">n</span> = <span class="keyword">cap</span> - 1;
    <span class="keyword">n</span> |= <span class="keyword">n</span> &gt;&gt;&gt; 1;
    <span class="keyword">n</span> |= <span class="keyword">n</span> &gt;&gt;&gt; 2;
    <span class="keyword">n</span> |= <span class="keyword">n</span> &gt;&gt;&gt; 4;
    <span class="keyword">n</span> |= <span class="keyword">n</span> &gt;&gt;&gt; 8;
    <span class="keyword">n</span> |= <span class="keyword">n</span> &gt;&gt;&gt; 16;
    <span class="keyword">return</span> (<span class="keyword">n</span> &lt; 0) ? 1 : (<span class="keyword">n</span> &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : <span class="keyword">n</span> + 1;
}
</code></pre><p>阀值为6是因为之后进行resize操作的时候更新了阀值</p>
<pre><code>阀值 = 容量 * 加载因子 = <span class="number">8</span> * <span class="number">0.75</span> = <span class="number">6</span>
</code></pre><p>HashMap的扩容会把原先哈希表的容量扩大两倍。扩大之后，会对节点重新进行处理。</p>
<p>哈希表上的节点的状态有3种，分别是单节点，无节点，链表，扩容对于这3种状态的处理方式如下：</p>
<p>以8节点为原先容量，扩容为16容量讲解。</p>
<ol>
<li>单节点：由于容量扩大两倍，相当于左移1位。扩容前与00000111[7，n - 1 = 8 - 1]进行与操作。扩容后与00001111[15, n - 1 = 16 - 1]进行与操作。所以最终的结果要是还是在原位置，要么在原位置 +8(+old capacity) 位置</li>
<li>无节点：不处理</li>
<li>链表：遍历各个节点，每个节点的处理方式跟单节点一样，结果分成2种，还在原位置和原位置 +8 位置</li>
</ol>
<p>单节点处理示意图如下，这么设计的原因就是不需要再次计算hash值，只需要移动位置(+old capacity)即可：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap07.jpg" alt="">   </p>
<p>下图是一个HashMap扩容之后的效果图（省去了索引为7橙色链表的虚线，太多线条了）：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/hashmap08.jpg" alt="">   </p>
<p>哈希表扩容是使用resize方法完成：</p>
<pre><code><span class="keyword">final</span> Node&lt;K,V&gt;[] resize() {
    Node&lt;K,V&gt;[] oldTab = table;
    <span class="keyword">int</span> oldCap = (oldTab == <span class="keyword">null</span>) ? <span class="number">0</span> : oldTab.length;
    <span class="keyword">int</span> oldThr = threshold;
    <span class="keyword">int</span> newCap, newThr = <span class="number">0</span>;
    <span class="keyword">if</span> (oldCap &gt; <span class="number">0</span>) { <span class="comment">// 如果老容量大于0，说明哈希表中已经有数据了，然后进行扩容</span>
        <span class="keyword">if</span> (oldCap &gt;= MAXIMUM_CAPACITY) { <span class="comment">// 超过最大容量的话，不扩容</span>
            threshold = Integer.MAX_VALUE;
            <span class="keyword">return</span> oldTab;
        }
        <span class="keyword">else</span> <span class="keyword">if</span> ((newCap = oldCap &lt;&lt; <span class="number">1</span>) &lt; MAXIMUM_CAPACITY &amp;&amp; <span class="comment">// 容量加倍</span>
                 oldCap &gt;= DEFAULT_INITIAL_CAPACITY) <span class="comment">// 如果老的容量超过默认容量的话</span>
            newThr = oldThr &lt;&lt; <span class="number">1</span>; <span class="comment">// 阀值加倍</span>
    }
    <span class="keyword">else</span> <span class="keyword">if</span> (oldThr &gt; <span class="number">0</span>) <span class="comment">// 根据thresold初始化数组</span>
        newCap = oldThr;
    <span class="keyword">else</span> {               <span class="comment">// 使用默认配置</span>
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (<span class="keyword">int</span>)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
    <span class="keyword">if</span> (newThr == <span class="number">0</span>) {
        <span class="keyword">float</span> ft = (<span class="keyword">float</span>)newCap * loadFactor;
        newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (<span class="keyword">float</span>)MAXIMUM_CAPACITY ?
                  (<span class="keyword">int</span>)ft : Integer.MAX_VALUE);
    }
    threshold = newThr;
    @SuppressWarnings({<span class="string">"rawtypes"</span>,<span class="string">"unchecked"</span>})
        Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap];
    table = newTab;
    <span class="keyword">if</span> (oldTab != <span class="keyword">null</span>) {
        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; oldCap; ++j) { <span class="comment">// 扩容之后进行rehash操作</span>
            Node&lt;K,V&gt; e;
            <span class="keyword">if</span> ((e = oldTab[j]) != <span class="keyword">null</span>) {
                oldTab[j] = <span class="keyword">null</span>;
                <span class="keyword">if</span> (e.<span class="keyword">next</span> == <span class="keyword">null</span>)
                    newTab[e.hash &amp; (newCap - <span class="number">1</span>)] = e; <span class="comment">// 单节点扩容</span>
                <span class="keyword">else</span> <span class="keyword">if</span> (e <span class="keyword">instanceof</span> TreeNode)
                    ((TreeNode&lt;K,V&gt;)e).split(<span class="keyword">this</span>, newTab, j, oldCap); <span class="comment">// 红黑树方式处理</span>
                <span class="keyword">else</span> { <span class="comment">// 链表扩容</span>
                    Node&lt;K,V&gt; loHead = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;
                    Node&lt;K,V&gt; hiHead = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;
                    Node&lt;K,V&gt; <span class="keyword">next</span>;
                    <span class="keyword">do</span> {
                        <span class="keyword">next</span> = e.<span class="keyword">next</span>;
                        <span class="keyword">if</span> ((e.hash &amp; oldCap) == <span class="number">0</span>) {
                            <span class="keyword">if</span> (loTail == <span class="keyword">null</span>)
                                loHead = e;
                            <span class="keyword">else</span>
                                loTail.<span class="keyword">next</span> = e;
                            loTail = e;
                        } 
                        <span class="keyword">else</span> {
                            <span class="keyword">if</span> (hiTail == <span class="keyword">null</span>)
                                hiHead = e;
                            <span class="keyword">else</span>
                                hiTail.<span class="keyword">next</span> = e;
                            hiTail = e;
                        }
                    } <span class="keyword">while</span> ((e = <span class="keyword">next</span>) != <span class="keyword">null</span>);
                    <span class="keyword">if</span> (loTail != <span class="keyword">null</span>) {
                        loTail.<span class="keyword">next</span> = <span class="keyword">null</span>;
                        newTab[j] = loHead;
                    }
                    <span class="keyword">if</span> (hiTail != <span class="keyword">null</span>) {
                        hiTail.<span class="keyword">next</span> = <span class="keyword">null</span>;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    <span class="keyword">return</span> newTab;
}
</code></pre><h2 id="HashMap注意的地方">HashMap注意的地方</h2><ol>
<li>HashMap底层是个哈希表，使用拉链法解决冲突</li>
<li>HashMap内部存储的数据是无序的，这是因为HashMap内部的数组的下表是根据hash值算出来的</li>
<li>HashMap允许key为null</li>
<li>HashMap不是一个线程安全的类</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>Map是一个映射键和值的对象。类似于Python中的字典。</p>
<p>HashMap为什么会出现呢?</p>
<p>因为数组这种数据结构，虽然遍历简单，但是插入和删除操作复杂，需要移动数组内部的元素；链表这种数据结构，插入和删除操作简单，但是查找复杂，只能一个一个地遍历。</p>
<p>有没有一种新的数据结构，插入数据简单，同时查找也简单？ 这个时候就出现了哈希表这种数据结构。 这是一种折中的方式，插入没链表快，查询没数组快。</p>
<p>wiki上就是这么定义哈希表的：</p>
<p>散列表（Hash table，也叫哈希表），是根据关键字（Key value）而直接访问在内存存储位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录，这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做散列表。</p>]]>
    
    </summary>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="map" scheme="http://fangjian0423.github.io/tags/map/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk LinkedList工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/27/jdk_linkedlist/"/>
    <id>http://fangjian0423.github.io/2016/03/27/jdk_linkedlist/</id>
    <published>2016-03-27T09:35:27.000Z</published>
    <updated>2016-03-29T16:34:48.000Z</updated>
    <content type="html"><![CDATA[<p>List接口的实现类之一ArrayList的内部实现是一个数组，而另外一个实现LinkedList内部实现是使用双向链表。</p>
<p>LinkedList在内部定义了一个叫做Node类型的内部类，这个Node就是一个节点，链表中的节点，这个节点有3个属性，分别是元素item(当前节点要表示的值), 前节点prev(当前节点之前位置上的一个节点)，后节点next(当前节点后面位置的一个节点)。 </p>
<p>LinkedList关于数据的插入，删除操作都会处理这些节点的前后关系。而不像ArrayList那样只需要移动元素的位置即可。</p>
<a id="more"></a>
<h2 id="源码分析">源码分析</h2><p>在分析LinkedList之前，我们先看下它里面的内部类Node，也就是节点的定义：</p>
<pre><code>private static class <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; {
    E item; // 节点所表示的值
    <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; next; // 后节点
    <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; prev; // 前节点

    <span class="keyword">Node</span><span class="identifier"></span><span class="title">(Node</span><span class="tag">&lt;E&gt;</span> prev, E element, <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; next) {
        this.item = element;
        this.next = next;
        this.prev = prev;
    }
}
</code></pre><p>LinkedList的3个属性：</p>
<pre><code>transient int size = <span class="number">0</span>; // 集合链表内节点数量

transient <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; first; // 集合链表的首节点

transient <span class="keyword">Node</span><span class="identifier"></span><span class="title">&lt;E</span>&gt; last; // 集合链表的尾节点
</code></pre><h3 id="add(E_e)">add(E e)</h3><p><strong> 添加元素到链表的最后一个位置 </strong></p>
<pre><code>public boolean add(<span class="keyword">E</span> <span class="keyword">e</span>) {
    linkLast(<span class="keyword">e</span>);
    <span class="keyword">return</span> true;
}

void linkLast(<span class="keyword">E</span> <span class="keyword">e</span>) {
    final Node&lt;<span class="keyword">E</span>&gt; <span class="keyword">l</span> = last;
    final Node&lt;<span class="keyword">E</span>&gt; newNode = new Node&lt;&gt;(<span class="keyword">l</span>, <span class="keyword">e</span>, null); <span class="comment">// 由于是添加元素的链表尾部，所以也就是这个新的节点是最后1个节点，它的前节点肯定是目前链表的尾节点，它的后节点为null</span>
    last = newNode; <span class="comment">// 尾节点变成新的节点</span>
    <span class="keyword">if</span> (<span class="keyword">l</span> == null) <span class="comment">// 如果一开始尾节点还没设置，那么说明这个新的节点是第一个节点，那么首节点也就是这个第一个节点</span>
        first = newNode;
    <span class="keyword">else</span> <span class="comment">// 否则，说明新节点不是第一个节点，处理节点前后关系</span>
        <span class="keyword">l</span>.next = newNode;
    size++; <span class="comment">// 节点数量+1</span>
    modCount++;
}
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedlist01.jpg" alt=""></p>
<p>上图第一个图就表示一个已经有1，2这2个节点的LinkedList调用add方法，第二个图表示添加一个值为3的元素后的情况。原先的尾节点2的后节点变成的新节点3，新节点3的前节点是原先的尾节点2，新节点3的后节点为null。同时链表的尾节点变成了3.</p>
<h3 id="add(int_index,_E_element)">add(int index, E element)</h3><p><strong> 添加元素到列表中的指定位置 </strong></p>
<pre><code><span class="keyword">public</span> <span class="keyword">void</span> add(<span class="keyword">int</span> <span class="keyword">index</span>, E element) {
    checkPositionIndex(<span class="keyword">index</span>); <span class="comment">// 检查索引的合法性，不能超过链表个数，不能小于0</span>

    <span class="keyword">if</span> (<span class="keyword">index</span> == size) <span class="comment">// 如果是在链表尾部插入节点，那么直接调用linkLast方法，上面已经分析过</span>
        linkLast(element);
    <span class="keyword">else</span> <span class="comment">// 不在链表尾部插入节点的话，调用linkBefore方法，参数为要插入的元素值和节点对象</span>
        linkBefore(element, node(<span class="keyword">index</span>));
}
</code></pre><p>先看一下node方法是如何根据索引找到对应的节点的：</p>
<pre><code>Node&lt;E&gt; node(<span class="keyword">int</span> index) {
    <span class="comment">// 用了一个小算法，如果索引比链表数量的一半还要小，从前往后找，这样只需要花O(n/2)的时间获取节点</span>
    <span class="keyword">if</span> (index &lt; (<span class="keyword">size</span> &gt;&gt; <span class="number">1</span>)) {
        Node&lt;E&gt; x = first;
        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index; i++)
            x = x.<span class="keyword">next</span>;
        <span class="keyword">return</span> x;
    } <span class="keyword">else</span> { <span class="comment">// 否则从后往前找</span>
        Node&lt;E&gt; x = last;
        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="keyword">size</span> - <span class="number">1</span>; i &gt; index; i--)
            x = x.prev;
        <span class="keyword">return</span> x;
    }
}

<span class="keyword">void</span> linkBefore(E e, Node&lt;E&gt; succ) { <span class="comment">// succ节点表示要新插入节点应该在的位置</span>
    <span class="keyword">final</span> Node&lt;E&gt; pred = succ.prev;
    <span class="keyword">final</span> Node&lt;E&gt; newNode = <span class="keyword">new</span> Node&lt;&gt;(pred, e, succ); <span class="comment">// 1：新节点的前节点就是succ节点的前节点，新节点的后节点是succ节点</span>
    succ.prev = newNode; <span class="comment">// 2：succ的前节点就是新节点</span>
    <span class="keyword">if</span> (pred == <span class="keyword">null</span>)    <span class="comment">// prev=null表示succ节点就是head首节点，这样的话只需要重新set一下首节点即可，首节点的后节点在步骤1以及设置过了</span>
        first = newNode;
    <span class="keyword">else</span> <span class="comment">// succ不是首节点的话执行步骤3</span>
        pred.<span class="keyword">next</span> = newNode; <span class="comment">// 3：succ节点的前节点的后节点就是新节点</span>
    <span class="keyword">size</span>++; <span class="comment">// 节点数量+1</span>
    modCount++;
} 
</code></pre><p>上面代码中注释的1，2，3点就在下图中表示：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedlist02.jpg" alt=""></p>
<p>LinkedList还提供了2种特殊的add方法，分别是addFirst和addLast方法，处理添加首节点和尾节点，原理都是差不多的，处理链表之间的关联关系即可。</p>
<h3 id="remove(int_index)">remove(int index)</h3><p><strong> 移除指定位置上的节点 </strong></p>
<pre><code><span class="keyword">public</span> E remove(<span class="keyword">int</span> index) {
    checkElementIndex(index); <span class="comment">// 检查索引的合法性，不能超过链表个数，不能小于0 </span>
    <span class="keyword">return</span> unlink(node(index));
}

E unlink(Node&lt;E&gt; x) {
    <span class="keyword">final</span> E element = x.item;
    <span class="keyword">final</span> Node&lt;E&gt; <span class="keyword">next</span> = x.<span class="keyword">next</span>;
    <span class="keyword">final</span> Node&lt;E&gt; prev = x.prev;

    <span class="keyword">if</span> (prev == <span class="keyword">null</span>) { <span class="comment">// 特殊情况，删除的是头节点</span>
        first = <span class="keyword">next</span>;
    } <span class="keyword">else</span> {
        prev.<span class="keyword">next</span> = <span class="keyword">next</span>; <span class="comment">// 1</span>
        x.prev = <span class="keyword">null</span>; <span class="comment">// 1</span>
    }

    <span class="keyword">if</span> (<span class="keyword">next</span> == <span class="keyword">null</span>) { <span class="comment">// 特殊情况，删除的是尾节点</span>
        last = prev;
    } <span class="keyword">else</span> {
        <span class="keyword">next</span>.prev = prev; <span class="comment">// 2</span>
        x.<span class="keyword">next</span> = <span class="keyword">null</span>; <span class="comment">// 2</span>
    }

    x.item = <span class="keyword">null</span>; <span class="comment">// 3</span>
    <span class="keyword">size</span>--; <span class="comment">// 链表数量减一</span>
    modCount++;
    <span class="keyword">return</span> element;
}
</code></pre><p>上面代码中注释的1，2，3点就在下图中表示：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/linkedlist03.jpg" alt=""></p>
<h3 id="get(int_index)">get(int index)</h3><p><strong> 得到索引位置上的元素 </strong></p>
<pre><code><span class="keyword">public</span> E get(<span class="keyword">int</span> <span class="keyword">index</span>) {
    checkElementIndex(<span class="keyword">index</span>); <span class="comment">// 检查索引的合法性，不能超过链表个数，不能小于0 </span>
    <span class="keyword">return</span> node(<span class="keyword">index</span>).item; <span class="comment">// 直接找到节点，返回节点的元素值即可</span>
}
</code></pre><h2 id="LinkedList和ArrayList的比较">LinkedList和ArrayList的比较</h2><ol>
<li>LinkedList和ArrayList的设计理念完全不一样，ArrayList基于数组，而LinkedList基于节点，也就是链表。所以LinkedList内部没有容量这个概念，因为是链表，链表是无界的</li>
<li>两者的使用场景不同，ArrayList适用于读多写少的场合。LinkedList适用于写多读少的场合。 刚好相反。 那是因为LinkedList要找节点的话必须要遍历一个一个节点，直到找到为止。而ArrayList完全不需要，因为ArrayList内部维护着一个数组，直接根据索引拿到需要的元素即可。</li>
<li>两个都是List接口的实现类，都是一种集合</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>List接口的实现类之一ArrayList的内部实现是一个数组，而另外一个实现LinkedList内部实现是使用双向链表。</p>
<p>LinkedList在内部定义了一个叫做Node类型的内部类，这个Node就是一个节点，链表中的节点，这个节点有3个属性，分别是元素item(当前节点要表示的值), 前节点prev(当前节点之前位置上的一个节点)，后节点next(当前节点后面位置的一个节点)。 </p>
<p>LinkedList关于数据的插入，删除操作都会处理这些节点的前后关系。而不像ArrayList那样只需要移动元素的位置即可。</p>]]>
    
    </summary>
    
      <category term="collection" scheme="http://fangjian0423.github.io/tags/collection/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[jdk ArrayList工作原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/27/jdk_arraylist/"/>
    <id>http://fangjian0423.github.io/2016/03/27/jdk_arraylist/</id>
    <published>2016-03-27T05:33:17.000Z</published>
    <updated>2016-03-29T16:34:38.000Z</updated>
    <content type="html"><![CDATA[<p>list是一种有序的集合(an ordered collection), 通常也会被称为序列(sequence)，使用list可以精确地控制每个元素的插入，可以通过索引值找到对应list中的各个项，也可以在list中查询元素。</p>
<p>以前的几段话摘自jdk文档的说明。</p>
<p>其实list就相当于一个动态的数组，也就是链表，普通的数组长度大小都是固定的，而list是一个动态的数组，当list的长度满了，再次插入数据到list当中的时候，list会自动地扩展它的长度。</p>
<a id="more"></a>
<h2 id="ArrayList源码分析">ArrayList源码分析</h2><p>首先我们先分析一个List接口的实现类之一，也是最常用的ArrayList的源码。</p>
<p>ArrayList底层使用一个数组完成数据的添加，查询，删除，修改。这个数组就是下面提到的elementData。</p>
<p>这里分析的代码是基于jdk1.7的。</p>
<p>ArrayList类的属性如下：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="built_in">int</span> DEFAULT_CAPACITY = <span class="number">10</span>; <span class="comment">// 集合的默认容量</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">Object</span>[] EMPTY_ELEMENTDATA = {}; <span class="comment">// 一个空集合数组，容量为0</span>
<span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">Object</span>[] elementData; <span class="comment">// 存储集合数据的数组，默认值为null</span>
<span class="keyword">private</span> <span class="built_in">int</span> <span class="built_in">size</span>; <span class="comment">// ArrayList集合中数组的当前有效长度，比如数组的容量是5，size是1 表示容量为5的数组目前已经有1条记录了，其余4条记录还是为空</span>
</code></pre><p>接下来看一下ArrayList的构造函数：</p>
<p>ArrayList有3个构造函数，分别是</p>
<pre><code><span class="keyword">public</span> ArrayList(<span class="keyword">int</span> initialCapacity) { <span class="comment">// 带有集合容量参数的构造函数</span>
    <span class="comment">// 调用父类AbstractList的方法构造函数</span>
    <span class="keyword">super</span>();
    <span class="keyword">if</span> (initialCapacity &lt; <span class="number">0</span>) <span class="comment">// 如果集合的容量小于0，这明显是个错误数值，直接抛出异常</span>
        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Illegal Capacity: "</span>+
                                           initialCapacity);
    <span class="keyword">this</span>.elementData = <span class="keyword">new</span> Object[initialCapacity]; <span class="comment">// 初始化elementData属性，确定容量</span>
}

<span class="keyword">public</span> ArrayList() { <span class="comment">// 没有参数的构造函数</span>
    <span class="keyword">super</span>(); <span class="comment">// 调用父类AbstractList的方法构造函数</span>
    <span class="keyword">this</span>.elementData = EMPTY_ELEMENTDATA; <span class="comment">// 让elementData和ArrayList的EMPTY_ELEMENTDATA这个空数组使用同一个引用</span>
}

<span class="keyword">public</span> ArrayList(Collection&lt;? <span class="keyword">extends</span> E&gt; c) { <span class="comment">// 参数是一个集合的构造函数</span>
    elementData = c.toArray(); <span class="comment">// elementData直接使用参数集合内部的数组</span>
    <span class="keyword">size</span> = elementData.length; <span class="comment">// 初始化数组当前有效长度</span>
    <span class="comment">// c.toArray方法可能不会返回一个Object[]结果，需要做一层判断。这个一个Java的bug，可以在http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6260652查看</span>
    <span class="keyword">if</span> (elementData.getClass() != Object[].<span class="keyword">class</span>)
        elementData = Arrays.copyOf(elementData, <span class="keyword">size</span>, Object[].<span class="keyword">class</span>);
}
</code></pre><p>接下来挑几个重要的方法讲解一下：</p>
<h3 id="add(E_e)_方法">add(E e) 方法</h3><p>这个方法的作用就是把 <strong>元素添加到集合的最后面</strong> </p>
<p>源码：</p>
<pre><code><span class="function"><span class="keyword">public</span> boolean <span class="title">add</span><span class="params">(E e)</span> </span>{
    ensureCapacityInternal(size + <span class="number">1</span>);  <span class="comment">// 调用ensureCapacityInternal，参数是集合当前的长度。确保集合容量够大，不够的话需要扩容</span>
    elementData[size++] = e; <span class="comment">// 数组容量够的话，直接添加元素到数组最后一个位置即可，同时修改集合当前有效长度</span>
    <span class="keyword">return</span> <span class="literal">true</span>;
}

<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureCapacityInternal</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>{
    <span class="keyword">if</span> (elementData == EMPTY_ELEMENTDATA) { <span class="comment">// 如果数组是个空数组，说明调用的是无参的构造函数</span>
        <span class="comment">// 如果调用的是无参构造函数，说明数组容量为0，那就需要使用默认容量</span>
        minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity);
    }

    ensureExplicitCapacity(minCapacity);
}    

<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureExplicitCapacity</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>{
    modCount++;

    <span class="comment">// 如果集合需要的最小长度比数组容量要大，那么就需要扩容，已经放不下了</span>
    <span class="keyword">if</span> (minCapacity - elementData.length &gt; <span class="number">0</span>)
        grow(minCapacity);
}

<span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">grow</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>{ <span class="comment">// 扩容的实现</span>
    <span class="keyword">int</span> oldCapacity = elementData.length;
    <span class="keyword">int</span> newCapacity = oldCapacity + (oldCapacity &gt;&gt; <span class="number">1</span>); <span class="comment">// 长度扩大1.5倍</span>
    <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>)
        newCapacity = minCapacity;
    <span class="keyword">if</span> (newCapacity - MAX_ARRAY_SIZE &gt; <span class="number">0</span>)
        newCapacity = hugeCapacity(minCapacity);
    <span class="comment">// 将数组拷贝到新长度的数组中</span>
    elementData = Arrays.copyOf(elementData, newCapacity);
}
</code></pre><p>以下面这段代码讲解一下扩容的机制：</p>
<pre><code><span class="comment">// 初始化一个容量为5的数组</span>
ArrayList&lt;Integer&gt; <span class="built_in">list</span> = <span class="keyword">new</span> ArrayList&lt;Integer&gt;(<span class="number">5</span>);
<span class="built_in">list</span>.add(<span class="number">1</span>);
<span class="built_in">list</span>.add(<span class="number">2</span>);
<span class="built_in">list</span>.add(<span class="number">3</span>);
<span class="built_in">list</span>.add(<span class="number">4</span>);
<span class="built_in">list</span>.add(<span class="number">5</span>);
<span class="comment">// 当添加第6个元素的时候，数组进行了扩容，扩容1.5倍(5+5/2=7)</span>
<span class="built_in">list</span>.add(<span class="number">6</span>);
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/arraylist01.jpg" alt=""></p>
<p>上图2个白色的空间就是扩容出来的，添加第6个元素之后，最后一个元素没被设置。</p>
<h3 id="add(int_index,_E_element)_方法">add(int index, E element) 方法</h3><p>这个方法的作用是 <strong>在指定位置插入数据</strong>，该方法的缺点就是如果集合数据量很大，移动元素位置将会话费不少时间：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">void</span> add(<span class="keyword">int</span> <span class="keyword">index</span>, E element) {
    rangeCheckForAdd(<span class="keyword">index</span>); <span class="comment">// 检查索引位置的正确的，不能小于0也不能大于数组有效长度</span>

    ensureCapacityInternal(size + <span class="number">1</span>);  <span class="comment">// 扩容检测</span>
    System.arraycopy(elementData, <span class="keyword">index</span>, elementData, <span class="keyword">index</span> + <span class="number">1</span>,
                     size - <span class="keyword">index</span>); <span class="comment">// 移动数组位置，数据量很大的话，性能变差</span>
    elementData[<span class="keyword">index</span>] = element; <span class="comment">// 指定的位置插入数据</span>
    size++; <span class="comment">// 数组有效长度+1</span>
}
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/arraylist03.jpg" alt=""></p>
<p>上图就表示要在容量为5的数组中的第4个位置插入6这个元素，会进行3个步骤：</p>
<ol>
<li>容量为5，再次加入元素，需要扩容，扩容出2个白色的空间</li>
<li>扩容之后，5和4这2个元素都移到后面那个位置上</li>
<li>移动完毕之后空出了第4个位置，插入元素6</li>
</ol>
<h3 id="remove(int_index)">remove(int index)</h3><p>remove方法就是 <strong>移除对应坐标值上的数据</strong></p>
<pre><code><span class="keyword">public</span> E remove(<span class="keyword">int</span> <span class="keyword">index</span>) {
  rangeCheck(<span class="keyword">index</span>); <span class="comment">// 检查索引值是否合法</span>

  modCount++;
  E oldValue = elementData(<span class="keyword">index</span>); <span class="comment">// 得到对应索引位置上的元素</span>

  <span class="keyword">int</span> numMoved = size - <span class="keyword">index</span> - <span class="number">1</span>; <span class="comment">// 需要移动的数量</span>
  <span class="keyword">if</span> (numMoved &gt; <span class="number">0</span>)
      System.arraycopy(elementData, <span class="keyword">index</span>+<span class="number">1</span>, elementData, <span class="keyword">index</span>,
                       numMoved); <span class="comment">// 从后往前移，留出最后一个元素</span>
  elementData[--size] = <span class="keyword">null</span>; <span class="comment">// 清楚对应位置上的对象，让gc回收</span>

  <span class="keyword">return</span> oldValue;
}
</code></pre><p>比如要移除5个元素中的第3个元素，首先要把4和5这2个位置的元素分别set到3和4这2个位置上，set完之后最后一个位置也就是第5个位置set为null。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/arraylist02.jpg" alt=""></p>
<h3 id="remove(Object_o)">remove(Object o)</h3><p> <strong>找出数组中的元素，然后移除</strong></p>
<pre><code><span class="comment">// 跟remove索引元素一样，这个方法是根据equals比较</span>
<span class="keyword">public</span> <span class="keyword">boolean</span> remove(Object o) {
    <span class="keyword">if</span> (o == <span class="keyword">null</span>) {
        <span class="comment">// ArrayList允许元素为null，所以对null值的删除在这个分支里进行</span>
        <span class="keyword">for</span> (<span class="keyword">int</span> <span class="keyword">index</span> = <span class="number">0</span>; <span class="keyword">index</span> &lt; size; <span class="keyword">index</span>++)
            <span class="keyword">if</span> (elementData[<span class="keyword">index</span>] == <span class="keyword">null</span>) {
                fastRemove(<span class="keyword">index</span>);
                <span class="keyword">return</span> <span class="keyword">true</span>;
            }
    } <span class="keyword">else</span> {
        <span class="comment">// 效率比较低，需要从第1个元素开始遍历直到找到equals相等的元素后才进行删除，删除同样需要移动元素</span>
        <span class="keyword">for</span> (<span class="keyword">int</span> <span class="keyword">index</span> = <span class="number">0</span>; <span class="keyword">index</span> &lt; size; <span class="keyword">index</span>++)
            <span class="keyword">if</span> (o.equals(elementData[<span class="keyword">index</span>])) {
                fastRemove(<span class="keyword">index</span>);
                <span class="keyword">return</span> <span class="keyword">true</span>;
            }
    }
    <span class="keyword">return</span> <span class="keyword">false</span>;
}
</code></pre><h3 id="clear">clear</h3><p><strong>清除list中的所有数据</strong></p>
<pre><code><span class="keyword">public</span> <span class="keyword">void</span> <span class="built_in">clear</span>() {
  modCount++;

  <span class="comment">// 遍历集合数据，全部set为null</span>
  <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">size</span>; i++)
      elementData[i] = <span class="keyword">null</span>;

  <span class="built_in">size</span> = <span class="number">0</span>; <span class="comment">// 数组有效长度变成0</span>
}
</code></pre><h3 id="set(int_index,_E_element)">set(int index, E element)</h3><p><strong>用element值替换下标值为index的值</strong></p>
<pre><code><span class="keyword">public</span> E set(<span class="keyword">int</span> <span class="keyword">index</span>, E element) {
  rangeCheck(<span class="keyword">index</span>); <span class="comment">// 检查索引值是否合法</span>

  E oldValue = elementData(<span class="keyword">index</span>); 
  elementData[<span class="keyword">index</span>] = element; <span class="comment">// 直接替换</span>
  <span class="keyword">return</span> oldValue;
}
</code></pre><h3 id="get(int_index)">get(int index)</h3><p><strong>得到下标值为index的元素</strong></p>
<pre><code><span class="keyword">public</span> E get(<span class="keyword">int</span> <span class="keyword">index</span>) {
    rangeCheck(<span class="keyword">index</span>); <span class="comment">// 检查索引值是否合法</span>

    <span class="keyword">return</span> elementData(<span class="keyword">index</span>); <span class="comment">// 直接返回下标值</span>
}
</code></pre><h3 id="addAll">addAll</h3><p><strong>在列表的结尾添加一个Collection集合</strong></p>
<pre><code><span class="keyword">public</span> <span class="keyword">boolean</span> addAll(Collection&lt;? <span class="keyword">extends</span> E&gt; c) {
    Object[] a = c.toArray();
    <span class="keyword">int</span> numNew = a.length;
    ensureCapacityInternal(<span class="keyword">size</span> + numNew);  <span class="comment">// 扩容检测</span>
    System.arraycopy(a, <span class="number">0</span>, elementData, <span class="keyword">size</span>, numNew); <span class="comment">// 直接在数组后面添加新的数组中的所有元素</span>
    <span class="keyword">size</span> += numNew; <span class="comment">// 更新有效长度</span>
    <span class="keyword">return</span> numNew != <span class="number">0</span>;
}
</code></pre><h3 id="toArray">toArray</h3><p><strong>根据elementData数组拷贝一份新的数组</strong></p>
<pre><code><span class="keyword">public</span> <span class="keyword">Object</span>[] toArray() {
    <span class="keyword">return</span> Arrays.copyOf(elementData, <span class="built_in">size</span>);
}
</code></pre><h2 id="ArrayList的注意点">ArrayList的注意点</h2><ol>
<li>当数据量很大的时候，ArrayList内部操作元素的时候会移动位置，很耗性能</li>
<li>ArrayList虽然可以自动扩展长度，但是数据量一大，扩展的也多，会造成很多空间的浪费</li>
<li>ArrayList有一个内部私有类，SubList。ArrayList提供一个subList方法用于构造这个SubList。这里需要注意的是SubList和ArrayList使用的数据引用是同一个对象，在SubList中操作数据和在ArrayList中操作数据都会影响双方。</li>
<li>ArrayList允许加入null元素</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>list是一种有序的集合(an ordered collection), 通常也会被称为序列(sequence)，使用list可以精确地控制每个元素的插入，可以通过索引值找到对应list中的各个项，也可以在list中查询元素。</p>
<p>以前的几段话摘自jdk文档的说明。</p>
<p>其实list就相当于一个动态的数组，也就是链表，普通的数组长度大小都是固定的，而list是一个动态的数组，当list的长度满了，再次插入数据到list当中的时候，list会自动地扩展它的长度。</p>]]>
    
    </summary>
    
      <category term="collection" scheme="http://fangjian0423.github.io/tags/collection/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/categories/jdk/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java线程池ThreadPoolExecutor源码分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/22/java-threadpool-analysis/"/>
    <id>http://fangjian0423.github.io/2016/03/22/java-threadpool-analysis/</id>
    <published>2016-03-22T12:56:11.000Z</published>
    <updated>2016-03-22T12:56:15.000Z</updated>
    <content type="html"><![CDATA[<p>ThreadPoolExecutor是jdk内置线程池的一个实现，基本上大部分情况都会使用这个线程池完成各项操作。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/thread-pool.jpeg" alt=""></p>
<a id="more"></a>
<p>本文分析ThreadPoolExecutor的实现原理。</p>
<h2 id="ThreadPoolExecutor的状态和属性">ThreadPoolExecutor的状态和属性</h2><p>ThreadPoolExecutor的属性在之前的一篇<a href="http://fangjian0423.github.io/2015/07/24/java-poolthread/">java内置的线程池笔记</a>文章中解释过了，本文不再解释。</p>
<p>ThreadPoolExecutor线程池有5个状态，分别是：</p>
<ol>
<li>RUNNING：可以接受新的任务，也可以处理阻塞队列里的任务</li>
<li>SHUTDOWN：不接受新的任务，但是可以处理阻塞队列里的任务</li>
<li>STOP：不接受新的任务，不处理阻塞队列里的任务，中断正在处理的任务</li>
<li>TIDYING：过渡状态，也就是说所有的任务都执行完了，当前线程池已经没有有效的线程，这个时候线程池的状态将会TIDYING，并且将要调用terminated方法</li>
<li>TERMINATED：终止状态。terminated方法调用完成以后的状态</li>
</ol>
<p>状态之间可以进行转换：</p>
<p>RUNNING -&gt; SHUTDOWN：手动调用shutdown方法，或者ThreadPoolExecutor要被GC回收的时候调用finalize方法，finalize方法内部也会调用shutdown方法</p>
<p>(RUNNING or SHUTDOWN) -&gt; STOP：调用shutdownNow方法</p>
<p>SHUTDOWN -&gt; TIDYING：当队列和线程池都为空的时候</p>
<p>STOP -&gt; TIDYING：当线程池为空的时候</p>
<p>TIDYING -&gt; TERMINATED：terminated方法调用完成之后</p>
<p>ThreadPoolExecutor内部还保存着线程池的有效线程个数。</p>
<p>状态和线程数在ThreadPoolExecutor内部使用一个整型变量保存，没错，一个变量表示两种含义。</p>
<p>为什么一个整型变量既可以保存状态，又可以保存数量？ 分析一下：</p>
<p>首先，我们知道java中1个整型占4个字节，也就是32位，所以1个整型有32位。</p>
<p>所以整型1用二进制表示就是：00000000000000000000000000000001</p>
<p>整型-1用二进制表示就是：11111111111111111111111111111111(这个是补码，不懂的同学可以看下原码，反码，补码的知识)</p>
<p>在ThreadPoolExecutor，整型中32位的前3位用来表示线程池状态，后3位表示线程池中有效的线程数。</p>
<pre><code><span class="comment">// 前3位表示状态，所有线程数占29位</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> COUNT_BITS = Integer.<span class="keyword">SIZE</span> - <span class="number">3</span>;
</code></pre><p>线程池容量大小为 1 &lt;&lt; 29 - 1 = 00011111111111111111111111111111(二进制)，代码如下</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> final <span class="keyword">int</span> CAPACITY   = (<span class="number">1</span> &lt;&lt; COUNT_BITS) - <span class="number">1</span>;
</code></pre><p>RUNNING状态 -1 &lt;&lt; 29 = 11111111111111111111111111111111 &lt;&lt; 29 = 11100000000000000000000000000000(前3位为111)：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> RUNNING    = -<span class="number">1</span> &lt;&lt; COUNT_BITS;
</code></pre><p>SHUTDOWN状态 0 &lt;&lt; 29 = 00000000000000000000000000000000 &lt;&lt; 29 = 00000000000000000000000000000000(前3位为000)</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SHUTDOWN   =  <span class="number">0</span> &lt;&lt; COUNT_BITS;
</code></pre><p>STOP状态 1 &lt;&lt; 29 = 00000000000000000000000000000001 &lt;&lt; 29 = 00100000000000000000000000000000(前3位为001)：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> STOP       =  <span class="number">1</span> &lt;&lt; COUNT_BITS;
</code></pre><p>TIDYING状态 2 &lt;&lt; 29 = 00000000000000000000000000000010 &lt;&lt; 29 = 01000000000000000000000000000000(前3位为010)：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TIDYING    =  <span class="number">2</span> &lt;&lt; COUNT_BITS;
</code></pre><p>TERMINATED状态 3 &lt;&lt; 29 = 00000000000000000000000000000011 &lt;&lt; 29 = 01100000000000000000000000000000(前3位为011)：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TERMINATED =  <span class="number">3</span> &lt;&lt; COUNT_BITS;    
</code></pre><p>清楚状态位之后，下面是获得状态和线程数的内部方法：</p>
<pre><code><span class="comment">// 得到线程数，也就是后29位的数字。 直接跟CAPACITY做一个与操作即可，CAPACITY就是的值就 1 &lt;&lt; 29 - 1 = 00011111111111111111111111111111。 与操作的话前面3位肯定为0，相当于直接取后29位的值</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="function"><span class="keyword">int</span> <span class="title">workerCountOf</span><span class="params">(<span class="keyword">int</span> c)</span>  </span>{ <span class="keyword">return</span> c &amp; CAPACITY; }

<span class="comment">// 得到状态，CAPACITY的非操作得到的二进制位11100000000000000000000000000000，然后做在一个与操作，相当于直接取前3位的的值</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="function"><span class="keyword">int</span> <span class="title">runStateOf</span><span class="params">(<span class="keyword">int</span> c)</span>     </span>{ <span class="keyword">return</span> c &amp; ~CAPACITY; }

<span class="comment">// 或操作。相当于更新数量和状态两个操作</span>
<span class="keyword">private</span> <span class="keyword">static</span> <span class="function"><span class="keyword">int</span> <span class="title">ctlOf</span><span class="params">(<span class="keyword">int</span> rs, <span class="keyword">int</span> wc)</span> </span>{ <span class="keyword">return</span> rs | wc; }
</code></pre><p>线程池初始化状态线程数变量：</p>
<pre><code><span class="comment">// 初始化状态和数量，状态为RUNNING，线程数为0</span>
<span class="keyword">private</span> <span class="keyword">final</span> AtomicInteger ctl = <span class="keyword">new</span> AtomicInteger(ctlOf(RUNNING, <span class="number">0</span>));
</code></pre><h2 id="ThreadPoolExecutor执行任务">ThreadPoolExecutor执行任务</h2><p>使用ThreadPoolExecutor执行任务的时候，可以使用execute或submit方法，submit方法如下：</p>
<pre><code><span class="keyword">public</span> Future&lt;?&gt; submit(Runnable <span class="keyword">task</span>) {
    <span class="keyword">if</span> (<span class="keyword">task</span> == <span class="keyword">null</span>) <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();
    RunnableFuture&lt;<span class="keyword">Void</span>&gt; ftask = newTaskFor(<span class="keyword">task</span>, <span class="keyword">null</span>);
    execute(ftask);
    <span class="keyword">return</span> ftask;
}
</code></pre><p>很明显地看到，submit方法内部使用了execute方法，而且submit方法是有返回值的。在调用execute方法之前，使用FutureTask包装一个Runnable，这个FutureTask就是返回值。</p>
<p>由于submit方法内部调用execute方法，所以execute方法就是执行任务的方法，来看一下execute方法，execute方法内部分3个步骤进行处理。</p>
<ol>
<li>如果当前正在执行的Worker数量比corePoolSize(基本大小)要小。直接创建一个新的Worker执行任务，会调用addWorker方法</li>
<li>如果当前正在执行的Worker数量大于等于corePoolSize(基本大小)。将任务放到阻塞队列里，如果阻塞队列没满并且状态是RUNNING的话，直接丢到阻塞队列，否则执行第3步。丢到阻塞队列之后，还需要再做一次验证(丢到阻塞队列之后可能另外一个线程关闭了线程池或者刚刚加入到队列的线程死了)。如果这个时候线程池不在RUNNING状态，把刚刚丢入队列的任务remove掉，调用reject方法，否则查看Worker数量，如果Worker数量为0，起一个新的Worker去阻塞队列里拿任务执行</li>
<li>丢到阻塞失败的话，会调用addWorker方法尝试起一个新的Worker去阻塞队列拿任务并执行任务，如果这个新的Worker创建失败，调用reject方法</li>
</ol>
<p>上面说的Worker可以暂时理解为一个执行任务的线程。</p>
<p>execute方法源码如下，上面提到的3个步骤对应源码中的3个注释：</p>
<pre><code><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span>(<span class="params">Runnable command</span>) </span>{
    <span class="keyword">if</span> (command == <span class="keyword">null</span>)
        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();
    <span class="keyword">int</span> c = ctl.<span class="keyword">get</span>();
    <span class="keyword">if</span> (workerCountOf(c) &lt; corePoolSize) {   <span class="comment">// 第一个步骤，满足线程池中的线程大小比基本大小要小</span>
        <span class="keyword">if</span> (addWorker(command, <span class="keyword">true</span>)) <span class="comment">// addWorker方法第二个参数true表示使用基本大小</span>
            <span class="keyword">return</span>;
        c = ctl.<span class="keyword">get</span>();
    }
    <span class="keyword">if</span> (isRunning(c) &amp;&amp; workQueue.offer(command)) { <span class="comment">// 第二个步骤，线程池的线程大小比基本大小要大，并且线程池还在RUNNING状态，阻塞队列也没满的情况，加到阻塞队列里</span>
        <span class="keyword">int</span> recheck = ctl.<span class="keyword">get</span>();
        <span class="keyword">if</span> (! isRunning(recheck) &amp;&amp; remove(command)) <span class="comment">// 虽然满足了第二个步骤，但是这个时候可能突然线程池关闭了，所以再做一层判断</span>
            reject(command);
        <span class="function"><span class="keyword">else</span> <span class="title">if</span> (<span class="params">workerCountOf(recheck</span>) </span>== <span class="number">0</span>)
            addWorker(<span class="keyword">null</span>, <span class="keyword">false</span>);
    }
    <span class="function"><span class="keyword">else</span> <span class="title">if</span> (<span class="params">!addWorker(command, <span class="keyword">false</span></span>)) <span class="comment">// 第三个步骤，直接使用线程池最大大小。addWorker方法第二个参数false表示使用最大大小</span>
        <span class="title">reject</span>(<span class="params">command</span>)</span>;
}
</code></pre><p>addWorker关系着如何起一个线程，再看addWorker方法之前，先看一下ThreadPoolExecutor的一个内部类Worker, Worker是一个AQS的实现类(为何设计成一个AQS在闲置Worker里会说明)，同时也是一个实现Runnable的类，使用独占锁，它的构造函数只接受一个Runnable参数，内部保存着这个Runnable属性，还有一个thread线程属性用于包装这个Runnable(这个thread属性使用ThreadFactory构造，在构造函数内完成thread线程的构造)，另外还有一个completedTasks计数器表示这个Worker完成的任务数。Worker类复写了run方法，使用ThreadPoolExecutor的runWorker方法(在addWorker方法里调用)，直接启动Worker的话，会调用ThreadPoolExecutor的runWork方法。<strong>需要特别注意的是这个Worker是实现了Runnable接口的，thread线程属性使用ThreadFactory构造Thread的时候，构造的Thread中使用的Runnable其实就是Worker。</strong>下面的Worker的源码：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Worker</span>    
    <span class="keyword">extends</span> <span class="title">AbstractQueuedSynchronizer</span>
    <span class="keyword">implements</span> <span class="title">Runnable</span>
</span>{
    <span class="comment">/**
     * This class will never be serialized, but we provide a
     * serialVersionUID to suppress a javac warning.
     */</span>
    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">6138294804551838833</span>L;

    <span class="comment">/** Thread this worker is running in.  Null if factory fails. */</span>
    <span class="keyword">final</span> Thread thread;
    <span class="comment">/** Initial task to run.  Possibly null. */</span>
    Runnable firstTask;
    <span class="comment">/** Per-thread task counter */</span>
    <span class="keyword">volatile</span> <span class="keyword">long</span> completedTasks;

    <span class="comment">/**
     * Creates with given first task and thread from ThreadFactory.
     * <span class="doctag">@param</span> firstTask the first task (null if none)
     */</span>
    Worker(Runnable firstTask) {
        <span class="comment">// 使用ThreadFactory构造Thread，这个构造的Thread内部的Runnable就是本身，也就是Worker。所以得到Worker的thread并start的时候，会执行Worker的run方法，也就是执行ThreadPoolExecutor的runWorker方法</span>
        setState(-<span class="number">1</span>); 把状态位设置成-<span class="number">1</span>，这样任何线程都不能得到Worker的锁，除非调用了unlock方法。这个unlock方法会在runWorker方法中一开始就调用，这是为了确保Worker构造出来之后，没有任何线程能够得到它的锁，除非调用了runWorker之后，其他线程才能获得Worker的锁
        <span class="keyword">this</span>.firstTask = firstTask;
        <span class="keyword">this</span>.thread = getThreadFactory().newThread(<span class="keyword">this</span>);
    }

    <span class="comment">/** Delegates main run loop to outer runWorker  */</span>
    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
        runWorker(<span class="keyword">this</span>);
    }

    <span class="comment">// Lock methods</span>
    <span class="comment">//</span>
    <span class="comment">// The value 0 represents the unlocked state.</span>
    <span class="comment">// The value 1 represents the locked state.</span>

    <span class="keyword">protected</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isHeldExclusively</span><span class="params">()</span> </span>{
        <span class="keyword">return</span> getState() != <span class="number">0</span>;
    }

    <span class="keyword">protected</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> unused)</span> </span>{
        <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, <span class="number">1</span>)) {
            setExclusiveOwnerThread(Thread.currentThread());
            <span class="keyword">return</span> <span class="keyword">true</span>;
        }
        <span class="keyword">return</span> <span class="keyword">false</span>;
    }

    <span class="keyword">protected</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryRelease</span><span class="params">(<span class="keyword">int</span> unused)</span> </span>{
        setExclusiveOwnerThread(<span class="keyword">null</span>);
        setState(<span class="number">0</span>);
        <span class="keyword">return</span> <span class="keyword">true</span>;
    }

    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span>        </span>{ acquire(<span class="number">1</span>); }
    <span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryLock</span><span class="params">()</span>  </span>{ <span class="function"><span class="keyword">return</span> <span class="title">tryAcquire</span><span class="params">(<span class="number">1</span>)</span></span>; }
    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">unlock</span><span class="params">()</span>      </span>{ release(<span class="number">1</span>); }
    <span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isLocked</span><span class="params">()</span> </span>{ <span class="function"><span class="keyword">return</span> <span class="title">isHeldExclusively</span><span class="params">()</span></span>; }

    <span class="function"><span class="keyword">void</span> <span class="title">interruptIfStarted</span><span class="params">()</span> </span>{
        Thread t;
        <span class="keyword">if</span> (getState() &gt;= <span class="number">0</span> &amp;&amp; (t = thread) != <span class="keyword">null</span> &amp;&amp; !t.isInterrupted()) {
            <span class="keyword">try</span> {
                t.interrupt();
            } <span class="keyword">catch</span> (SecurityException ignore) {
            }
        }
    }
}
</code></pre><p>接下来看一下addWorker源码：</p>
<pre><code><span class="comment">// 两个参数，firstTask表示需要跑的任务。boolean类型的core参数为true的话表示使用线程池的基本大小，为false使用线程池最大大小</span>
<span class="comment">// 返回值是boolean类型，true表示新任务被接收了，并且执行了。否则是false</span>
<span class="keyword">private</span> <span class="built_in">boolean</span> addWorker(Runnable firstTask, <span class="built_in">boolean</span> core) {
    retry:
    <span class="keyword">for</span> (;;) {
        <span class="built_in">int</span> c = ctl.<span class="built_in">get</span>();
        <span class="built_in">int</span> rs = runStateOf(c); <span class="comment">// 线程池当前状态</span>

        <span class="comment">// 这个判断转换成 rs &gt;= SHUTDOWN &amp;&amp; (rs != SHUTDOWN || firstTask != null || workQueue.isEmpty)。 </span>
        <span class="comment">// 概括为3个条件：</span>
        <span class="comment">// 1. 线程池不在RUNNING状态并且状态是STOP、TIDYING或TERMINATED中的任意一种状态</span>

        <span class="comment">// 2. 线程池不在RUNNING状态，线程池接受了新的任务 </span>

        <span class="comment">// 3. 线程池不在RUNNING状态，阻塞队列为空。  满足这3个条件中的任意一个的话，拒绝执行任务</span>

        <span class="keyword">if</span> (rs &gt;= SHUTDOWN &amp;&amp;
            ! (rs == SHUTDOWN &amp;&amp;
               firstTask == <span class="keyword">null</span> &amp;&amp;
               ! workQueue.isEmpty()))
            <span class="keyword">return</span> <span class="keyword">false</span>;

        <span class="keyword">for</span> (;;) {
            <span class="built_in">int</span> wc = workerCountOf(c); <span class="comment">// 线程池线程个数</span>
            <span class="keyword">if</span> (wc &gt;= CAPACITY ||
                wc &gt;= (core ? corePoolSize : maximumPoolSize)) <span class="comment">// 如果线程池线程数量超过线程池最大容量或者线程数量超过了基本大小(core参数为true，core参数为false的话判断超过最大大小)</span>
                <span class="keyword">return</span> <span class="keyword">false</span>; <span class="comment">// 超过直接返回false</span>
            <span class="keyword">if</span> (compareAndIncrementWorkerCount(c)) <span class="comment">// 没有超过各种大小的话，cas操作线程池线程数量+1，cas成功的话跳出循环</span>
                <span class="keyword">break</span> retry;
            c = ctl.<span class="built_in">get</span>();  <span class="comment">// 重新检查状态</span>
            <span class="keyword">if</span> (runStateOf(c) != rs) <span class="comment">// 如果状态改变了，重新循环操作</span>
                <span class="keyword">continue</span> retry;
            <span class="comment">// else CAS failed due to workerCount change; retry inner loop</span>
        }
    }
    <span class="comment">// 走到这一步说明cas操作成功了，线程池线程数量+1</span>
    <span class="built_in">boolean</span> workerStarted = <span class="keyword">false</span>; <span class="comment">// 任务是否成功启动标识</span>
    <span class="built_in">boolean</span> workerAdded = <span class="keyword">false</span>; <span class="comment">// 任务是否添加成功标识</span>
    Worker w = <span class="keyword">null</span>;
    <span class="keyword">try</span> {
        <span class="keyword">final</span> ReentrantLock mainLock = <span class="keyword">this</span>.mainLock; <span class="comment">// 得到线程池的可重入锁</span>
        w = <span class="keyword">new</span> Worker(firstTask); <span class="comment">// 基于任务firstTask构造worker</span>
        <span class="keyword">final</span> Thread t = w.thread; <span class="comment">// 使用Worker的属性thread，这个thread是使用ThreadFactory构造出来的</span>
        <span class="keyword">if</span> (t != <span class="keyword">null</span>) { <span class="comment">// ThreadFactory构造出的Thread有可能是null，做个判断</span>
            mainLock.lock(); <span class="comment">// 锁住，防止并发</span>
            <span class="keyword">try</span> {
                <span class="comment">// 在锁住之后再重新检测一下状态</span>
                <span class="built_in">int</span> c = ctl.<span class="built_in">get</span>();
                <span class="built_in">int</span> rs = runStateOf(c);

                <span class="keyword">if</span> (rs &lt; SHUTDOWN ||
                    (rs == SHUTDOWN &amp;&amp; firstTask == <span class="keyword">null</span>)) { <span class="comment">// 如果线程池在RUNNING状态或者线程池在SHUTDOWN状态并且任务是个null</span>
                    <span class="keyword">if</span> (t.isAlive()) <span class="comment">// 判断线程是否还活着，也就是说线程已经启动并且还没死掉</span>
                        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalThreadStateException(); <span class="comment">// 如果存在已经启动并且还没死的线程，抛出异常</span>
                    workers.<span class="built_in">add</span>(w); <span class="comment">// worker添加到线程池的workers属性中，是个HashSet</span>
                    <span class="built_in">int</span> s = workers.<span class="built_in">size</span>(); <span class="comment">// 得到目前线程池中的线程个数</span>
                    <span class="keyword">if</span> (s &gt; largestPoolSize) <span class="comment">// 如果线程池中的线程个数超过了线程池中的最大线程数时，更新一下这个最大线程数</span>
                        largestPoolSize = s;
                    workerAdded = <span class="keyword">true</span>; <span class="comment">// 标识一下任务已经添加成功</span>
                }
            } <span class="keyword">finally</span> {
                mainLock.unlock(); <span class="comment">// 解锁</span>
            }
            <span class="keyword">if</span> (workerAdded) { <span class="comment">// 如果任务添加成功，运行任务，改变一下任务成功启动标识</span>
                t.start(); <span class="comment">// 启动线程，这里的t是Worker中的thread属性，所以相当于就是调用了Worker的run方法</span>
                workerStarted = <span class="keyword">true</span>;
            }
        }
    } <span class="keyword">finally</span> {
        <span class="keyword">if</span> (! workerStarted) <span class="comment">// 如果任务启动失败，调用addWorkerFailed方法</span>
            addWorkerFailed(w);
    }
    <span class="keyword">return</span> workerStarted;
}
</code></pre><p>Worker中的线程start的时候，调用Worker本身run方法，这个run方法之前分析过，调用外部类ThreadPoolExecutor的runWorker方法，直接看runWorker方法：</p>
<pre><code><span class="keyword">final</span> <span class="keyword">void</span> runWorker(Worker w) {
    Thread wt = Thread.currentThread(); <span class="comment">// 得到当前线程</span>
    Runnable <span class="keyword">task</span> = w.firstTask; <span class="comment">// 得到Worker中的任务task，也就是用户传入的task</span>
    w.firstTask = <span class="keyword">null</span>; <span class="comment">// 将Worker中的任务置空</span>
    w.unlock(); <span class="comment">// allow interrupts。 </span>
    <span class="keyword">boolean</span> completedAbruptly = <span class="keyword">true</span>;
    <span class="keyword">try</span> {
        <span class="comment">// 如果worker中的任务不为空，继续知否，否则使用getTask获得任务。一直死循环，除非得到的任务为空才退出</span>
        <span class="keyword">while</span> (<span class="keyword">task</span> != <span class="keyword">null</span> || (<span class="keyword">task</span> = getTask()) != <span class="keyword">null</span>) {
            w.lock();  <span class="comment">// 如果拿到了任务，给自己上锁，表示当前Worker已经要开始执行任务了，已经不是闲置Worker(闲置Worker的解释请看下面的线程池关闭)</span>
            <span class="comment">// 在执行任务之前先做一些处理。 1. 如果线程池已经处于STOP状态并且当前线程没有被中断，中断线程 2. 如果线程池还处于RUNNING或SHUTDOWN状态，并且当前线程已经被中断了，重新检查一下线程池状态，如果处于STOP状态并且没有被中断，那么中断线程</span>
            <span class="keyword">if</span> ((runStateAtLeast(ctl.get(), STOP) ||
                 (Thread.interrupted() &amp;&amp;
                  runStateAtLeast(ctl.get(), STOP))) &amp;&amp;
                !wt.isInterrupted())
                wt.interrupt();
            <span class="keyword">try</span> {
                beforeExecute(wt, <span class="keyword">task</span>); <span class="comment">// 任务执行前需要做什么，ThreadPoolExecutor是个空实现</span>
                Throwable thrown = <span class="keyword">null</span>;
                <span class="keyword">try</span> {
                    <span class="keyword">task</span>.run(); <span class="comment">// 真正的开始执行任务，调用的是run方法，而不是start方法。这里run的时候可能会被中断，比如线程池调用了shutdownNow方法</span>
                } <span class="keyword">catch</span> (RuntimeException x) { <span class="comment">// 任务执行发生的异常全部抛出，不在runWorker中处理</span>
                    thrown = x; <span class="keyword">throw</span> x;
                } <span class="keyword">catch</span> (Error x) {
                    thrown = x; <span class="keyword">throw</span> x;
                } <span class="keyword">catch</span> (Throwable x) {
                    thrown = x; <span class="keyword">throw</span> <span class="keyword">new</span> Error(x);
                } <span class="keyword">finally</span> {
                    afterExecute(<span class="keyword">task</span>, thrown); <span class="comment">// 任务执行结束需要做什么，ThreadPoolExecutor是个空实现</span>
                }
            } <span class="keyword">finally</span> {
                <span class="keyword">task</span> = <span class="keyword">null</span>;
                w.completedTasks++; <span class="comment">// 记录执行任务的个数</span>
                w.unlock(); <span class="comment">// 执行完任务之后，解锁，Worker变成闲置Worker</span>
            }
        }
        completedAbruptly = <span class="keyword">false</span>;
    } <span class="keyword">finally</span> {
        processWorkerExit(w, completedAbruptly); <span class="comment">// 回收Worker方法</span>
    }
}
</code></pre><p>我们看一下getTask方法是如何获得任务的：</p>
<pre><code><span class="comment">// 如果发生了以下四件事中的任意一件，那么Worker需要被回收：</span>
<span class="comment">// 1. Worker个数比线程池最大大小要大</span>
<span class="comment">// 2. 线程池处于STOP状态</span>
<span class="comment">// 3. 线程池处于SHUTDOWN状态并且阻塞队列为空</span>
<span class="comment">// 4. 使用超时时间从阻塞队列里拿数据，并且超时之后没有拿到数据(allowCoreThreadTimeOut || workerCount &gt; corePoolSize)</span>
<span class="keyword">private</span> <span class="function">Runnable <span class="title">getTask</span><span class="params">()</span> </span>{
    <span class="keyword">boolean</span> timedOut = <span class="keyword">false</span>; <span class="comment">// 如果使用超时时间并且也没有拿到任务的标识</span>

    retry:
    <span class="keyword">for</span> (;;) {
        <span class="keyword">int</span> c = ctl.get();
        <span class="keyword">int</span> rs = runStateOf(c);

        <span class="comment">// 如果线程池是SHUTDOWN状态并且阻塞队列为空的话，worker数量减一，直接返回null(SHUTDOWN状态还会处理阻塞队列任务，但是阻塞队列为空的话就结束了)，如果线程池是STOP状态的话，worker数量建议，直接返回null(STOP状态不处理阻塞队列任务)[方法一开始注释的2，3两点，返回null，开始Worker回收]</span>
        <span class="keyword">if</span> (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) {
            decrementWorkerCount();
            <span class="keyword">return</span> <span class="keyword">null</span>;
        }

        <span class="keyword">boolean</span> timed;      <span class="comment">// 标记从队列中取任务时是否设置超时时间，如果为true说明这个worker可能需要回收，为false的话这个worker会一直存在，并且阻塞当前线程等待阻塞队列中有数据</span>

        <span class="keyword">for</span> (;;) {
            <span class="keyword">int</span> wc = workerCountOf(c); <span class="comment">// 得到当前线程池Worker个数</span>
            <span class="comment">// allowCoreThreadTimeOut属性默认为false，表示线程池中的核心线程在闲置状态下还保留在池中；如果是true表示核心线程使用keepAliveTime这个参数来作为超时时间</span>
            <span class="comment">// 如果worker数量比基本大小要大的话，timed就为true，需要进行回收worker</span>
            timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; 

            <span class="keyword">if</span> (wc &lt;= maximumPoolSize &amp;&amp; ! (timedOut &amp;&amp; timed)) <span class="comment">// 方法一开始注释的1，4两点，会进行下一步worker数量减一</span>
                <span class="keyword">break</span>;
            <span class="keyword">if</span> (compareAndDecrementWorkerCount(c)) <span class="comment">// worker数量减一，返回null，之后会进行Worker回收工作</span>
                <span class="keyword">return</span> <span class="keyword">null</span>;
            c = ctl.get();  <span class="comment">// 重新检查线程池状态</span>
            <span class="keyword">if</span> (runStateOf(c) != rs) <span class="comment">// 线程池状态改变的话重新开始外部循环，否则继续内部循环</span>
                <span class="keyword">continue</span> retry;
            <span class="comment">// else CAS failed due to workerCount change; retry inner loop</span>
        }

        <span class="keyword">try</span> {
            <span class="comment">// 如果需要设置超时时间，使用poll方法，否则使用take方法一直阻塞等待阻塞队列新进数据</span>
            Runnable r = timed ?
                workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :
                workQueue.take();
            <span class="keyword">if</span> (r != <span class="keyword">null</span>)
                <span class="keyword">return</span> r;
            timedOut = <span class="keyword">true</span>;
        } <span class="keyword">catch</span> (InterruptedException retry) {
            timedOut = <span class="keyword">false</span>; <span class="comment">// 闲置Worker被中断</span>
        }
    }
}
</code></pre><p>如果getTask返回的是null，那说明阻塞队列已经没有任务并且当前调用getTask的Worker需要被回收，那么会调用processWorkerExit方法进行回收：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">void</span> processWorkerExit(Worker w, <span class="built_in">boolean</span> completedAbruptly) {
    <span class="keyword">if</span> (completedAbruptly) <span class="comment">// 如果Worker没有正常结束流程调用processWorkerExit方法，worker数量减一。如果是正常结束的话，在getTask方法里worker数量已经减一了</span>
        decrementWorkerCount();

    <span class="keyword">final</span> ReentrantLock mainLock = <span class="keyword">this</span>.mainLock;
    mainLock.lock(); <span class="comment">// 加锁，防止并发问题</span>
    <span class="keyword">try</span> {
        completedTaskCount += w.completedTasks; <span class="comment">// 记录总的完成任务数</span>
        workers.remove(w); <span class="comment">// 线程池的worker集合删除掉需要回收的Worker</span>
    } <span class="keyword">finally</span> {
        mainLock.unlock(); <span class="comment">// 解锁</span>
    }

    tryTerminate(); <span class="comment">// 尝试结束线程池</span>

    <span class="built_in">int</span> c = ctl.<span class="built_in">get</span>();
    <span class="keyword">if</span> (runStateLessThan(c, STOP)) {  <span class="comment">// 如果线程池还处于RUNNING或者SHUTDOWN状态</span>
        <span class="keyword">if</span> (!completedAbruptly) { <span class="comment">// Worker是正常结束流程的话</span>
            <span class="built_in">int</span> <span class="built_in">min</span> = allowCoreThreadTimeOut ? <span class="number">0</span> : corePoolSize;
            <span class="keyword">if</span> (<span class="built_in">min</span> == <span class="number">0</span> &amp;&amp; ! workQueue.isEmpty())
                <span class="built_in">min</span> = <span class="number">1</span>;
            <span class="keyword">if</span> (workerCountOf(c) &gt;= <span class="built_in">min</span>)
                <span class="keyword">return</span>; <span class="comment">// 不需要新开一个Worker</span>
        }
        <span class="comment">// 新开一个Worker代替原先的Worker</span>
        <span class="comment">// 新开一个Worker需要满足以下3个条件中的任意一个：</span>
        <span class="comment">// 1. 用户执行的任务发生了异常</span>
        <span class="comment">// 2. Worker数量比线程池基本大小要小</span>
        <span class="comment">// 3. 阻塞队列不空但是没有任何Worker在工作</span>
        addWorker(<span class="keyword">null</span>, <span class="keyword">false</span>);
    }
}
</code></pre><p>在回收Worker的时候线程池会尝试结束自己的运行，tryTerminate方法：</p>
<pre><code><span class="keyword">final</span> void tryTerminate() {
    <span class="keyword">for</span> (;;) {
        int <span class="built_in">c</span> = ctl.<span class="keyword">get</span>();
        <span class="comment">// 满足3个条件中的任意一个，不终止线程池</span>
        <span class="comment">// 1. 线程池还在运行，不能终止</span>
        <span class="comment">// 2. 线程池处于TIDYING或TERMINATED状态，说明已经在关闭了，不允许继续处理</span>
        <span class="comment">// 3. 线程池处于SHUTDOWN状态并且阻塞队列不为空，这时候还需要处理阻塞队列的任务，不能终止线程池</span>
        <span class="keyword">if</span> (isRunning(<span class="built_in">c</span>) ||
            runStateAtLeast(<span class="built_in">c</span>, <span class="type">TIDYING</span>) ||
            (runStateOf(<span class="built_in">c</span>) == <span class="type">SHUTDOWN</span> &amp;&amp; ! workQueue.isEmpty()))
            <span class="keyword">return</span>;
        <span class="comment">// 走到这一步说明线程池已经不在运行，阻塞队列已经没有任务，但是还要回收正在工作的Worker</span>
        <span class="keyword">if</span> (workerCountOf(<span class="built_in">c</span>) != <span class="number">0</span>) {
             <span class="comment">// 由于线程池不运行了，调用了线程池的关闭方法，在解释线程池的关闭原理的时候会说道这个方法</span>
            interruptIdleWorkers(<span class="type">ONLY_ONE</span>); <span class="comment">// 中断闲置Worker，直到回收全部的Worker。这里没有那么暴力，只中断一个，中断之后退出方法，中断了Worker之后，Worker会回收，然后还是会调用tryTerminate方法，如果还有闲置线程，那么继续中断</span>
            <span class="keyword">return</span>;
        }
        <span class="comment">// 走到这里说明worker已经全部回收了，并且线程池已经不在运行，阻塞队列已经没有任务。可以准备结束线程池了</span>
        <span class="keyword">final</span> <span class="type">ReentrantLock</span> mainLock = this.mainLock;
        mainLock.lock(); <span class="comment">// 加锁，防止并发</span>
        <span class="keyword">try</span> {
            <span class="keyword">if</span> (ctl.compareAndSet(<span class="built_in">c</span>, ctlOf(<span class="type">TIDYING</span>, <span class="number">0</span>))) { <span class="comment">// cas操作，将线程池状态改成TIDYING</span>
                <span class="keyword">try</span> {
                    terminated(); <span class="comment">// 调用terminated方法</span>
                } finally {
                    ctl.<span class="keyword">set</span>(ctlOf(<span class="type">TERMINATED</span>, <span class="number">0</span>)); <span class="comment">// terminated方法调用完毕之后，状态变为TERMINATED</span>
                    termination.signalAll();
                }
                <span class="keyword">return</span>;
            }
        } finally {
            mainLock.unlock(); <span class="comment">// 解锁</span>
        }
        <span class="comment">// else retry on failed CAS</span>
    }
}
</code></pre><p>解释了这么多，对线程池的启动并且执行任务做一个总结：</p>
<p>首先，构造线程池的时候，需要一些参数。一些重要的参数解释在 <a href="http://fangjian0423.github.io/2015/07/24/java-poolthread/">java内置的线程池笔记</a> 文章中的结尾已经说明了一下重要参数的意义。</p>
<p>线程池构造完毕之后，如果用户调用了execute或者submit方法的时候，最后都会使用execute方法执行。</p>
<p>execute方法内部分3种情况处理任务：</p>
<ol>
<li>如果当前正在执行的Worker数量比corePoolSize(基本大小)要小。直接创建一个新的Worker执行任务，会调用addWorker方法</li>
<li>如果当前正在执行的Worker数量大于等于corePoolSize(基本大小)。将任务放到阻塞队列里，如果阻塞队列没满并且状态是RUNNING的话，直接丢到阻塞队列，否则执行第3步</li>
<li>丢到阻塞失败的话，会调用addWorker方法尝试起一个新的Worker去阻塞队列拿任务并执行任务，如果这个新的Worker创建失败，调用reject方法</li>
</ol>
<p>线程池中的这个基本大小指的是Worker的数量。一个Worker是一个Runnable的实现类，会被当做一个线程进行启动。Worker内部带有一个Runnable属性firstTask，这个firstTask可以为null，为null的话Worker会去阻塞队列拿任务执行，否则会先执行这个任务，执行完毕之后再去阻塞队列继续拿任务执行。</p>
<p>所以说如果Worker数量超过了基本大小，那么任务都会在阻塞队列里，当Worker执行完了它的第一个任务之后，就会去阻塞队列里拿其他任务继续执行。</p>
<p>Worker在执行的时候会根据一些参数进行调节，比如Worker数量超过了线程池基本大小或者超时时间到了等因素，这个时候Worker会被线程池回收，线程池会尽量保持内部的Worker数量不超过基本大小。</p>
<p>另外Worker执行任务的时候调用的是Runnable的run方法，而不是start方法，调用了start方法就相当于另外再起一个线程了。</p>
<p>Worker在回收的时候会尝试终止线程池。尝试关闭线程池的时候，会检查是否还有Worker在工作，检查线程池的状态，没问题的话会将状态过度到TIDYING状态，之后调用terminated方法，terminated方法调用完成之后将线程池状态更新到TERMINATED。</p>
<h2 id="ThreadPoolExecutor的关闭">ThreadPoolExecutor的关闭</h2><p>线程池的启动过程分析好了之后，接下来看线程池的关闭操作：</p>
<p>shutdown方法，关闭线程池，关闭之后阻塞队列里的任务不受影响，会继续被Worker处理，但是新的任务不会被接受：</p>
<pre><code><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">shutdown</span><span class="params">()</span> </span>{
    <span class="keyword">final</span> ReentrantLock mainLock = <span class="keyword">this</span>.mainLock;
    mainLock.lock(); <span class="comment">// 关闭的时候需要加锁，防止并发</span>
    <span class="keyword">try</span> {
        checkShutdownAccess(); <span class="comment">// 检查关闭线程池的权限</span>
        advanceRunState(SHUTDOWN); <span class="comment">// 把线程池状态更新到SHUTDOWN</span>
        interruptIdleWorkers(); <span class="comment">// 中断闲置的Worker</span>
        onShutdown(); <span class="comment">// 钩子方法，默认不处理。ScheduledThreadPoolExecutor会做一些处理</span>
    } <span class="keyword">finally</span> {
        mainLock.unlock(); <span class="comment">// 解锁</span>
    }
    tryTerminate(); <span class="comment">// 尝试结束线程池，上面已经分析过了</span>
}
</code></pre><p>interruptIdleWorkers方法，注意，这个方法打断的是闲置Worker，打断闲置Worker之后，getTask方法会返回null，然后Worker会被回收。那什么是闲置Worker呢？</p>
<p>闲置Worker是这样解释的：Worker运行的时候会去阻塞队列拿数据(getTask方法)，拿的时候如果没有设置超时时间，那么会一直阻塞等待阻塞队列进数据，这样的Worker就被称为闲置Worker。由于Worker也是一个AQS，在runWorker方法里会有一对lock和unlock操作，这对lock操作是为了确保Worker不是一个闲置Worker。</p>
<p>所以Worker被设计成一个AQS是为了根据Worker的锁来判断是否是闲置线程，是否可以被强制中断。</p>
<p>下面我们看下interruptIdleWorkers方法：</p>
<pre><code><span class="comment">// 调用他的一个重载方法，传入了参数false，表示要中断所有的正在运行的闲置Worker，如果为true表示只打断一个闲置Worker</span>
<span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">interruptIdleWorkers</span><span class="params">()</span> </span>{
    interruptIdleWorkers(<span class="keyword">false</span>);
}

<span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">interruptIdleWorkers</span><span class="params">(<span class="keyword">boolean</span> onlyOne)</span> </span>{
    <span class="keyword">final</span> ReentrantLock mainLock = <span class="keyword">this</span>.mainLock;
    mainLock.lock(); <span class="comment">// 中断闲置Worker需要加锁，防止并发</span>
    <span class="keyword">try</span> {
        <span class="keyword">for</span> (Worker w : workers) { 
            Thread t = w.thread; <span class="comment">// 拿到worker中的线程</span>
            <span class="keyword">if</span> (!t.isInterrupted() &amp;&amp; w.tryLock()) { <span class="comment">// Worker中的线程没有被打断并且Worker可以获取锁，这里Worker能获取锁说明Worker是个闲置Worker，在阻塞队列里拿数据一直被阻塞，没有数据进来。如果没有获取到Worker锁，说明Worker还在执行任务，不进行中断(shutdown方法不会中断正在执行的任务)</span>
                <span class="keyword">try</span> {
                    t.interrupt();  <span class="comment">// 中断Worker线程</span>
                } <span class="keyword">catch</span> (SecurityException ignore) {
                } <span class="keyword">finally</span> {
                    w.unlock(); <span class="comment">// 释放Worker锁</span>
                }
            }
            <span class="keyword">if</span> (onlyOne) <span class="comment">// 如果只打断1个Worker的话，直接break退出，否则，遍历所有的Worker</span>
                <span class="keyword">break</span>;
        }
    } <span class="keyword">finally</span> {
        mainLock.unlock(); <span class="comment">// 解锁</span>
    }
}
</code></pre><p>shutdown方法将线程池状态改成SHUTDOWN，线程池还能继续处理阻塞队列里的任务，并且会回收一些闲置的Worker。但是shutdownNow方法不一样，它会把线程池状态改成STOP状态，这样不会处理阻塞队列里的任务，也不会处理新的任务：</p>
<pre><code><span class="comment">// shutdownNow方法会有返回值的，返回的是一个任务列表，而shutdown方法没有返回值</span>
<span class="function"><span class="keyword">public</span> List&lt;Runnable&gt; <span class="title">shutdownNow</span>(<span class="params"></span>) </span>{
    List&lt;Runnable&gt; tasks;
    final ReentrantLock mainLock = <span class="keyword">this</span>.mainLock;
    mainLock.<span class="keyword">lock</span>(); <span class="comment">// shutdownNow操作也需要加锁，防止并发</span>
    <span class="keyword">try</span> {
        checkShutdownAccess(); <span class="comment">// 检查关闭线程池的权限</span>
        advanceRunState(STOP); <span class="comment">// 把线程池状态更新到STOP</span>
        interruptWorkers(); <span class="comment">// 中断Worker的运行</span>
        tasks = drainQueue();
    } <span class="keyword">finally</span> {
        mainLock.unlock(); <span class="comment">// 解锁</span>
    }
    tryTerminate(); <span class="comment">// 尝试结束线程池，上面已经分析过了</span>
    <span class="keyword">return</span> tasks;
}
</code></pre><p>shutdownNow的中断和shutdown方法不一样，调用的是interruptWorkers方法：</p>
<pre><code><span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">interruptWorkers</span><span class="params">()</span> </span>{
    <span class="keyword">final</span> ReentrantLock mainLock = <span class="keyword">this</span>.mainLock;
    mainLock.lock(); <span class="comment">// 中断Worker需要加锁，防止并发</span>
    <span class="keyword">try</span> {
        <span class="keyword">for</span> (Worker w : workers)
            w.interruptIfStarted(); <span class="comment">// 中断Worker的执行</span>
    } <span class="keyword">finally</span> {
        mainLock.unlock(); <span class="comment">// 解锁</span>
    }
}
</code></pre><p>Worker的interruptIfStarted方法中断Worker的执行：</p>
<pre><code><span class="function"><span class="keyword">void</span> <span class="title">interruptIfStarted</span><span class="params">()</span> </span>{
   Thread t;
   <span class="comment">// Worker无论是否被持有锁，只要还没被中断，那就中断Worker</span>
   <span class="keyword">if</span> (getState() &gt;= <span class="number">0</span> &amp;&amp; (t = thread) != <span class="keyword">null</span> &amp;&amp; !t.isInterrupted()) {
       <span class="keyword">try</span> {
           t.interrupt(); <span class="comment">// 强行中断Worker的执行</span>
       } <span class="keyword">catch</span> (SecurityException ignore) {
       }
   }
}
</code></pre><p>线程池关闭总结：    </p>
<p>线程池的关闭主要是两个方法，shutdown和shutdownNow方法。</p>
<p>shutdown方法会更新状态到SHUTDOWN，不会影响阻塞队列里任务的执行，但是不会执行新进来的任务。同时也会回收闲置的Worker，闲置Worker的定义上面已经说过了。</p>
<p>shutdownNow方法会更新状态到STOP，会影响阻塞队列的任务执行，也不会执行新进来的任务。同时会回收所有的Worker。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>ThreadPoolExecutor是jdk内置线程池的一个实现，基本上大部分情况都会使用这个线程池完成各项操作。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/thread-pool.jpeg" alt=""></p>]]>
    
    </summary>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="thread" scheme="http://fangjian0423.github.io/tags/thread/"/>
    
      <category term="java" scheme="http://fangjian0423.github.io/categories/java/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java可重入锁ReentrantLock分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/19/java-ReentrantLock-analysis/"/>
    <id>http://fangjian0423.github.io/2016/03/19/java-ReentrantLock-analysis/</id>
    <published>2016-03-19T06:31:58.000Z</published>
    <updated>2016-03-19T06:59:23.000Z</updated>
    <content type="html"><![CDATA[<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/ReentrantLock.png" alt=""><br>Java中的可重入锁ReentrantLock很常见，可以用它来代替内置锁synchronized，ReentrantLock是语法级别的锁，所以比内置锁更加灵活。</p>
<a id="more"></a>
<p>下面这段代码是ReentrantLock的一个例子：</p>
<pre><code><span class="keyword">class</span> <span class="title">Context</span> {
    <span class="keyword">private</span> ReentrantLock <span class="keyword">lock</span> = <span class="keyword">new</span> ReentrantLock();
    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">method</span>(<span class="params"></span>) </span>{
        <span class="keyword">lock</span>.<span class="keyword">lock</span>();
        System.<span class="keyword">out</span>.println(<span class="string">"do atomic operation"</span>);
        <span class="keyword">try</span> {
            Thread.sleep(<span class="number">3000</span>l);
        } <span class="keyword">catch</span> (InterruptedException e) {
            e.printStackTrace();
        } <span class="keyword">finally</span> {
            <span class="keyword">lock</span>.unlock();
        }
    }
}

<span class="keyword">class</span> <span class="title">MyThread</span> <span class="title">implements</span> <span class="title">Runnable</span> {
    <span class="keyword">private</span> Context context;
    <span class="function"><span class="keyword">public</span> <span class="title">MyThread</span>(<span class="params">Context context</span>) </span>{
        <span class="keyword">this</span>.context = context;
    }
    @<span class="function">Override
    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span>(<span class="params"></span>) </span>{
        context.method();
    }
}
</code></pre><p>main方法：</p>
<pre><code><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>{
    Context context = <span class="keyword">new</span> Context();
    ExecutorService executorService = Executors.newFixedThreadPool(<span class="number">5</span>);
    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i ++ ) {
        executorService.submit(<span class="keyword">new</span> MyThread(context));
    }
}
</code></pre><p>输出结果，每隔3秒输出：</p>
<pre><code><span class="keyword">do</span> atomic operation   
</code></pre><p>如果没有使用可重入锁的话，那么一次性输出5条 do atomic operation。</p>
<p>ReentrantLock中有3个内部类，分别是Sync、FairSync和NonfairSync。</p>
<p>Sync是一个继承AQS的抽象类，使用独占锁，复写了tryRelease方法。tryAcquire方法由它的两个FairSync(公平锁)和NonfairSync(非公平锁)实现。</p>
<p>AQS相关的内容可以参考文章末尾的参考资料，这篇文章写得非常棒。</p>
<p>ReentrantLock的lock方法使用sync的lock方法，Sync的lock方法是个抽象方法，由公平锁和非公平锁去实现。unlock方法直接使用AQS的release方法。所以说公平锁和非公平锁的释放锁过程是一样的，不一样的是获取锁过程。</p>
<p>先来看一下unlock方法，unlock方法调用的AQS的release方法，也就是调用了tryRelease方法，tryRelease方法调完之后恢复第一个挂起的线程：</p>
<pre><code><span class="keyword">protected</span> <span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryRelease</span><span class="params">(<span class="keyword">int</span> releases)</span> </span>{
    <span class="keyword">int</span> c = getState() - releases; <span class="comment">// 释放</span>
    <span class="keyword">if</span> (Thread.currentThread() != getExclusiveOwnerThread()) <span class="comment">// 如果当前线程不是独占线程，直接抛出异常</span>
        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalMonitorStateException();
    <span class="keyword">boolean</span> free = <span class="keyword">false</span>;
    <span class="keyword">if</span> (c == <span class="number">0</span>) { <span class="comment">// 由于是可重入锁，需要判断是否全部释放了</span>
        free = <span class="keyword">true</span>;
        setExclusiveOwnerThread(<span class="keyword">null</span>); <span class="comment">// 全部释放的话直接把独占线程设置为null</span>
    }
    setState(c);
    <span class="keyword">return</span> free;
}

<span class="comment">// 恢复线程</span>
<span class="keyword">public</span> <span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">release</span><span class="params">(<span class="keyword">int</span> arg)</span> </span>{
    <span class="keyword">if</span> (tryRelease(arg)) {
        Node h = head;  <span class="comment">// 恢复第一个挂起的线程</span>
        <span class="keyword">if</span> (h != <span class="keyword">null</span> &amp;&amp; h.waitStatus != <span class="number">0</span>)
            unparkSuccessor(h);
        <span class="keyword">return</span> <span class="keyword">true</span>;
    }
    <span class="keyword">return</span> <span class="keyword">false</span>;
}
</code></pre><p>ReentrantLock的lock方法就是获取锁的方法，AQS中线程对锁的的竞争结果只有两种，要么获取到了锁；要么没有获取到锁，没有获取的锁线程被挂起等待被唤醒。</p>
<p>公平锁FairSync的lock方法：</p>
<pre><code><span class="keyword">final</span> <span class="function"><span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>{
    <span class="comment">// acquire方法内部调用tryAcquire方法</span>
    <span class="comment">// 公平锁的获取锁方法，对于没有获取到的线程，会按照队列的方式挂起线程</span>
    acquire(<span class="number">1</span>);
}

<span class="keyword">protected</span> <span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>{
    <span class="keyword">final</span> Thread current = Thread.currentThread();
    <span class="keyword">int</span> c = getState();
    <span class="keyword">if</span> (c == <span class="number">0</span>) {
        <span class="comment">// 公平锁这里多了一个!hasQueuedPredecessors()判断，表示是否有线程在队列里等待的时间比当前线程要长，如果有等待时间更长的线程，那么放弃获取锁</span>
        <span class="keyword">if</span> (!hasQueuedPredecessors() &amp;&amp;
            compareAndSetState(<span class="number">0</span>, acquires)) {
            setExclusiveOwnerThread(current);
            <span class="keyword">return</span> <span class="keyword">true</span>;
        }
    }
    <span class="keyword">else</span> <span class="keyword">if</span> (current == getExclusiveOwnerThread()) {
        <span class="keyword">int</span> nextc = c + acquires;
        <span class="keyword">if</span> (nextc &lt; <span class="number">0</span>)
            <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">"Maximum lock count exceeded"</span>);
        setState(nextc);
        <span class="keyword">return</span> <span class="keyword">true</span>;
    }
    <span class="keyword">return</span> <span class="keyword">false</span>;
}
</code></pre><p>非公平锁NonfairSync的lock方法：</p>
<pre><code><span class="keyword">final</span> <span class="function"><span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>{
    <span class="comment">// 非公平锁的获取锁</span>
    <span class="comment">// 跟公平锁的区别就在这里。直接对状态位state进行cas操作，成功就获取锁，这是一种抢占式的方式。不成功跟公平锁一样进入队列挂起线程</span>
    <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, <span class="number">1</span>))
        setExclusiveOwnerThread(Thread.currentThread());
    <span class="keyword">else</span>
        acquire(<span class="number">1</span>);
}

<span class="comment">// 调用Sync的nonfairTryAcquire方法</span>
<span class="keyword">protected</span> <span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">tryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>{
    <span class="function"><span class="keyword">return</span> <span class="title">nonfairTryAcquire</span><span class="params">(acquires)</span></span>;
}

<span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">nonfairTryAcquire</span><span class="params">(<span class="keyword">int</span> acquires)</span> </span>{
    <span class="keyword">final</span> Thread current = Thread.currentThread();
    <span class="keyword">int</span> c = getState();
    <span class="keyword">if</span> (c == <span class="number">0</span>) {
        <span class="keyword">if</span> (compareAndSetState(<span class="number">0</span>, acquires)) {
            setExclusiveOwnerThread(current);
            <span class="keyword">return</span> <span class="keyword">true</span>;
        }
    }
    <span class="keyword">else</span> <span class="keyword">if</span> (current == getExclusiveOwnerThread()) {
        <span class="keyword">int</span> nextc = c + acquires;
        <span class="keyword">if</span> (nextc &lt; <span class="number">0</span>) <span class="comment">// overflow</span>
            <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">"Maximum lock count exceeded"</span>);
        setState(nextc);
        <span class="keyword">return</span> <span class="keyword">true</span>;
    }
    <span class="keyword">return</span> <span class="keyword">false</span>;
}
</code></pre><p>如上述源码和注释所说，公平锁和非公平锁的最主要区别就是获取锁的方式不一样。</p>
<p>公平锁获取锁的时候，首先先读取状态位state，然后再做判断，之后使用cas设置状态位。能获取锁的线程就获取锁，不能获取锁的线程被挂起进入队列。之后再来的线程的等待时间没有已经在队列里的线程等待时间长，所以会一直进入等待队列。 公平锁类似于排队买火车票一样，后面来的人没有前面来的人等待时间长，会一直在队尾被加入到队列里。</p>
<p>非公平锁获取锁的时候，立马就使用cas判断设置状态位，是一种抢占式的方式。同时非公平锁也没有等待时间长的线程会优先获取锁这个概念。非公平锁类似吃饭排队，但是总会有那么几个人试图插队。</p>
<p>公平锁和非公平锁的还有另外一个差别，前面已经分析过了。就是公平锁获取锁多了一个判断条件，当前线程的等待时间没有队列里的线程等待时间长的话，不能获取锁；而非公平锁没有这个条件。</p>
<p>ReentrantLock的默认构造函数使用的是NonfairSync，如果想使用FairSync，使用带有boolean参数的构造函数，传入true表示FairSync，否则是NonfairSync。</p>
<p>ReentrantLock内部还提供了一些有用的方法：</p>
<p>hasQueuedThreads： 查询是否有线程在等待队列里<br>hasQueuedThread(Thread thread)：查询线程是否在等待队列里<br>isHeldByCurrentThread：当前线程是否持有锁<br>getQueueLength：队列中的挂起线程个数</p>
<p>等等还有其他的一些有用方法。</p>
<p>总结：</p>
<p>ReentrantLock可重入锁内部有3个类，Sync、FairSync和NonfairSync。</p>
<p>Sync是一个继承AQS的抽象类，并发的控制就是通过Sync实现的(当然是使用AQS实现的，AQS是Java并发包的一个同步基础类)，它复写了tryRelease方法，它有2个子类FairSync和NonfairSync，也就是公平锁和非公平锁。</p>
<p>由于Sync复写了tryRelease方法，它的2个子类公平锁和非公平锁没有再次复写这个方法，所以公平锁和非公平锁的释放锁操作是一样的，释放锁也就是唤醒等待队列中的第一个被挂起的线程。</p>
<p>虽然公平锁和非公平锁的释放锁方式一样，但是它们的获取锁方式不一样，公平锁获取锁的时候，如果1个线程获取到了锁，其他线程都会被挂起并且进入等待队列，后面来的线程的等待时间没有队列里的线程等待时间长的话，那么就放弃获取锁，进入等待队列。非公平锁获取锁的方式是一种抢占式的方式，不考虑等待时间的问题，无论哪个线程获取到了锁，其他线程就进入等待队列。</p>
<p>参考资料：</p>
<p><a href="http://ifeve.com/jdk1-8-abstractqueuedsynchronizer/" target="_blank" rel="external">http://ifeve.com/jdk1-8-abstractqueuedsynchronizer/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/ReentrantLock.png" alt=""><br>Java中的可重入锁ReentrantLock很常见，可以用它来代替内置锁synchronized，ReentrantLock是语法级别的锁，所以比内置锁更加灵活。</p>]]>
    
    </summary>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="thread" scheme="http://fangjian0423.github.io/tags/thread/"/>
    
      <category term="java" scheme="http://fangjian0423.github.io/categories/java/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java AtomicInteger原理分析]]></title>
    <link href="http://fangjian0423.github.io/2016/03/16/java-AtomicInteger-analysis/"/>
    <id>http://fangjian0423.github.io/2016/03/16/java-AtomicInteger-analysis/</id>
    <published>2016-03-16T12:32:35.000Z</published>
    <updated>2016-03-19T07:36:38.000Z</updated>
    <content type="html"><![CDATA[<p>Java中的AtomicInteger大家应该很熟悉，它是为了解决多线程访问Integer变量导致结果不正确所设计的一个基于多线程并且支持原子操作的Integer类。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/java-AtomicInteger-analysis.jpeg" alt=""></p>
<a id="more"></a>
<p>它的使用也非常简单：</p>
<pre><code>AtomicInteger ai = <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>);
ai.addAndGet(<span class="number">5</span>); <span class="comment">// 5</span>
ai.getAndAdd(<span class="number">1</span>); <span class="comment">// 5</span>
ai.get(); <span class="comment">// 6</span>
</code></pre><p>AtomicInteger内部有一个变量UnSafe：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">Unsafe</span> <span class="keyword">unsafe</span> = <span class="keyword">Unsafe</span>.getUnsafe();
</code></pre><p>Unsafe类是一个可以执行不安全、容易犯错的操作的一个特殊类。虽然Unsafe类中所有方法都是public的，但是这个类只能在一些被信任的代码中使用。Unsafe的源码可以在这里看 -&gt; <a href="http://www.docjar.com/html/api/sun/misc/Unsafe.java.html" target="_blank" rel="external">UnSafe源码</a>。</p>
<p>Unsafe类可以执行以下几种操作：</p>
<ol>
<li>分配内存，释放内存：在方法allocateMemory，reallocateMemory，freeMemory中，有点类似c中的malloc，free方法</li>
<li>可以定位对象的属性在内存中的位置，可以修改对象的属性值。使用objectFieldOffset方法</li>
<li>挂起和恢复线程，被封装在LockSupport类中供使用</li>
<li>CAS操作(CompareAndSwap，比较并交换，是一个原子操作)</li>
</ol>
<p>AtomicInteger中用的就是Unsafe的CAS操作。</p>
<p>Unsafe中的int类型的CAS操作方法：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">native</span> <span class="function"><span class="keyword">boolean</span> <span class="title">compareAndSwapInt</span><span class="params">(Object o, <span class="keyword">long</span> offset,
                                                <span class="keyword">int</span> expected,
                                                <span class="keyword">int</span> x)</span></span>;
</code></pre><p>参数o就是要进行cas操作的对象，offset参数是内存位置，expected参数就是期望的值，x参数是需要更新到的值。</p>
<p>也就是说，如果我把1这个数字属性更新到2的话，需要这样调用：</p>
<pre><code>compareAndSwapInt(<span class="keyword">this</span>, valueOffset, <span class="number">1</span>, <span class="number">2</span>)
</code></pre><p>valueOffset字段表示内存位置，可以在AtomicInteger对象中使用unsafe得到：</p>
<pre><code><span class="keyword">static</span> {
  <span class="keyword">try</span> {
    valueOffset = unsafe.objectFieldOffset
        (AtomicInteger.<span class="keyword">class</span>.getDeclaredField(<span class="string">"value"</span>));
  } <span class="keyword">catch</span> (Exception ex) { <span class="keyword">throw</span> <span class="keyword">new</span> Error(ex); }
}
</code></pre><p>AtomicInteger内部使用变量value表示当前的整型值，这个整型变量还是volatile的，表示内存可见性，一个线程修改value之后保证对其他线程的可见性：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">int</span> <span class="keyword">value</span>;
</code></pre><p>AtomicInteger内部还封装了一下CAS，定义了一个compareAndSet方法，只需要2个参数：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">final</span> <span class="function"><span class="keyword">boolean</span> <span class="title">compareAndSet</span><span class="params">(<span class="keyword">int</span> expect, <span class="keyword">int</span> update)</span> </span>{
    <span class="function"><span class="keyword">return</span> unsafe.<span class="title">compareAndSwapInt</span><span class="params">(<span class="keyword">this</span>, valueOffset, expect, update)</span></span>;
}
</code></pre><p>addAndGet方法，addAndGet方法内部使用一个死循环，先得到当前的值value，然后再把当前的值加一，加完之后使用cas原子操作让当前值加一处理正确。当然cas原子操作不一定是成功的，所以做了一个死循环，当cas操作成功的时候返回数据。这里由于使用了cas原子操作，所以不会出现多线程处理错误的问题。比如线程A得到current为1，线程B也得到current为1；线程A的next值为2，进行cas操作并且成功的时候，将value修改成了2；这个时候线程B也得到next值为2，当进行cas操作的时候由于expected值已经是2，而不是1了；所以cas操作会失败，下一次循环的时候得到的current就变成了2；也就不会出现多线程处理问题了：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> addAndGet(<span class="keyword">int</span> delta) {
    <span class="keyword">for</span> (;;) {
        <span class="keyword">int</span> current = get();
        <span class="keyword">int</span> <span class="keyword">next</span> = current + delta;
        <span class="keyword">if</span> (compareAndSet(current, <span class="keyword">next</span>))
            <span class="keyword">return</span> <span class="keyword">next</span>;
    }
}
</code></pre><p>incrementAndGet方法，跟addAndGet方法类似，只不过next值变成了current+1：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> incrementAndGet() {
    <span class="keyword">for</span> (;;) {
        <span class="keyword">int</span> current = get();
        <span class="keyword">int</span> <span class="keyword">next</span> = current + <span class="number">1</span>;
        <span class="keyword">if</span> (compareAndSet(current, <span class="keyword">next</span>))
            <span class="keyword">return</span> <span class="keyword">next</span>;
    }
}
</code></pre><p>getAndAdd方法，跟addAndGet方法一样，返回值变成了current：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> getAndAdd(<span class="keyword">int</span> delta) {
    <span class="keyword">for</span> (;;) {
        <span class="keyword">int</span> current = get();
        <span class="keyword">int</span> <span class="keyword">next</span> = current + delta;
        <span class="keyword">if</span> (compareAndSet(current, <span class="keyword">next</span>))
            <span class="keyword">return</span> current;
    }
}
</code></pre><p>缺点：</p>
<p>虽然AtomicInteger中的cas操作可以实现非阻塞的原子操作，但是会产生ABA问题，</p>
<p>参考资料：</p>
<p><a href="http://blog.csdn.net/ghsau/article/details/38471987" target="_blank" rel="external">http://blog.csdn.net/ghsau/article/details/38471987</a></p>
<p><a href="http://blog.csdn.net/aesop_wubo/article/details/7537278" target="_blank" rel="external">http://blog.csdn.net/aesop_wubo/article/details/7537278</a></p>
<p><a href="http://ifeve.com/sun-misc-unsafe/" target="_blank" rel="external">http://ifeve.com/sun-misc-unsafe/</a></p>
<p><a href="http://ifeve.com/java-atomic/" target="_blank" rel="external">http://ifeve.com/java-atomic/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Java中的AtomicInteger大家应该很熟悉，它是为了解决多线程访问Integer变量导致结果不正确所设计的一个基于多线程并且支持原子操作的Integer类。</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/java-AtomicInteger-analysis.jpeg" alt=""></p>]]>
    
    </summary>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="java" scheme="http://fangjian0423.github.io/categories/java/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java根类Object的方法说明]]></title>
    <link href="http://fangjian0423.github.io/2016/03/12/java-Object-method/"/>
    <id>http://fangjian0423.github.io/2016/03/12/java-Object-method/</id>
    <published>2016-03-12T09:44:59.000Z</published>
    <updated>2016-03-12T09:45:05.000Z</updated>
    <content type="html"><![CDATA[<p>Java中的Object类是所有类的父类，它提供了以下11个方法：</p>
<ol>
<li>public final native Class&lt;?&gt; getClass()</li>
<li>public native int hashCode()</li>
<li>public boolean equals(Object obj)</li>
<li>protected native Object clone() throws CloneNotSupportedException</li>
<li>public String toString()</li>
<li>public final native void notify()</li>
<li>public final native void notifyAll()</li>
<li>public final native void wait(long timeout) throws InterruptedException</li>
<li>public final void wait(long timeout, int nanos) throws InterruptedException </li>
<li>public final void wait() throws InterruptedException</li>
<li>protected void finalize() throws Throwable { }</li>
</ol>
<p>下面我们一个个方法进行分析，看这些方法到底有什么作用：</p>
<h3 id="getClass方法">getClass方法</h3><p>getClass方法是一个final方法，不允许子类重写，并且也是一个native方法。</p>
<p>返回当前运行时对象的Class对象，注意这里是运行时，比如以下代码中n是一个Number类型的实例，但是java中数值默认是Integer类型，所以getClass方法返回的是java.lang.Integer：</p>
<pre><code><span class="string">"str"</span>.getClass() // <span class="class"><span class="keyword">class</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">String</span></span>
<span class="string">"str"</span>.getClass == String.class // <span class="literal">true</span>
Number n = <span class="number">0</span>;
<span class="class"><span class="keyword">Class</span>&lt;? <span class="keyword">extends</span> <span class="title">Number</span>&gt; <span class="title">c</span> = <span class="title">n</span>.<span class="title">getClass</span>(); // <span class="title">class</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Integer</span></span>
</code></pre><h3 id="hashCode方法">hashCode方法</h3><p>hashCode方法也是一个native方法。</p>
<p>该方法返回对象的哈希码，主要使用在哈希表中，比如JDK中的HashMap。</p>
<p>哈希码的通用约定如下：</p>
<ol>
<li>在java程序执行过程中，在一个对象没有被改变的前提下，无论这个对象被调用多少次，hashCode方法都会返回相同的整数值。对象的哈希码没有必要在不同的程序中保持相同的值。</li>
<li>如果2个对象使用equals方法进行比较并且相同的话，那么这2个对象的hashCode方法的值也必须相等。</li>
<li>如果根据equals方法，得到两个对象不相等，那么这2个对象的hashCode值不需要必须不相同。但是，不相等的对象的hashCode值不同的话可以提高哈希表的性能。</li>
</ol>
<p>通常情况下，不同的对象产生的哈希码是不同的。默认情况下，对象的哈希码是通过将该对象的内部地址转换成一个整数来实现的。</p>
<p>String的hashCode方法实现如下， 计算方法是 s[0]<em>31^(n-1) + s[1]</em>31^(n-2) + … + s[n-1]，其中s[0]表示字符串的第一个字符，n表示字符串长度：</p>
<pre><code><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>{
    <span class="keyword">int</span> h = hash;
    <span class="keyword">if</span> (h == <span class="number">0</span> &amp;&amp; value.length &gt; <span class="number">0</span>) {
        <span class="keyword">char</span> val[] = value;

        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; value.length; i++) {
            h = <span class="number">31</span> * h + val[i];
        }
        hash = h;
    }
    <span class="keyword">return</span> h;
}
</code></pre><p>比如”fo”的hashCode = 102 <em> 31^1 + 111 = 3273， “foo”的hashCode = 102 </em> 31^2 + 111 * 31^1 + 111 = 101574 (‘f’的ascii码为102, ‘o’的ascii码为111)</p>
<p>hashCode在哈希表HashMap中的应用：</p>
<pre><code><span class="comment">// Student类，只重写了hashCode方法</span>
<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> Student {

    <span class="keyword">private</span> String name;
    <span class="keyword">private</span> <span class="keyword">int</span> age;

    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>{
        <span class="keyword">this</span>.name = name;
        <span class="keyword">this</span>.age = age;
    }

    @<span class="function">Override
    <span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>{
        <span class="keyword">return</span> name.hashCode();
    }
}

Map&lt;Student, String&gt; <span class="built_in">map</span> = <span class="keyword">new</span> HashMap&lt;Student, String&gt;();
Student stu1 = <span class="keyword">new</span> Student(<span class="string">"fo"</span>, <span class="number">11</span>);
Student stu2 = <span class="keyword">new</span> Student(<span class="string">"fo"</span>, <span class="number">22</span>);
<span class="built_in">map</span>.put(stu1, <span class="string">"fo"</span>);
<span class="built_in">map</span>.put(stu2, <span class="string">"fo"</span>);
</code></pre><p>上面这段代码中，map中有2个元素stu1和stu2。但是这2个元素是在哈希表中的同一个数组项中的位置，也就是在同一串链表中。 但是为什么stu1和stu2的hashCode相同，但是两条元素都插到map里了，这是因为map判断重复数据的条件是 <strong>两个对象的哈希码相同并且(两个对象是同一个对象或者两个对象相等[equals为true])</strong>。 所以再给Student重写equals方法，并且只比较name的话，这样map就只有1个元素了。</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>{
    <span class="keyword">if</span> (<span class="keyword">this</span> == o) <span class="keyword">return</span> <span class="keyword">true</span>;
    <span class="keyword">if</span> (o == <span class="keyword">null</span> || getClass() != o.getClass()) <span class="keyword">return</span> <span class="keyword">false</span>;
    Student student = (Student) o;
    <span class="keyword">return</span> <span class="keyword">this</span>.name.equals(student.name);
}
</code></pre><p>这个例子直接说明了hashCode中通用约定的第三点：</p>
<p>第三点：如果根据equals方法，得到两个对象不相等，那么这2个对象的hashCode值不需要必须不相同。但是，不相等的对象的hashCode值不同的话可以提高哈希表的性能。 –&gt; 上面例子一开始没有重写equals方法，导致两个对象不相等，但是这两个对象的hashCode值一样，所以导致这两个对象在同一串链表中，影响性能。</p>
<p>当然，还有第三种情况，那就是equals方法相等，但是hashCode的值不相等。</p>
<p>这种情况也就是违反了通用约定的第二点：</p>
<p>第二点：<strong>如果2个对象使用equals方法进行比较并且相同的话，那么这2个对象的hashCode方法的值也必须相等。</strong> 违反这一点产生的后果就是如果一个stu1实例是Student(“fo”, 11)，stu2实例是Student(“fo”, 11)，那么这2个实例是相等的，但是他们的hashCode不一样，这样是导致哈希表中都会存入stu1实例和stu2实例，但是实际情况下，stu1和stu2是重复数据，只允许存在一条数据在哈希表中。所以这一点是非常重点的，再强调一下：<strong>如果2个对象的equals方法相等，那么他们的hashCode值也必须相等，反之，如果2个对象hashCode值相等，但是equals不相等，这样会影响性能，所以还是建议2个方法都一起重写。</strong></p>
<h3 id="equals方法">equals方法</h3><p>比较两个对象是否相等。Object类的默认实现，即比较2个对象的内存地址是否相等：</p>
<pre><code><span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span> </span>{
    <span class="keyword">return</span> (<span class="keyword">this</span> == obj);
}
</code></pre><p>equals方法在非空对象引用上的特性：</p>
<ol>
<li>reflexive，自反性。任何非空引用值x，对于x.equals(x)必须返回true</li>
<li>symmetric，对称性。任何非空引用值x和y，如果x.equals(y)为true，那么y.equals(x)也必须为true</li>
<li>transitive，传递性。任何非空引用值x、y和z，如果x.equals(y)为true并且y.equals(z)为true，那么x.equals(z)也必定为true</li>
<li>consistent，一致性。任何非空引用值x和y，多次调用 x.equals(y) 始终返回 true 或始终返回 false，前提是对象上 equals 比较中所用的信息没有被修改</li>
<li>对于任何非空引用值 x，x.equals(null) 都应返回 false</li>
</ol>
<p>Object类的equals方法对于任何非空引用值x和y，当x和y引用同一个对象时，此方法才返回true。这个也就是我们常说的地址相等。</p>
<p>注意点：如果重写了equals方法，通常有必要重写hashCode方法，这点已经在hashCode方法中说明了。</p>
<h3 id="clone方法">clone方法</h3><p>创建并返回当前对象的一份拷贝。一般情况下，对于任何对象 x，表达式 x.clone() != x 为true，x.clone().getClass() == x.getClass() 也为true。</p>
<p>Object类的clone方法是一个protected的native方法。</p>
<p>由于Object本身没有实现Cloneable接口，所以不重写clone方法并且进行调用的话会发生CloneNotSupportedException异常。</p>
<h3 id="toString方法">toString方法</h3><p>Object对象的默认实现，即输出类的名字@实例的哈希码的16进制：</p>
<pre><code><span class="keyword">public</span> <span class="built_in">String</span> toString() {
    <span class="keyword">return</span> getClass()<span class="built_in">.</span>getName() + <span class="string">"@"</span> + <span class="built_in">Integer</span><span class="built_in">.</span>toHexString(hashCode());
}
</code></pre><p>toString方法的结果应该是一个简明但易于读懂的字符串。建议Object所有的子类都重写这个方法。</p>
<h3 id="notify方法">notify方法</h3><p>notify方法是一个native方法，并且也是final的，不允许子类重写。</p>
<p>唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果所有的线程都在此对象上等待，那么只会选择一个线程。选择是任意性的，并在对实现做出决定时发生。一个线程在对象监视器上等待可以调用wait方法。</p>
<p>直到当前线程放弃对象上的锁之后，被唤醒的线程才可以继续处理。被唤醒的线程将以常规方式与在该对象上主动同步的其他所有线程进行竞争。例如，唤醒的线程在作为锁定此对象的下一个线程方面没有可靠的特权或劣势。</p>
<p>notify方法只能被作为此对象监视器的所有者的线程来调用。一个线程要想成为对象监视器的所有者，可以使用以下3种方法：</p>
<ol>
<li>执行对象的同步实例方法</li>
<li>使用synchronized内置锁</li>
<li>对于Class类型的对象，执行同步静态方法</li>
</ol>
<p>一次只能有一个线程拥有对象的监视器。</p>
<p>如果当前线程不是此对象监视器的所有者的话会抛出IllegalMonitorStateException异常</p>
<p>注意点：</p>
<p><strong>因为notify只能在拥有对象监视器的所有者线程中调用，否则会抛出IllegalMonitorStateException异常</strong></p>
<h3 id="notifyAll方法">notifyAll方法</h3><p>跟notify一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。</p>
<p>同样，如果当前线程不是对象监视器的所有者，那么调用notifyAll同样会发生IllegalMonitorStateException异常。</p>
<p>以下这段代码直接调用notify或者notifyAll方法会发生IllegalMonitorStateException异常，这是因为调用这两个方法需要当前线程是对象监视器的所有者：</p>
<pre><code>Factory <span class="literal">factory</span> = <span class="keyword">new</span> Factory();
<span class="literal">factory</span>.notify();
<span class="literal">factory</span>.notifyAll();
</code></pre><h3 id="wait(long_timeout)_throws_InterruptedException方法">wait(long timeout) throws InterruptedException方法</h3><p>wait(long timeout)方法同样是一个native方法，并且也是final的，不允许子类重写。</p>
<p>wait方法会让当前线程等待直到另外一个线程调用对象的notify或notifyAll方法，或者超过参数设置的timeout超时时间。</p>
<p><strong>跟notify和notifyAll方法一样，当前线程必须是此对象的监视器所有者，否则还是会发生IllegalMonitorStateException异常。</strong></p>
<p>wait方法会让当前线程(我们先叫做线程T)将其自身放置在对象的等待集中，并且放弃该对象上的所有同步要求。出于线程调度目的，线程T是不可用并处于休眠状态，直到发生以下四件事中的任意一件：</p>
<ol>
<li>其他某个线程调用此对象的notify方法，并且线程T碰巧被任选为被唤醒的线程</li>
<li>其他某个线程调用此对象的notifyAll方法</li>
<li>其他某个线程调用Thread.interrupt方法中断线程T</li>
<li>时间到了参数设置的超时时间。如果timeout参数为0，则不会超时，会一直进行等待</li>
</ol>
<p>所以可以理解wait方法相当于放弃了当前线程对对象监视器的所有者(也就是说释放了对象的锁)</p>
<p>之后，线程T会被等待集中被移除，并且重新进行线程调度。然后，该线程以常规方式与其他线程竞争，以获得在该对象上同步的权利；一旦获得对该对象的控制权，该对象上的所有其同步声明都将被恢复到以前的状态，这就是调用wait方法时的情况。然后，线程T从wait方法的调用中返回。所以，从wait方法返回时，该对象和线程T的同步状态与调用wait方法时的情况完全相同。</p>
<p>在没有被通知、中断或超时的情况下，线程还可以唤醒一个所谓的虚假唤醒 (spurious wakeup)。虽然这种情况在实践中很少发生，但是应用程序必须通过以下方式防止其发生，即对应该导致该线程被提醒的条件进行测试，如果不满足该条件，则继续等待。换句话说，等待应总是发生在循环中，如下面的示例：</p>
<pre><code><span class="keyword">synchronized</span> (obj) {
    <span class="keyword">while</span> (&lt;condition does not hold&gt;)
        obj.wait(timeout);
        ... <span class="comment">// Perform action appropriate to condition</span>
}
</code></pre><p>如果当前线程在等待之前或在等待时被任何线程中断，则会抛出InterruptedException异常。在按上述形式恢复此对象的锁定状态时才会抛出此异常。</p>
<h3 id="wait(long_timeout,_int_nanos)_throws_InterruptedException方法">wait(long timeout, int nanos) throws InterruptedException方法</h3><p>跟wait(long timeout)方法类似，多了一个nanos参数，这个参数表示额外时间（以毫微秒为单位，范围是 0-999999）。 所以超时的时间还需要加上nanos毫秒。</p>
<p>需要注意的是 wait(0, 0)和wait(0)效果是一样的，即一直等待。</p>
<h3 id="wait()_throws_InterruptedException方法">wait() throws InterruptedException方法</h3><p>跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念。</p>
<p>以下这段代码直接调用wait方法会发生IllegalMonitorStateException异常，这是因为调用wait方法需要当前线程是对象监视器的所有者：</p>
<pre><code>Factory <span class="literal">factory</span> = <span class="keyword">new</span> Factory();
<span class="literal">factory</span>.wait();
</code></pre><p>一般情况下，wait方法和notify方法会一起使用的，wait方法阻塞当前线程，notify方法唤醒当前线程，一个使用wait和notify方法的生产者消费者例子代码如下：</p>
<pre><code>public <span class="class"><span class="keyword">class</span> <span class="title">WaitNotifyTest</span> </span>{

    public <span class="literal">static</span> <span class="keyword">void</span> main(<span class="built_in">String</span>[] args) {
        Factory <span class="literal">factory</span> = <span class="keyword">new</span> Factory();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Producer(<span class="literal">factory</span>, <span class="number">5</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Producer(<span class="literal">factory</span>, <span class="number">5</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Producer(<span class="literal">factory</span>, <span class="number">20</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Producer(<span class="literal">factory</span>, <span class="number">30</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Consumer(<span class="literal">factory</span>, <span class="number">10</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Consumer(<span class="literal">factory</span>, <span class="number">20</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Consumer(<span class="literal">factory</span>, <span class="number">5</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Consumer(<span class="literal">factory</span>, <span class="number">5</span>)).start();
        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Consumer(<span class="literal">factory</span>, <span class="number">20</span>)).start();
    }

}

<span class="class"><span class="keyword">class</span> <span class="title">Factory</span> </span>{

    public <span class="literal">static</span> <span class="keyword">final</span> Integer MAX_NUM = <span class="number">50</span>;

    private <span class="built_in">int</span> currentNum = <span class="number">0</span>;

    public <span class="keyword">void</span> consume(<span class="built_in">int</span> <span class="built_in">num</span>) throws InterruptedException {
        synchronized (<span class="keyword">this</span>) {
            <span class="keyword">while</span>(currentNum - <span class="built_in">num</span> &lt; <span class="number">0</span>) {
                <span class="keyword">this</span>.wait();
            }
            currentNum -= <span class="built_in">num</span>;
            System.out.println(<span class="string">"consume "</span> + <span class="built_in">num</span> + <span class="string">", left: "</span> + currentNum);
            <span class="keyword">this</span>.notifyAll();
        }
    }

    public <span class="keyword">void</span> produce(<span class="built_in">int</span> <span class="built_in">num</span>) throws InterruptedException {
        synchronized (<span class="keyword">this</span>) {
            <span class="keyword">while</span>(currentNum + <span class="built_in">num</span> &gt; MAX_NUM) {
                <span class="keyword">this</span>.wait();
            }
            currentNum += <span class="built_in">num</span>;
            System.out.println(<span class="string">"produce "</span> + <span class="built_in">num</span> + <span class="string">", left: "</span> + currentNum);
            <span class="keyword">this</span>.notifyAll();
        }
    }

}

<span class="class"><span class="keyword">class</span> <span class="title">Producer</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>{
    private Factory <span class="literal">factory</span>;
    private <span class="built_in">int</span> <span class="built_in">num</span>;
    public Producer(Factory <span class="literal">factory</span>, <span class="built_in">int</span> <span class="built_in">num</span>) {
        <span class="keyword">this</span>.<span class="literal">factory</span> = <span class="literal">factory</span>;
        <span class="keyword">this</span>.<span class="built_in">num</span> = <span class="built_in">num</span>;
    }
    <span class="annotation">@Override</span>
    public <span class="keyword">void</span> run() {
        <span class="keyword">try</span> {
            <span class="literal">factory</span>.produce(<span class="built_in">num</span>);
        } <span class="keyword">catch</span> (InterruptedException e) {
            e.printStackTrace();
        }
    }
}


<span class="class"><span class="keyword">class</span> <span class="title">Consumer</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>{
    private Factory <span class="literal">factory</span>;
    private <span class="built_in">int</span> <span class="built_in">num</span>;
    public Consumer(Factory <span class="literal">factory</span>, <span class="built_in">int</span> <span class="built_in">num</span>) {
        <span class="keyword">this</span>.<span class="literal">factory</span> = <span class="literal">factory</span>;
        <span class="keyword">this</span>.<span class="built_in">num</span> = <span class="built_in">num</span>;
    }
    <span class="annotation">@Override</span>
    public <span class="keyword">void</span> run() {
        <span class="keyword">try</span> {
            <span class="literal">factory</span>.consume(<span class="built_in">num</span>);
        } <span class="keyword">catch</span> (InterruptedException e) {
            e.printStackTrace();
        }
    }
}
</code></pre><p>注意的是Factory类的produce和consume方法都将Factory实例锁住了，锁住之后线程就成为了对象监视器的所有者，然后才能调用wait和notify方法。</p>
<p>输出：</p>
<pre><code>produce <span class="number">5</span>, left: <span class="number">5</span>
produce <span class="number">20</span>, left: <span class="number">25</span>
produce <span class="number">5</span>, left: <span class="number">30</span>
consume <span class="number">10</span>, left: <span class="number">20</span>
produce <span class="number">30</span>, left: <span class="number">50</span>
consume <span class="number">20</span>, left: <span class="number">30</span>
consume <span class="number">5</span>, left: <span class="number">25</span>
consume <span class="number">5</span>, left: <span class="number">20</span>
consume <span class="number">20</span>, left: <span class="number">0</span>
</code></pre><h3 id="finalize方法">finalize方法</h3><p>finalize方法是一个protected方法，Object类的默认实现是不进行任何操作。 </p>
<p>该方法的作用是实例被垃圾回收器回收的时候触发的操作，就好比 “死前的最后一波挣扎”。</p>
<p>直接写个弱引用例子：</p>
<pre><code>Car car = <span class="keyword">new</span> Car(<span class="number">9999</span>, <span class="string">"black"</span>);
WeakReference&lt;Car&gt; carWeakReference = <span class="keyword">new</span> WeakReference&lt;Car&gt;(car);

<span class="keyword">int</span> i = <span class="number">0</span>;
<span class="keyword">while</span>(<span class="keyword">true</span>) {
    <span class="keyword">if</span>(carWeakReference.<span class="keyword">get</span>() != <span class="keyword">null</span>) {
        i++;
        System.<span class="keyword">out</span>.println(<span class="string">"Object is alive for "</span>+i+<span class="string">" loops - "</span>+carWeakReference);
    } <span class="keyword">else</span> {
        System.<span class="keyword">out</span>.println(<span class="string">"Object has been collected."</span>);
        <span class="keyword">break</span>;
    }
}

<span class="keyword">class</span> <span class="title">Car</span> {
    <span class="keyword">private</span> <span class="keyword">double</span> price;
    <span class="keyword">private</span> String colour;

    <span class="function"><span class="keyword">public</span> <span class="title">Car</span>(<span class="params"><span class="keyword">double</span> price, String colour</span>)</span>{
        <span class="keyword">this</span>.price = price;
        <span class="keyword">this</span>.colour = colour;
    }

    <span class="comment">// get set method</span>

    @<span class="function">Override
    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">finalize</span>(<span class="params"></span>) throws Throwable </span>{
        System.<span class="keyword">out</span>.println(<span class="string">"i will be destroyed"</span>);
    }
}
</code></pre><p>输出：</p>
<pre><code>....
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26417</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26418</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26419</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26420</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26421</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> <span class="keyword">is</span> alive <span class="keyword">for</span> <span class="number">26422</span> loops - java.lang.<span class="keyword">ref</span>.<span class="type">WeakReference</span>@<span class="number">7</span>c2f1622
<span class="type">Object</span> has been collected.
i will be destroyed
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Java中的Object类是所有类的父类，它提供了11个方法，分别是getClass，hashCode，clone，toString，notify，notifyAll，wait，finalze等方法 ...]]>
    
    </summary>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="jdk" scheme="http://fangjian0423.github.io/tags/jdk/"/>
    
      <category term="java" scheme="http://fangjian0423.github.io/categories/java/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Avro介绍]]></title>
    <link href="http://fangjian0423.github.io/2016/02/21/avro-intro/"/>
    <id>http://fangjian0423.github.io/2016/02/21/avro-intro/</id>
    <published>2016-02-20T17:23:22.000Z</published>
    <updated>2016-03-16T06:55:43.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Avro介绍">Avro介绍</h2><p>Apache Avro是一个数据序列化系统。</p>
<p>Avro所提供的属性：</p>
<p>1.丰富的数据结构<br>2.使用快速的压缩二进制数据格式<br>3.提供容器文件用于持久化数据<br>4.远程过程调用RPC<br>5.简单的动态语言结合功能，Avro 和动态语言结合后，读写数据文件和使用 RPC 协议都不需要生成代码，而代码生成作为一种可选的优化只值得在静态类型语言中实现。</p>
<h2 id="Avro的Schema">Avro的Schema</h2><p>Avro的Schema用JSON表示。Schema定义了简单数据类型和复杂数据类型。</p>
<h3 id="基本类型">基本类型</h3><p>其中简单数据类型有以下8种：</p>
<table>
<thead>
<tr>
<th style="text-align:center">类型</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">null</td>
<td style="text-align:center">没有值</td>
</tr>
<tr>
<td style="text-align:center">boolean</td>
<td style="text-align:center">布尔值</td>
</tr>
<tr>
<td style="text-align:center">int</td>
<td style="text-align:center">32位有符号整数</td>
</tr>
<tr>
<td style="text-align:center">long</td>
<td style="text-align:center">64位有符号整数</td>
</tr>
<tr>
<td style="text-align:center">float</td>
<td style="text-align:center">单精度（32位）的IEEE 754浮点数</td>
</tr>
<tr>
<td style="text-align:center">double</td>
<td style="text-align:center">双精度（64位）的IEEE 754浮点数</td>
</tr>
<tr>
<td style="text-align:center">bytes</td>
<td style="text-align:center">8位无符号字节序列</td>
</tr>
<tr>
<td style="text-align:center">string</td>
<td style="text-align:center">字符串</td>
</tr>
</tbody>
</table>
<p>基本类型没有属性，基本类型的名字也就是类型的名字，比如：</p>
<pre><code>{"<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>}
</code></pre><h3 id="复杂类型">复杂类型</h3><p>Avro提供了6种复杂类型。分别是Record，Enum，Array，Map，Union和Fixed。</p>
<h4 id="Record">Record</h4><p>Record类型使用的类型名字是 “record”，还支持其它属性的设置：</p>
<p>name：record类型的名字(必填)</p>
<p>namespace：命名空间(可选)</p>
<p>doc：这个类型的文档说明(可选)</p>
<p>aliases：record类型的别名，是个字符串数组(可选)</p>
<p>fields：record类型中的字段，是个对象数组(必填)。每个字段需要以下属性：</p>
<ol>
<li>name：字段名字(必填)</li>
<li>doc：字段说明文档(可选)</li>
<li>type：一个schema的json对象或者一个类型名字(必填)</li>
<li>default：默认值(可选)</li>
<li>order：排序(可选)，只有3个值ascending(默认)，descending或ignore</li>
<li>aliases：别名，字符串数组(可选)</li>
</ol>
<p>一个Record类型例子，定义一个元素类型是Long的链表：</p>
<pre><code><span class="collection">{
  <span class="string">"type"</span>: <span class="string">"record"</span>, 
  <span class="string">"name"</span>: <span class="string">"LongList"</span>,
  <span class="string">"aliases"</span>: <span class="collection">[<span class="string">"LinkedLongs"</span>]</span>,                      // old name for this
  <span class="string">"fields"</span> : <span class="collection">[
    <span class="collection">{<span class="string">"name"</span>: <span class="string">"value"</span>, <span class="string">"type"</span>: <span class="string">"long"</span>}</span>,             // each element has a long
    <span class="collection">{<span class="string">"name"</span>: <span class="string">"next"</span>, <span class="string">"type"</span>: <span class="collection">[<span class="string">"null"</span>, <span class="string">"LongList"</span>]</span>}</span> // optional next element
  ]</span>
}</span>
</code></pre><h4 id="Enum">Enum</h4><p>枚举类型的类型名字是”enum”，还支持其它属性的设置：</p>
<p>name：枚举类型的名字(必填)<br>namespace：命名空间(可选)<br>aliases：字符串数组，别名(可选)<br>doc：说明文档(可选)<br>symbols：字符串数组，所有的枚举值(必填)，不允许重复数据。</p>
<p>一个枚举类型的例子：</p>
<pre><code>{ "<span class="attribute">type</span>": <span class="value"><span class="string">"enum"</span></span>,
  "<span class="attribute">name</span>": <span class="value"><span class="string">"Suit"</span></span>,
  "<span class="attribute">symbols</span>" : <span class="value">[<span class="string">"SPADES"</span>, <span class="string">"HEARTS"</span>, <span class="string">"DIAMONDS"</span>, <span class="string">"CLUBS"</span>]
</span>}
</code></pre><h4 id="Array">Array</h4><p>数组类型的类型名字是”array”并且只支持一个属性：</p>
<p>items：数组元素的schema</p>
<p>一个数组例子：</p>
<pre><code>{"<span class="attribute">type</span>": <span class="value"><span class="string">"array"</span></span>, "<span class="attribute">items</span>": <span class="value"><span class="string">"string"</span></span>}
</code></pre><h4 id="Map">Map</h4><p>Map类型的类型名字是”map”并且只支持一个属性：</p>
<p>values：map值的schema</p>
<p>Map的key必须是字符串。</p>
<p>一个Map例子：</p>
<pre><code>{"<span class="attribute">type</span>": <span class="value"><span class="string">"map"</span></span>, "<span class="attribute">values</span>": <span class="value"><span class="string">"long"</span></span>}
</code></pre><h4 id="Union">Union</h4><p>组合类型，表示各种类型的组合，使用数组进行组合。比如[“null”, “string”]表示类型可以为null或者string。</p>
<p>组合类型的默认值是看组合类型的第一个元素，因此如果一个组合类型包括null类型，那么null类型一般都会放在第一个位置，这样子的话这个组合类型的默认值就是null。</p>
<p>组合类型中不允许同一种类型的元素的个数不会超过1个，除了record，fixed和enum。比如组合类中有2个array类型或者2个map类型，这是不允许的。</p>
<p>组合类型不允许嵌套组合类型。</p>
<h4 id="Fixed">Fixed</h4><p>混合类型的类型名字是fixed，支持以下属性：</p>
<p>name：名字(必填)<br>namespace：命名空间(可选)<br>aliases：字符串数组，别名(可选)<br>size：一个整数，表示每个值的字节数(必填)</p>
<p>比如16个字节数的fixed类型例子如下：</p>
<pre><code>{"<span class="attribute">type</span>": <span class="value"><span class="string">"fixed"</span></span>, "<span class="attribute">size</span>": <span class="value"><span class="number">16</span></span>, "<span class="attribute">name</span>": <span class="value"><span class="string">"md5"</span></span>}
</code></pre><h2 id="1个Avro例子">1个Avro例子</h2><p>首先定义一个User的schema：</p>
<pre><code>{
"<span class="attribute">namespace</span>": <span class="value"><span class="string">"example.avro"</span></span>,
 "<span class="attribute">type</span>": <span class="value"><span class="string">"record"</span></span>,
 "<span class="attribute">name</span>": <span class="value"><span class="string">"User"</span></span>,
 "<span class="attribute">fields</span>": <span class="value">[
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"name"</span></span>, "<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_number"</span></span>,  "<span class="attribute">type</span>": <span class="value"><span class="string">"int"</span></span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_color"</span></span>, "<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>}
 ]
</span>}
</code></pre><p>User有3个属性，分别是name，favorite_number和favorite_color。</p>
<p>json文件内容：</p>
<pre><code>{"<span class="attribute">name</span>":<span class="value"><span class="string">"format"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value"><span class="number">1</span></span>,"<span class="attribute">favorite_color</span>":<span class="value"><span class="string">"red"</span></span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"format2"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value"><span class="number">2</span></span>,"<span class="attribute">favorite_color</span>":<span class="value"><span class="string">"black"</span></span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"format3"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value"><span class="number">666</span></span>,"<span class="attribute">favorite_color</span>":<span class="value"><span class="string">"blue"</span></span>}
</code></pre><p>使用avro工具将json文件转换成avro文件：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">fromjson</span> <span class="tag">--schema-file</span> <span class="tag">user</span><span class="class">.avsc</span> <span class="tag">user</span><span class="class">.json</span> &gt; <span class="tag">user</span><span class="class">.avro</span>
</code></pre><p>可以设置压缩格式：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">fromjson</span> <span class="tag">--codec</span> <span class="tag">snappy</span> <span class="tag">--schema-file</span> <span class="tag">user</span><span class="class">.avsc</span> <span class="tag">user</span><span class="class">.json</span> &gt; <span class="tag">user2</span><span class="class">.avro</span>
</code></pre><p>将avro文件反转换成json文件：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">tojson</span> <span class="tag">user</span><span class="class">.avro</span>
<span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">--pretty</span> <span class="tag">tojson</span> <span class="tag">user</span><span class="class">.avro</span>
</code></pre><p>得到avro文件的meta：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">getmeta</span> <span class="tag">user</span><span class="class">.avro</span>
</code></pre><p>输出：</p>
<pre><code>avro.codec    null
avro.<span class="keyword">schema</span>    {<span class="string">"type"</span>:<span class="string">"record"</span>,<span class="string">"name"</span>:<span class="string">"User"</span>,<span class="string">"namespace"</span>:<span class="string">"example.avro"</span>,<span class="string">"fields"</span>:[{<span class="string">"name"</span>:<span class="string">"name"</span>,<span class="string">"type"</span>:<span class="string">"string"</span>},{<span class="string">"name"</span>:<span class="string">"favorite_number"</span>,<span class="string">"type"</span>:<span class="string">"int"</span>},{<span class="string">"name"</span>:<span class="string">"favorite_color"</span>,<span class="string">"type"</span>:<span class="string">"string"</span>}]}
</code></pre><p>得到avro文件的schema：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">getschema</span> <span class="tag">user</span><span class="class">.avro</span>
</code></pre><p>将文本文件转换成avro文件：</p>
<pre><code><span class="tag">java</span> <span class="tag">-jar</span> <span class="tag">avro-tools-1</span><span class="class">.8</span><span class="class">.0</span><span class="class">.jar</span> <span class="tag">fromtext</span> <span class="tag">user</span><span class="class">.txt</span> <span class="tag">usertxt</span><span class="class">.avro</span>
</code></pre><h2 id="Avro使用生成的代码进行序列化和反序列化">Avro使用生成的代码进行序列化和反序列化</h2><p>以上面一个例子的schema为例讲解。</p>
<p>Avro可以根据schema自动生成对应的类：</p>
<pre><code>java -jar /path/to/avro-tools-<span class="number">1.8</span>.<span class="number">0</span><span class="class">.jar</span> compile schema user<span class="class">.avsc</span> .
</code></pre><p>user.avsc的namespace为example.avro，name为User。最终在当前目录生成的example/avro目录下有个User.java文件。</p>
<pre><code>├── <span class="tag">example</span>
│   └── <span class="tag">avro</span>
│       └── <span class="tag">User</span><span class="class">.java</span>
</code></pre><p><strong>使用Avro生成的代码创建User：</strong></p>
<pre><code><span class="keyword">User</span> <span class="title">user1</span> = new User();
user1.setName(<span class="string">"Format"</span>);
user1.setFavoriteColor(<span class="string">"red"</span>);
user1.setFavoriteNumber(<span class="number">666</span>);

<span class="keyword">User</span> <span class="title">user2</span> = new User(<span class="string">"Format2"</span>, <span class="number">66</span>, <span class="string">"blue"</span>);

<span class="keyword">User</span> <span class="title">user3</span> = User.newBuilder()
                .setName(<span class="string">"Format3"</span>)
                .setFavoriteNumber(<span class="number">6</span>)
                .setFavoriteColor(<span class="string">"black"</span>).build();
</code></pre><p>可以使用有参的构造函数和无参的构造函数，也可以使用Builder构造User。</p>
<p><strong>序列化：</strong></p>
<p>DatumWrite接口用来把java对象转换成内存中的序列化格式，SpecificDatumWriter用来生成类并且指定生成的类型。</p>
<p>最后使用DataFileWriter来进行具体的序列化，create方法指定文件和schema信息，append方法用来写数据，最后写完后close文件。</p>
<pre><code>DatumWriter&lt;User&gt; userDatumWriter = <span class="keyword">new</span> SpecificDatumWriter&lt;User&gt;(User.<span class="keyword">class</span>);
        DataFileWriter&lt;User&gt; dataFileWriter = <span class="keyword">new</span> DataFileWriter&lt;User&gt;(userDatumWriter);
dataFileWriter.create(user1.getSchema(), <span class="keyword">new</span> <span class="keyword">File</span>(<span class="string">"users.avro"</span>));
dataFileWriter.<span class="keyword">append</span>(user1);
dataFileWriter.<span class="keyword">append</span>(user2);
dataFileWriter.<span class="keyword">append</span>(user3);
dataFileWriter.close();
</code></pre><p><strong>反序列化：</strong></p>
<p>反序列化跟序列化很像，相应的Writer换成Reader。这里只创建一个User对象是为了性能优化，每次都重用这个User对象，如果文件量很大，对象分配和垃圾收集处理的代价很昂贵。如果不考虑性能，可以使用 for (User user : dataFileReader) 循环遍历对象</p>
<pre><code><span class="keyword">File</span> <span class="keyword">file</span> = <span class="keyword">new</span> <span class="keyword">File</span>(<span class="string">"users.avro"</span>);
DatumReader&lt;User&gt; userDatumReader = <span class="keyword">new</span> SpecificDatumReader&lt;User&gt;(User.<span class="keyword">class</span>);
DataFileReader&lt;User&gt; dataFileReader = <span class="keyword">new</span> DataFileReader&lt;User&gt;(<span class="keyword">file</span>, userDatumReader);
User user = <span class="keyword">null</span>;
<span class="keyword">while</span>(dataFileReader.hasNext()) {
    user = dataFileReader.<span class="keyword">next</span>(user);
    System.out.<span class="keyword">println</span>(user);
}
</code></pre><p>打印出：</p>
<pre><code>{"<span class="attribute">name</span>": <span class="value"><span class="string">"Format"</span></span>, "<span class="attribute">favorite_number</span>": <span class="value"><span class="number">666</span></span>, "<span class="attribute">favorite_color</span>": <span class="value"><span class="string">"red"</span></span>}
{"<span class="attribute">name</span>": <span class="value"><span class="string">"Format2"</span></span>, "<span class="attribute">favorite_number</span>": <span class="value"><span class="number">66</span></span>, "<span class="attribute">favorite_color</span>": <span class="value"><span class="string">"blue"</span></span>}
{"<span class="attribute">name</span>": <span class="value"><span class="string">"Format3"</span></span>, "<span class="attribute">favorite_number</span>": <span class="value"><span class="number">6</span></span>, "<span class="attribute">favorite_color</span>": <span class="value"><span class="string">"black"</span></span>}
</code></pre><h2 id="Avro不使用生成的代码进行序列化和反序列化">Avro不使用生成的代码进行序列化和反序列化</h2><p>虽然Avro为我们提供了根据schema自动生成类的方法，我们也可以自己创建类，不使用Avro的自动生成工具。</p>
<p><strong>创建User：</strong></p>
<p>首先使用Parser读取schema信息并且创建Schema类：</p>
<pre><code>Schema schema = <span class="keyword">new</span> Schema.Parser().parse(<span class="keyword">new</span> <span class="keyword">File</span>(<span class="string">"user.avsc"</span>));
</code></pre><p>有了Schema之后可以创建record：</p>
<pre><code>GenericRecord user1 = <span class="keyword">new</span> GenericData.Record(schema);
user1.<span class="keyword">put</span>(<span class="string">"name"</span>, <span class="string">"Format"</span>);
user1.<span class="keyword">put</span>(<span class="string">"favorite_number"</span>, <span class="number">666</span>);
user1.<span class="keyword">put</span>(<span class="string">"favorite_color"</span>, <span class="string">"red"</span>);

GenericRecord user2 = <span class="keyword">new</span> GenericData.Record(schema);
user2.<span class="keyword">put</span>(<span class="string">"name"</span>, <span class="string">"Format2"</span>);
user2.<span class="keyword">put</span>(<span class="string">"favorite_number"</span>, <span class="number">66</span>);
user2.<span class="keyword">put</span>(<span class="string">"favorite_color"</span>, <span class="string">"blue"</span>);
</code></pre><p>使用GenericRecord表示User，GenericRecord会根据schema验证字段是否正确，如果put进了不存在的字段 user1.put(“favorite_animal”, “cat”) ，那么运行的时候会得到AvroRuntimeException异常。</p>
<p><strong>序列化：</strong></p>
<p>序列化跟生成的User类似，只不过schema是自己构造的，不是User中拿的。</p>
<pre><code>Schema schema = <span class="keyword">new</span> Schema.Parser().parse(<span class="keyword">new</span> File(<span class="string">"user.avsc"</span>));
GenericRecord user1 = <span class="keyword">new</span> GenericData.Record(schema);
user1.<span class="keyword">put</span>(<span class="string">"name"</span>, <span class="string">"Format"</span>);
user1.<span class="keyword">put</span>(<span class="string">"favorite_number"</span>, <span class="number">666</span>);
user1.<span class="keyword">put</span>(<span class="string">"favorite_color"</span>, <span class="string">"red"</span>);

GenericRecord user2 = <span class="keyword">new</span> GenericData.Record(schema);
user2.<span class="keyword">put</span>(<span class="string">"name"</span>, <span class="string">"Format2"</span>);
user2.<span class="keyword">put</span>(<span class="string">"favorite_number"</span>, <span class="number">66</span>);
user2.<span class="keyword">put</span>(<span class="string">"favorite_color"</span>, <span class="string">"blue"</span>);

DatumWriter&lt;GenericRecord&gt; datumWriter = <span class="keyword">new</span> SpecificDatumWriter&lt;GenericRecord&gt;(schema);
DataFileWriter&lt;GenericRecord&gt; dataFileWriter = <span class="keyword">new</span> DataFileWriter&lt;GenericRecord&gt;(datumWriter);
dataFileWriter.create(schema, <span class="keyword">new</span> File(<span class="string">"users2.avro"</span>));
dataFileWriter.<span class="built_in">append</span>(user1);
dataFileWriter.<span class="built_in">append</span>(user2);
dataFileWriter.<span class="keyword">close</span>();
</code></pre><p><strong>反序列化：</strong></p>
<p>反序列化跟生成的User类似，只不过schema是自己构造的，不是User中拿的。</p>
<pre><code>Schema schema = <span class="keyword">new</span> Schema.Parser().parse(<span class="keyword">new</span> <span class="keyword">File</span>(<span class="string">"user.avsc"</span>));
<span class="keyword">File</span> <span class="keyword">file</span> = <span class="keyword">new</span> <span class="keyword">File</span>(<span class="string">"users2.avro"</span>);
DatumReader&lt;GenericRecord&gt; datumReader = <span class="keyword">new</span> SpecificDatumReader&lt;GenericRecord&gt;(schema);
DataFileReader&lt;GenericRecord&gt; dataFileReader = <span class="keyword">new</span> DataFileReader&lt;GenericRecord&gt;(<span class="keyword">file</span>, datumReader);
GenericRecord user = <span class="keyword">null</span>;
<span class="keyword">while</span>(dataFileReader.hasNext()) {
    user = dataFileReader.<span class="keyword">next</span>(user);
    System.out.<span class="keyword">println</span>(user);
}
</code></pre><p>打印出：</p>
<pre><code>{"<span class="attribute">name</span>": <span class="value"><span class="string">"Format"</span></span>, "<span class="attribute">favorite_number</span>": <span class="value"><span class="number">666</span></span>, "<span class="attribute">favorite_color</span>": <span class="value"><span class="string">"red"</span></span>}
{"<span class="attribute">name</span>": <span class="value"><span class="string">"Format2"</span></span>, "<span class="attribute">favorite_number</span>": <span class="value"><span class="number">66</span></span>, "<span class="attribute">favorite_color</span>": <span class="value"><span class="string">"blue"</span></span>}
</code></pre><h2 id="一些注意点">一些注意点</h2><p>Avro解析json文件的时候，如果类型是Record并且里面有字段是union并且允许空值的话，需要进行转换。因为[“bytes”, “string”]和[“int”,”long”]这2个union类型在json中是有歧义的，第一个union在json中都会被转换成string类型，第二个union在json中都会被转换成数字类型。</p>
<p>所以如果json值的null的话，在avro提供的json中直接写null，否则使用只有一个键值对的对象，键是类型，值的具体的值。</p>
<p>比如：</p>
<pre><code>{
"<span class="attribute">namespace</span>": <span class="value"><span class="string">"example.avro"</span></span>,
 "<span class="attribute">type</span>": <span class="value"><span class="string">"record"</span></span>,
 "<span class="attribute">name</span>": <span class="value"><span class="string">"User"</span></span>,
 "<span class="attribute">fields</span>": <span class="value">[
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"name"</span></span>, "<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_number"</span></span>,  "<span class="attribute">type</span>": <span class="value">[<span class="string">"int"</span>,<span class="string">"null"</span>]</span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_color"</span></span>, "<span class="attribute">type</span>": <span class="value">[<span class="string">"string"</span>,<span class="string">"null"</span>]</span>}
 ]
</span>}
</code></pre><p>在要转换成json文件的时候要写成这样：</p>
<pre><code>{"<span class="attribute">name</span>":<span class="value"><span class="string">"format"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value">{"<span class="attribute">int</span>":<span class="value"><span class="number">1</span></span>}</span>,"<span class="attribute">favorite_color</span>":<span class="value">{"<span class="attribute">string</span>":<span class="value"><span class="string">"red"</span></span>}</span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"format2"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value"><span class="literal">null</span></span>,"<span class="attribute">favorite_color</span>":<span class="value">{"<span class="attribute">string</span>":<span class="value"><span class="string">"black"</span></span>}</span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"format3"</span></span>,"<span class="attribute">favorite_number</span>":<span class="value">{"<span class="attribute">int</span>":<span class="value"><span class="number">66</span></span>}</span>,"<span class="attribute">favorite_color</span>":<span class="value"><span class="literal">null</span></span>}
</code></pre><h2 id="Spark读取Avro文件">Spark读取Avro文件</h2><p>直接遍历avro文件，得到GenericRecord进行处理：</p>
<pre><code>val conf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(<span class="string">"local"</span>)</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(<span class="string">"AvroTest"</span>)</span></span>

val sc = new <span class="function"><span class="title">SparkContext</span><span class="params">(conf)</span></span>

val rdd = sc<span class="class">.hadoopFile</span>[AvroWrapper[GenericRecord], NullWritable, AvroInputFormat[GenericRecord]](this<span class="class">.getClass</span><span class="class">.getResource</span>(<span class="string">"/"</span>)<span class="class">.toString</span> + <span class="string">"users.avro"</span>)

val nameRdd = rdd.<span class="function"><span class="title">map</span><span class="params">(s =&gt; s._1.datum()</span></span>.<span class="function"><span class="title">get</span><span class="params">(<span class="string">"name"</span>)</span></span>.toString)

nameRdd.<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>
</code></pre><h2 id="使用Avro需要注意的地方">使用Avro需要注意的地方</h2><p>笔者使用Avro的时候暂时遇到了下面2个坑。先记录一下，以后遇到新的坑会更新这篇文章。</p>
<p>1.如果定义了unions类型的字段，而且unions中有null选项的schema，比如如下schema：</p>
<pre><code>{
"<span class="attribute">namespace</span>": <span class="value"><span class="string">"example.avro"</span></span>,
 "<span class="attribute">type</span>": <span class="value"><span class="string">"record"</span></span>,
 "<span class="attribute">name</span>": <span class="value"><span class="string">"User2"</span></span>,
 "<span class="attribute">fields</span>": <span class="value">[
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"name"</span></span>, "<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_number"</span></span>,  "<span class="attribute">type</span>": <span class="value">[<span class="string">"null"</span>,<span class="string">"int"</span>]</span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_color"</span></span>, "<span class="attribute">type</span>": <span class="value">[<span class="string">"null"</span>,<span class="string">"string"</span>]</span>}
 ]
</span>}
</code></pre><p>这样的schema，如果不使用Avro自动生成的model代码进行insert，并且insert中的model数据有null数据的话。然后用spark读avro文件的话，会报org.apache.avro.AvroTypeException: Found null, expecting int … 这样的错误。</p>
<p>这一点很奇怪，但是使用Avro生成的Model进行insert的话，sprak读取就没有任何问题。 很困惑。</p>
<p>2.如果使用了Map类型的字段，avro生成的model中的Map的Key默认类型为CharSequence。这种model我们insert数据的话，用String是没有问题的。但是spark读取之后要根据Key拿这个Map数据的时候，永远得到的是null。</p>
<p>stackoverflow上有一个页面说到了这个问题。<a href="http://stackoverflow.com/questions/19728853/apache-avro-map-uses-charsequence-as-key" target="_blank" rel="external">http://stackoverflow.com/questions/19728853/apache-avro-map-uses-charsequence-as-key
</a></p>
<p>需要在map类型的字段里加上”avro.java.string”: “String”这个选项, 然后compile的时候使用-string参数即可。</p>
<p>比如以下这个schema：</p>
<pre><code>{
"<span class="attribute">namespace</span>": <span class="value"><span class="string">"example.avro"</span></span>,
 "<span class="attribute">type</span>": <span class="value"><span class="string">"record"</span></span>,
 "<span class="attribute">name</span>": <span class="value"><span class="string">"User3"</span></span>,
 "<span class="attribute">fields</span>": <span class="value">[
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"name"</span></span>, "<span class="attribute">type</span>": <span class="value"><span class="string">"string"</span></span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_number"</span></span>,  "<span class="attribute">type</span>": <span class="value">[<span class="string">"null"</span>,<span class="string">"int"</span>]</span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"favorite_color"</span></span>, "<span class="attribute">type</span>": <span class="value">[<span class="string">"null"</span>,<span class="string">"string"</span>]</span>},
     {"<span class="attribute">name</span>": <span class="value"><span class="string">"scores"</span></span>, "<span class="attribute">type</span>": <span class="value">[<span class="string">"null"</span>, {"<span class="attribute">type</span>": <span class="value"><span class="string">"map"</span></span>, "<span class="attribute">values</span>": <span class="value"><span class="string">"string"</span></span>, "<span class="attribute">avro.java.string</span>": <span class="value"><span class="string">"String"</span></span>}]</span>}
 ]
</span>}
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Apache Avro是一个数据序列化系统, 提供丰富的数据结构，使用快速的压缩二进制数据格式，提供容器文件用于持久化数据 ...]]>
    
    </summary>
    
      <category term="avro" scheme="http://fangjian0423.github.io/tags/avro/"/>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="avro" scheme="http://fangjian0423.github.io/categories/avro/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark DataFrame介绍]]></title>
    <link href="http://fangjian0423.github.io/2016/02/17/spark-sql/"/>
    <id>http://fangjian0423.github.io/2016/02/17/spark-sql/</id>
    <published>2016-02-17T01:22:22.000Z</published>
    <updated>2016-02-17T03:25:25.000Z</updated>
    <content type="html"><![CDATA[<h2 id="DataFrame是什么">DataFrame是什么</h2><p>DataFrame是一个以命名列方式组织的分布式数据集。在概念上，它跟关系型数据库中的一张表或者1个Python(或者R)中的data frame一样，但是比他们更优化。DataFrame可以根据结构化的数据文件、hive表、外部数据库或者已经存在的RDD构造。</p>
<h2 id="DataFrame的创建">DataFrame的创建</h2><p>Spark DataFrame可以从一个已经存在的RDD、hive表或者数据源中创建。</p>
<p>以下一个例子就表示一个DataFrame基于一个json文件创建：</p>
<pre><code>val sc: SparkContext <span class="comment">// An existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SQLContext</span>(sc)

val df = sqlContext<span class="class">.read</span><span class="class">.json</span>(<span class="string">"examples/src/main/resources/people.json"</span>)

<span class="comment">// Displays the content of the DataFrame to stdout</span>
df.<span class="function"><span class="title">show</span><span class="params">()</span></span>
</code></pre><h2 id="DataFrame的操作">DataFrame的操作</h2><p>直接以1个例子来说明DataFrame的操作：</p>
<p>json文件内容：</p>
<pre><code>{"<span class="attribute">name</span>":<span class="value"><span class="string">"Michael"</span></span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"Andy"</span></span>, "<span class="attribute">age</span>":<span class="value"><span class="number">30</span></span>}
{"<span class="attribute">name</span>":<span class="value"><span class="string">"Justin"</span></span>, "<span class="attribute">age</span>":<span class="value"><span class="number">19</span></span>}
</code></pre><p>程序内容：</p>
<pre><code>val conf = new SparkConf().setMaster("local").setAppName("DataFrameTest")

val sc = new SparkContext(conf)

val sqlContext = new SQLContext(sc)

val df = sqlContext.read.json(this.getClass.getResource("/").toString + "people.json")

<span class="header">  /** 展示DataFrame的内容
+----+-------+</span>
<span class="header">| age|   name|
+----+-------+</span>
|null|Michael|
|  30|   Andy|
|  19| Justin|
<span class="code">+----+</span>-------+  
<span class="code">  **/</span>
df.show()

/** 以树的形式打印出DataFrame的schema
root
<span class="code"> |-- age: long (nullable = true)</span>
<span class="code"> |-- name: string (nullable = true)</span>
*<span class="strong">*/
df.printSchema()

</span><span class="header">/** 打印出name列的数据
+-------+</span>
<span class="header">|   name|
+-------+</span>
|Michael|
|   Andy|
| Justin|
<span class="code">+-------+</span>   
*<span class="strong">*/
df.select("name").show()

</span><span class="header">/** 打印出name列和age列+1的数据，DataFrame的apply方法返回Column
+-------+---------+</span>
<span class="header">|   name|(age + 1)|
+-------+---------+</span>
|Michael|     null|
|   Andy|       31|
<span class="header">| Justin|       20|
+-------+---------+</span>
*<span class="strong">*/
df.select(df("name"), df("age") + 1).show()

</span><span class="header">/** 添加过滤条件，过滤出age字段大于21的数据
+---+----+</span>
<span class="header">|age|name|
+---+----+</span>
<span class="header">| 30|Andy|
+---+----+</span>
*<span class="strong">*/
df.filter(df("age") &gt; 21).show()

</span><span class="header">/** 以age字段分组进行统计
+----+-----+</span>
<span class="header">| age|count|
+----+-----+</span>
|null|    1|
|  19|    1|
<span class="header">|  30|    1|
+----+-----+</span>
*<span class="strong">*/
df.groupBy(df("age")).count().show()</span>
</code></pre><h2 id="使用反射推断出Schema">使用反射推断出Schema</h2><p>Spark SQL的Scala接口支持将包括case class数据的RDD转换成DataFrame。</p>
<p>case class定义表的schema，case class的属性会被读取并且成为列的名字，这里case class也可以被当成别的case class的属性或者是复杂的类型，比如Sequence或Array。</p>
<p>RDD会被隐式转换成DataFrame并且被注册成一个表，这个表可以被用在查询语句中：</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SQLContext</span>(sc)
<span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span>
import sqlContext<span class="class">.implicits</span>._

<span class="comment">// Define the schema using a case class.</span>
<span class="comment">// <span class="doctag">Note:</span> Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span>
<span class="comment">// you can use custom classes that implement the Product interface.</span>
case class <span class="function"><span class="title">Person</span><span class="params">(name: String, age: Int)</span></span>

<span class="comment">// Create an RDD of Person objects and register it as a table.</span>
val people = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"examples/src/main/resources/people.txt"</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(_.split(<span class="string">","</span>)</span></span>).<span class="function"><span class="title">map</span><span class="params">(p =&gt; Person(p(<span class="number">0</span>)</span></span>, <span class="function"><span class="title">p</span><span class="params">(<span class="number">1</span>)</span></span><span class="class">.trim</span><span class="class">.toInt</span>)).<span class="function"><span class="title">toDF</span><span class="params">()</span></span>
people.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"people"</span>)</span></span>

<span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span>
val teenagers = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span></span>

<span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span>
<span class="comment">// The columns of a row in the result can be accessed by field index:</span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>

<span class="comment">// or by field name:</span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t.getAs[String](<span class="string">"name"</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>

<span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(_.getValuesMap[Any](List(<span class="string">"name"</span>, <span class="string">"age"</span>)</span></span>)).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>
<span class="comment">// Map("name" -&gt; "Justin", "age" -&gt; 19)</span>
</code></pre><h2 id="使用编程指定Schema">使用编程指定Schema</h2><p>当case class不能提前确定（例如，记录的结构是经过编码的字符串，或者一个文本集合将会被解析，不同的字段投影给不同的用户），一个 DataFrame 可以通过三步来创建。</p>
<p>1.从原来的 RDD 创建一个行的 RDD<br>2.创建由一个 StructType 表示的模式与第一步创建的 RDD 的行结构相匹配<br>3.在行 RDD 上通过 applySchema 方法应用模式</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
<span class="variable"><span class="keyword">val</span> sqlContext</span> = new org.apache.spark.sql.SQLContext(sc)

<span class="comment">// Create an RDD</span>
<span class="variable"><span class="keyword">val</span> people</span> = sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)

<span class="comment">// The schema is encoded in a string</span>
<span class="variable"><span class="keyword">val</span> schemaString</span> = <span class="string">"name age"</span>

<span class="comment">// Import Row.</span>
<span class="keyword">import</span> org.apache.spark.sql.Row;

<span class="comment">// Import Spark SQL data types</span>
<span class="keyword">import</span> org.apache.spark.sql.types.{StructType,StructField,StringType};

<span class="comment">// Generate the schema based on the string of schema</span>
<span class="variable"><span class="keyword">val</span> schema</span> =
  StructType(
    schemaString.split(<span class="string">" "</span>).map(fieldName =&gt; StructField(fieldName, StringType, <span class="literal">true</span>)))

<span class="comment">// Convert records of the RDD (people) to Rows.</span>
<span class="variable"><span class="keyword">val</span> rowRDD</span> = people.map(_.split(<span class="string">","</span>)).map(p =&gt; Row(p(<span class="number">0</span>), p(<span class="number">1</span>).trim))

<span class="comment">// Apply the schema to the RDD.</span>
<span class="variable"><span class="keyword">val</span> peopleDataFrame</span> = sqlContext.createDataFrame(rowRDD, schema)

<span class="comment">// Register the DataFrames as a table.</span>
peopleDataFrame.registerTempTable(<span class="string">"people"</span>)

<span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span>
<span class="variable"><span class="keyword">val</span> results</span> = sqlContext.sql(<span class="string">"SELECT name FROM people"</span>)

<span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span>
<span class="comment">// The columns of a row in the result can be accessed by field index or by field name.</span>
results.map(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)).collect().foreach(println)
</code></pre><h2 id="数据源">数据源</h2><p>Spark SQL默认使用的数据源是parquet(可以通过spark.sql.sources.default修改)。</p>
<pre><code>val df = sqlContext<span class="class">.read</span><span class="class">.load</span>(<span class="string">"examples/src/main/resources/users.parquet"</span>)
df.<span class="function"><span class="title">select</span><span class="params">(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>)</span></span><span class="class">.write</span><span class="class">.save</span>(<span class="string">"namesAndFavColors.parquet"</span>)
</code></pre><p>可以在读取数据源的时候指定一些往外的参数。数据源也可以使用全名称，比如org.apache.spark.sql.parquet，但是内置的数据源可以使用短名称，比如json, parquet, jdbc。任何类型的DataFrame都可以使用这种方式转换成其他类型：</p>
<pre><code>val df = sqlContext<span class="class">.read</span><span class="class">.format</span>(<span class="string">"json"</span>).<span class="function"><span class="title">load</span><span class="params">(<span class="string">"examples/src/main/resources/people.json"</span>)</span></span>
df.<span class="function"><span class="title">select</span><span class="params">(<span class="string">"name"</span>, <span class="string">"age"</span>)</span></span><span class="class">.write</span><span class="class">.format</span>(<span class="string">"parquet"</span>).<span class="function"><span class="title">save</span><span class="params">(<span class="string">"namesAndAges.parquet"</span>)</span></span>
</code></pre><p>使用read方法读取数据源得到DataFrame，还可以使用sql直接查询文件的方式：</p>
<pre><code>val df = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`"</span>)</span></span>
</code></pre><p>保存模式：</p>
<p>保存方法会需要一个可选参数SaveMode，用于处理已经存在的数据。这些保存模式内部不会用到锁的概念，也不是一个原子操作。如果使用了Overwrite这种保存模式，那么写入数据前会清空之前的老数据。</p>
<table>
<thead>
<tr>
<th style="text-align:center">Scala/Java</th>
<th style="text-align:center">具体值</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">SaveMode.ErrorIfExists (默认值)</td>
<td style="text-align:center">“error” (默认值)</td>
<td style="text-align:center">当保存DataFrame到数据源的时候，如果数据源文件已经存在，那么会抛出异常</td>
</tr>
<tr>
<td style="text-align:center">SaveMode.Append</td>
<td style="text-align:center">“append”</td>
<td style="text-align:center">如果数据源文件已经存在，append到文件末尾</td>
</tr>
<tr>
<td style="text-align:center">SaveMode.Overwrite</td>
<td style="text-align:center">“overwrite”</td>
<td style="text-align:center">如果数据源文件已经存在，清空数据</td>
</tr>
<tr>
<td style="text-align:center">SaveMode.Ignore</td>
<td style="text-align:center">“ignore”</td>
<td style="text-align:center">如果数据源文件已经存在，不做任何处理。跟SQL中的 CREATE TABLE IF NOT EXISTS 类似</td>
</tr>
</tbody>
</table>
<p>持久化表：</p>
<p>当使用HiveContext的时候，使用saveAsTable方法可以把DataFrame持久化成表。跟registerTempTable方法不一样，saveAsTable方法会把DataFrame持久化成表，并且创建一个数据的指针到HiveMetastore对象中。只要获得了同一个HiveMetastore对象的链接，当Spark程序重启的时候，saveAsTable持久化后的表依然会存在。一个DataFrame持久化成一个table也可以通过SQLContext的table方法，参数就是表的名字。</p>
<p>默认情况下，saveAsTable方法会创建一个”被管理的表”，被管理的表的意思是说表中数据的位置会被HiveMetastore所控制，如果表被删除了，HiveMetastore中的数据也相当于被删除了。</p>
<h3 id="Parquet_Files">Parquet Files</h3><p>parquet是一种基于列的存储格式，并且可以被很多框架所支持。Spark SQL支持parquet文件的读和写操作，并且会自动维护原始数据的schema，当写一个parquet文件的时候，所有的列都允许为空。</p>
<h4 id="加载Parquet文件">加载Parquet文件</h4><pre><code><span class="comment">// sqlContext from the previous example is used in this example.</span>
<span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span>
import sqlContext<span class="class">.implicits</span>._

val people: RDD[Person] = ... <span class="comment">// An RDD of case class objects, from the previous example.</span>

<span class="comment">// The RDD is implicitly converted to a DataFrame by implicits, allowing it to be stored using Parquet.</span>
people<span class="class">.write</span><span class="class">.parquet</span>(<span class="string">"people.parquet"</span>)

<span class="comment">// Read in the parquet file created above. Parquet files are self-describing so the schema is preserved.</span>
<span class="comment">// The result of loading a Parquet file is also a DataFrame.</span>
val parquetFile = sqlContext<span class="class">.read</span><span class="class">.parquet</span>(<span class="string">"people.parquet"</span>)

<span class="comment">//Parquet files can also be registered as tables and then used in SQL statements.</span>
parquetFile.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"parquetFile"</span>)</span></span>
val teenagers = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span></span>
teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>
</code></pre><h4 id="Parquet文件的Partition">Parquet文件的Partition</h4><p>Parquet文件可以根据列自动进行分区，只需要调用DataFrameWriter的partitionBy方法即可，该方法需要的参数是需要进行分区的列。比如需要分区成这样：</p>
<pre><code>path
└── <span class="keyword">to</span>
    └── table
        ├── gender=male
        │   ├── <span class="attribute">...</span>
        │   │
        │   ├── country=US
        │   │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
        │   ├── country=<span class="literal">CN</span>
        │   │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
        │   └── <span class="attribute">...</span>
        └── gender=female
            ├── <span class="attribute">...</span>
            │
            ├── country=US
            │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
            ├── country=<span class="literal">CN</span>
            │   └── <span class="built_in">data</span><span class="built_in">.</span>parquet
            └── <span class="attribute">...</span>
</code></pre><p>这个需要DataFrame就需要4列，分别是name，age，gender和country，write的时候如下：</p>
<pre><code>dataFrame<span class="class">.write</span><span class="class">.partitionBy</span>(<span class="string">"gender"</span>, <span class="string">"country"</span>).<span class="function"><span class="title">parquet</span><span class="params">(<span class="string">"path"</span>)</span></span>
</code></pre><h4 id="Schema_Merging">Schema Merging</h4><p>像ProtocolBuffer，Avro，Thrift一样，Parquet也支持schema的扩展。</p>
<p>由于schema的自动扩展是一次昂贵的操作，所以默认情况下不是开启的，可以根据以下设置打开：</p>
<p>读parquet文件的时候设置参数mergeSchema为true或者设置全局的sql属性spark.sql.parquet.mergeSchema为true：</p>
<pre><code><span class="comment">// sqlContext from the previous example is used in this example.</span>
<span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span>
import sqlContext<span class="class">.implicits</span>._

<span class="comment">// Create a simple DataFrame, stored into a partition directory</span>
val df1 = sc.<span class="function"><span class="title">makeRDD</span><span class="params">(<span class="number">1</span> to <span class="number">5</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(i =&gt; (i, i * <span class="number">2</span>)</span></span>).<span class="function"><span class="title">toDF</span><span class="params">(<span class="string">"single"</span>, <span class="string">"double"</span>)</span></span>
df1<span class="class">.write</span><span class="class">.parquet</span>(<span class="string">"data/test_table/key=1"</span>)

<span class="comment">// Create another DataFrame in a new partition directory,</span>
<span class="comment">// adding a new column and dropping an existing column</span>
val df2 = sc.<span class="function"><span class="title">makeRDD</span><span class="params">(<span class="number">6</span> to <span class="number">10</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(i =&gt; (i, i * <span class="number">3</span>)</span></span>).<span class="function"><span class="title">toDF</span><span class="params">(<span class="string">"single"</span>, <span class="string">"triple"</span>)</span></span>
df2<span class="class">.write</span><span class="class">.parquet</span>(<span class="string">"data/test_table/key=2"</span>)

<span class="comment">// Read the partitioned table</span>
val df3 = sqlContext<span class="class">.read</span><span class="class">.option</span>(<span class="string">"mergeSchema"</span>, <span class="string">"true"</span>).<span class="function"><span class="title">parquet</span><span class="params">(<span class="string">"data/test_table"</span>)</span></span>
df3.<span class="function"><span class="title">printSchema</span><span class="params">()</span></span>

<span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span>
<span class="comment">// with the partitioning column appeared in the partition directory paths.</span>
<span class="comment">// root</span>
<span class="comment">// |-- single: int (nullable = true)</span>
<span class="comment">// |-- double: int (nullable = true)</span>
<span class="comment">// |-- triple: int (nullable = true)</span>
<span class="comment">// |-- key : int (nullable = true)</span>
</code></pre><h3 id="JSON数据源">JSON数据源</h3><p>本文之前的一个例子就是使用的JSON数据源，使用SQLContext.read.json()读取一个带有String类型的RDD或者一个json文件。</p>
<p>需要注意的是json文件不是一个典型的json格式的文件，每一行都是一个json对象。</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SQLContext</span>(sc)

<span class="comment">// A JSON dataset is pointed to by path.</span>
<span class="comment">// The path can be either a single text file or a directory storing text files.</span>
val path = <span class="string">"examples/src/main/resources/people.json"</span>
val people = sqlContext<span class="class">.read</span><span class="class">.json</span>(path)

<span class="comment">// The inferred schema can be visualized using the printSchema() method.</span>
people.<span class="function"><span class="title">printSchema</span><span class="params">()</span></span>
<span class="comment">// root</span>
<span class="comment">//  |-- age: integer (nullable = true)</span>
<span class="comment">//  |-- name: string (nullable = true)</span>

<span class="comment">// Register this DataFrame as a table.</span>
people.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"people"</span>)</span></span>

<span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span>
val teenagers = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span></span>

<span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span>
<span class="comment">// an RDD[String] storing one JSON object per string.</span>
val anotherPeopleRDD = sc.parallelize(
  <span class="string">""</span><span class="string">"{"</span>name<span class="string">":"</span>Yin<span class="string">","</span>address<span class="string">":{"</span>city<span class="string">":"</span>Columbus<span class="string">","</span>state<span class="string">":"</span>Ohio<span class="string">"}}"</span><span class="string">""</span> :: Nil)
val anotherPeople = sqlContext<span class="class">.read</span><span class="class">.json</span>(anotherPeopleRDD)
</code></pre><h3 id="Hive表">Hive表</h3><p>需要使用HiveContext。</p>
<pre><code><span class="comment">// sc is an existing SparkContext.</span>
val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.hive</span><span class="class">.HiveContext</span>(sc)

sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"</span>)</span></span>
sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)</span></span>

<span class="comment">// Queries are expressed in HiveQL</span>
sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"FROM src SELECT key, value"</span>)</span></span>.<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span>
</code></pre><h3 id="JDBC">JDBC</h3><p>直接使用load方法加载：</p>
<pre><code>sqlContext<span class="built_in">.</span>load(<span class="string">"jdbc"</span>, <span class="built_in">Map</span>(<span class="string">"url"</span> <span class="subst">-&gt; </span><span class="string">"jdbc:mysql://localhost:3306/your_database?user=your_user&amp;password=your_password"</span>, <span class="string">"dbtable"</span> <span class="subst">-&gt; </span><span class="string">"your_table"</span>))
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[DataFrame是一个以命名列方式组织的分布式数据集。在概念上，它跟关系型数据库中的一张表或者1个Python(或者R)中的data frame一样，但是比他们更优化。DataFrame可以根据结构化的数据文件、hive表、外部数据库或者已经存在的RDD构造 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/tags/spark/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark Streaming编程指南笔记]]></title>
    <link href="http://fangjian0423.github.io/2016/02/10/sparkstreaming-programming-guide/"/>
    <id>http://fangjian0423.github.io/2016/02/10/sparkstreaming-programming-guide/</id>
    <published>2016-02-09T17:36:17.000Z</published>
    <updated>2016-02-09T17:46:12.000Z</updated>
    <content type="html"><![CDATA[<h2 id="概述">概述</h2><p>Spark Streaming是Spark核心API的扩展，用于处理实时数据流。Spark Streaming处理的数据源可以是Kafka，Flume，Twitter，ZeroMQ，Kinesis或者Tcp Sockets，这些数据可以使用map，reduce，join，window方法进行处转换，还可以直接使用Spark内置的机器学习算法，图算法包来处理数据。</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt=""></p>
<p>最终处理后的数据可以存入文件系统，数据库。</p>
<p>Spark Streaming内部接收到实时数据之后，会把数据分成几个批次，这些批次数据会被Spark引擎处理并生成各个批次的结果。</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-flow.png" alt=""></p>
<p>Spark Streaming提供了一个叫做<strong>discretized stream 或者 DStream</strong>的抽象概念，表示一段连续的数据流。DStream会在数据源中的数据流中创建，或者在别的DStream中使用类似map，join方法创建。一个DStream表示一个RDD序列。</p>
<h2 id="一个快速例子">一个快速例子</h2><p>以一个TCP Socket监听接收数据，并计算单词的个数为例子讲解。</p>
<p>首先，需要import Spark Streaming中的一些类和StreamingContext中的一些隐式转换。我们会创建一个带有2个线程，1秒一个批次的StreamingContext。</p>
<pre><code><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>
<span class="keyword">import</span> org.apache.spark.streaming.{<span class="type">Seconds</span>, <span class="type">StreamingContext</span>}

<span class="class"><span class="keyword">object</span> <span class="title">SparkStreamTest</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">App</span> {</span>

  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)

  <span class="comment">// 创建一个StreamingContext，每1秒钟处理一次计算程序</span>
  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))

  <span class="comment">// 使用StreamingContext创建DStream，DStream表示TCP源中的流数据. lines这个DStream表示接收到的服务器数据，每一行都是文本</span>
  <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)

  <span class="comment">// 使用flatMap将每一行中的文本转换成每个单词，并产生一个新的DStream。</span>
  <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))

  <span class="comment">// 使用map方法将每个单词转换成tuple</span>
  <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))

  <span class="comment">// 使用reduceByKey计算出每个单词的出现次数</span>
  <span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)

  wordCounts.print()

  ssc.start() <span class="comment">// 开始计算</span>
  ssc.awaitTermination() <span class="comment">// 等待计算结束</span>
}
</code></pre><p>在运行这段代码之前，首先先起一个netcat服务：</p>
<pre><code>nc -lk <span class="number">9999</span>
</code></pre><p>之后比如输入hello world之后，控制台会打印出如下数据：</p>
<pre><code><span class="code">-------------------------------------------
Time: 1454684570000 ms
-------------------------------------------</span>
(hello,1)
(world,1)
</code></pre><h2 id="基础概念">基础概念</h2><h3 id="Linking(SparkStreaming的连接)">Linking(SparkStreaming的连接)</h3><p>写Spark Streaming程序需要一些依赖。使用maven的话加入以下依赖：</p>
<pre><code><span class="tag">&lt;<span class="title">dependency</span>&gt;</span>
    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span>
    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-streaming_2.10<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span>
    <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.6.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span>
<span class="tag">&lt;/<span class="title">dependency</span>&gt;</span>
</code></pre><p>使用sbt的话，加入以下依赖：</p>
<pre><code><span class="title">libraryDependencies</span> += <span class="string">"org.apache.spark"</span> % <span class="string">"spark-streaming_2.10"</span> % <span class="string">"1.6.0"</span>
</code></pre><p>SparkStreaming核心不提供一些数据源的依赖，需要手动添加，一些数据源对应的Artifact如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">数据源</th>
<th style="text-align:center">Artifact</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Kafka</td>
<td style="text-align:center">spark-streaming-kafka_2.10</td>
</tr>
<tr>
<td style="text-align:center">Flume</td>
<td style="text-align:center">spark-streaming-flume_2.10</td>
</tr>
<tr>
<td style="text-align:center">Kinesis</td>
<td style="text-align:center">spark-streaming-kinesis-asl_2.10 [Amazon Software License]</td>
</tr>
<tr>
<td style="text-align:center">Twitter</td>
<td style="text-align:center">spark-streaming-twitter_2.10</td>
</tr>
<tr>
<td style="text-align:center">ZeroMQ</td>
<td style="text-align:center">spark-streaming-zeromq_2.10</td>
</tr>
<tr>
<td style="text-align:center">MQTT</td>
<td style="text-align:center">spark-streaming-mqtt_2.10</td>
</tr>
</tbody>
</table>
<h3 id="StreamingContext的初始化">StreamingContext的初始化</h3><p>StreamingContext的创建是Spark Streaming程序中最重要的一环。</p>
<p>可以根据SparkConf对象创建出StreamingContext对象：</p>
<pre><code>import org<span class="class">.apache</span><span class="class">.spark</span>._
import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.streaming</span>._

val conf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(appName)</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(master)</span></span>
val ssc = new <span class="function"><span class="title">StreamingContext</span><span class="params">(conf, Seconds(<span class="number">1</span>)</span></span>)
</code></pre><p>appName参数是应用程序的名字，在cluster UI中显示的就是这个名字。master参数的意义跟spark中master参数的意义是一样的。</p>
<p>StreamingContext内部会创建SparkContext，可以使用StreamingContext内部的sparkContext获得。</p>
<pre><code>ssc<span class="class">.sparkContext</span> <span class="comment">// 得到SparkContext</span>
</code></pre><p>StreamingContext也可以根据SparkContext创建：</p>
<pre><code>import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.streaming</span>._

val sc = ...                <span class="comment">// existing SparkContext</span>
val ssc = new <span class="function"><span class="title">StreamingContext</span><span class="params">(sc, Seconds(<span class="number">1</span>)</span></span>)
</code></pre><p>StreamingContext创建之后，可以做以下几点：</p>
<p>1.创建DStreams定义数据源<br>2.使用DStreams的transformation和output operations用于计算<br>3.使用streamingContext的start方法接收数据<br>4.使用streamingContext的awaitTermination方法等待处理结果<br>5.可以使用streamingContext的stop方法停止程序</p>
<p>一些需要注意的点：</p>
<p>1.context开始启动之后，一些streaming的计算不允许发生<br>2.context停掉之后不能重启<br>3.一个JVM在同一时刻只能有一个StreamingContext可以激活<br>4.StreamingContext中的stop方法内部也会stop SparkContext。如果只想stop StreamingContext，那么调用stop方法的时候参数设置为false<br>5.一个SparkContext可以用来创建多个StreamingContexts，只要上一个StreamingContext在下一个StreamingContext创建之前停掉</p>
<h3 id="Discretized_Streams_(DStreams)">Discretized Streams (DStreams)</h3><p>DStreams和Discretized Streams在Spark Streaming中代表相同的意思：</p>
<p>1.一段连续的数据流<br>2.数据源中接收到的数据流<br>3.使用transforming处理过的流数据</p>
<p>Spark内部一个DStream表示一段连续的RDD。DStream中每段RDD表示一段时间内的RDD，效果如下：</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream.png" alt=""></p>
<p>DStream可以使用一些transformation操作将内部的RDD转换成另外一种RDD。比如之前的一个单词统计例子中就将一行文本的DStream转换成每个单词的DStream，过程如下：</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream-ops.png" alt=""></p>
<h3 id="Input_DStreams_and_Receivers(数据源和接收器)">Input DStreams and Receivers(数据源和接收器)</h3><p>Input DStreams是DStreams从streaming source中接收到的输入流数据。在之前分析的一个单词统计例子中，lines就是个Input DStream，表示接收到的服务器数据，每一行都是文本。</p>
<p>每一个Input DStream(除了file stream)都会关联一个Receiver对象，这个Receiver对象的作用是接收数据源中的数据并存储在内存中。</p>
<p>Spark Streaming提供了两种类型的内置数据源：</p>
<p>1.基础数据源。可以直接使用StreamingContext的API，比如文件系统，socket连接，Akka。<br>2.高级数据源。比如Flume，Kafka，Kinesis，Twitter等可以使用工具类的数据源。使用这些数据源需要对应的依赖，在Linking章节中已经介绍过。</p>
<p>如果想在streaming程序中并行地接收多个数据源，需要创建多个Input DStream，有个多个Input DStream的话那就会对应地有多个Receiver。但是需要记住的是，Spark的worker/executor模式是一个相当耗时的任务，因此服务器的配置需要够好才能支撑多个Input DStream。</p>
<p>一些注意点：</p>
<p>1.当本地跑Spark Streaming程序的时候，不要使用”local”或者”local[1]”设置master URL。因为这两种master URL只会使用1个线程。当使用比如Flume，Kafka，socket这些数据源的时候，因为只有一个线程跑receiver接收数据，那么没有其他线程去处理接收后的数据了。所以，当在本地跑Spark Streaming程序的时候，需要将master URL设置为local[n]，n需要大于receiver的个数。<br>2.服务器的核数需要大于receiver的个数。否则程序只会接收数据，而不会处理数据。</p>
<h4 id="Basic_Sources(基础数据源)">Basic Sources(基础数据源)</h4><p>基础数据源刚刚分析过，StreamingContext的API可以使用如文件系统，socket连接，Akka作为输入源。socket连接本文以开始的例子中已经使用过了。</p>
<p>文件系统的输入源会读取文件或任何支持HDFS API(比如HDFS，S3，NFS)的文件系统的数据：</p>
<pre><code>streamingContext.fileStream[<span class="link_label">KeyClass, ValueClass, InputFormatClass</span>](<span class="link_url">dataDirectory</span>)
</code></pre><p>Spark Streaming会监测dataDirectory目录并且会处理这个目录中新创建的文件(老文件写新数据的话不会被支持)。使用文件数据源还需要这几点：</p>
<p>1.所有文件的数据格式必须相同<br>2.dataDirectory目录中的文件必须是新创建的，也可以是从别的目录move进来的<br>3.文件内部的数据更改之后，新更改的数据不会被处理</p>
<p>对于简单的文件，可以使用streamingContext的textFileStream方法处理。</p>
<h4 id="Advanced_Sources(高级数据源)">Advanced Sources(高级数据源)</h4><p>高级数据源需要一些非Spark依赖。Spark Streaming把创建DStream的API移到了各自的API里。如果想创建一个使用Twitter的数据源，需要做以下三步：</p>
<p>1.添加对应的Twitter依赖spark-streaming-twitter_2.10到项目里<br>2.import这个类TwitterUtils，使用TwitterUtils.createStream创建DStream<br>3.部署</p>
<h4 id="Custom_Sources(自定义数据源)">Custom Sources(自定义数据源)</h4><p>要实现一个自定义的数据源，需要实现一个自定义的receiver</p>
<h4 id="Receiver_Reliability(接收器的可靠性)">Receiver Reliability(接收器的可靠性)</h4><p>基于可靠性的数据源分为两种。</p>
<p>1.可靠的接收器(Reliable Receiver)：一个可靠的接收器接收到数据之后会给数据源发送消息表示自己已经接收到数据<br>2.不可靠的接收器(Unreliable Receiver)：一个不可靠的接收器不会发送消息给数据源。</p>
<p>想写出一个可靠的接收器可以参考 <a href="http://spark.apache.org/docs/latest/streaming-custom-receivers.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/streaming-custom-receivers.html</a></p>
<h3 id="DStreams的Transformations操作">DStreams的Transformations操作</h3><p>DStream的Transformations操作跟RDD的Transformations操作类似，</p>
<table>
<thead>
<tr>
<th style="text-align:center">Transformation</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">map(func)</td>
<td style="text-align:center">根据func函数生成一个新的DStream</td>
</tr>
<tr>
<td style="text-align:center">flatMap(func)</td>
<td style="text-align:center">跟map方法类似，但是每一项可以返回多个值。func函数的返回值是一个集合</td>
</tr>
<tr>
<td style="text-align:center">filter(func)</td>
<td style="text-align:center">根据func函数返回true的数据集</td>
</tr>
<tr>
<td style="text-align:center">repartition(numPartitions)</td>
<td style="text-align:center">重新给 DStream 分区</td>
</tr>
<tr>
<td style="text-align:center">union(otherStream)</td>
<td style="text-align:center">取2个DStream的并集，得到一个新的DStream</td>
</tr>
<tr>
<td style="text-align:center">count()</td>
<td style="text-align:center">返回一个新的只有一个元素的DStream，这个元素就是DStream中的所有RDD的个数</td>
</tr>
<tr>
<td style="text-align:center">reduce(func)</td>
<td style="text-align:center">返回一个新的只有一个元素的DStream，这个元素就是DStream中的所有RDD通过func函数聚合得到的结果</td>
</tr>
<tr>
<td style="text-align:center">countByValue()</td>
<td style="text-align:center">如果DStream的类型为K，那么返回一个新的DStream，这个新的DStream中的元素类型是(K, Long)，K是原先DStream的值，Long表示这个Key有多少次</td>
</tr>
<tr>
<td style="text-align:center">reduceByKey(func, [numTasks])</td>
<td style="text-align:center">本文的例子使用过这个方法，对于是键值对(K,V)的DStream，返回一个新的DStream以K为键，各个value使用func函数操作得到的聚合结果为value</td>
</tr>
<tr>
<td style="text-align:center">join(otherStream, [numTasks])</td>
<td style="text-align:center">基于(K, V)键值对的DStream，如果对(K, W)的键值对DStream使用join操作，可以产生(K, (V, W))键值对的DStream</td>
</tr>
<tr>
<td style="text-align:center">cogroup(otherStream, [numTasks])</td>
<td style="text-align:center">跟join方法类似，不过是基于(K, V)的DStream，cogroup基于(K, W)的DStream，产生(K, (Seq[V], Seq[W]))的DStream</td>
</tr>
<tr>
<td style="text-align:center">transform(func)</td>
<td style="text-align:center">基于DStream中的每个RDD调用func函数，func函数的参数是个RDD，返回值也是个RDD</td>
</tr>
<tr>
<td style="text-align:center">updateStateByKey(func)</td>
<td style="text-align:center">对于每个key都会调用func函数处理先前的状态和所有新的状态。比如就可以用来做累加，这个方法跟reduceByKey类似，但比它更加灵活</td>
</tr>
</tbody>
</table>
<h4 id="UpdateStateByKey操作">UpdateStateByKey操作</h4><p>使用UpdateStateByKey方法需要做以下两步：</p>
<p>1.定义状态：状态可以是任意的数据类型<br>2.定义状态更新函数：这个函数需要根据输入流把先前的状态和所有新的状态</p>
<p>不管有没有新数据进来，在每个批次中，Spark都会对所有存在的key调用func方法，如果func函数返回None，那么key-value键值对不会被处理。</p>
<p>以一个例子来讲解updateStateByKey方法，这个例子会统计每个单词的个数在一个文本输入流里：</p>
<p>runningCount是一个状态并且是Int类型，所以这个状态的类型是Int，runningCount是先前的状态，newValues是所有新的状态，是一个集合，函数如下：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span><span class="params">(newValues: Seq[Int], runningCount: Option[Int])</span>:</span> Option[Int] = {
    val newCount = ...  // add the new values <span class="keyword">with</span> the previous running count to get the new count
    Some(newCount)
}
</code></pre><p>updateStateByKey方法的调用：</p>
<pre><code>val runningCounts = pairs.updateStateByKey[<span class="link_label">Int</span>](<span class="link_url">updateFunction _</span>)
</code></pre><h4 id="Transform操作">Transform操作</h4><p>Transform操作针对的是RDD-RDD的操作，所以可以用来处理那些没有在DStream API中暴露的处理任意的RDD操作。比如在DStream中的每次批次没有join rdd的API，所以可以使用transform操作：</p>
<pre><code><span class="variable"><span class="keyword">val</span> spamInfoRDD</span> = ssc.sparkContext.newAPIHadoopRDD(...) <span class="comment">// RDD containing spam information</span>

<span class="variable"><span class="keyword">val</span> cleanedDStream</span> = wordCounts.transform(rdd =&gt; {
  rdd.join(spamInfoRDD).filter(...) <span class="comment">// join data stream with spam information to do data cleaning</span>
  ...
})
</code></pre><h4 id="Window操作">Window操作</h4><p>window操作效果图如下图所示，把几个批次的DStream合并成一个DStream：</p>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream-window.png" alt=""></p>
<p>每个window操作都需要2个参数：</p>
<p>1.window length。每个window对应的批次数(上图中是3，time1-time3是一个window, time3-time5也是一个window)<br>2.sliding interval。每个window之间的间隔时间，上图下方的window1，window3，window5的间隔。上图这个值为2</p>
<p>这两个参数必须是批次间隔的倍数。上个批次间隔值为1。</p>
<p>以1个例子来讲解window操作，基于本文一开始的那个例子，生成最后30秒的数据，每10秒为单位，这里就需要使用reduceByKeyAndWindow方法：</p>
<pre><code>val windowedWordCounts = pairs.<span class="function"><span class="title">reduceByKeyAndWindow</span><span class="params">((a:Int,b:Int)</span></span> =&gt; (<span class="tag">a</span> + b), <span class="function"><span class="title">Seconds</span><span class="params">(<span class="number">30</span>)</span></span>, <span class="function"><span class="title">Seconds</span><span class="params">(<span class="number">10</span>)</span></span>)
</code></pre><p>其他的一些window操作：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Transformation</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">window(windowLength, slideInterval)</td>
<td style="text-align:center">根据window操作的2个参数得到新的DStream</td>
</tr>
<tr>
<td style="text-align:center">countByWindow(windowLength, slideInterval)</td>
<td style="text-align:center">基于window操作的count操作</td>
</tr>
<tr>
<td style="text-align:center">reduceByWindow(func, windowLength, slideInterval)</td>
<td style="text-align:center">基于window操作的reduce操作</td>
</tr>
<tr>
<td style="text-align:center">reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</td>
<td style="text-align:center">基于window操作的reduceByKey操作</td>
</tr>
<tr>
<td style="text-align:center">reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</td>
<td style="text-align:center">跟reduceByKeyAndWindow方法类似，更有效率，invFunc方法跟func方法的参数返回值一样，表示从window离开的数据</td>
</tr>
<tr>
<td style="text-align:center">countByValueAndWindow(windowLength, slideInterval, [numTasks])</td>
<td style="text-align:center">基于window操作的countByValue操作</td>
</tr>
</tbody>
</table>
<h4 id="Join操作">Join操作</h4><p>DStream可以很容易地join其他DStream：</p>
<pre><code><span class="label">val</span> <span class="keyword">stream1: </span>DStream[<span class="keyword">String, </span><span class="keyword">String] </span>= ...
<span class="label">val</span> <span class="keyword">stream2: </span>DStream[<span class="keyword">String, </span><span class="keyword">String] </span>= ...
<span class="label">val</span> joinedStream = <span class="keyword">stream1.join(stream2)</span>
</code></pre><p>还可以使用leftOuterJoin，rightOuterJoin，fullOuterJoin等。同样地，也可以在window操作后的DStream中使用join：</p>
<pre><code>val windowedStream1 = stream1.<span class="function"><span class="title">window</span><span class="params">(Seconds(<span class="number">20</span>)</span></span>)
val windowedStream2 = stream2.<span class="function"><span class="title">window</span><span class="params">(Minutes(<span class="number">1</span>)</span></span>)
val joinedStream = windowedStream1.<span class="function"><span class="title">join</span><span class="params">(windowedStream2)</span></span>
</code></pre><p>基于rdd的join：</p>
<pre><code><span class="variable"><span class="keyword">val</span> dataset</span>: RDD[String, String] = ...
<span class="variable"><span class="keyword">val</span> windowedStream</span> = stream.window(Seconds(<span class="number">20</span>))...
<span class="variable"><span class="keyword">val</span> joinedStream</span> = windowedStream.transform { rdd =&gt; rdd.join(dataset) }
</code></pre><h3 id="DStream的输出操作">DStream的输出操作</h3><p>输出操作允许DStream中的数据输出到外部系统，比如像数据库、文件系统等。</p>
<table>
<thead>
<tr>
<th style="text-align:center">输出操作</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">print()</td>
<td style="text-align:center">打印出DStream中每个批次的前10条数据</td>
</tr>
<tr>
<td style="text-align:center">saveAsTextFiles(prefix, [suffix])</td>
<td style="text-align:center">把DStream中的数据保存到文本文件里。每次批次的文件名根据参数prefix和suffix生成：”prefix-TIME_IN_MS[.suffix]”</td>
</tr>
<tr>
<td style="text-align:center">saveAsObjectFiles(prefix, [suffix])</td>
<td style="text-align:center">把DStream中的数据按照Java序列化的方式保存Sequence文件里，文件名规则跟saveAsTextFiles方法一样</td>
</tr>
<tr>
<td style="text-align:center">saveAsHadoopFiles(prefix, [suffix])</td>
<td style="text-align:center">把DStream中的数据保存到Hadoop文件里，文件名规则跟saveAsTextFiles方法一样</td>
</tr>
<tr>
<td style="text-align:center">foreachRDD(func)</td>
<td style="text-align:center">遍历DStream中的每段RDD，遍历的过程中可以将RDD中的数据保存到外部系统中</td>
</tr>
</tbody>
</table>
<h4 id="foreachRDD中的设计模式">foreachRDD中的设计模式</h4><p>foreachRDD方法会遍历DStream中的每段RDD，遍历的过程中可以将RDD中的数据保存到外部系统中。这个方法很实用，所以理解foreachRDD方法显得很重要。</p>
<p>将数据写到外部系统通常都需要一个connection对象，所以很多时候都会不经意地创建这个connection对象：</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  val connection = createNewConnection()  <span class="comment">// executed at the driver</span>
  rdd.<span class="keyword">foreach</span> { record =&gt;
    connection.send(record) <span class="comment">// executed at the worker</span>
  }
}
</code></pre><p>这种写法是不正确的。这里connection需要被序列化并且发送到worker，而且connection对象会跨机器传递，会发生序列化错误(connection对象是不可序列化的)，初始化错误(connection对象需要在worker中初始化)。这个错误的解决方案就是在worker中创建connection对象。</p>
<p>为每条记录创建connection也是一个很常见的错误：</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  rdd.foreach { record =&gt;
    <span class="keyword">val</span> connection = createNewConnection<span class="literal">()</span>
    connection.send(record)
    connection.close<span class="literal">()</span>
  }
}
</code></pre><p>因为创建connection对象是一种很耗资源，很耗时间的操作。对于每条数据都创建一个connection代驾更大。所有可以使用rdd.foreachPartition方法，这个方法会创建单一的connection并且在一个RDD分区中所有数据都使用这个connection：</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  rdd.foreachPartition { partitionOfRecords =&gt;
    <span class="keyword">val</span> connection = createNewConnection<span class="literal">()</span>
    partitionOfRecords.foreach(record =&gt; connection.send(record))
    connection.close<span class="literal">()</span>
  }
}
</code></pre><p>一种更好的方式就是使用ConnectionPool，ConnectionPool可以重用connection对象在多个批次和RDD中。</p>
<pre><code>dstream.foreachRDD { rdd =&gt;
  rdd.foreachPartition { partitionOfRecords =&gt;
    <span class="comment">// ConnectionPool is a static, lazily initialized pool of connections</span>
    val connection = ConnectionPool.getConnection()
    partitionOfRecords.<span class="keyword">foreach</span>(record =&gt; connection.send(record))
    ConnectionPool.returnConnection(connection)  <span class="comment">// return to the pool for future reuse</span>
  }
}
</code></pre><p>其他需要注意的点：</p>
<p>1.DStream的输出操作也是延迟执行的，就像RDD的action操作一样。RDD的action操作在DStream的输出操作内部执行的话会强制Spark Streaming执行。如果程序里没有任何输出操作，或者有比如像dstream.foreachRDD操作一样内部没有rdd的action操作的话，这样就不会执行任意操作，会被Spark忽略。<br>2.默认情况下，在一个时间点下，只有一个输出操作被执行。它们是根据程序里的编写顺序执行的。</p>
<h3 id="DataFrame_and_SQL_Operations">DataFrame and SQL Operations</h3><p>在Spark Streaming中可以使用DataFrames and SQL操作。</p>
<pre><code><span class="comment">/** DataFrame operations inside your streaming program */</span>

<span class="variable"><span class="keyword">val</span> words</span>: DStream[String] = ...

words.foreachRDD { rdd =&gt;

  <span class="comment">// Get the singleton instance of SQLContext</span>
  <span class="variable"><span class="keyword">val</span> sqlContext</span> = SQLContext.getOrCreate(rdd.sparkContext)
  <span class="keyword">import</span> sqlContext.implicits._

  <span class="comment">// Convert RDD[String] to DataFrame</span>
  <span class="variable"><span class="keyword">val</span> wordsDataFrame</span> = rdd.toDF(<span class="string">"word"</span>)

  <span class="comment">// Register as table</span>
  wordsDataFrame.registerTempTable(<span class="string">"words"</span>)

  <span class="comment">// Do word count on DataFrame using SQL and print it</span>
  <span class="variable"><span class="keyword">val</span> wordCountsDataFrame</span> = 
    sqlContext.sql(<span class="string">"select word, count(*) as total from words group by word"</span>)
  wordCountsDataFrame.show()
}
</code></pre><h3 id="Caching_/_Persistence">Caching / Persistence</h3><p>跟RDD类似，DStream也允许将数据保存到内存中，使用persist方法可以做到这一点。</p>
<p>但是基于window和state的操作，reduceByWindow,reduceByKeyAndWindow,updateStateByKey它们就是隐式的保存了，系统已经帮它自动保存了。</p>
<p>从网络接收的数据(比如Kafka, Flume, sockets等)，默认是保存在两个节点来实现容错性，以序列化的方式保存在内存当中。</p>
<h3 id="Checkpointing">Checkpointing</h3><p>一个Spark Streaming程序必须是全天工作的，所以如果万一系统挂掉了或者JVM挂掉之后是要有容错性的。Spark Streaming需在容错存储系统做checkpoint，这样才能够处理错误信息。有两种类型的数据需要做checkpoint：</p>
<p>1.metadata checkpointing：元数据检查点。主要包括3个元数据：<br>配置：创建streaming程序的的配置信息<br>DStream操作：streaming程序中DStream的操作集合<br>未完成的批次：在队列中未完成的批次<br>2.data checkpointing：数据检查点。保存已经生成的RDD数据。在一些有状态的transformation操作中，一些RDD数据会依赖之前批次的RDD数据，随时时间的推移，这种依赖情况就会越发严重。为了解决这个问题，需要保存这些有依赖关系的RDD数据到存储系统中(比如HDFS)来剪断这种依赖关系</p>
<p>什么时候需要启用checkpoint？</p>
<p>满足以下2个条件中的任意1个即可启用checkpoint:</p>
<p>1.使用了有状态的transformation。比如使用了updateStateByKey或reduceByKeyAndWindow方法后，就需要启用checkpoint<br>2.恢复挂掉的程序。可以根据metadata数据恢复程序</p>
<p>一些比较简单的streaming程序没有用到有状态的transformation，并且也可以接受程序挂掉之后丢失部分数据，那么就没有必要启用checkpoint。</p>
<p>如何配置checkpoint？</p>
<p>checkpoint的配置需要设置一个目录，使用streamingContext.checkpoint(checkpointDirectory)方法。</p>
<pre><code>// Function to <span class="operator"><span class="keyword">create</span> <span class="keyword">and</span> setup a <span class="keyword">new</span> StreamingContext
<span class="keyword">def</span> functionToCreateContext(): StreamingContext = {
    val ssc = <span class="keyword">new</span> StreamingContext(...)   // <span class="keyword">new</span> <span class="keyword">context</span>
    val <span class="keyword">lines</span> = ssc.socketTextStream(...) // <span class="keyword">create</span> DStreams
    ...
    ssc.checkpoint(checkpointDirectory)   // <span class="keyword">set</span> checkpoint <span class="keyword">directory</span>
    ssc
}

// <span class="keyword">Get</span> StreamingContext <span class="keyword">from</span> checkpoint <span class="keyword">data</span> <span class="keyword">or</span> <span class="keyword">create</span> a <span class="keyword">new</span> one
val <span class="keyword">context</span> = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _)

// <span class="keyword">Do</span> additional setup <span class="keyword">on</span> <span class="keyword">context</span> that needs <span class="keyword">to</span> be done,
// irrespective <span class="keyword">of</span> whether it <span class="keyword">is</span> being started <span class="keyword">or</span> restarted
<span class="keyword">context</span>. ...

// <span class="keyword">Start</span> the <span class="keyword">context</span>
<span class="keyword">context</span>.<span class="keyword">start</span>()
<span class="keyword">context</span>.awaitTermination()</span>
</code></pre><p>因为检查操作会导致保存到hdfs上的开销，所以设置这个时间间隔，要很慎重。对于小批次的数据，比如一秒的，检查操作会大大降低吞吐量。但是检查的间隔太长，会导致任务变大。通常来说，5-10秒的检查间隔时间是比较合适的。</p>
]]></content>
    <summary type="html">
    <![CDATA[Spark Streaming是Spark核心API的扩展，用于处理实时数据流。Spark Streaming处理的数据源可以是Kafka，Flume，Twitter，ZeroMQ，Kinesis或者Tcp Sockets ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/tags/spark/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark编程指南笔记]]></title>
    <link href="http://fangjian0423.github.io/2016/01/27/spark-programming-guide/"/>
    <id>http://fangjian0423.github.io/2016/01/27/spark-programming-guide/</id>
    <published>2016-01-26T16:22:21.000Z</published>
    <updated>2016-01-26T16:24:07.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Spark初始化">Spark初始化</h2><p>使用Spark编程第一件要做的事就是初始化SparkContext对象，SparkContext对象会告诉Spark如何使用Spark集群。</p>
<p>SparkContext会使用SparkConf中的一些配置信息，所以构造SparkContext对象之前需要构造一个SparkConf对象。</p>
<p>一个JVM上的SparkContext只有一个是激活的，如果要构造一个新的SparkContext，必须stop一个已经激活的SparkContext。</p>
<pre><code>val conf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(<span class="string">"local"</span>)</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(<span class="string">"Test"</span>)</span></span>
</code></pre><p>  val sc = new SparkContext(conf)</p>
<p>appName为Test，这个name会在cluster UI上展示，master是一个Spark，Mesos，YARN cluster URL或者local。 具体的值可以参考 <a href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="external">master-url解释</a>。</p>
<h2 id="RDD(Resilient_Distributed_Datasets)">RDD(Resilient Distributed Datasets)</h2><p>Spark提出的最主要抽象概念是RDD(弹性分布式数据集)，它是一个有容错机制并且可以被并行操作的元素集合。</p>
<p>有两种方式可以创建RDD:</p>
<p>1.使用一个已存在的集合进行并行计算<br>2.使用外部数据集，比如共享的文件系统，HDFS，HBase以及任何支持Hadoop InputFormat的数据源</p>
<h3 id="并行集合">并行集合</h3><p>使用SparkContext的parallelize方法构造并行集合。</p>
<pre><code>val dataSet = <span class="function"><span class="title">Array</span><span class="params">(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span></span>
val rdd = sc.<span class="function"><span class="title">parallelize</span><span class="params">(dataSet)</span></span>
rdd.<span class="function"><span class="title">reduce</span><span class="params">(_ + _)</span></span> <span class="comment">// 15</span>
</code></pre><p>parallelize方法有一个参数slices，表示数据集切分的份数。Spark会在集群上为每一个分片起一个任务。如果不设置的话，Spark会根据集群的情况，自动设置slices的数字。</p>
<h3 id="外部数据集">外部数据集</h3><p>文本文件可以使用SparkContext的textFile方法构造RDD。这个方法接收一个URI参数(也可以包括本地的文件)，并且以每行的方式读取文件内容。</p>
<p>比如data.txt文件里一行一个数字，取所有数字的和：</p>
<pre><code>val rdd = sc.textFile(this.getClass.getResource(<span class="string">"/data.txt"</span>).<span class="built_in">toString</span>)

rdd.<span class="built_in">reduce</span> {
   (a, b) =&gt; (a.toInt + b.toInt).<span class="built_in">toString</span>
}

<span class="comment">// 另外一种方式</span>
rdd.<span class="built_in">map</span>(s =&gt; s.toInt).<span class="built_in">reduce</span>(<span class="number">_</span> + <span class="number">_</span>)
</code></pre><p>Spark中所有基于文件的输入方法，都支持目录，压缩文件，通配符读取文件。比如</p>
<pre><code>sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"/data/*.txt"</span>)</span></span>
sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"/data"</span>)</span></span>
</code></pre><p>textFile方法也可以传入第二个可选参数来控制文件的分片数量。默认情况下，Spark会为文件的每一个块（在HDFS中块的大小默认是64MB）创建一个分片。但是你也可以通过传入一个更大的值来要求Spark建立更多的分片。注意，分片的数量绝不能小于文件块的数量。</p>
<p>除了文本文件之外，Spark还支持其他格式的输入：</p>
<p>1.SparkContext的wholeTextFiles方法会读取一个包含很多小文件的目录，并以filename，content为键值对的方式返回结果。<br>2.对于SequenceFiles，可以使用SparkContext的sequenceFile[K, V]方法创建。像 IntWritable和Text一样，它们必须是 Hadoop 的 Writable 接口的子类。另外，对于几种通用 Writable 类型，Spark 允许你指定原生类型来替代。例如：sequencFile[Int, String] 将会自动读取 IntWritable 和 Texts。<br>3.对于其他类型的 Hadoop 输入格式，你可以使用 SparkContext.hadoopRDD 方法，它可以接收任意类型的 JobConf 和输入格式类，键类型和值类型。按照像 Hadoop 作业一样的方法设置输入源就可以了。<br>4.RDD.saveAsObjectFile 和 SparkContext.objectFile 提供了以 Java 序列化的简单方式来保存 RDD。虽然这种方式没有 Avro 高效，但也是一种简单的方式来保存任意的 RDD。</p>
<h2 id="RDD操作">RDD操作</h2><h3 id="RDD操作基础">RDD操作基础</h3><p>RDD支持两种类型的操作。</p>
<p>1.transformations。从一个数据集产生一个新的数据集。比如map方法，就可以根据旧的数据集产生新的数据集。<br>2.actions。在一个数据集中进行聚合操作，并且返回一个最终的结果。</p>
<p>Spark中所有的transformations操作都是lazy的，就是说它们并不会立刻真的计算出结果。相反，它们仅仅是记录下了转换操作的操作对象（比如：一个文件）。只有当一个启动操作被执行，要向驱动程序返回结果时，转化操作才会真的开始计算。这样的设计使得Spark运行更加高效——比如，我们会发觉由map操作产生的数据集将会在reduce操作中用到，之后仅仅是返回了reduce的最终的结果而不是map产生的庞大数据集。</p>
<p>在默认情况下，每一个由转化操作得到的RDD都会在每次执行启动操作时重新计算生成。但是，你也可以通过调用persist(或cache)方法来将RDD持久化到内存中，这样Spark就可以在下次使用这个数据集时快速获得。Spark同样提供了对将RDD持久化到硬盘上或在多个节点间复制的支持。</p>
<p>一个计算文件中每行的字符串个数和所有字符串个数的和例子：</p>
<pre><code>val rdd = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"data.txt"</span>)</span></span>
val lineLengths = rdd.<span class="function"><span class="title">map</span><span class="params">(s =&gt; s.length)</span></span>
val totalLength = lineLengths.<span class="function"><span class="title">reduce</span><span class="params">(_ + _)</span></span>
</code></pre><p>lineLengths对象是一个transformations结果，所以它不是马上就开始执行的，当运行lineLengths.reduce的时候lineLengths才会开始去计算。如果之后还会用到这个lineLengths。可以在reduce方法之前加上:</p>
<pre><code>lineLengths.<span class="function"><span class="title">persist</span><span class="params">()</span></span>
</code></pre><h3 id="使用函数">使用函数</h3><p>Spark很多方法都可以使用函数完成。</p>
<p>使用对象：</p>
<pre><code><span class="class"><span class="keyword">object</span> <span class="title">SparkFunction</span> {</span>
  <span class="function"><span class="keyword">def</span> <span class="title">strLength</span> =</span> (s: <span class="type">String</span>) =&gt; s.length
}

<span class="keyword">val</span> lineLengths = rdds.map(<span class="type">SparkFunction</span>.strLength)
lineLengths.reduce(_ + _)
</code></pre><p>使用类：</p>
<pre><code><span class="class"><span class="keyword">class</span> <span class="title">SparkCls</span> </span>{
  def <span class="func"><span class="keyword">func</span> = <span class="params">(s: String)</span></span> =&gt; s.length
  def buildRdd(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = rdd.<span class="built_in">map</span>(<span class="func"><span class="keyword">func</span>)
}
<span class="title">new</span> <span class="title">SparkCls</span><span class="params">()</span></span>.buildRdd(rdds).<span class="built_in">reduce</span>(<span class="number">_</span> + <span class="number">_</span>)
</code></pre><h3 id="闭包">闭包</h3><pre><code><span class="tag">var</span> counter = <span class="number">0</span>
<span class="tag">var</span> rdd = sc.<span class="function"><span class="title">parallelize</span><span class="params">(data)</span></span>

<span class="comment">// Wrong: Don't do this!!</span>
rdd.<span class="function"><span class="title">foreach</span><span class="params">(x =&gt; counter += x)</span></span>

<span class="function"><span class="title">println</span><span class="params">(<span class="string">"Counter value: "</span> + counter)</span></span>
</code></pre><p>上述代码如果在local模式并且在一个JVM的情况下使用是可以得到正确的值的。这是因为所有的RDD和变量counter都是同一块内存上。</p>
<p>然后在集群模式下，上述代码的结果可能就不会是我们想要的正确结果。集群模式下，Spark会在RDD分成多个任务，每个任务都会被对应的executor执行。在executor执行之前，Spark会计算每个闭包。上面这个例子foreach方法和counter就组成了一个闭包。这个闭包会被序列化并且发送给每个executor。在local模式下，因为只有一个executor，所以共享相同的闭包。然后在集群模式下，有多个executor，并且各个executor在不同的节点上都有自己的闭包的拷贝。</p>
<p>所以counter变量就已经不再是节点上的变量了。虽然counter变量在内存上依然存在，但是它对于executor已经不可见，executor只知道它是序列化后的闭包的一份拷贝。因此如果counter的操作都是在闭包下的话，counter的值还是为0。</p>
<p>Spark提供了一种Accumulator的概念用来处理集群模式下的变量更新问题。</p>
<p>另外一个要注意的是不要使用foreach或者map方法打印数据。在一台机器上，这个操作是没有问题的。但是如果在集群上，不一定会打印出全部的数据。可以使用collect方法将RDD放到调用节点上。所以rdd.collect().foreach(println)是可以打印出数据的，但是可能数据量过大，会导致OOM。所以最好的方式还是使用take方法：rdd.take(100).foreach(println)。</p>
<h3 id="使用键值对">使用键值对</h3><p>Spark也支持键值对的操作，这在分组和聚合操作时候用得到。当键值对中的键为自定义对象时，需要自定义该对象的equals()和hashCode()方法。</p>
<p>一个使用键值对的单词统计例子：</p>
<pre><code>// 使用map方法将单词文本转换成一个键值对，(word, num)。 num初始值为<span class="number">1</span>
val pairs = rdd.map(s =&gt; (s, <span class="number">1</span>))
val reduceRdd = pairs.reduceByKey(_ + _)
val <span class="literal">result</span> = reduceRdd.sortByKey().collect()
<span class="literal">result</span>.foreach(println)
</code></pre><h3 id="Spark内置的Transformations">Spark内置的Transformations</h3><table>
<thead>
<tr>
<th style="text-align:center">转换</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">map(func)</td>
<td style="text-align:center">根据func函数生成一个新的rdd数据集</td>
</tr>
<tr>
<td style="text-align:center">filter(func)</td>
<td style="text-align:center">根据func函数返回true的数据集</td>
</tr>
<tr>
<td style="text-align:center">flatMap(func)</td>
<td style="text-align:center">跟map方法类似，但是每一项可以返回多个值。func函数的返回值是一个集合</td>
</tr>
<tr>
<td style="text-align:center">mapPartitions(func)</td>
<td style="text-align:center">跟map方法类似，但是是在每个partition上运行的。func函数的参数是一个Iteraror，返回值也是一个Iterator。如果map方法需要创建一个额外的对象，使用mapPartitions方法比map方法高效得多</td>
</tr>
<tr>
<td style="text-align:center">mapPartitionsWithIndex(func)</td>
<td style="text-align:center">作用跟mapPartitions方法一样，只是func方法多了一个index参数。 func方法定义 (Int, Iterator[T]) =&gt; Iterator[U]</td>
</tr>
<tr>
<td style="text-align:center">sample(withReplacement, fraction, seed)</td>
<td style="text-align:center">根据 fraction 指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed 用于指定随机数生成器种子</td>
</tr>
<tr>
<td style="text-align:center">union(otherDataset)</td>
<td style="text-align:center">取2个rdd的并集，得到一个新的rdd</td>
</tr>
<tr>
<td style="text-align:center">intersection(otherDataset)</td>
<td style="text-align:center">取2个rdd的交集，得到一个新的rdd。这个新的rdd没有重复的数据</td>
</tr>
<tr>
<td style="text-align:center">distinct([numTasks])</td>
<td style="text-align:center">返回一个新的没有重复数据的数据集</td>
</tr>
<tr>
<td style="text-align:center">groupByKey([numTasks])</td>
<td style="text-align:center">将一个(K,V)的键值对RDD转换成一个(K, Iterable[V])的新的键值对RDD。注意点：如果group的目的是为了做聚合计算(比如总和或者平均值)，使用reduceByKey或者aggregateByKey性能更好。</td>
</tr>
<tr>
<td style="text-align:center">reduceByKey(func, [numTasks])</td>
<td style="text-align:center">跟groupByKey方法一样，也是操作(K, V)的键值对RDD。返回值同样是一个(K, V)的键值对RDD，func函数的定义：(V, V) =&gt; V，也就是每两个值的值</td>
</tr>
<tr>
<td style="text-align:center">aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</td>
<td style="text-align:center">跟reduceByKey作用类似，zeroValue参数表示初始值，这个初始值的类型可以跟rdd中的键值对的值的类型不同。seqOp参数是个函数，定义为(U, V) =&gt; U，U类型是初始化zeroValue的类型，V类型是一开始rdd的键值对的值的类型。这个函数表示用来与初始值zeroValue进行比较，取一个新的值，需要注意的是这个新的值会作为参数出现在下一次key相等的情况下。 combOp参数也是个函数，定义为(U, U) =&gt; U，U类型也是初始值的类型。这个函数相当于reduce方法中的函数，用来做聚合操作</td>
</tr>
<tr>
<td style="text-align:center">sortByKey([ascending], [numTasks])</td>
<td style="text-align:center">对一个(K, V)键值对的RDD进行排行，返回一个基于K排序的新的RDD</td>
</tr>
<tr>
<td style="text-align:center">join(otherDataset, [numTasks])</td>
<td style="text-align:center">基于(K, V)键值对的rdd，如果对(K, W)的键值对rdd使用join操作，可以产生(K, (V, W))键值对的rdd。类似数据库中的join操作，spark还提供leftOuterJoin, rightOuterJoin, fullOuterJoin方法</td>
</tr>
<tr>
<td style="text-align:center">cogroup(otherDataset, [numTasks])</td>
<td style="text-align:center">跟join方法类似，不过是基于(K, V)的rdd，cogroup基于(K, W)的rdd，产生(K, (Iterable[V], Iterable[W]))的rdd。这个方法也叫做groupWith</td>
</tr>
<tr>
<td style="text-align:center">cartesian(otherDataset)</td>
<td style="text-align:center">笛卡尔积。 有K的rdd与V的rdd进行笛卡尔积，会生成(K, V)的rdd</td>
</tr>
<tr>
<td style="text-align:center">pipe(command, [envVars])</td>
<td style="text-align:center">对rdd进行管道操作。 就像shell命令一样</td>
</tr>
<tr>
<td style="text-align:center">coalesce(numPartitions)</td>
<td style="text-align:center">减少 RDD 的分区数到指定值。在过滤大量数据之后，可以执行此操作</td>
</tr>
<tr>
<td style="text-align:center">repartition(numPartitions)</td>
<td style="text-align:center">重新给 RDD 分区</td>
</tr>
<tr>
<td style="text-align:center">repartitionAndSortWithinPartitions(partitioner)</td>
<td style="text-align:center">重新给 RDD 分区，并且每个分区内以记录的 key 排序</td>
</tr>
</tbody>
</table>
<p>以这个数据为例：</p>
<pre><code><span class="literal">i</span>
am
<span class="keyword">format</span>
let
<span class="keyword">us</span>
go
hoho
good
nice
<span class="keyword">format</span>
is
nice
haha
haha
haha
<span class="keyword">scala</span> is cool, nice
</code></pre><p>一些Transformations操作：</p>
<pre><code>rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; s<span class="built_in">.</span>length) <span class="comment">// 每一行文本转换成长度</span>
rdd<span class="built_in">.</span>filter(s =&gt; s<span class="built_in">.</span>length == <span class="number">1</span>) <span class="comment">// 取文本长度为1的数据</span>
rdd<span class="built_in">.</span>flatMap(s =&gt; s<span class="built_in">.</span>split(<span class="string">","</span>)) <span class="comment">// 把有 , 的字符串转换成多行</span>
rdd<span class="built_in">.</span>sample(<span class="literal">false</span>, <span class="number">1.0</span>)
rdd<span class="built_in">.</span>union(rdd2)
rdd<span class="built_in">.</span>intersection(rdd2)
rdd<span class="built_in">.</span>distinct(rdd2)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>groupByKey()<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    print(<span class="built_in">pair</span><span class="built_in">.</span>_1) ; print(<span class="string">" ** "</span>)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
  }
}
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>reduceByKey {
  (x, y) =&gt; x + y
}<span class="built_in">.</span>foreach {
  (<span class="built_in">pair</span>) =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>aggregateByKey(<span class="number">0</span>)(
  (a, b) =&gt; {
    math<span class="built_in">.</span><span class="keyword">max</span>(a, b)
  }, (a, b) =&gt; {
    a + b
  }
)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, <span class="number">1</span>))<span class="built_in">.</span>sortByKey(<span class="literal">true</span>)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span><span class="keyword">join</span>(rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>charAt(<span class="number">0</span>)<span class="built_in">.</span>toUpper<span class="built_in">.</span>toString)))<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_2)
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>cogroup(rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>charAt(<span class="number">0</span>)<span class="built_in">.</span>toUpper<span class="built_in">.</span>toString)))<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">"======"</span>)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_1<span class="built_in">.</span>toList<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2<span class="built_in">.</span>_2<span class="built_in">.</span>toList<span class="built_in">.</span>mkString(<span class="string">"-"</span>))
    println(<span class="string">"**"</span> * <span class="number">8</span>)
  }
}

rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; s<span class="built_in">.</span>length)<span class="built_in">.</span>cartesian(rdd)<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; {
    println(<span class="built_in">pair</span><span class="built_in">.</span>_1)
    println(<span class="built_in">pair</span><span class="built_in">.</span>_2)
    println(<span class="string">"**"</span> * <span class="number">8</span>)
  }
}
</code></pre><h3 id="Spark内置的Actions">Spark内置的Actions</h3><table>
<thead>
<tr>
<th style="text-align:center">动作</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">reduce(func)</td>
<td style="text-align:center">聚合操作。之前很多例子都使用了reduce方法。这个功能必须可交换且可关联的，从而可以正确的被并行执行。</td>
</tr>
<tr>
<td style="text-align:center">collect()</td>
<td style="text-align:center">返回rdd中所有的元素，返回值类型是Array。这个方法经常用来取数据量比较小的集合</td>
</tr>
<tr>
<td style="text-align:center">count()</td>
<td style="text-align:center">rdd中的元素个数</td>
</tr>
<tr>
<td style="text-align:center">first()</td>
<td style="text-align:center">返回数据集的第一个元素</td>
</tr>
<tr>
<td style="text-align:center">take(n)</td>
<td style="text-align:center">返回数据集的前n个元素，返回值是个Array</td>
</tr>
<tr>
<td style="text-align:center">takeSample(withReplacement, num, [seed])</td>
<td style="text-align:center">返回一个数组，在数据集中随机采样 num 个元素组成，可以选择是否用随机数替换不足的部分，seed 用于指定的随机数生成器种子返回数据集的前n个元素，返回值是个Array</td>
</tr>
<tr>
<td style="text-align:center">takeOrdered(n, [ordering])</td>
<td style="text-align:center">返回自然顺序或者自定义顺序的前 n 个元素</td>
</tr>
<tr>
<td style="text-align:center">saveAsTextFile(path)</td>
<td style="text-align:center">把数据集中的元素写到文件里，可以写到本地文件系统上，hdfs上或者任意Hadoop支持的文件系统上。Spark会调用元素的toString方法将其转换成文本的一行</td>
</tr>
<tr>
<td style="text-align:center">saveAsSequenceFile(path)</td>
<td style="text-align:center">跟saveAsTextFile方法类似，但是是写成SequenceFile文件格式，也是支持写到本地文件系统上，hdfs上或者任意Hadoop支持的文件系统上。这个方法只能作用于键值对的RDD</td>
</tr>
<tr>
<td style="text-align:center">saveAsObjectFile(path)</td>
<td style="text-align:center">跟saveAsTextFile方法类似，是使用Java的序列化的方式保存文件</td>
</tr>
<tr>
<td style="text-align:center">countByKey()</td>
<td style="text-align:center">计算键值的数量。对键值对(K, V)的rdd数据集，返回(K, Int)的Map</td>
</tr>
<tr>
<td style="text-align:center">foreach(func)</td>
<td style="text-align:center">使用func遍历rdd数据集中的各个元素。这通常用于边缘效果，例如更新一个Accumulator，或者和外部存储系统进行交互</td>
</tr>
</tbody>
</table>
<p>一些Actions操作：</p>
<pre><code>rdd<span class="built_in">.</span>saveAsTextFile(<span class="string">"file:///tmp/data01"</span>)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>saveAsSequenceFile(<span class="string">"file:///tmp/data02"</span>)
rdd<span class="built_in">.</span><span class="built_in">map</span>(s =&gt; (s, s<span class="built_in">.</span>length))<span class="built_in">.</span>countByKey()<span class="built_in">.</span>foreach {
  <span class="built_in">pair</span> =&gt; println(<span class="built_in">pair</span><span class="built_in">.</span>_1 + <span class="string">" "</span> + <span class="built_in">pair</span><span class="built_in">.</span>_2)
}
</code></pre><h2 id="RDD的持久化(Persistence)">RDD的持久化(Persistence)</h2><p>Spark的一个重要功能就是在将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。当持久化一个RDD的时候，每个存储着这个RDD的分片节点都会计算然后保存到内存中以便下次再次使用。这使得接下来的计算过程速度能够加快（经常能加快超过十倍的速度）。缓存是加快迭代算法和快速交互过程速度的关键工具。</p>
<p>可以使用persist或者cache方法让rdd持久化。在第一次被计算产生之后，它就会始终停留在节点的内存中。Spark的缓存是具有容错性的——如果RDD的任意一个分片丢失了，Spark就会依照这个RDD产生的转化过程自动重算一遍。</p>
<p>另外，每个持久化后的RDD可以使用不用级别的存储级别。比如可以存在硬盘中，可以存在内存中，还可以将这个数据集在节点之间复制，或者使用 Tachyon 将它储存到堆外。这些存储级别都是通过向 persist() 传递一个 StorageLevel 对象（Scala, Java, Python）来设置的。</p>
<p>Spark的一些存储级别如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">存储级别</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">MEMORY_ONLY</td>
<td style="text-align:center">默认级别。将RDD作为反序列化的的对象存储在JVM中。如果不能被内存装下，一些分区将不会被缓存，并且在需要的时候被重新计算</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_AND_DISK</td>
<td style="text-align:center">默认级别。将RDD作为反序列化的的对象存储在JVM中。如果不能被内存装下，会存在硬盘上，并且在需要的时候被重新计算</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_ONLY_SER</td>
<td style="text-align:center">将RDD作为序列化的的对象进行存储（每一分区占用一个字节数组）。通常来说，这比将对象反序列化的空间利用率更高，尤其当使用fast serializer,但在读取时会比较占用CPU</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_AND_DISK_SER</td>
<td style="text-align:center">与MEMORY_ONLY_SER相似，但是把超出内存的分区将存储在硬盘上而不是在每次需要的时候重新计算</td>
</tr>
<tr>
<td style="text-align:center">DISK_ONLY</td>
<td style="text-align:center">只存储RDD分区到硬盘上</td>
</tr>
<tr>
<td style="text-align:center">MEMORY_ONLY_2, MEMORY_AND_DISK_2 等</td>
<td style="text-align:center">与上述的存储级别一样，但是将每一个分区都复制到两个集群结点上</td>
</tr>
</tbody>
</table>
<p>存储级别的选择：</p>
<p>如果你的 RDD 可以很好的与默认的存储级别契合，就不需要做任何修改了。这已经是 CPU 使用效率最高的选项，它使得 RDD的操作尽可能的快。</p>
<p>如果不行，试着使用 MEMORY_ONLY_SER 并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</p>
<p>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</p>
<p>如果你想有快速故障恢复能力，使用复制存储级别。例如：用 Spark 来响应web应用的请求。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在 RDD 上持续的运行任务，而不需要等待丢失的分区被重新计算。</p>
<p>如果你想要定义你自己的存储级别，比如复制因子为3而不是2，可以使用 StorageLevel 单例对象的 apply()方法。</p>
<h2 id="共享变量">共享变量</h2><p>通常情况下，当一个函数在远程集群节点上通过Spark操作(比如map或者reduce)，Spark会对涉及到的变量的所有副本执行这个函数。这些变量都会被拷贝到每台机器上，而且这个过程不会被反馈到驱动程序。通常情况下，在任务之间读写共享变量是很低效的。但是Spark仍然提供了有限的两种共享变量类型用于常见的使用场景：broadcast variables 和 accumulators。</p>
<h3 id="broadcast_variables(广播变量)">broadcast variables(广播变量)</h3><p>广播变量允许程序员在每台机器上保持一个只读变量的缓存而不是将一个变量的拷贝传递给各个任务。这些变量是可以被使用的，比如，给每个节点传递一份大输入数据集的拷贝是很耗时的。Spark试图使用高效的广播算法来分布广播变量，以此来降低通信花销。可以通过SparkContext.broadcast(v)来从变量v创建一个广播变量。这个广播变量是v的一个包装，同时它的值可以调用value方法获得：</p>
<pre><code>val broadcastVar = sc.<span class="function"><span class="title">broadcast</span><span class="params">(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span></span>)

broadcastVar<span class="class">.value</span> <span class="comment">// Array(1, 2, 3)</span>
</code></pre><p>一个广播变量被创建之后，在所有函数中都应当使用它来代替原来的变量v，这样就可以包装v在节点之间只被传递一次。另外，v变量在被广播之后不应该再被修改，这样可以确保每一个节点上储存的广播变量的一致性。</p>
<h3 id="accumulators(累加器)">accumulators(累加器)</h3><p>累加器是在一个相关过程中只能被”累加”的变量，对这个变量的操作可以有效地被并行化。它们可以被用于实现计数器（就像在MapReduce过程中）或求和运算。Spark原生支持对数字类型的累加器，程序员也可以为其他新的类型添加支持。累加器被以一个名字创建之后，会在Spark的UI中显示出来。这有助于了解计算的累进过程（注意：目前Python中不支持这个特性）。</p>
<p>可以通过SparkContext.accumulator(v)来从变量v创建一个累加器。在集群中运行的任务随后可以使用add方法或+=操作符（在Scala和Python中）来向这个累加器中累加值。但是，他们不能读取累加器中的值。只有驱动程序可以读取累加器中的值，通过累加器的value方法。</p>
<p>以下的代码展示了向一个累加器中累加数组元素的过程：</p>
<pre><code>val accum = sc.<span class="function"><span class="title">accumulator</span><span class="params">(<span class="number">0</span>, <span class="string">"My Accumulator"</span>)</span></span>
sc.<span class="function"><span class="title">parallelize</span><span class="params">(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span></span>).<span class="function"><span class="title">foreach</span><span class="params">(x =&gt; accum += x)</span></span>
accum<span class="class">.value</span> <span class="comment">// 10</span>
</code></pre><p>这段代码利用了累加器对Int类型的内建支持，程序员可以通过继承 AccumulatorParam 类来创建自己想要的类型支持。AccumulatorParam 的接口提供了两个方法：zero 用于为你的数据类型提供零值；addInPlace 用于计算两个值得和。比如，假设我们有一个 Vector类表示数学中的向量，我们可以这样写：</p>
<pre><code>object VectorAccumulatorParam extends AccumulatorParam[Vector] {
  <span class="function"><span class="keyword">def</span> <span class="title">zero</span><span class="params">(initialValue: Vector)</span>:</span> Vector = {
    Vector.zeros(initialValue.size)
  }
  <span class="function"><span class="keyword">def</span> <span class="title">addInPlace</span><span class="params">(v1: Vector, v2: Vector)</span>:</span> Vector = {
    v1 += v2
  }
}

// Then, create an Accumulator of this type:
val vecAccum = sc.accumulator(new Vector(...))(VectorAccumulatorParam)
</code></pre><p>累加器的更新操作只会被运行一次，Spark 提供了保证，每个任务中对累加器的更新操作都只会被运行一次。比如，重启一个任务不会再次更新累加器。在转化过程中，用户应该留意每个任务的更新操作在任务或作业重新运算时是否被执行了超过一次。</p>
<p>累加器不会改变Spark的惰性求值模型。如果累加器在对RDD的操作中被更新了，它们的值只会在启动操作中作为 RDD 计算过程中的一部分被更新。所以，在一个懒惰的转化操作中调用累加器的更新，并没法保证会被及时运行。下面的代码段展示了这一点：</p>
<pre><code><span class="title">accum</span> = sc.accumulator(<span class="number">0</span>)
<span class="typedef"><span class="keyword">data</span>.map<span class="container">(<span class="title">lambda</span> <span class="title">x</span> =&gt; <span class="title">acc</span>.<span class="title">add</span>(<span class="title">x</span>)</span>; f<span class="container">(<span class="title">x</span>)</span>)</span>
</code></pre><h2 id="参考资料">参考资料</h2><p><a href="http://spark.apache.org/docs/latest/programming-guide.html/" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[Spark编程指南笔记，参考官方文档的编程指南，翻译再加上一些自己写的代码 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/tags/spark/"/>
    
      <category term="spark" scheme="http://fangjian0423.github.io/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[记录Flume使用KafkaSource的时候Channel队列满了之后发生的怪异问题]]></title>
    <link href="http://fangjian0423.github.io/2016/01/19/flume-channel-full-exception/"/>
    <id>http://fangjian0423.github.io/2016/01/19/flume-channel-full-exception/</id>
    <published>2016-01-19T12:07:35.000Z</published>
    <updated>2016-03-04T17:54:42.000Z</updated>
    <content type="html"><![CDATA[<p>Flume的这个问题纠结了2个月，因为之前实在太忙了，没有时间来研究这个问题产生的原理，今天终于研究出来了，找出了这个问题所在。</p>
<p>先来描述一下这个问题的现象：</p>
<p>Flume的Source用的是KafkaSource，Sink用的是Custom Sink，由于这个Custom Sink写的有一点小问题，比如batchSize是5000次，第4000条就会发生exception，这样每次都会写入4000条数据。Sink处理的时候都会发生异常，每次都会rollback，rollback方面的知识可以参考<a href="http://fangjian0423.github.io/2016/01/03/flume-transaction/">Flume Transaction介绍</a>。</p>
<p>这样造成的后果有3个：</p>
<p>1.Channel中的数据满了。会发生以下异常：</p>
<pre><code>Caused by: org.apache.flume.ChannelFullException: Space <span class="keyword">for</span> commit <span class="keyword">to</span> queue couldn<span class="attribute">'t</span> be acquired. Sinks are likely <span class="keyword">not</span> keeping up <span class="keyword">with</span> sources, <span class="keyword">or</span> the <span class="keyword">buffer</span> size <span class="keyword">is</span> too tight
</code></pre><p>2.Sink会一直写数据，造成数据量暴增。</p>
<p>3.如果用了interceptor，且修改了event中的数据，那么会重复处理这些修改完后的event数据。</p>
<p>前面2个很容易理解，Sink发生异常，transaction rollback，导致channel中的队列满了。</p>
<p>关键是第三点，很让人费解。</p>
<p>以一段伪需求和伪代码为例，TestInterceptor的intercept方法：</p>
<p>比如处理一段json：</p>
<pre><code>{"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">languages</span>": <span class="value">[<span class="string">"java"</span>, <span class="string">"scala"</span>, <span class="string">"javascript"</span>]</span>}
</code></pre><p>使用interceptor处理成:</p>
<pre><code>[{"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"java"</span></span>}, {"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"scala"</span></span>}, {"<span class="attribute">name</span>": <span class="value"><span class="string">"format"</span></span>, "<span class="attribute">language</span>": <span class="value"><span class="string">"javascript"</span></span>}]
</code></pre><p>interceptor代码如下：</p>
<pre><code><span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span>(<span class="params">Event <span class="keyword">event</span></span>) </span>{
    Model model = <span class="keyword">null</span>;
    String jsonStr = <span class="keyword">new</span> String(<span class="keyword">event</span>.getBody(), <span class="string">"UTF-8"</span>);
    <span class="keyword">try</span> {
        model = parseJsonStr(jsonStr);
    } <span class="keyword">catch</span> (Exception e) {
        log.error(<span class="string">"convert json data error"</span>);
    }
    <span class="keyword">event</span>.setBody(model.getJsonString().getBytes());
    <span class="keyword">return</span> <span class="keyword">event</span>;
}
</code></pre><p>当Channel中的队列已经满了以后，上述代码会打印出convert json data error，而且jsonStr的内容居然是转换后的数据，这一点一开始让我十分费解，误以为transaction rollback之后会修改source中的数据。后来debug源码发现错误在Source中。</p>
<p>后来发现并不是这样的。</p>
<p>KafkaSource中有一个属性eventList，是个ArrayList。用来接收kafka consume的message。</p>
<p>直接说明KafkaSource的process方法源码：</p>
<pre><code><span class="keyword">public</span> Status process() <span class="keyword">throws</span> EventDeliveryException {

  <span class="built_in">byte</span>[] kafkaMessage;
  <span class="built_in">byte</span>[] kafkaKey;
  Event event;
  Map&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; headers;
  <span class="keyword">long</span> batchStartTime = System.currentTimeMillis();
  <span class="keyword">long</span> batchEndTime = System.currentTimeMillis() + timeUpperLimit;
  <span class="keyword">try</span> {

    <span class="comment">/** 这里读取kafka中的message **/</span>
    <span class="built_in">boolean</span> iterStatus = <span class="keyword">false</span>;
    <span class="keyword">while</span> (eventList.<span class="built_in">size</span>() &lt; batchUpperLimit &amp;&amp;
            System.currentTimeMillis() &lt; batchEndTime) {
      iterStatus = hasNext();
      <span class="keyword">if</span> (iterStatus) {
        <span class="comment">// get next message</span>
        MessageAndMetadata&lt;<span class="built_in">byte</span>[], <span class="built_in">byte</span>[]&gt; messageAndMetadata = it.next();
        kafkaMessage = messageAndMetadata.message();
        kafkaKey = messageAndMetadata.<span class="variable">key</span>();

        <span class="comment">// Add headers to event (topic, timestamp, and key)</span>
        headers = <span class="keyword">new</span> <span class="keyword">HashMap</span>&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;();
        headers.put(KafkaSourceConstants.TIMESTAMP,
                <span class="keyword">String</span>.valueOf(System.currentTimeMillis()));
        headers.put(KafkaSourceConstants.TOPIC, topic);
        headers.put(KafkaSourceConstants.KEY, <span class="keyword">new</span> <span class="keyword">String</span>(kafkaKey));
        <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
          <span class="built_in">log</span>.debug(<span class="string">"Message: {}"</span>, <span class="keyword">new</span> <span class="keyword">String</span>(kafkaMessage));
        }
        event = EventBuilder.withBody(kafkaMessage, headers);
        eventList.<span class="built_in">add</span>(event);
      }
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Waited: {} "</span>, System.currentTimeMillis() - batchStartTime);
        <span class="built_in">log</span>.debug(<span class="string">"Event #: {}"</span>, eventList.<span class="built_in">size</span>());
      }
    }
    <span class="comment">/** 这里读取kafka中的message **/</span>

    <span class="comment">// If we have events, send events to channel</span>
    <span class="comment">// clear the event list</span>
    <span class="comment">// and commit if Kafka doesn't auto-commit</span>
    <span class="keyword">if</span> (eventList.<span class="built_in">size</span>() &gt; <span class="number">0</span>) {
      <span class="comment">// 使用ChannelProcess将Source中读取的数据给各个Channel</span>
      <span class="comment">// 如果getChannelProcessor().processEventBatch(eventList);发生了异常，eventList不会被清空，而且processEventBatch方法会调用Interceptor处理event中的数据，event中的数据已经被转换。所以下一次会将转换后的event数据再次传给Interceptor。</span>
      getChannelProcessor().processEventBatch(eventList);
      eventList.<span class="built_in">clear</span>();
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Wrote {} events to channel"</span>, eventList.<span class="built_in">size</span>());
      }
      <span class="keyword">if</span> (!kafkaAutoCommitEnabled) {
        <span class="comment">// commit the read transactions to Kafka to avoid duplicates</span>
        consumer.commitOffsets();
      }
    }
    <span class="keyword">if</span> (!iterStatus) {
      <span class="keyword">if</span> (<span class="built_in">log</span>.isDebugEnabled()) {
        <span class="built_in">log</span>.debug(<span class="string">"Returning with backoff. No more data to read"</span>);
      }
      <span class="keyword">return</span> Status.BACKOFF;
    }
    <span class="keyword">return</span> Status.READY;
  } <span class="keyword">catch</span> (Exception e) {
    <span class="built_in">log</span>.error(<span class="string">"KafkaSource EXCEPTION, {}"</span>, e);
    <span class="keyword">return</span> Status.BACKOFF;
  }
}
</code></pre><p>上述代码已经加了备注，再重申一下：ChannelProcess的processEventBatch方法会调用Interceptor处理event中的数据。所以如果Channel中的队列满了，那么processEventBatch方法会发生异常，发生异常之后eventList中的没有进入channel的数据已经被Interceptor修改，且不会被清空。因此下次还是会使用这些数据，所以会发生convert json data error错误。</p>
<p>画了一个序列图如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/flume-channel-full01.png" alt=""></p>
<p>第6步添加event到channel中的时候，队列已满，所以会抛出异常。最终异常被KafkaSource捕捉，但是eventList内部的部分数据已经被interceptor修改过。</p>
<p>多个channel的影响：</p>
<p>如果有多个channel，这个问题也会影响。比如有2个channel，c1和c2。c1的sink没有问题，一直稳定执行，c2对应的sink是一个CustomSink，会有问题。这样c2中的队列迟早会爆满，爆满之后，ChannelProcess批量处理event的时候，由于c2的队列满了，所以Source中的eventList永远不会被清空，eventList永远不会被清空的话，所有的channel都会被影响到，这就好比水源被污染之后，所有的用水都会受到影响。</p>
<p>举个例子：source为s1，c1对应的sink是k1，c2对应的sink是k2。k1和k2的batchSize都是5000，k2处理第4000条数据的时候总会发生异常，进行回滚。k1很稳定。这样c2迟早会爆满，爆满之后s1的eventList一直不能clear，这样也会导致c1一直在处理，所以k1的数据量跟k2一样也会暴增。</p>
<p>要避免本文所说的这一系列情况，最好的做法就是sink必须要加上很好的异常处理机制，不是任何情况都可以rollback的，要根据需求做对应的处理。</p>
]]></content>
    <summary type="html">
    <![CDATA[记录Flume使用KafkaSource的时候Channel队列满了之后发生的怪异问题。数据量暴增，Channel队列爆满 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Kafka介绍]]></title>
    <link href="http://fangjian0423.github.io/2016/01/13/kafka-intro/"/>
    <id>http://fangjian0423.github.io/2016/01/13/kafka-intro/</id>
    <published>2016-01-13T15:54:51.000Z</published>
    <updated>2016-01-13T15:55:05.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Kafka介绍">Kafka介绍</h2><p>Kafka是一个分布式的发布-订阅消息系统(Producer-Consumer)，是一种快速、可扩展的、分区的和可复制的日志服务。</p>
<p>kafka中的几个概念：</p>
<p>Topic：用来区别各种message。比如A系统的所有message的Topic为A，B系统的所有message的Topic为B。</p>
<p>Broker：已发布的消息保存在一组服务器中，这组服务器就被称为Broker或Kafka集群。</p>
<p>Producer：生产者，用于发布消息，发布消息到kafka broker。</p>
<p>Consumer：消费者，订阅消息，订阅kafka broker中的已经被发布的消息。</p>
<p>下图是几个概念的说明：</p>
<p>producer发布消息到kafka cluster(也就是kafka broker)，然后发布后的这些消息被consumer订阅。</p>
<p>从图中也可以看出来，kafka支持多个producer和多个consumer。</p>
<p><img src="http://kafka.apache.org/images/producer_consumer.png" alt=""></p>
<h2 id="Kafka存储机制">Kafka存储机制</h2><p>Partition：Kafka中每个Topic都会有一个或多个Partition，由于kafka将数据直接写到硬盘里，这里的Partition对应一个文件夹，文件夹下存储这个Partition的所有消息和索引。如果有2个Topic，分别有3个和4个Partition。那么总共有7个文件夹。Kafka内部会根据一个算法，根据消息得出一个值，然后根据这个值放入对应的partition目录中的段文件里。</p>
<p>比如在一台机器上创建partition为3，topic为test01和partition为4，topic为test02的2个topic。</p>
<p>创建完之后 /tmp/kafka-logs中就会有7个文件夹，分别是 </p>
<p>test01-0<br>test01-1<br>test01-2<br>test02-0<br>test02-1<br>test02-2<br>test02-3</p>
<p>Segment：组成Partiton的组件。一个Partition代表一个文件夹，而Segment则是这个文件夹下的各个文件。每个Segmenet文件有大小限制，在配置文件中用log.segment.bytes配置。</p>
<pre><code><span class="built_in">log</span>.<span class="keyword">segment</span>.<span class="keyword">bytes</span>=<span class="number">1073741824</span>
</code></pre><p>当文件的大小超过1073741824字节的时候，会创建第一个段文件。需要注意的是这里每个段文件中的消息数量不一定相等，因为虽然他们的字节数一样，但是每个消息的字节数是不一样的，所以每个段文件中的消息数量不一定相等。</p>
<p>每个段文件由2部分组成，分别是index file和log file，表示索引文件和日志(数据)文件。这2个文件一一对应。</p>
<p>第一个segment文件从0开始，后续每个segment文件名是上一个segment文件的最后一条message的offset值，数值最大为64位long大小，19位数字字符长度，没有数字用0填充。</p>
<p>下面是做的一个例子，partition和replication-factor都为1，每个segmenet文件的大小是5M。有500000条message，一共生成了4对文件，这里00000000000000137200.log文件表示是00000000000000000000.log中存储了137199个message，这个文件开始存储第137200个message。</p>
<p>00000000000000000000.index<br>00000000000000000000.log</p>
<p>00000000000000137200.index<br>00000000000000137200.log</p>
<p>00000000000000271600.index<br>00000000000000271600.log</p>
<p>00000000000000406000.index<br>00000000000000406000.log</p>
<p>offset：用来标识message在partition中的下标，用来定位message。</p>
<p>Kafka内部存储结构可以参考<a href="http://tech.meituan.com/kafka-fs-design-theory.html" target="_blank" rel="external">Kafka文件存储机制那些事</a>文章里的讲解。</p>
<p>一个kafka producer例子：</p>
<pre><code><span class="keyword">import</span> java.util.<span class="type">Properties</span>
<span class="keyword">import</span> kafka.producer.{<span class="type">KeyedMessage</span>, <span class="type">Producer</span>, <span class="type">ProducerConfig</span>}

<span class="class"><span class="keyword">object</span> <span class="title">TestProducer</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">App</span> {</span>

  <span class="keyword">val</span> events = <span class="number">500000</span>
  <span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>()
  <span class="keyword">val</span> brokers = <span class="string">"localhost:9092"</span>
  props.put(<span class="string">"metadata.broker.list"</span>, brokers)
  props.put(<span class="string">"serializer.class"</span>, <span class="string">"kafka.serializer.StringEncoder"</span>)
  props.put(<span class="string">"producer.type"</span>, <span class="string">"async"</span>)

  <span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">ProducerConfig</span>(props)

  <span class="keyword">val</span> topic = <span class="string">"format03"</span>

  <span class="keyword">val</span> producer = <span class="keyword">new</span> <span class="type">Producer</span>[<span class="type">String</span>, <span class="type">String</span>](config)

  <span class="keyword">for</span>(nEvents &lt;- <span class="type">Range</span>(<span class="number">0</span>, events)) {
    <span class="keyword">val</span> msg = <span class="string">"Message"</span> + nEvents
    <span class="keyword">val</span> data = <span class="keyword">new</span> <span class="type">KeyedMessage</span>[<span class="type">String</span>, <span class="type">String</span>](topic, msg)
    producer.send(data)
  }

  producer.close()

}
</code></pre><h2 id="参考资料">参考资料</h2><p><a href="http://tech.meituan.com/kafka-fs-design-theory.html" target="_blank" rel="external">Kafka文件存储机制那些事</a><br><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-1/" target="_blank" rel="external">Kafka剖析（一）：Kafka背景及架构介绍</a><br><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-2/" target="_blank" rel="external">Kafka设计解析（二）：Kafka High Availability （上）</a><br><a href="http://www.infoq.com/cn/articles/kafka-analysis-part-3/" target="_blank" rel="external">Kafka设计解析（三）：Kafka High Availability （下）</a></p>
]]></content>
    <summary type="html">
    <![CDATA[Kafka是一个分布式的发布-订阅消息系统(Producer-Consumer)，是一种快速、可扩展的、分区的和可复制的日志服务。Kafka中有几个概念，分别是Topic，Broker，Producer，Consumer等 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="log" scheme="http://fangjian0423.github.io/tags/log/"/>
    
      <category term="kafka" scheme="http://fangjian0423.github.io/categories/kafka/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Flume Transaction介绍]]></title>
    <link href="http://fangjian0423.github.io/2016/01/03/flume-transaction/"/>
    <id>http://fangjian0423.github.io/2016/01/03/flume-transaction/</id>
    <published>2016-01-03T09:35:53.000Z</published>
    <updated>2016-01-11T14:19:42.000Z</updated>
    <content type="html"><![CDATA[<p>Flume中有一个Transaction的概念。本文仅分析Transaction的实现类MemoryTransaction的实现原理，JdbcTransaction的原理跟数据库中的Transaction类似。</p>
<p>Transaction接口定义如下：</p>
<pre><code><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">begin</span><span class="params">()</span></span>;

<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">commit</span><span class="params">()</span></span>;

<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">rollback</span><span class="params">()</span></span>;

<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;
</code></pre><p>Transaction跟数据库中的Transaction概念类似，都有begin，commit，rollback，close方法。</p>
<p>Flume中的Transaction是在Channel中使用的，主要用来处理Source数据进入Channel的过程和Channel中的数据被Sink处理的过程。下面会从这2个方面根据源码分析Transaction的原理。</p>
<p>在分析具体的事务操作之前，看一下MemoryTransaction中的各个方法实现原理。</p>
<p>首先先看一下事务的获取方法：</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function">Transaction <span class="title">getTransaction</span><span class="params">()</span> </span>{

    <span class="keyword">if</span> (!initialized) {
      <span class="keyword">synchronized</span> (<span class="keyword">this</span>) {
        <span class="keyword">if</span> (!initialized) {
          initialize();
          initialized = <span class="keyword">true</span>;
        }
      }
    }
    <span class="comment">// currentTransaction是一个ThreadLocal对象</span>
    BasicTransactionSemantics transaction = currentTransaction.get();
    <span class="comment">// 如果是第一次获取事务或者当前事务的已经close。那么会重新create一个新的事务</span>
    <span class="keyword">if</span> (transaction == <span class="keyword">null</span> || transaction.getState().equals(
            BasicTransactionSemantics.State.CLOSED)) {
      transaction = createTransaction();
      currentTransaction.set(transaction);
    }
    <span class="keyword">return</span> transaction;
}
</code></pre><p><strong>再重复一下，第一次拿事务或者事务关闭之后，才会重新去构造一个新的事务。各个线程之间的事务都是独立的</strong></p>
<p>MemoryTransaction是MemoryChannel中的一个内部类。</p>
<p>然后介绍一下MemoryTransaction和MemoryChannel中的几个重要属性。</p>
<p>MemoryTransaction中有2个阻塞队列，分别是putList和takeList。putList放Source进来的数据，Sink从MemoryChannel中的queue中拿数据，然后这个数据丢到takeList中。</p>
<p>MemoryChannel中有个阻塞队列queue。每次事务commit的时候都会把putList中的数据丢到queue中。</p>
<p>begin方法MemoryTransaction没做任何处理，就不分析了。</p>
<p>put方法：</p>
<pre><code>@<span class="function">Override
<span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doPut</span>(<span class="params">Event <span class="keyword">event</span></span>) throws InterruptedException </span>{
  channelCounter.incrementEventPutAttemptCount();
  <span class="keyword">int</span> eventByteSize = (<span class="keyword">int</span>)Math.ceil(estimateEventSize(<span class="keyword">event</span>)/byteCapacitySlotSize);

  <span class="keyword">if</span> (!putList.offer(<span class="keyword">event</span>)) {
    <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(
      <span class="string">"Put queue for MemoryTransaction of capacity "</span> +
        putList.size() + <span class="string">" full, consider committing more frequently, "</span> +
        <span class="string">"increasing capacity or increasing thread count"</span>);
  }
  putByteCounter += eventByteSize;
}
</code></pre><p>put方法把数据丢入putList中，这个也就是之前分析的putList这个属性的作用，putList放Source进来的数据。</p>
<p>commit方法的关键性代码：</p>
<pre><code>@Override
<span class="keyword">protected</span> <span class="keyword">void</span> doCommit() <span class="keyword">throws</span> InterruptedException {
  <span class="built_in">int</span> puts = putList.<span class="built_in">size</span>();
  <span class="built_in">int</span> takes = takeList.<span class="built_in">size</span>();
  <span class="keyword">synchronized</span>(queueLock) {
    <span class="keyword">if</span>(puts &gt; <span class="number">0</span> ) {
      <span class="comment">// 清空putList，丢到外部类MemoryChannel中的queue队列里</span>
      <span class="keyword">while</span>(!putList.isEmpty()) {
        <span class="comment">// MemoryChannel中的queue队列</span>
        <span class="keyword">if</span>(!queue.offer(putList.removeFirst())) {
          <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Queue add failed, this shouldn't be able to happen"</span>);
        }
      }
    }
    putList.<span class="built_in">clear</span>();
    takeList.<span class="built_in">clear</span>();
  }
}
</code></pre><p>rollback方法关键性代码：</p>
<pre><code>@Override
<span class="keyword">protected</span> <span class="keyword">void</span> doRollback() {
  <span class="built_in">int</span> takes = takeList.<span class="built_in">size</span>();
  <span class="keyword">synchronized</span>(queueLock) {
    Preconditions.checkState(queue.remainingCapacity() &gt;= takeList.<span class="built_in">size</span>(), <span class="string">"Not enough space in memory channel "</span> +
        <span class="string">"queue to rollback takes. This should never happen, please report"</span>);
    <span class="comment">// 把takeList中的数据放回到queue中</span>
    <span class="keyword">while</span>(!takeList.isEmpty()) {
      queue.addFirst(takeList.removeLast());
    }
    putList.<span class="built_in">clear</span>();
  }
}
</code></pre><p>发生异常后才会调用rollback方法。也就是说take方法被调用之后，由于take方法是从queue中拿数据，并且放到takeList里。所以回滚的时候需要把takeList中的数据还给queue。</p>
<p>MemoryTransaction的close方法只是把状态改成了CLOSED，其他没做什么，就不分析了。</p>
<p>MemoryTransaction的take方法：</p>
<p>take方法从queue中拉出数据，然后放到takeList中。</p>
<pre><code>@<span class="function">Override
<span class="keyword">protected</span> Event <span class="title">doTake</span>(<span class="params"></span>) throws InterruptedException </span>{
  channelCounter.incrementEventTakeAttemptCount();
  <span class="keyword">if</span>(takeList.remainingCapacity() == <span class="number">0</span>) {
    <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(<span class="string">"Take list for MemoryTransaction, capacity "</span> +
        takeList.size() + <span class="string">" full, consider committing more frequently, "</span> +
        <span class="string">"increasing capacity, or increasing thread count"</span>);
  }
  <span class="keyword">if</span>(!queueStored.tryAcquire(keepAlive, TimeUnit.SECONDS)) {
    <span class="keyword">return</span> <span class="keyword">null</span>;
  }
  Event <span class="keyword">event</span>;
  synchronized(queueLock) {
    <span class="keyword">event</span> = queue.poll();
  }
  Preconditions.checkNotNull(<span class="keyword">event</span>, <span class="string">"Queue.poll returned NULL despite semaphore "</span> +
      <span class="string">"signalling existence of entry"</span>);
  takeList.put(<span class="keyword">event</span>);

  <span class="keyword">int</span> eventByteSize = (<span class="keyword">int</span>)Math.ceil(estimateEventSize(<span class="keyword">event</span>)/byteCapacitySlotSize);
  takeByteCounter += eventByteSize;

  <span class="keyword">return</span> <span class="keyword">event</span>;
}
</code></pre><p>Source数据进入Channel过程中Transaction的处理过程：</p>
<p>ChannelProcessor处理这个过程：</p>
<pre><code><span class="keyword">for</span> (Channel reqChannel : reqChannelQueue.keySet()) {
  <span class="comment">// 获取事务</span>
  Transaction tx = reqChannel.getTransaction();
  Preconditions.checkNotNull(tx, <span class="string">"Transaction object must not be null"</span>);
  <span class="keyword">try</span> {
    <span class="comment">// 事务开始</span>
    tx.begin();
    <span class="comment">// 获取Source处理的一个批次中的所有Event</span>
    List&lt;Event&gt; batch = reqChannelQueue.get(reqChannel);

    <span class="keyword">for</span> (Event event : batch) {
      <span class="comment">// MemoryChannel的put方法会MemoryTransaction的put方法。</span>
      reqChannel.put(event);
    }
    <span class="comment">// 提交事务</span>
    tx.commit();
  } <span class="keyword">catch</span> (Throwable t) {
      <span class="comment">// 发生异常回滚事务</span>
    tx.rollback();
    <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) {
      LOG.<span class="keyword">error</span>(<span class="string">"Error while writing to required channel: "</span> +
          reqChannel, t);
      <span class="keyword">throw</span> (Error) t;
    } <span class="keyword">else</span> {
      <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(<span class="string">"Unable to put batch on required "</span> +
          <span class="string">"channel: "</span> + reqChannel, t);
    }
  } <span class="keyword">finally</span> {
    <span class="keyword">if</span> (tx != <span class="keyword">null</span>) {
      <span class="comment">// 最后结束事务</span>
      tx.close();
    }
  }
}
</code></pre><p>Channel中的数据被Sink处理的过程：</p>
<p>以hdfs sink为例讲解：</p>
<pre><code><span class="keyword">public</span> <span class="function">Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>{
    Channel channel = getChannel();
    Transaction transaction = channel.getTransaction();
    List&lt;BucketWriter&gt; writers = Lists.newArrayList();
    transaction.begin();
    <span class="keyword">try</span> {
      <span class="keyword">int</span> txnEventCount = <span class="number">0</span>;
      <span class="keyword">for</span> (txnEventCount = <span class="number">0</span>; txnEventCount &lt; batchSize; txnEventCount++) {
        Event event = channel.take();
        <span class="keyword">if</span> (event == <span class="keyword">null</span>) {
          <span class="keyword">break</span>;
        }

      ... 

      transaction.commit();

      <span class="keyword">if</span> (txnEventCount &lt; <span class="number">1</span>) {
        <span class="keyword">return</span> Status.BACKOFF;
      } <span class="keyword">else</span> {
        sinkCounter.addToEventDrainSuccessCount(txnEventCount);
        <span class="keyword">return</span> Status.READY;
      }
    } <span class="keyword">catch</span> (IOException eIO) {
      transaction.rollback();
      LOG.warn(<span class="string">"HDFS IO error"</span>, eIO);
      <span class="keyword">return</span> Status.BACKOFF;
    } <span class="keyword">catch</span> (Throwable th) {
      transaction.rollback();
      LOG.<span class="keyword">error</span>(<span class="string">"process failed"</span>, th);
      <span class="keyword">if</span> (th <span class="keyword">instanceof</span> Error) {
        <span class="keyword">throw</span> (Error) th;
      } <span class="keyword">else</span> {
        <span class="keyword">throw</span> <span class="keyword">new</span> EventDeliveryException(th);
      }
    } <span class="keyword">finally</span> {
      transaction.close();
    }
 }
</code></pre><p>也是一样的流程，begin，take，commit or rollback，close。</p>
<p>总结：</p>
<ol>
<li><p>MemoryTransaction是MemoryChannel中的一个内部类，内部有2个阻塞队列putList和takeList。MemoryChannel内部有个queue阻塞队列。</p>
</li>
<li><p>putList接收Source交给Channel的event数据，takeList保存Channel交给Sink的event数据。</p>
</li>
<li><p>如果是Source交给Channel任务完成，进行commit的时候。会把putList中的所有event放到MemoryChannel中的queue。</p>
</li>
<li><p>如果是Source交给Channel任务失败，进行rollback的时候。程序就不会继续走下去，比如KafkaSource需要commitOffsets，如果任务失败就不会commitOffsets。</p>
</li>
<li><p>如果是Sink处理完Channel带来的event，进行commit的时候。会清空takeList中的event数据，因为已经没consume。</p>
</li>
<li><p>如果是Sink处理Channel带来的event失败的话，进行rollback的时候。会把takeList中的event写回到queue中。</p>
</li>
</ol>
<p>缺点：</p>
<p>Flume的Transaction跟数据库的Transaction不一样。数据库中的事务回滚之后所有操作的数据都会进行处理。而Flume的却不能还原。比如HDFSSink写数据到HDFS的时候需要rollback，比如本来要写入10000条数据，但是写到5000条的时候rollback，那么已经写入的5000条数据不能回滚，而那10000条数据回到了阻塞队列里，下次再写入的时候还会重新写入这10000条数据。这样就多了5000条重复数据，这是flume设计上的缺陷。</p>
]]></content>
    <summary type="html">
    <![CDATA[Flume中有一个Transaction的概念。本文仅分析Transaction的实现类MemoryTransaction的实现原理，JdbcTransaction的原理跟数据库中的Transaction类似 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[2015总结]]></title>
    <link href="http://fangjian0423.github.io/2016/01/01/2015_end/"/>
    <id>http://fangjian0423.github.io/2016/01/01/2015_end/</id>
    <published>2015-12-31T16:22:35.000Z</published>
    <updated>2015-12-31T16:48:01.000Z</updated>
    <content type="html"><![CDATA[<h2 id="2015年总结">2015年总结</h2><p>又是一年年底，2015年年底了。总结，总结，总结，还是分3个部分。</p>
<h3 id="工作">工作</h3><p>今年2月份春节过完之后就换了工作，之后在这家公司工作到了现在。</p>
<p>来新公司主要的目的是为了做大数据的，但是一进来就是给别人擦屁股，改bug = =。 后来大数据的东西只做了一部分，大概3个月的时间，之后被拉过去做公众号相关的东西了。没办法，公司人手太少，自己负责的etl这部分还有很多不完善的地方，可惜没有时间继续做下去。</p>
<p>公司人少没时间做是一个原因，但是更重要的原因还是自己懒了。</p>
<h3 id="技术">技术</h3><p>2015年，对技术做以下总结：</p>
<ol>
<li>继续写博客，15年写了43篇博客。相比去年的34篇博客，今年写的更多了。但是这43篇博客的质量其实都很一般，没有去年写的springmvc源码分析好。 但是想要写出好的文章得花费很长的时间。争取之后的文章写得多又能写得好。</li>
<li>了解了大数据方面的知识。包括hadoop，hdfs，flume，spark，hbase，elasticsearch，sqoop，hive等方面的知识。由于自己在公司负责的是etl方面的内容，所以对flume了解的比较多，然后又用base和elasticsearch存储了一些东西，所以对这两块内容也比较了解。但是对这些东西，也只是停留在使用的基础上，没有深入了解到内部的结构，明年会深入了解这些内容的。</li>
<li>github贡献了几个开源项目。包括flume，一个es教程，waterfall等。flume的HBaseSink在stop的时候居然没有把serializer关闭掉。给flume提了个pull request，但是flume居然不接受在github提出的pull request，只能在apache jira上处理ticket，但是新建了一个ticket之后居然不能assign给任何人，所以也就没人处理了，有点尴尬。</li>
<li>玩了会grails。由于公司内部的后台系统是用grails搭建的，所以自然就得会grails，grails内部用groovy写的。用了之后发现grails的调试在intellij中特别的慢，而且只要进了一个闭包，调试就特别麻烦。grails项目大了之后启动也非常地慢，有时候还会莫名其妙地出现一些错误，重启一下就好了。综上原因，对grails不是非常喜欢。</li>
<li>spring-boot的使用。公司内部发现grails项目大了之后启动会非常慢，后来开始使用micro-service就行新项目的开发。spring-boot其实是各个框架的整合，包括hibernate，spring，springdata等。提供了一些封装好的方便的方法，但是发现使用了spring-boot之后有些它内部定义好的内容你不看文档是不会知道的，而且有的东西文档里也没有说，所以只能看源码。这个算是使用spring-boot的一个弊端吧。</li>
<li>scala的学习。今年把scala in action这本书看完了。这本书很一般，很多scala的东西感觉都没有讲清楚。自己也把spray-json(scala写的一个json库，很小)的源码看了一遍。其实感觉看源码学语法也是不错的一个方法。</li>
<li>可以勉强算一个全栈了。公司技术少，前端更是只有一个。所以今年做了一些前端的工作，感觉自己一个人能搞定一个公众号的前后端了。写前端的时候使用了angularjs。后来又了解了js的一些打包工具，grunt，gulp等。给公司做了《超级邀请》这个公众号的开发任务，这个公众号的前台页面和接口都是自己写的。</li>
<li>了解了java高级部分的一些知识。知道了java并发的一些内容，知道了内置锁，信号量，栅栏，闭锁等知识。java内存模型也了解了一点，知道了happens-before，java的内存通信是通过共享内存实现的。jvm的书买了，但是还没开始看，是个弱项。</li>
<li>代码质量。今年写的代码质量感觉还是很烂，比去年也就稍微好了一点，依旧很烂。希望16年能写出更好的代码，而不仅仅只是为了完成任务的代码。</li>
</ol>
<h3 id="生活">生活</h3><p>宅，宅，宅。</p>
<p>不过双12买了把ukulele。但是还没开始学 = =，尴尬。不过买了就不会浪费，之后会开始学。</p>
<p>玩了高达模型，跟同事学的。搭了2个MG，武者MK2和迅雷高达。1个bb版强袭高达，年底又买了个pg强袭。pg还没开始搭，搭完绝壁炫酷到爆炸。</p>
<p>附上自己搭的高达图片，就放一张吧。毕竟是写总结的，不是介绍高达的：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/gundam1.JPG" alt=""></p>
<h3 id="2016年计划">2016年计划</h3><p>去年定的2015年计划：</p>
<ol>
<li>今年真的没看书。 有的书看了一点点就没看了，java并发的书看了100多页就没下文了。。 要养成看书的习惯。 (☑。今年把这本书看完了，还看了scala in action。其实还远远不够)</li>
<li><p>技术方面的计划：<br>redis(×。看了elasticsearch)<br>docker(×，而是看了大数据)<br>github贡献开源框架(☑)<br>nio(×)<br>并发包(☑)<br>Mina(×)<br>Netty(×)<br>Python深入(×，深入学习了scala)<br>搜索相关的知识(×)<br>看源码的时候多想想作者的思路以及架构方面，不用特别在意细节(☑)</p>
</li>
<li><p>继续写博客(☑)</p>
</li>
<li>做让生活变得更有趣的事，比如guitar(☑。ukulele，高达)</li>
</ol>
<p>2015的计划虽然只完成了一半，但是由于工作中接触大数据。所以很多内容都没看，转而去看大数据方面的知识了。所以总体完成度还是可以的，算80%。</p>
<p>2016年计划以及展望：</p>
<ol>
<li>继续看书，要看更多的书。 15年居然只看了两本书，对不起自己…. 16年要5本+。</li>
<li><p>技术方面<br>大数据的深入学习，不仅仅局限于会使用。包括spark，es，hbase，hadoop等。<br>分布式方面的学习<br>docker<br>scala的继续深入，要开始用scala写代码<br>netty<br>github继续贡献开源项目<br>jvm</p>
</li>
<li><p>继续写博客</p>
</li>
<li>继续玩高达</li>
<li>学会ukulele</li>
<li>希望能做一些逼格高一点的东西，而不仅仅是做一些功能性的东西</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[2015年年终总结]]>
    
    </summary>
    
      <category term="杂事" scheme="http://fangjian0423.github.io/tags/%E6%9D%82%E4%BA%8B/"/>
    
      <category term="总结" scheme="http://fangjian0423.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spray-json源码分析]]></title>
    <link href="http://fangjian0423.github.io/2015/12/23/scala-spray-json/"/>
    <id>http://fangjian0423.github.io/2015/12/23/scala-spray-json/</id>
    <published>2015-12-22T17:47:35.000Z</published>
    <updated>2016-03-12T10:19:50.000Z</updated>
    <content type="html"><![CDATA[<h2 id="spray-json介绍以及内部结构">spray-json介绍以及内部结构</h2><p><a href="https://github.com/spray/spray-json" target="_blank" rel="external">spray-json</a>是scala的一个轻量的，简洁的，简单的关于JSON实现。</p>
<p>同时也是<a href="http://spray.io/" target="_blank" rel="external">spray</a>项目的json模块。</p>
<p>本文分析spray-json的源码。</p>
<p>在分析spray-json的源码之前，我们先介绍一下spray-json的使用方法以及里面的几个概念。</p>
<p>首先是spray-json的一个使用例子，里面有各种黑魔法：</p>
<pre><code><span class="keyword">val</span> str = <span class="string">"""{
    "name": "Ed",
    "age": 24
}"""</span>

<span class="comment">// 黑魔法。不是String的parseJson方法，而是使用了隐式转换，隐式转换成PimpedString类。PimpedString里有parseJson方法，转换成JsValue对象</span>
<span class="keyword">val</span> jsonVal = str.parseJson <span class="comment">// jsonVal是个JsObject对象的实例</span>

<span class="comment">// jsonVal是个JsObject对象，也是个JsValue实例。JsValue对象都有compactPrint和prettyPrint方法</span>
println(jsonVal.compactPrint) <span class="comment">// 压缩打印</span>
println(jsonVal.prettyPrint) <span class="comment">// 格式化打印</span>

<span class="comment">// 手动构建一个JsObject</span>
<span class="keyword">val</span> jsonObj = <span class="type">JsObject</span>(
    (<span class="string">"name"</span>, <span class="type">JsString</span>(<span class="string">"format"</span>)), (<span class="string">"age"</span>, <span class="type">JsNumber</span>(<span class="number">99</span>))
)

println(jsonObj.compactPrint)
println(jsonObj.prettyPrint)

<span class="comment">// 黑方法，不是List的toJson方法，而是使用了隐式转换，隐式转换成PimpedAny类，PimpedAny类里有toList方法，转换成对应的类型</span>
<span class="keyword">val</span> jsonList = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>).toJson

<span class="comment">// JsValue的toString方法引用了compactPrint方法</span>
println(jsonList)
</code></pre><p>然后是spray-json里的几个概念介绍：</p>
<p>1.JsValue: 抽象类。对原生json中的各种数据类型的抽象。它的实现类JsArray对应json里的数组；JsBoolean对应json里的布尔值；JsNumber对应json里的数值 …</p>
<p>2.JsonFormat: 一个trait，是json序列化和反序列话的抽象。继承JsonReader(JsValue转换成对象的抽象)和JsonWriter(对象转换成JsValue的抽象)。BasicFormats和CollectionFormats、AdditionalFormats等这些trait里都有各种JsonFormat的隐式转换。</p>
<p>3.RootJsonFormat。json文档的抽象，跟JsonFormat一样，只不过RootJsonFormat只支持JsArray和JsObject。因为这是一个json文档对象，只有一个json对象或者一个json数组才能称得上是一个json文档对象。</p>
<p>4.JsonPrinter: 一个trait。json打印成字符串的抽象。具体的实现特质有CompactPrinter(压缩后的字符串)和PrettyPrinter(格式化后的字符串)</p>
<p>5.DefaultJsonProtocol: 继承BasicFormats，混入StandardFormats、CollectionFormats、ProductFormats、AdditionalFormats的特质。我们需要转换一些基础类型或者集合类型的时候需要import这个trait。</p>
<h2 id="json_package对象">json package对象</h2><p>json package对象里定义了一些隐式转换方法和一些实用方法。</p>
<pre><code>package object json {

  // JsField。 一个二元元组，代表json中的一个项(key为String，<span class="keyword">value</span>为任意json类型)
  <span class="keyword">type</span> JsField = (String, JsValue)

  // 反序列化异常
  def deserializationError(msg: String, cause: Throwable = null, fieldNames: List[String] = Nil) = throw new DeserializationException(msg, cause, fieldNames)
  // 序列化异常
  def serializationError(msg: String) = throw new SerializationException(msg)
  // jsonReader方法，是个泛型。使用了隐式参数，返回值是这个隐式参数的引用。也就是说只要调用了jsonReader方法，那么就会自动去找对应泛型类型的实现
  def jsonReader[T](<span class="type">implicit</span> reader: JsonReader[T]) = reader
  // 跟jsonReader方法一个道理。只要调用了jsonWriter方法，那么就会自动去找对应泛型类型的实现
  def jsonWriter[T](<span class="type">implicit</span> writer: JsonWriter[T]) = writer 

  // 隐式转换方法。上面例子的toList使用了这个隐式转换
  <span class="type">implicit</span> def pimpAny[T](<span class="built_in">any</span>: T) = new PimpedAny(<span class="built_in">any</span>)
  // 隐式方法。上面例子的parseJson使用了这个隐式转换
  <span class="type">implicit</span> def pimpString(string: String) = new PimpedString(string)
}

package json {

  // 反序列异常类的定义，上面的deserializationError方法实例化了这个类
  <span class="keyword">case</span> <span class="keyword">class</span> DeserializationException(msg: String, cause: Throwable = null, fieldNames: List[String] = Nil) <span class="keyword">extends</span> RuntimeException(msg, cause)
  // 序列异常类的定义，上面的serializationError方法实例化了这个类
  <span class="keyword">class</span> SerializationException(msg: String) <span class="keyword">extends</span> RuntimeException(msg)

  // 上面的隐式方法pimpAny实例化了这个类。黑魔法toJson方法，不是List的toJson方法，而是List隐式转换成PimpedAny，然后调用PimpedAny的toJson方法。toJson方法的参数是个隐式参数，跟上面代码里的jsonWriter方法一样，会找对应泛型类型的JsonWriter实现类，然后调用JsonWriter的<span class="built_in">write</span>方法
  <span class="keyword">private</span>[json] <span class="keyword">class</span> PimpedAny[T](<span class="built_in">any</span>: T) {
    def toJson(<span class="type">implicit</span> writer: JsonWriter[T]): JsValue = writer.<span class="built_in">write</span>(<span class="built_in">any</span>)
  }

  // 上面的隐式方法pimpString实例化了这个类。黑魔法parseJson方法，不是String的parseJson方法，而是String隐式转换成PimpedString，然后调用PimpedString的parseJson方法
  <span class="keyword">private</span>[json] <span class="keyword">class</span> PimpedString(string: String) {
    @deprecated(<span class="string">"deprecated in favor of parseJson"</span>, <span class="string">"1.2.6"</span>)
    def asJson: JsValue = parseJson
    def parseJson: JsValue = JsonParser(string)
  }
}
</code></pre><h2 id="JsValue(原生json中的各种数据类型的抽象)">JsValue(原生json中的各种数据类型的抽象)</h2><p>JsValue是原生json中各种数据类型的抽象，是个抽象类，直接看JsValue的定义:</p>
<pre><code><span class="keyword">sealed</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">JsValue</span> {</span>
     <span class="comment">// 重载的toString方法引用了compactPrint方法，会打印出json的压缩格式</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span> =</span> compactPrint
  <span class="function"><span class="keyword">def</span> <span class="title">toString</span>(</span>printer: (<span class="type">JsValue</span> =&gt; <span class="type">String</span>)) = printer(<span class="keyword">this</span>)
  <span class="comment">// 压缩打印，使用了CompactPrinter。CompactPrinter是一个JsonPrinter的子类</span>
  <span class="function"><span class="keyword">def</span> <span class="title">compactPrint</span> =</span> <span class="type">CompactPrinter</span>(<span class="keyword">this</span>)
  <span class="comment">// 格式化打印，PrettyPrinter。PrettyPrinter也是一个JsonPrinter的子类</span>
  <span class="function"><span class="keyword">def</span> <span class="title">prettyPrint</span> =</span> <span class="type">PrettyPrinter</span>(<span class="keyword">this</span>)
  <span class="comment">// 转换成对应的类。jsonReader方法在json package里定义，已经分析过。会找对应那个的JsonReader实现类。然后调用read方法</span>
  <span class="function"><span class="keyword">def</span> <span class="title">convertTo</span>[</span><span class="type">T</span> :<span class="type">JsonReader</span>]: <span class="type">T</span> = jsonReader[<span class="type">T</span>].read(<span class="keyword">this</span>)

  <span class="comment">// 转换成JsObject对象，除了JsObject对象重写了这个，返回了自身。其他类型的JsValue都会抛出DeserializationException异常</span>
  <span class="function"><span class="keyword">def</span> <span class="title">asJsObject</span>(</span>errorMsg: <span class="type">String</span> = <span class="string">"JSON object expected"</span>): <span class="type">JsObject</span> = deserializationError(errorMsg)

  <span class="function"><span class="keyword">def</span> <span class="title">asJsObject</span>:</span> <span class="type">JsObject</span> = asJsObject()

  <span class="annotation">@deprecated</span>(<span class="string">"Superceded by 'convertTo'"</span>, <span class="string">"1.1.0"</span>)
  <span class="function"><span class="keyword">def</span> <span class="title">fromJson</span>[</span><span class="type">T</span> :<span class="type">JsonReader</span>]: <span class="type">T</span> = convertTo
}
</code></pre><p>JsNumber是数值类型的抽象：</p>
<pre><code><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JsNumber</span>(</span>value: <span class="type">BigDecimal</span>) <span class="keyword">extends</span> <span class="type">JsValue</span>

<span class="comment">// JsNumber中定义了几个方便的构造方法</span>
<span class="class"><span class="keyword">object</span> <span class="title">JsNumber</span> {</span>
  <span class="keyword">val</span> zero: <span class="type">JsNumber</span> = apply(<span class="number">0</span>)
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Int</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Long</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Double</span>) = n <span class="keyword">match</span> {
    <span class="keyword">case</span> n <span class="keyword">if</span> n.isNaN      =&gt; <span class="type">JsNull</span>
    <span class="keyword">case</span> n <span class="keyword">if</span> n.isInfinity =&gt; <span class="type">JsNull</span>
    <span class="keyword">case</span> _                 =&gt; <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  }
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">BigInt</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">String</span>) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>n: <span class="type">Array</span>[<span class="type">Char</span>]) = <span class="keyword">new</span> <span class="type">JsNumber</span>(<span class="type">BigDecimal</span>(n))
}
</code></pre><p>JsString是字符串类型的抽象：</p>
<pre><code><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JsString</span>(</span>value: <span class="type">String</span>) <span class="keyword">extends</span> <span class="type">JsValue</span>

<span class="comment">// JsString中定义了几个方便的方法</span>
<span class="class"><span class="keyword">object</span> <span class="title">JsString</span> {</span>
    <span class="keyword">val</span> empty = <span class="type">JsString</span>(<span class="string">""</span>)
    <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>value: <span class="type">Symbol</span>) = <span class="keyword">new</span> <span class="type">JsString</span>(value.name)
}
</code></pre><p>JsObject是对象类型的抽象：</p>
<pre><code><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JsObject</span>(</span>fields: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">JsValue</span>]) <span class="keyword">extends</span> <span class="type">JsValue</span> {
  <span class="comment">// 重写了asJsObject方法</span>
  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">asJsObject</span>(</span>errorMsg: <span class="type">String</span>) = <span class="keyword">this</span>
  <span class="comment">// 根据字段名获得对应的JsValue</span>
  <span class="function"><span class="keyword">def</span> <span class="title">getFields</span>(</span>fieldNames: <span class="type">String</span>*): immutable.<span class="type">Seq</span>[<span class="type">JsValue</span>] = fieldNames.flatMap(fields.get)(collection.breakOut)
}

<span class="class"><span class="keyword">object</span> <span class="title">JsObject</span> {</span>
  <span class="keyword">val</span> empty = <span class="type">JsObject</span>(<span class="type">Map</span>.empty[<span class="type">String</span>, <span class="type">JsValue</span>])
  <span class="comment">// 使用多个JsField构造JsObject。这里的JsField就是代表一个(String, JsValue)</span>
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>members: <span class="type">JsField</span>*) = <span class="keyword">new</span> <span class="type">JsObject</span>(<span class="type">Map</span>(members: _*))
  <span class="annotation">@deprecated</span>(<span class="string">"Use JsObject(JsValue*) instead"</span>, <span class="string">"1.3.0"</span>)
  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>members: <span class="type">List</span>[<span class="type">JsField</span>]) = <span class="keyword">new</span> <span class="type">JsObject</span>(<span class="type">Map</span>(members: _*))
}
</code></pre><h2 id="JsonFormat(JsonWriter和JsonReader的子类)">JsonFormat(JsonWriter和JsonReader的子类)</h2><p>一个trait，是json序列化和反序列话的抽象。继承JsonReader(JsValue转换成对象的抽象)和JsonWriter(对象转换成JsValue的抽象)。</p>
<p>JsonReader的定义，把一个JsValue转换成对应的类型：</p>
<pre><code>trait JsonReader[T] {
  <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(json: JsValue)</span>:</span> T
}
</code></pre><p>JsonWriter的定义，把一个类型转换成JsValue：</p>
<pre><code>trait JsonWriter[T] {
  <span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(obj: T)</span>:</span> JsValue
}
</code></pre><p>json package对象里的jsonReader和jsonWriter方法有个隐式参数，我们也分析过：只要调用了jsonReader(JsonWriter)方法，那么就会自动去找对应泛型类型的实现。</p>
<p>AdditionalFormats、BasicFormats、CollectionFormats、StandardFormats等都定义了各种JsonFormat。</p>
<p>比如Int类型就找IntJsonFormat，String类型就找StringJsonFormat …  这些基础类型的JsonFormat都定义在BasicFormats这个trait中。</p>
<p>我们就分析几个BasicFormats中定义的基础类型JsonFormat：</p>
<p>Int基本类型的JsonFormat：</p>
<pre><code><span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">IntJsonFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">JsonFormat</span>[</span><span class="type">Int</span>] {
    <span class="comment">// write方法继承自JsonWriter。 直接实例化一个JsNumber对象</span>
    <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>x: <span class="type">Int</span>) = <span class="type">JsNumber</span>(x)
    <span class="comment">// read方法继承自JsonReader。读取JsNumber中对应的值</span>
    <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value <span class="keyword">match</span> {
      <span class="keyword">case</span> <span class="type">JsNumber</span>(x) =&gt; x.intValue
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected Int as JsNumber, but got "</span> + x)
    }
}
</code></pre><p>String基本类型的JsonFormat：</p>
<pre><code><span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">StringJsonFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">JsonFormat</span>[</span><span class="type">String</span>] {
    <span class="comment">// write方法实例化一个JsString</span>
    <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>x: <span class="type">String</span>) = {
      require(x ne <span class="literal">null</span>)
      <span class="type">JsString</span>(x)
    }
    <span class="comment">// read方法读取JsString中对应的字符串</span>
    <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value <span class="keyword">match</span> {
      <span class="keyword">case</span> <span class="type">JsString</span>(x) =&gt; x
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected String as JsString, but got "</span> + x)
    }
}
</code></pre><p>…..</p>
<p>CollectionFormats中定义了几个集合类的JsonFormat:</p>
<p>List类型的JsonFormat：</p>
<pre><code><span class="type">implicit</span> def listFormat[T :JsonFormat] = new RootJsonFormat[List[T]] {
    // 将List转换成JsArray对象。遍历list中的各个元素，对每个元素调用toJson方法。最后JsArray里的每个元素都是JsValue
    def <span class="built_in">write</span>(list: List[T]) = JsArray(list.map(_.toJson).toVector)
    // JsArray转换成List。对JsArray里的各个JsValue调用convertTo转换成对应的类型
    def read(<span class="keyword">value</span>: JsValue): List[T] = <span class="keyword">value</span> match {
      <span class="keyword">case</span> JsArray(elements) =&gt; elements.map(_.convertTo[T])(collection.breakOut)
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected List as JsArray, but got "</span> + x)
    }
}
</code></pre><p>Map类型的JsonFormat：</p>
<pre><code><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">mapFormat</span>[</span><span class="type">K</span> :<span class="type">JsonFormat</span>, <span class="type">V</span> :<span class="type">JsonFormat</span>] = <span class="keyword">new</span> <span class="type">RootJsonFormat</span>[<span class="type">Map</span>[<span class="type">K</span>, <span class="type">V</span>]] {
    <span class="comment">// 遍历Map中的每一个二元元组。如果元组的第一项不是String，直接抛出SerializationException异常。否则构造key为元组第一项字符串，value为元组第二项的JsVaue对象。</span>
    <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>m: <span class="type">Map</span>[<span class="type">K</span>, <span class="type">V</span>]) = <span class="type">JsObject</span> {
      m.map { field =&gt;
        field._1.toJson <span class="keyword">match</span> {
          <span class="keyword">case</span> <span class="type">JsString</span>(x) =&gt; x -&gt; field._2.toJson
          <span class="keyword">case</span> x =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SerializationException</span>(<span class="string">"Map key must be formatted as JsString, not '"</span> + x + <span class="string">"'"</span>)
        }
      }
    }
    <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value <span class="keyword">match</span> {
      <span class="keyword">case</span> x: <span class="type">JsObject</span> =&gt; x.fields.map { field =&gt;
        (<span class="type">JsString</span>(field._1).convertTo[<span class="type">K</span>], field._2.convertTo[<span class="type">V</span>])
      } (collection.breakOut)
      <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected Map as JsObject, but got "</span> + x)
    }
}
</code></pre><p>AdditionalFormats中定义了一些helper和额外的JsonFormat：</p>
<pre><code><span class="comment">// JsValue的JsonFormat，JsValue调用convertTo或者toJson返回的就是自身</span>
<span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">JsValueFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">JsonFormat</span>[</span><span class="type">JsValue</span>] {
  <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>value: <span class="type">JsValue</span>) = value
  <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value
}


<span class="keyword">implicit</span> <span class="class"><span class="keyword">object</span> <span class="title">RootJsObjectFormat</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">RootJsonFormat</span>[</span><span class="type">JsObject</span>] {
  <span class="function"><span class="keyword">def</span> <span class="title">write</span>(</span>value: <span class="type">JsObject</span>) = value
  <span class="function"><span class="keyword">def</span> <span class="title">read</span>(</span>value: <span class="type">JsValue</span>) = value.asJsObject
}
</code></pre><p>StandardFormats中定义了Option、Either的JsonFormat，1元-7元元组的JsonFormat。</p>
<pre><code><span class="type">implicit</span> def tuple1Format[A :JF] = new JF[Tuple1[A]] {
  def <span class="built_in">write</span>(t: Tuple1[A]) = t._1.toJson
  def read(<span class="keyword">value</span>: JsValue) = Tuple1(<span class="keyword">value</span>.convertTo[A])
}

<span class="type">implicit</span> def tuple2Format[A :JF, B :JF] = new RootJsonFormat[(A, B)] {
  def <span class="built_in">write</span>(t: (A, B)) = JsArray(t._1.toJson, t._2.toJson)
  def read(<span class="keyword">value</span>: JsValue) = <span class="keyword">value</span> match {
    <span class="keyword">case</span> JsArray(Seq(a, b)) =&gt; (a.convertTo[A], b.convertTo[B])
    <span class="keyword">case</span> x =&gt; deserializationError(<span class="string">"Expected Tuple2 as JsArray, but got "</span> + x)
  }
}
</code></pre><h2 id="JsonPrinter(将JsValue打印成原生json字符串)">JsonPrinter(将JsValue打印成原生json字符串)</h2><p>JsonPrinter继承一个函数对象，这个函数的输入是个JsValue，输出是String：</p>
<pre><code><span class="class"><span class="keyword">trait</span> <span class="title">JsonPrinter</span> <span class="keyword"><span class="keyword">extends</span></span> (</span><span class="type">JsValue</span> =&gt; <span class="type">String</span>)
</code></pre><p>内部定义了一个抽象方法：</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">print</span><span class="params">(<span class="symbol">x:</span> <span class="constant">JsValue</span>, <span class="symbol">sb:</span> <span class="constant">JStringBuilder</span>)</span></span>
</code></pre><p>CompactPrinter继承JsonPrinter，压缩打印：</p>
<p>实现的print方法：</p>
<pre><code>def <span class="built_in">print</span>(x: JsValue, sb: StringBuilder) {
    x match {
      <span class="keyword">case</span> JsObject(x) =&gt; <span class="built_in">print</span>Object(x, sb)
      <span class="keyword">case</span> JsArray(x)  =&gt; <span class="built_in">print</span>Array(x, sb)
      <span class="keyword">case</span> _ =&gt; <span class="built_in">print</span>Leaf(x, sb)
    }
}
</code></pre><p>PrettyPrinter也继承JsonPrinter，格式化打印：</p>
<p>实现的print方法：</p>
<pre><code><span class="keyword">def</span> print(<span class="string">x:</span> JsValue, <span class="string">sb:</span> StringBuilder) {
  print(x, sb, <span class="number">0</span>)
}

  <span class="comment">// indent参数是格式化打印的关键</span>
<span class="keyword">protected</span> <span class="keyword">def</span> print(<span class="string">x:</span> JsValue, <span class="string">sb:</span> StringBuilder, <span class="string">indent:</span> Int) {
  x match {
    <span class="keyword">case</span> JsObject(x) =&gt; printObject(x, sb, indent)
    <span class="keyword">case</span> JsArray(x)  =&gt; printArray(x, sb, indent)
    <span class="keyword">case</span> _ =&gt; printLeaf(x, sb)
  }
}
</code></pre><h2 id="DefaultJsonProtocol(整合了多个JsonFormat)">DefaultJsonProtocol(整合了多个JsonFormat)</h2><p>直接看源码：</p>
<pre><code><span class="class"><span class="keyword">trait</span> <span class="title">DefaultJsonProtocol</span>
</span>    <span class="keyword">extends</span> <span class="type">BasicFormats</span>
    <span class="keyword">with</span> <span class="type">StandardFormats</span>
    <span class="keyword">with</span> <span class="type">CollectionFormats</span>
    <span class="keyword">with</span> <span class="type">ProductFormats</span>
    <span class="keyword">with</span> <span class="type">AdditionalFormats</span>

<span class="class"><span class="keyword">object</span> <span class="title">DefaultJsonProtocol</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">DefaultJsonProtocol</span></span>
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[spray-json是scala的一个轻量的，简洁的，简单的关于JSON实现。同时也是spray项目的json模块，本文分析spray-json的源码 ...]]>
    
    </summary>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/tags/jvm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/categories/scala/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala 隐式转换和隐式参数]]></title>
    <link href="http://fangjian0423.github.io/2015/12/20/scala-implicit/"/>
    <id>http://fangjian0423.github.io/2015/12/20/scala-implicit/</id>
    <published>2015-12-20T07:38:22.000Z</published>
    <updated>2016-03-12T10:18:09.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>Scala的implicit功能很强大，可以自动地给对象”添加一个属性”。 这里打上引号的原因是Scala内部进行编译的时候会自动加上隐式转换函数。</p>
<p>很多Scala开源框架内部都大量使用了implicit。因为implicit真的很强大，写得好的implicit可以让代码更优雅。但个人感觉implicit也有一些缺点，比如使用了implicit之后，看源码或者使用一些library的时候无法下手，因为你根本不知道作者哪里写了implicit。这个也会对初学者造成一些困扰。</p>
<p>比如Scala中Option就有一个implicit可以将Option转换成Iterable：</p>
<pre><code>val <span class="built_in">list</span> = List(<span class="number">1</span>, <span class="number">2</span>)
val <span class="built_in">map</span> = Map(<span class="number">1</span> -&gt; <span class="number">11</span>, <span class="number">2</span> -&gt; <span class="number">22</span>, <span class="number">3</span> -&gt; <span class="number">33</span>)

val newList = <span class="built_in">list</span>.flatMap {
    num =&gt; <span class="built_in">map</span>.get(num) <span class="comment">// map.get方法返回的是Option，可以被隐式转换成Iterable</span>
} 
</code></pre><p>以下是implicit的一个小例子。</p>
<p>比如以下一个例子，定义一个Int类型的变量num，但是赋值给了一个Double类型的数值。这时候就会编译错误：</p>
<pre><code><span class="variable"><span class="keyword">val</span> num</span>: <span class="typename">Int</span> = <span class="number">3.5</span> <span class="comment">// Compile Error</span>
</code></pre><p>但是我们加了一个隐式转换之后，就没问题了:</p>
<pre><code><span class="type">implicit</span> def double2Int(d: <span class="type">Double</span>) = d.toInt

val num: <span class="built_in">Int</span> = <span class="number">3.5</span> // <span class="number">3</span>， 这段代码会被编译成 val num: <span class="built_in">Int</span> = double2Int(<span class="number">3.5</span>)
</code></pre><h2 id="隐式转换规则">隐式转换规则</h2><h3 id="标记规则(Marking_Rule)">标记规则(Marking Rule)</h3><p>任何变量，函数或者对象都可以用implicit这个关键字进行标记，表示可以进行隐式转换。</p>
<pre><code><span class="type">implicit</span> def intToString(x: <span class="built_in">Int</span>) = x.toString
</code></pre><p>编译器可能会将x + y 转换成 convert(x) + y 如果convert被标记成implicit。</p>
<h3 id="作用域规则(Scope_Rule)">作用域规则(Scope Rule)</h3><p>在一个作用域内，一个隐式转换必须是一个唯一的标识。</p>
<p>比如说MyUtils这个object里有很多隐式转换。x + y 不会使用MyUtils里的隐式转换。 除非import进来。 import MyUtils._</p>
<p>Scala编译器还能在companion class中去找companion object中定义的隐式转换。</p>
<pre><code><span class="keyword">object</span> Player {
    implicit def getClub(player: Player): Club = Club(player.clubName)
}

<span class="class"><span class="keyword">class</span> <span class="title">Player</span></span>(<span class="variable"><span class="keyword">val</span> name</span>: String, <span class="variable"><span class="keyword">val</span> age</span>: <span class="typename">Int</span>, <span class="variable"><span class="keyword">val</span> clubName</span>: String) {

}

<span class="variable"><span class="keyword">val</span> p</span> = new Player(<span class="string">"costa"</span>, <span class="number">27</span>, <span class="string">"Chelsea"</span>)

println(p.welcome) <span class="comment">// Chelsea welcome you here!</span>
println(p.playerNum) <span class="comment">// 21</span>
</code></pre><h3 id="一次编译只隐式转换一次(One-at-a-time_Rule)">一次编译只隐式转换一次(One-at-a-time Rule)</h3><p>Scala不会把 x + y 转换成 convert1(convert2(x)) + y</p>
<h2 id="隐式转换类型">隐式转换类型</h2><h3 id="隐式转换成正确的类型">隐式转换成正确的类型</h3><p>这种类型是Scala编译器对隐式转换的第一选择。 比如说编译器看到一个类型的X的数据，但是需要一个类型为Y的数据，那么就会去找把X类型转换成Y类型的隐式转换。</p>
<p>本文一开始的double2Int方法就是这种类型的隐式转换。</p>
<pre><code><span class="type">implicit</span> def double2Int(d: <span class="type">Double</span>) = d.toInt

val num: <span class="built_in">Int</span> = <span class="number">3.5</span> // <span class="number">3</span>
</code></pre><p>当编译器发现变量num是个Int类型，并且用Double类型给它赋值的时候，会报错。 但是在报错之前，编译器会查找Double =&gt; Int的隐式转换。然后发现了double2Int这个隐式转换函数。于是就使用了隐式转换。</p>
<h3 id="方法调用的隐式转换">方法调用的隐式转换</h3><p>比如这段代码  obj.doSomeThing。 比如obj对象没有doSomeThing这个方法，编译器会会去查找拥有doSomeThing方法的类型，并且看obj类型是否有隐式转换成有doSomeThing类型的函数。有的话就是将obj对象隐式转换成拥有doSomeThing方法的对象。</p>
<p>以下是一个例子：</p>
<pre><code><span class="keyword">case</span> <span class="keyword">class</span> Person(<span class="keyword">name</span>: String, age: <span class="built_in">Int</span>) {
    def +(num: <span class="built_in">Int</span>) = age + num
    def +(p: Person) = age + p.age
  }

val person = Person(<span class="string">"format"</span>, <span class="number">99</span>)
println(person + <span class="number">1</span>) // <span class="number">100</span>
//  println(<span class="number">1</span> + person)  报错，因为<span class="built_in">Int</span>的+方法没有有Person参数的重载方法

<span class="type">implicit</span> def personAddAge(x: <span class="built_in">Int</span>) = Person(<span class="string">"unknown"</span>, x)

println(<span class="number">1</span> + person) // <span class="number">100</span>
</code></pre><p>有了隐式转换方法之后，编译器检查 1 + person 表达式，发现Int的+方法没有有Person参数的重载方法。在放弃之前查看是否有将Int类型的对象转换成以Person为参数的+方法的隐式转换函数，于是找到了，然后就进行了隐式转换。</p>
<p>Scala的Predef中也使用了方法调用的隐式转换。</p>
<pre><code>Map(<span class="number">1</span> -&gt; <span class="number">11</span>, <span class="number">2</span> -&gt; <span class="number">22</span>)
</code></pre><p>上面这段Map中的参数是个二元元组。 Int没有 -&gt; 方法。 但是在Predef中定义了：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> ArrowAssoc[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    @inline def -&gt; [B](y: B): Tuple2[A, B] = Tuple2(self, y)
    def →[B](y: B): Tuple2[A, B] = -&gt;(y)
}
</code></pre><h3 id="隐式参数">隐式参数</h3><p>隐式参数的意义是当方法需要多个参数的时候，可以定义一些隐式参数，这些隐式参数可以被自动加到方法填充的参数里，而不必手填充。</p>
<pre><code>def implicitParamFunc(<span class="keyword">name</span>: String)(<span class="type">implicit</span> tiger: Tiger, lion: Lion): <span class="keyword">Unit</span> = {
    println(<span class="keyword">name</span> + <span class="string">" have a tiget and a lion, their names are: "</span> + tiger.<span class="keyword">name</span> + <span class="string">", "</span> + lion.<span class="keyword">name</span>)
}

object Zoo {
    <span class="type">implicit</span> val tiger = Tiger(<span class="string">"tiger1"</span>)
    <span class="type">implicit</span> val lion = Lion(<span class="string">"lion1"</span>)
}

<span class="keyword">import</span> Zoo._

implicitParamFunc(<span class="string">"format"</span>)
</code></pre><p>上面这个代码中implicitParamFunc中的第二个参数定义成了隐式参数。</p>
<p>然后在Zoo对象里定义了两个隐式变量，import进来之后，调用implicitParamFunc方法的时候这两个变量被自动填充到了参数里。</p>
<p>这里需要注意的是不仅仅方法中的参数需要被定义成隐式参数，对应的隐式参数的变量也需要被定义成隐式变量。</p>
<h2 id="其他">其他</h2><p>对象中的隐式转换可以只import自己需要的。</p>
<pre><code>object MyUtils {
    <span class="type">implicit</span> def a ...
    <span class="type">implicit</span> def b ...
}

<span class="keyword">import</span> MyUtils.a
</code></pre><p>隐式转换修饰符implicit可以修饰class，method，变量，object。</p>
<p>修饰方法和变量的隐式转换本文已经介绍过，就不继续说了。</p>
<p>修饰class的隐式转换，它的作用跟修饰method的隐式转换类似：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMarker(val start: <span class="built_in">Int</span>) {
    def --&gt;(<span class="keyword">end</span>: <span class="built_in">Int</span>) = start to <span class="keyword">end</span>
}

<span class="number">1</span> --&gt; <span class="number">10</span> // <span class="built_in">Range</span>(<span class="number">1</span>, <span class="number">10</span>)
</code></pre><p>上段代码可以改造成使用Value Class完成类的隐式转换：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMaker(start: <span class="built_in">Int</span>) <span class="keyword">extends</span> AnyVal {
    def --&gt;(<span class="keyword">end</span>: <span class="built_in">Int</span>) = start to <span class="keyword">end</span>
}
</code></pre><p>修饰object的隐式转换：</p>
<pre><code>trait Calculate[T] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: T, y: T)</span>:</span> T
}

implicit object IntCal extends Calculate[Int] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: Int, y: Int)</span>:</span> Int = x + y
}

implicit object ListCal extends Calculate[List[Int]] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: List[Int], y: List[Int])</span>:</span> List[Int] = x ::: y
}

<span class="function"><span class="keyword">def</span> <span class="title">implicitObjMethod</span>[<span class="title">T</span>]<span class="params">(x: T, y: T)</span><span class="params">(implicit cal: Calculate[T])</span>:</span> Unit = {
    println(x + <span class="string">" + "</span> + y + <span class="string">" = "</span> + cal.add(x, y))
}

implicitObjMethod(<span class="number">1</span>, <span class="number">2</span>) // <span class="number">1</span> + <span class="number">2</span> = <span class="number">3</span>
implicitObjMethod(List(<span class="number">1</span>, <span class="number">2</span>), List(<span class="number">3</span>, <span class="number">4</span>)) // List(<span class="number">1</span>, <span class="number">2</span>) + List(<span class="number">3</span>, <span class="number">4</span>) = List(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Scala的implicit功能很强大，可以自动地给对象"添加一个属性"。 这里打上引号的原因是Scala内部进行编译的时候会自动加上隐式转换函数 ...]]>
    
    </summary>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/tags/jvm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/categories/scala/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[elasticsearch查询模板]]></title>
    <link href="http://fangjian0423.github.io/2015/11/07/elasticsearch-search-template/"/>
    <id>http://fangjian0423.github.io/2015/11/07/elasticsearch-search-template/</id>
    <published>2015-11-07T08:04:25.000Z</published>
    <updated>2015-12-20T17:06:16.000Z</updated>
    <content type="html"><![CDATA[<p>最近在公司又用到了elasticsearch，也用到了查询模板，顺便写篇文章记录一下查询模板的使用。</p>
<p>以1个需求为例讲解es模板的使用：</p>
<p><strong>页面上某个按钮在一段时间内的点击次数统计，并且可以以小时，天，月为单位进行汇总，并且需要去重。</strong></p>
<p>创建索引，只定义3个字段，user_id, user_name和create_time:</p>
<pre><code>-POST /<span class="variable">$ES</span>/event_index

{
  <span class="string">"mappings"</span>: {
    <span class="string">"event"</span>: {
      <span class="string">"_ttl"</span>: {
        <span class="string">"enabled"</span>: false
      },
      <span class="string">"_timestamp"</span>: {
        <span class="string">"enabled"</span>: true,
        <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
      },
      <span class="string">"properties"</span>: {
        <span class="string">"user_id"</span>: {
          <span class="string">"type"</span>: <span class="string">"string"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>,
          <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>
        },
        <span class="string">"create_time"</span>: {
          <span class="string">"type"</span>: <span class="string">"date"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>,
          <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>,
          <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
        },
        <span class="string">"user_name"</span>: {
          <span class="string">"type"</span>: <span class="string">"string"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>
        }
      }
    }
  }
}
</code></pre><p>定义对应的查询模板，模板名字stats，使用了Cardinality和DateHistogram这两个Aggregation<br>，其中Date Histogram嵌套在Cardinality里。在定义模板的时候，{ { } } 的表示是个参数，需要调用模板的时候传递进来:</p>
<pre><code>  -POST /<span class="variable">$ES</span>/_search/template/stats
{
    <span class="string">"template"</span>: {
        <span class="string">"query"</span>: {
            <span class="string">"bool"</span>: {
                <span class="string">"must"</span>: [
                    {
                        <span class="string">"range"</span>: {
                            <span class="string">"create_time"</span>: {
                                <span class="string">"gte"</span>: <span class="string">""</span>,
                                <span class="string">"lte"</span>: <span class="string">""</span>
                            }
                        }
                    }
                ]
            }
        },
        <span class="string">"size"</span>: <span class="number">0</span>,
        <span class="string">"aggs"</span>: {
            <span class="string">"stats_data"</span>: {
                <span class="string">"date_histogram"</span>: {
                    <span class="string">"field"</span>: <span class="string">"create_time"</span>,
                    <span class="string">"interval"</span>: <span class="string">""</span>
                },
                <span class="string">"aggs"</span>: {
                    <span class="string">"time"</span>: {
                        <span class="string">"cardinality"</span>: {
                            <span class="string">"field"</span>: <span class="string">"user_id"</span>
                        }
                    }
                }
            }
        }
    }
}
</code></pre><p>Cardinality Aggregation的作用就是类似sql中的distinct，去重。</p>
<p>Date Histogram Aggregation的作用是根据时间进行统计。内部有个interval属性表面统计的范畴。</p>
<p>下面加几条数据到event_index里：</p>
<pre><code>-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 12:00:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"2"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format2"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:30:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"3"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format3"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:30:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:50:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:55:00"</span>
}
</code></pre><p>11-07 12-13点有1条数据，1个用户<br>11-07 13-14点有4条数据，3个用户</p>
<p>使用模板查询：</p>
<pre><code>curl -XGET <span class="string">"<span class="variable">$ES</span>/event_index/_search/template"</span> -d'{
  <span class="string">"template"</span>: { <span class="string">"id"</span>: <span class="string">"stats"</span> }, 
  <span class="string">"params"</span>: { <span class="string">"earliest"</span>: <span class="string">"2015-11-07 00:00:00"</span>, <span class="string">"latest"</span>: <span class="string">"2015-11-07 23:59:59"</span>, <span class="string">"interval"</span>: <span class="string">"hour"</span> }
}'    
</code></pre><p>结果：</p>
<pre><code>{
    "<span class="attribute">took</span>": <span class="value"><span class="number">3</span></span>,
    "<span class="attribute">timed_out</span>": <span class="value"><span class="literal">false</span></span>,
    "<span class="attribute">_shards</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">successful</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">failed</span>": <span class="value"><span class="number">0</span>
    </span>}</span>,
    "<span class="attribute">hits</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">max_score</span>": <span class="value"><span class="number">0</span></span>,
        "<span class="attribute">hits</span>": <span class="value">[]
    </span>}</span>,
    "<span class="attribute">aggregations</span>": <span class="value">{
        "<span class="attribute">stats_data</span>": <span class="value">{
            "<span class="attribute">buckets</span>": <span class="value">[
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 12:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446897600000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">1</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">1</span>
                    </span>}
                </span>},
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 13:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446901200000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">4</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">3</span>
                    </span>}
                </span>}
            ]
        </span>}
    </span>}
</span>}
</code></pre><p>12点-13点的只有1条数据，1个用户。13-14点的有4条数据，3个用户。</p>
<p>以天(day)统计：</p>
<pre><code>curl -XGET <span class="string">"<span class="variable">$ES</span>/event_index/_search/template"</span> -d'{
  <span class="string">"template"</span>: { <span class="string">"id"</span>: <span class="string">"stats"</span> }, 
  <span class="string">"params"</span>: { <span class="string">"earliest"</span>: <span class="string">"2015-11-07 00:00:00"</span>, <span class="string">"latest"</span>: <span class="string">"2015-11-07 23:59:59"</span>, <span class="string">"interval"</span>: <span class="string">"day"</span> }
}'    
</code></pre><p>结果：</p>
<pre><code>{
    "<span class="attribute">took</span>": <span class="value"><span class="number">4</span></span>,
    "<span class="attribute">timed_out</span>": <span class="value"><span class="literal">false</span></span>,
    "<span class="attribute">_shards</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">successful</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">failed</span>": <span class="value"><span class="number">0</span>
    </span>}</span>,
    "<span class="attribute">hits</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">max_score</span>": <span class="value"><span class="number">0</span></span>,
        "<span class="attribute">hits</span>": <span class="value">[]
    </span>}</span>,
    "<span class="attribute">aggregations</span>": <span class="value">{
        "<span class="attribute">stats_data</span>": <span class="value">{
            "<span class="attribute">buckets</span>": <span class="value">[
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 00:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446854400000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">5</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">3</span>
                    </span>}
                </span>}
            ]
        </span>}
    </span>}
</span>}
</code></pre><p>11-07这一天有5条数据，3个用户。</p>
<p>本文只是简单说明了es查询模板的使用，也简单使用了2个aggregation。更多内容可以去官网查看相关资料。</p>
]]></content>
    <summary type="html">
    <![CDATA[近在公司又用到了elasticsearch，也用到了查询模板，顺便写篇文章记录一下查询模板的使用 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="elasticsearch" scheme="http://fangjian0423.github.io/tags/elasticsearch/"/>
    
      <category term="elasticsearch" scheme="http://fangjian0423.github.io/categories/elasticsearch/"/>
    
  </entry>
  
</feed>
