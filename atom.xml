<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Format's Notes]]></title>
  <subtitle><![CDATA[吃饭睡觉撸代码]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://fangjian0423.github.io/"/>
  <updated>2015-12-20T17:11:57.000Z</updated>
  <id>http://fangjian0423.github.io/</id>
  
  <author>
    <name><![CDATA[Format]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Scala 隐式转换和隐式参数]]></title>
    <link href="http://fangjian0423.github.io/2015/12/20/scala-implicit/"/>
    <id>http://fangjian0423.github.io/2015/12/20/scala-implicit/</id>
    <published>2015-12-20T07:38:22.000Z</published>
    <updated>2015-12-20T17:11:57.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>Scala的implicit功能很强大，可以自动地给对象”添加一个属性”。 这里打上引号的原因是Scala内部进行编译的时候会自动加上隐式转换函数。</p>
<p>很多Scala开源框架内部都大量使用了implicit。因为implicit真的很强大，写得好的implicit可以让代码更优雅。但个人感觉implicit也有一些缺点，比如使用了implicit之后，看源码或者使用一些library的时候无法下手，因为你根本不知道作者哪里写了implicit。这个也会对初学者造成一些困扰。</p>
<p>比如Scala中Option就有一个implicit可以将Option转换成Iterable：</p>
<pre><code>val <span class="built_in">list</span> = List(<span class="number">1</span>, <span class="number">2</span>)
val <span class="built_in">map</span> = Map(<span class="number">1</span> -&gt; <span class="number">11</span>, <span class="number">2</span> -&gt; <span class="number">22</span>, <span class="number">3</span> -&gt; <span class="number">33</span>)

val newList = <span class="built_in">list</span>.flatMap {
    num =&gt; <span class="built_in">map</span>.get(num) <span class="comment">// map.get方法返回的是Option，可以被隐式转换成Iterable</span>
} 
</code></pre><p>以下是implicit的一个小例子。</p>
<p>比如以下一个例子，定义一个Int类型的变量num，但是赋值给了一个Double类型的数值。这时候就会编译错误：</p>
<pre><code><span class="variable"><span class="keyword">val</span> num</span>: <span class="typename">Int</span> = <span class="number">3.5</span> <span class="comment">// Compile Error</span>
</code></pre><p>但是我们加了一个隐式转换之后，就没问题了:</p>
<pre><code><span class="type">implicit</span> def double2Int(d: <span class="type">Double</span>) = d.toInt

val num: <span class="built_in">Int</span> = <span class="number">3.5</span> // <span class="number">3</span>， 这段代码会被编译成 val num: <span class="built_in">Int</span> = double2Int(<span class="number">3.5</span>)
</code></pre><h2 id="隐式转换规则">隐式转换规则</h2><h3 id="标记规则(Marking_Rule)">标记规则(Marking Rule)</h3><p>任何变量，函数或者对象都可以用implicit这个关键字进行标记，表示可以进行隐式转换。</p>
<pre><code><span class="type">implicit</span> def intToString(x: <span class="built_in">Int</span>) = x.toString
</code></pre><p>编译器可能会将x + y 转换成 convert(x) + y 如果convert被标记成implicit。</p>
<h3 id="作用域规则(Scope_Rule)">作用域规则(Scope Rule)</h3><p>在一个作用域内，一个隐式转换必须是一个唯一的标识。</p>
<p>比如说MyUtils这个object里有很多隐式转换。x + y 不会使用MyUtils里的隐式转换。 除非import进来。 import MyUtils._</p>
<p>Scala编译器还能在companion class中去找companion object中定义的隐式转换。</p>
<pre><code><span class="keyword">object</span> Player {
    implicit def getClub(player: Player): Club = Club(player.clubName)
}

<span class="class"><span class="keyword">class</span> <span class="title">Player</span></span>(<span class="variable"><span class="keyword">val</span> name</span>: String, <span class="variable"><span class="keyword">val</span> age</span>: <span class="typename">Int</span>, <span class="variable"><span class="keyword">val</span> clubName</span>: String) {

}

<span class="variable"><span class="keyword">val</span> p</span> = new Player(<span class="string">"costa"</span>, <span class="number">27</span>, <span class="string">"Chelsea"</span>)

println(p.welcome) <span class="comment">// Chelsea welcome you here!</span>
println(p.playerNum) <span class="comment">// 21</span>
</code></pre><h3 id="一次编译只隐式转换一次(One-at-a-time_Rule)">一次编译只隐式转换一次(One-at-a-time Rule)</h3><p>Scala不会把 x + y 转换成 convert1(convert2(x)) + y</p>
<h2 id="隐式转换类型">隐式转换类型</h2><h3 id="隐式转换成正确的类型">隐式转换成正确的类型</h3><p>这种类型是Scala编译器对隐式转换的第一选择。 比如说编译器看到一个类型的X的数据，但是需要一个类型为Y的数据，那么就会去找把X类型转换成Y类型的隐式转换。</p>
<p>本文一开始的double2Int方法就是这种类型的隐式转换。</p>
<pre><code><span class="type">implicit</span> def double2Int(d: <span class="type">Double</span>) = d.toInt

val num: <span class="built_in">Int</span> = <span class="number">3.5</span> // <span class="number">3</span>
</code></pre><p>当编译器发现变量num是个Int类型，并且用Double类型给它赋值的时候，会报错。 但是在报错之前，编译器会查找Double =&gt; Int的隐式转换。然后发现了double2Int这个隐式转换函数。于是就使用了隐式转换。</p>
<h3 id="方法调用的隐式转换">方法调用的隐式转换</h3><p>比如这段代码  obj.doSomeThing。 比如obj对象没有doSomeThing这个方法，编译器会会去查找拥有doSomeThing方法的类型，并且看obj类型是否有隐式转换成有doSomeThing类型的函数。有的话就是将obj对象隐式转换成拥有doSomeThing方法的对象。</p>
<p>以下是一个例子：</p>
<pre><code><span class="keyword">case</span> <span class="keyword">class</span> Person(<span class="keyword">name</span>: String, age: <span class="built_in">Int</span>) {
    def +(num: <span class="built_in">Int</span>) = age + num
    def +(p: Person) = age + p.age
  }

val person = Person(<span class="string">"format"</span>, <span class="number">99</span>)
println(person + <span class="number">1</span>) // <span class="number">100</span>
//  println(<span class="number">1</span> + person)  报错，因为<span class="built_in">Int</span>的+方法没有有Person参数的重载方法

<span class="type">implicit</span> def personAddAge(x: <span class="built_in">Int</span>) = Person(<span class="string">"unknown"</span>, x)

println(<span class="number">1</span> + person) // <span class="number">100</span>
</code></pre><p>有了隐式转换方法之后，编译器检查 1 + person 表达式，发现Int的+方法没有有Person参数的重载方法。在放弃之前查看是否有将Int类型的对象转换成以Person为参数的+方法的隐式转换函数，于是找到了，然后就进行了隐式转换。</p>
<p>Scala的Predef中也使用了方法调用的隐式转换。</p>
<pre><code>Map(<span class="number">1</span> -&gt; <span class="number">11</span>, <span class="number">2</span> -&gt; <span class="number">22</span>)
</code></pre><p>上面这段Map中的参数是个二元元组。 Int没有 -&gt; 方法。 但是在Predef中定义了：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> ArrowAssoc[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    @inline def -&gt; [B](y: B): Tuple2[A, B] = Tuple2(self, y)
    def →[B](y: B): Tuple2[A, B] = -&gt;(y)
}
</code></pre><h3 id="隐式参数">隐式参数</h3><p>隐式参数的意义是当方法需要多个参数的时候，可以定义一些隐式参数，这些隐式参数可以被自动加到方法填充的参数里，而不必手填充。</p>
<pre><code>def implicitParamFunc(<span class="keyword">name</span>: String)(<span class="type">implicit</span> tiger: Tiger, lion: Lion): <span class="keyword">Unit</span> = {
    println(<span class="keyword">name</span> + <span class="string">" have a tiget and a lion, their names are: "</span> + tiger.<span class="keyword">name</span> + <span class="string">", "</span> + lion.<span class="keyword">name</span>)
}

object Zoo {
    <span class="type">implicit</span> val tiger = Tiger(<span class="string">"tiger1"</span>)
    <span class="type">implicit</span> val lion = Lion(<span class="string">"lion1"</span>)
}

<span class="keyword">import</span> Zoo._

implicitParamFunc(<span class="string">"format"</span>)
</code></pre><p>上面这个代码中implicitParamFunc中的第二个参数定义成了隐式参数。</p>
<p>然后在Zoo对象里定义了两个隐式变量，import进来之后，调用implicitParamFunc方法的时候这两个变量被自动填充到了参数里。</p>
<p>这里需要注意的是不仅仅方法中的参数需要被定义成隐式参数，对应的隐式参数的变量也需要被定义成隐式变量。</p>
<h2 id="其他">其他</h2><p>对象中的隐式转换可以只import自己需要的。</p>
<pre><code>object MyUtils {
    <span class="type">implicit</span> def a ...
    <span class="type">implicit</span> def b ...
}

<span class="keyword">import</span> MyUtils.a
</code></pre><p>隐式转换修饰符implicit可以修饰class，method，变量，object。</p>
<p>修饰方法和变量的隐式转换本文已经介绍过，就不继续说了。</p>
<p>修饰class的隐式转换，它的作用跟修饰method的隐式转换类似：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMarker(val start: <span class="built_in">Int</span>) {
    def --&gt;(<span class="keyword">end</span>: <span class="built_in">Int</span>) = start to <span class="keyword">end</span>
}

<span class="number">1</span> --&gt; <span class="number">10</span> // <span class="built_in">Range</span>(<span class="number">1</span>, <span class="number">10</span>)
</code></pre><p>上段代码可以改造成使用Value Class完成类的隐式转换：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMaker(start: <span class="built_in">Int</span>) <span class="keyword">extends</span> AnyVal {
    def --&gt;(<span class="keyword">end</span>: <span class="built_in">Int</span>) = start to <span class="keyword">end</span>
}
</code></pre><p>修饰object的隐式转换：</p>
<pre><code>trait Calculate[T] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: T, y: T)</span>:</span> T
}

implicit object IntCal extends Calculate[Int] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: Int, y: Int)</span>:</span> Int = x + y
}

implicit object ListCal extends Calculate[List[Int]] {
    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x: List[Int], y: List[Int])</span>:</span> List[Int] = x ::: y
}

<span class="function"><span class="keyword">def</span> <span class="title">implicitObjMethod</span>[<span class="title">T</span>]<span class="params">(x: T, y: T)</span><span class="params">(implicit cal: Calculate[T])</span>:</span> Unit = {
    println(x + <span class="string">" + "</span> + y + <span class="string">" = "</span> + cal.add(x, y))
}

implicitObjMethod(<span class="number">1</span>, <span class="number">2</span>) // <span class="number">1</span> + <span class="number">2</span> = <span class="number">3</span>
implicitObjMethod(List(<span class="number">1</span>, <span class="number">2</span>), List(<span class="number">3</span>, <span class="number">4</span>)) // List(<span class="number">1</span>, <span class="number">2</span>) + List(<span class="number">3</span>, <span class="number">4</span>) = List(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Scala的implicit功能很强大，可以自动地给对象"添加一个属性"。 这里打上引号的原因是Scala内部进行编译的时候会自动加上隐式转换函数 ...]]>
    
    </summary>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/tags/jvm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[elasticsearch查询模板]]></title>
    <link href="http://fangjian0423.github.io/2015/11/07/elasticsearch-search-template/"/>
    <id>http://fangjian0423.github.io/2015/11/07/elasticsearch-search-template/</id>
    <published>2015-11-07T08:04:25.000Z</published>
    <updated>2015-12-20T17:06:16.000Z</updated>
    <content type="html"><![CDATA[<p>最近在公司又用到了elasticsearch，也用到了查询模板，顺便写篇文章记录一下查询模板的使用。</p>
<p>以1个需求为例讲解es模板的使用：</p>
<p><strong>页面上某个按钮在一段时间内的点击次数统计，并且可以以小时，天，月为单位进行汇总，并且需要去重。</strong></p>
<p>创建索引，只定义3个字段，user_id, user_name和create_time:</p>
<pre><code>-POST /<span class="variable">$ES</span>/event_index

{
  <span class="string">"mappings"</span>: {
    <span class="string">"event"</span>: {
      <span class="string">"_ttl"</span>: {
        <span class="string">"enabled"</span>: false
      },
      <span class="string">"_timestamp"</span>: {
        <span class="string">"enabled"</span>: true,
        <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
      },
      <span class="string">"properties"</span>: {
        <span class="string">"user_id"</span>: {
          <span class="string">"type"</span>: <span class="string">"string"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>,
          <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>
        },
        <span class="string">"create_time"</span>: {
          <span class="string">"type"</span>: <span class="string">"date"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>,
          <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>,
          <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
        },
        <span class="string">"user_name"</span>: {
          <span class="string">"type"</span>: <span class="string">"string"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>
        }
      }
    }
  }
}
</code></pre><p>定义对应的查询模板，模板名字stats，使用了Cardinality和DateHistogram这两个Aggregation<br>，其中Date Histogram嵌套在Cardinality里。在定义模板的时候，{ { } } 的表示是个参数，需要调用模板的时候传递进来:</p>
<pre><code>  -POST /<span class="variable">$ES</span>/_search/template/stats
{
    <span class="string">"template"</span>: {
        <span class="string">"query"</span>: {
            <span class="string">"bool"</span>: {
                <span class="string">"must"</span>: [
                    {
                        <span class="string">"range"</span>: {
                            <span class="string">"create_time"</span>: {
                                <span class="string">"gte"</span>: <span class="string">""</span>,
                                <span class="string">"lte"</span>: <span class="string">""</span>
                            }
                        }
                    }
                ]
            }
        },
        <span class="string">"size"</span>: <span class="number">0</span>,
        <span class="string">"aggs"</span>: {
            <span class="string">"stats_data"</span>: {
                <span class="string">"date_histogram"</span>: {
                    <span class="string">"field"</span>: <span class="string">"create_time"</span>,
                    <span class="string">"interval"</span>: <span class="string">""</span>
                },
                <span class="string">"aggs"</span>: {
                    <span class="string">"time"</span>: {
                        <span class="string">"cardinality"</span>: {
                            <span class="string">"field"</span>: <span class="string">"user_id"</span>
                        }
                    }
                }
            }
        }
    }
}
</code></pre><p>Cardinality Aggregation的作用就是类似sql中的distinct，去重。</p>
<p>Date Histogram Aggregation的作用是根据时间进行统计。内部有个interval属性表面统计的范畴。</p>
<p>下面加几条数据到event_index里：</p>
<pre><code>-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 12:00:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"2"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format2"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:30:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"3"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format3"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:30:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:50:00"</span>
}

-POST <span class="variable">$ES</span>/event_index/event
{
    <span class="string">"user_id"</span>: <span class="string">"1"</span>,
    <span class="string">"user_name"</span>: <span class="string">"format1"</span>,
    <span class="string">"create_time"</span>: <span class="string">"2015-11-07 13:55:00"</span>
}
</code></pre><p>11-07 12-13点有1条数据，1个用户<br>11-07 13-14点有4条数据，3个用户</p>
<p>使用模板查询：</p>
<pre><code>curl -XGET <span class="string">"<span class="variable">$ES</span>/event_index/_search/template"</span> -d'{
  <span class="string">"template"</span>: { <span class="string">"id"</span>: <span class="string">"stats"</span> }, 
  <span class="string">"params"</span>: { <span class="string">"earliest"</span>: <span class="string">"2015-11-07 00:00:00"</span>, <span class="string">"latest"</span>: <span class="string">"2015-11-07 23:59:59"</span>, <span class="string">"interval"</span>: <span class="string">"hour"</span> }
}'    
</code></pre><p>结果：</p>
<pre><code>{
    "<span class="attribute">took</span>": <span class="value"><span class="number">3</span></span>,
    "<span class="attribute">timed_out</span>": <span class="value"><span class="literal">false</span></span>,
    "<span class="attribute">_shards</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">successful</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">failed</span>": <span class="value"><span class="number">0</span>
    </span>}</span>,
    "<span class="attribute">hits</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">max_score</span>": <span class="value"><span class="number">0</span></span>,
        "<span class="attribute">hits</span>": <span class="value">[]
    </span>}</span>,
    "<span class="attribute">aggregations</span>": <span class="value">{
        "<span class="attribute">stats_data</span>": <span class="value">{
            "<span class="attribute">buckets</span>": <span class="value">[
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 12:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446897600000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">1</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">1</span>
                    </span>}
                </span>},
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 13:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446901200000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">4</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">3</span>
                    </span>}
                </span>}
            ]
        </span>}
    </span>}
</span>}
</code></pre><p>12点-13点的只有1条数据，1个用户。13-14点的有4条数据，3个用户。</p>
<p>以天(day)统计：</p>
<pre><code>curl -XGET <span class="string">"<span class="variable">$ES</span>/event_index/_search/template"</span> -d'{
  <span class="string">"template"</span>: { <span class="string">"id"</span>: <span class="string">"stats"</span> }, 
  <span class="string">"params"</span>: { <span class="string">"earliest"</span>: <span class="string">"2015-11-07 00:00:00"</span>, <span class="string">"latest"</span>: <span class="string">"2015-11-07 23:59:59"</span>, <span class="string">"interval"</span>: <span class="string">"day"</span> }
}'    
</code></pre><p>结果：</p>
<pre><code>{
    "<span class="attribute">took</span>": <span class="value"><span class="number">4</span></span>,
    "<span class="attribute">timed_out</span>": <span class="value"><span class="literal">false</span></span>,
    "<span class="attribute">_shards</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">successful</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">failed</span>": <span class="value"><span class="number">0</span>
    </span>}</span>,
    "<span class="attribute">hits</span>": <span class="value">{
        "<span class="attribute">total</span>": <span class="value"><span class="number">5</span></span>,
        "<span class="attribute">max_score</span>": <span class="value"><span class="number">0</span></span>,
        "<span class="attribute">hits</span>": <span class="value">[]
    </span>}</span>,
    "<span class="attribute">aggregations</span>": <span class="value">{
        "<span class="attribute">stats_data</span>": <span class="value">{
            "<span class="attribute">buckets</span>": <span class="value">[
                {
                    "<span class="attribute">key_as_string</span>": <span class="value"><span class="string">"2015-11-07 00:00:00"</span></span>,
                    "<span class="attribute">key</span>": <span class="value"><span class="number">1446854400000</span></span>,
                    "<span class="attribute">doc_count</span>": <span class="value"><span class="number">5</span></span>,
                    "<span class="attribute">time</span>": <span class="value">{
                        "<span class="attribute">value</span>": <span class="value"><span class="number">3</span>
                    </span>}
                </span>}
            ]
        </span>}
    </span>}
</span>}
</code></pre><p>11-07这一天有5条数据，3个用户。</p>
<p>本文只是简单说明了es查询模板的使用，也简单使用了2个aggregation。更多内容可以去官网查看相关资料。</p>
]]></content>
    <summary type="html">
    <![CDATA[近在公司又用到了elasticsearch，也用到了查询模板，顺便写篇文章记录一下查询模板的使用 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="elasticsearch" scheme="http://fangjian0423.github.io/tags/elasticsearch/"/>
    
      <category term="elasticsearch" scheme="http://fangjian0423.github.io/categories/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[html页面左右滑动菜单效果的实现]]></title>
    <link href="http://fangjian0423.github.io/2015/10/29/html-left-right-menu/"/>
    <id>http://fangjian0423.github.io/2015/10/29/html-left-right-menu/</id>
    <published>2015-10-29T13:15:13.000Z</published>
    <updated>2015-12-20T17:06:35.000Z</updated>
    <content type="html"><![CDATA[<h2 id="正文">正文</h2><p>最近要实现一个微信端页面左边弹出菜单的实现，效果如下：</p>
<iframe src="//jsfiddle.net/format/t0xda6zv/embedded/result,js,html,css" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
<p>绿色部分是左菜单内容，高度填充满整个页面。且有滚动条，并且滚动条内容随着滚动条的滚动不会影响正文的滚动，正文内容的滚动不会影响左菜单的滚动。</p>
<p>下面说下自己实现这种效果的思路。</p>
<p>1.首先由于左右两边的滚动不影响双方，这就需要将菜单和内容的position设置为绝对定位，设置都需要滚动条: overflow: auto;  。</p>
<p>2.菜单的内容会覆盖正文的内容：所以菜单的z-index比正文要大。</p>
<p>3.给左边的菜单加点width动画，需要显示的时候设置width，需要隐藏的时候width设置为0即可。</p>
<p>如果不想把菜单的内容覆盖在正文内容上面，而是正文内容向右偏移菜单的宽度：</p>
<iframe src="//jsfiddle.net/format/of445qxn/embedded/result,js,html,css" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
<p>这个效果与上一个效果一样，唯一的区别就是正文的z-index比菜单大，而且正文需要配置一个背景色。最后切换菜单的时候正文内容加点向右便宜的动画即可。</p>
<p>刚换了next主题…. 发现这个主题右边的sidebar也是这样的效果实现  →_→ 。 囧 。</p>
<h2 id="基础小知识">基础小知识</h2><p>上面第二个例子中内容的z-index比菜单的z-index要大，而且正文需要配置一个背景色。为什么正文需要配置一个背景色呢？</p>
<p><strong>因为html中的元素没指定背景色的话，那说明这个元素的背景色是透明的</strong></p>
<p>比如下面这个效果，上面的内容没设置背景色，所以是透明的，虽然它的z-index比上面的块要大，但是还是显示了。</p>
<iframe src="//jsfiddle.net/format/yxseu05z/embedded/result,html" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
<p>给上面的内容设置黄色的背景色就可以隐藏下面的内容的。</p>
<iframe src="//jsfiddle.net/format/kec1up6t/embedded/result,html" width="100%" height="500" frameborder="0" allowfullscreen></iframe>
]]></content>
    <summary type="html">
    <![CDATA[最近要实现一个微信端页面左边弹出菜单的实现，效果如下 ...]]>
    
    </summary>
    
      <category term="css" scheme="http://fangjian0423.github.io/tags/css/"/>
    
      <category term="html" scheme="http://fangjian0423.github.io/tags/html/"/>
    
      <category term="css" scheme="http://fangjian0423.github.io/categories/css/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[最近前端的一点总结]]></title>
    <link href="http://fangjian0423.github.io/2015/10/24/front-end/"/>
    <id>http://fangjian0423.github.io/2015/10/24/front-end/</id>
    <published>2015-10-24T12:15:13.000Z</published>
    <updated>2015-12-20T17:01:03.000Z</updated>
    <content type="html"><![CDATA[<p>这段时间被拉去当做前端了，做了快1个多月了，也好久没写博客了。 做了两个项目的前端，这周是第二个项目的启动，而且这周简直是灾难的一周，基本上都是23点后下班的，有两天还是凌晨2点。悲剧。</p>
<p>做前端经验不是很多，这个月还是学到了一些前端的自己没掌握的知识。做个总结吧。</p>
<p>1.box-sizing属性。</p>
<p>box-sizing是css3引入的。有两个值，分别content-box和border-box。 默认为content-box。这个属性的作用是这样的：</p>
<p>比如我们定义一个div，宽度和高度都是50px。padding 5px， border: 5px。 那么这个div实际的宽度和高度是：</p>
<pre><code><span class="number">50</span> + <span class="number">2</span> * <span class="number">5</span> + <span class="number">2</span> * <span class="number">5</span> = <span class="number">70</span>。
</code></pre><p>如果使用border-box的话，那么这个div的宽度还是50。 因为border-box会把padding和border都一起算到宽度和高度里面。所以使用border-box后div的高度和宽度为还是50。但是实际上真正显示内容的高度和宽度是 50 - 5 <em> 2 - 5 </em> 2 = 30。</p>
<p>直接来点实际的代码，使用content-box，也就是默认情况：</p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"background-color: blue; width: 100px; height: 100px;"</span>&gt;
    &lt;<span class="keyword">div</span> style=<span class="string">"background-color: yellow; width: 50px; height: 50px; padding: 5px; border: 5px solid red; box-sizing: content-box;"</span>&gt;&lt;/<span class="keyword">div</span>&gt;
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/content-box1.png" alt=""><br><img src="http://7x2wh6.com1.z0.glb.clouddn.com/content-box2.png" alt=""></p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"background-color: blue; width: 100px; height: 100px;"</span>&gt;
    &lt;<span class="keyword">div</span> style=<span class="string">"background-color: yellow; width: 50px; height: 50px; padding: 5px; border: 5px solid red; box-sizing: border-box;"</span>&gt;&lt;/<span class="keyword">div</span>&gt;
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png" alt=""><br><img src="http://7x2wh6.com1.z0.glb.clouddn.com/border-box2.png" alt=""></p>
<p>Bootstrap3中也大量使用了border-box。</p>
<p>2.white-space属性。</p>
<p>使用white-space是为了让多张图片在同一行展示，不会换行。</p>
<pre><code>&lt;<span class="tag">div</span> style=<span class="string">"width: 200px;"</span>&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
    &lt;<span class="tag">img</span> src=<span class="string">"http://7x2wh6.com1.z0.glb.clouddn.com/border-box1.png"</span> <span class="attribute">width</span>=<span class="string">"50px"</span>/&gt;
&lt;/div&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/white-space1.png" alt=""></p>
<p>将外层的div改为：</p>
<pre><code>&lt;<span class="tag">div</span> style=<span class="string">"width: 200px; white-space: nowrap; overflow: scroll;"</span>&gt;
</code></pre><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/white-space2.png" alt=""></p>
<p>white-space设置为nowrap后，文本内容不会换行直到遇到br标签为止。</p>
<p>3.CSS3的动画。</p>
<p>一开始没有使用CSS3的动画，使用了JQuery的animate，结果发现页面动画有点卡，后来改成了CSS3的动画。</p>
<p>因为都是一些比较简单的动画，所以只能写下简单的动画属性了。</p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"width: 50px; height: 50px; background-color: yellow; transition: width .2s;"</span>&gt;
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p>div的点击事件：</p>
<pre><code>$(<span class="string">"div"</span>).click(<span class="keyword">function</span>() {
    var <span class="variable">$div</span> = $(this);
    if(<span class="variable">$div</span>.width() == <span class="number">50</span>) {
      <span class="variable">$div</span>.width(<span class="string">"100"</span>);  
    } else {
      <span class="variable">$div</span>.width(<span class="string">"50"</span>);  
    }
});
</code></pre><p>动画还有延迟效果，这里就不举例了。</p>
<p>4.垂直居中</p>
<p>高度固定的垂直居中，设置line-height为容器高度，text-align为center即可：</p>
<pre><code>&lt;<span class="keyword">div</span> style=<span class="string">"width: 100px; height: 100px; background-color: yellow; line-height: 100px; text-align: center;"</span>&gt;
    我居中了
&lt;/<span class="keyword">div</span>&gt;
</code></pre><p>高度不固定的垂直居中：</p>
<pre><code>html, body {
  height: 100%;
}
body {
  display: -webkit-box;
  display: -webkit-flex;

  display: -moz-box;
  display: -moz-flex;

  display: -ms-flexbox;

  display: flex;
<span class="comment">
  /* 水平居中*/</span>
  -<span class="ruby">webkit-box-<span class="symbol">align:</span> center;
</span>  -<span class="ruby">moz-box-<span class="symbol">align:</span> center;
</span>  -<span class="ruby">ms-flex-<span class="symbol">pack:</span>center;<span class="regexp">/* IE 10 */</span>
</span>
  -<span class="ruby">webkit-justify-<span class="symbol">content:</span> center;
</span>  -<span class="ruby">moz-justify-<span class="symbol">content:</span> center;
</span>  justify-content: center;/* IE 11+,Firefox 22+,Chrome 29+,Opera 12.1*/
<span class="comment">
  /* 垂直居中 */</span>
  -<span class="ruby">webkit-box-<span class="symbol">pack:</span> center;
</span>  -<span class="ruby">moz-box-<span class="symbol">pack:</span> center;
</span>  -<span class="ruby">ms-flex-<span class="symbol">align:</span>center;<span class="regexp">/* IE 10 */</span>
</span>
  -<span class="ruby">webkit-align-<span class="symbol">items:</span> center;
</span>  -<span class="ruby">moz-align-<span class="symbol">items:</span> center;
</span>  align-items: center;
}

...
&lt;body&gt;
    垂直居中
&lt;/body&gt;
...
</code></pre><p>还有其他的一些比如绝对定位，固定定位，相对定位，overflow等问题就不一一举例了。 估计之后还是得做前端的一些工作，到时候用到了一些新内容的话还会继续更新前端相关的博客的。</p>
]]></content>
    <summary type="html">
    <![CDATA[这段时间被拉去当做前端了，做了快1个多月了，也好久没写博客了。 做了两个项目的前端，这周是第二个项目的启动 ...]]>
    
    </summary>
    
      <category term="css" scheme="http://fangjian0423.github.io/tags/css/"/>
    
      <category term="html" scheme="http://fangjian0423.github.io/tags/html/"/>
    
      <category term="css" scheme="http://fangjian0423.github.io/categories/css/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala的Predef介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/10/07/scala-predef/"/>
    <id>http://fangjian0423.github.io/2015/10/07/scala-predef/</id>
    <published>2015-10-06T16:37:20.000Z</published>
    <updated>2015-12-20T17:03:03.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>Scala每个程序都会自动import以下3个包，分别是 java.lang.<em>  ,  scala.</em> 和 Predef._ 。跟Java程序都会自动import java.lang一样。</p>
<p>Predef是一个单例对象，所以我们import进来之后，可以直接使用Predef中定义的方法。</p>
<h2 id="Predef中定义的方法和属性">Predef中定义的方法和属性</h2><h3 id="常用方法和类">常用方法和类</h3><pre><code><span class="function"><span class="keyword">def</span> <span class="title">classOf</span>[</span><span class="type">T</span>]: <span class="type">Class</span>[<span class="type">T</span>] = <span class="literal">null</span> <span class="comment">// This is a stub method. The actual implementation is filled in by the compiler.</span>

<span class="class"><span class="keyword">type</span> <span class="title">String</span>        =</span> java.lang.<span class="type">String</span>  <span class="comment">// scala中的String使用jdk中的String</span>
<span class="class"><span class="keyword">type</span> <span class="title">Class</span>[</span><span class="type">T</span>]      = java.lang.<span class="type">Class</span>[<span class="type">T</span>] <span class="comment">// scala中的Class使用jdk中的Class</span>

<span class="class"><span class="keyword">type</span> <span class="title">Function</span>[</span>-<span class="type">A</span>, +<span class="type">B</span>] = <span class="type">Function1</span>[<span class="type">A</span>, <span class="type">B</span>] <span class="comment">// Function1取别名Function</span>

<span class="class"><span class="keyword">type</span> <span class="title">Map</span>[</span><span class="type">A</span>, +<span class="type">B</span>] = immutable.<span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>] <span class="comment">// Map类型默认使用immutable包下的Map</span>
<span class="class"><span class="keyword">type</span> <span class="title">Set</span>[</span><span class="type">A</span>]     = immutable.<span class="type">Set</span>[<span class="type">A</span>] <span class="comment">// Set类型默认使用immutable包下的Set</span>
<span class="keyword">val</span> <span class="type">Map</span>         = immutable.<span class="type">Map</span> <span class="comment">// Map对象默认使用immutable包下的Map对象(下面测试用到)</span>
<span class="keyword">val</span> <span class="type">Set</span>         = immutable.<span class="type">Set</span> <span class="comment">// Set对象默认使用immutable包下的Set对象(下面测试用到)</span>
</code></pre><p>一些测试：</p>
<pre><code>// 默认的Map和<span class="operator"><span class="keyword">Set</span>都是immutable包下的，这里的<span class="keyword">Set</span>和<span class="keyword">Map</span>都是在Predef中定义的一个变量。
<span class="keyword">Map</span>(<span class="number">1</span> -&gt; <span class="number">1</span>, <span class="number">2</span> -&gt; <span class="number">2</span>) // scala.collection.immutable.<span class="keyword">Map</span>[<span class="built_in">Int</span>,<span class="built_in">Int</span>] = <span class="keyword">Map</span>(<span class="number">1</span> -&gt; <span class="number">1</span>, <span class="number">2</span> -&gt; <span class="number">2</span>)
<span class="keyword">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>) // scala.collection.immutable.<span class="keyword">Set</span>[<span class="built_in">Int</span>] = <span class="keyword">Set</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span>
</code></pre><h3 id="打印方法">打印方法</h3><pre><code>def <span class="function"><span class="title">print</span><span class="params">(x: Any)</span></span> = Console.<span class="function"><span class="title">print</span><span class="params">(x)</span></span>
def <span class="function"><span class="title">println</span><span class="params">()</span></span> = Console.<span class="function"><span class="title">println</span><span class="params">()</span></span>
def <span class="function"><span class="title">println</span><span class="params">(x: Any)</span></span> = Console.<span class="function"><span class="title">println</span><span class="params">(x)</span></span>
def <span class="function"><span class="title">printf</span><span class="params">(text: String, xs: Any*)</span></span> = Console.<span class="function"><span class="title">print</span><span class="params">(text.format(xs: _*)</span></span>)
</code></pre><p>因此，平时我们使用的println，print，printf这些打印方法都是在Predef中定义的，</p>
<h3 id="一些调试和错误方法">一些调试和错误方法</h3><pre><code><span class="comment">// 过期方法，抛出带有message消息的RuntimeException</span>
<span class="annotation">@deprecated</span>(<span class="string">"Use `sys.error(message)` instead"</span>, <span class="string">"2.9.0"</span>)
<span class="keyword">def</span> error(<span class="string">message:</span> String): Nothing = sys.error(message)

<span class="comment">// 断言。 参数是一个Boolean类型，失败抛出java.lang.AssertionError异常</span>
<span class="annotation">@elidable</span>(ASSERTION)
<span class="keyword">def</span> <span class="keyword">assert</span>(<span class="string">assertion:</span> Boolean) {
<span class="keyword">if</span> (!assertion)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assertion failed"</span>)
}

<span class="comment">// 跟上一个方法类似，多了一个message参数。抛出的异常就打印出这个message参数</span>
<span class="annotation">@elidable</span>(ASSERTION) <span class="annotation">@inline</span>
<span class="keyword">final</span> <span class="keyword">def</span> <span class="keyword">assert</span>(<span class="string">assertion:</span> Boolean, <span class="string">message:</span> =&gt; Any) {
<span class="keyword">if</span> (!assertion)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assertion failed: "</span>+ message)
}

<span class="comment">// 跟assert类似，唯一的区别是assume支持静态经验(static checker)</span>
<span class="annotation">@elidable</span>(ASSERTION)
<span class="keyword">def</span> assume(<span class="string">assumption:</span> Boolean) {
<span class="keyword">if</span> (!assumption)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assumption failed"</span>)
}

<span class="comment">// 跟assume一样，多了个message参数</span>
<span class="annotation">@elidable</span>(ASSERTION) <span class="annotation">@inline</span>
<span class="keyword">final</span> <span class="keyword">def</span> assume(<span class="string">assumption:</span> Boolean, <span class="string">message:</span> =&gt; Any) {
<span class="keyword">if</span> (!assumption)
  <span class="keyword">throw</span> <span class="keyword">new</span> java.lang.AssertionError(<span class="string">"assumption failed: "</span>+ message)
}

<span class="comment">// 跟assert类似，只不过抛出的是IllegalArgumentException异常</span>
<span class="keyword">def</span> require(<span class="string">requirement:</span> Boolean) {
<span class="keyword">if</span> (!requirement)
  <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"requirement failed"</span>)
}

<span class="comment">// 多个参数，作用如上一样</span>
<span class="annotation">@inline</span> <span class="keyword">final</span> <span class="keyword">def</span> require(<span class="string">requirement:</span> Boolean, <span class="string">message:</span> =&gt; Any) {
<span class="keyword">if</span> (!requirement)
  <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"requirement failed: "</span>+ message)
}
</code></pre><p>调试和错误方法测试：</p>
<pre><code><span class="function"><span class="title">assert</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>)</span></span> <span class="comment">// java.lang.AssertionError: assertion failed</span>
<span class="function"><span class="title">assert</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>, <span class="string">"test"</span>)</span></span> <span class="comment">// java.lang.AssertionError: assertion failed: test</span>
<span class="function"><span class="title">assume</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>)</span></span> <span class="comment">// java.lang.AssertionError: assumption failed</span>
<span class="function"><span class="title">assume</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>, <span class="string">"test"</span>)</span></span> <span class="comment">// java.lang.AssertionError: assumption failed: test</span>
<span class="function"><span class="title">require</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>)</span></span> <span class="comment">// java.lang.IllegalArgumentException: requirement failed</span>
<span class="function"><span class="title">require</span><span class="params">(<span class="number">1</span> == <span class="number">2</span>, <span class="string">"test"</span>)</span></span> <span class="comment">// java.lang.IllegalArgumentException: requirement failed: test</span>
</code></pre><h3 id="一个特殊的属性">一个特殊的属性</h3><p>Predef中有个 ??? 属性，抛出一个NotImplementedError：</p>
<pre><code><span class="function"><span class="keyword">def</span> ??? :</span> Nothing = throw new NotImplementedError
</code></pre><p>比如定义一些方法的时候，这个方法还没有实现，这个时候可以使用 ???， 而非TODO：</p>
<pre><code><span class="keyword">def</span> todoMethod(x: <span class="keyword">Int</span>): <span class="keyword">Int</span> = ???

todoMethod(<span class="number">2</span>) <span class="comment">// scala.NotImplementedError: an implementation is missing</span>
</code></pre><h2 id="Predef还有大量的隐式转换和隐式转换类">Predef还有大量的隐式转换和隐式转换类</h2><p>再讲Predef中的隐式转换和隐式转换类之前，先介绍一下这2个概念。</p>
<h3 id="隐式转换">隐式转换</h3><p>隐式转换的意思是一个方法中有一个类型的参数，并返回另外一个类型的返回值。比如一个Double类型的方法返回一个Int类型的返回值。</p>
<pre><code>def <span class="function"><span class="title">double2Int</span><span class="params">(d: Double)</span></span> = d<span class="class">.toInt</span>

<span class="function"><span class="title">double2Int</span><span class="params">(<span class="number">2.4</span>)</span></span> <span class="comment">// 2</span>

val <span class="tag">a</span>: Int = <span class="number">2.3</span> <span class="comment">// 报错</span>
</code></pre><p>重新定义double2Int，使其支持隐式转换：</p>
<pre><code><span class="type">implicit</span> def double2Int(d: <span class="type">Double</span>) = d.toInt

val a: <span class="built_in">Int</span> = <span class="number">2.3</span> // a: <span class="built_in">Int</span> = <span class="number">2</span>
</code></pre><h3 id="隐式转换类">隐式转换类</h3><p>当需要给Int类型添加一个 –&gt; 方法的时候，需要使用到隐式转换类。因为隐式转换只支持1个参数，所以只能通过隐式转换类完成。</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMaker(left: <span class="built_in">Int</span>) {
    def --&gt;(right: <span class="built_in">Int</span>) = left to right
}

val <span class="built_in">range</span> = <span class="number">1</span> --&gt; <span class="number">10</span> // <span class="built_in">Range</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)
</code></pre><p>隐式转换类有个弊端，那就是每次都会创建一个类的实例，有时候这完全没有必要。</p>
<p>scala提供了一个将隐式转换类转换成value class的方法，只需要继承AnyVal即可，还需要注意参数需要加上val标识符。</p>
<pre><code><span class="type">implicit</span> <span class="keyword">class</span> RangeMaker(val left: <span class="built_in">Int</span>) <span class="keyword">extends</span> AnyVal {
    def --&gt;(right: <span class="built_in">Int</span>) = left to right
}
</code></pre><h3 id="Predef中的隐式转换和隐式转换类">Predef中的隐式转换和隐式转换类</h3><p>-&gt; 方法：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> ArrowAssoc[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    @inline def -&gt; [B](y: B): Tuple2[A, B] = Tuple2(self, y)
    def →[B](y: B): Tuple2[A, B] = -&gt;(y)
}
</code></pre><p>生成一个二元元组，Tuple2。</p>
<pre><code><span class="number">1</span> → <span class="number">2</span>  <span class="comment">// (Int, Int) = (1, 2)</span>
<span class="number">1</span> -&gt; <span class="number">2</span> <span class="comment">// (Int, Int) = (1, 2)</span>
</code></pre><p>ensuring 方法：</p>
<pre><code>implicit final <span class="class"><span class="keyword">class</span> <span class="title">Ensuring</span>[<span class="title">A</span>](<span class="title">private</span> <span class="title">val</span> <span class="title">self</span>: <span class="title">A</span>) <span class="title">extends</span> <span class="title">AnyVal</span> {</span>
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">Boolean</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond)</span>;</span> <span class="keyword">self</span> }
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">Boolean</span>, <span class="symbol">msg:</span> =&gt; <span class="constant">Any</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond, msg)</span>;</span> <span class="keyword">self</span> }
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">A</span> =&gt; <span class="constant">Boolean</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond(<span class="keyword">self</span>)</span>);</span> <span class="keyword">self</span> }
    <span class="function"><span class="keyword">def</span> <span class="title">ensuring</span><span class="params">(<span class="symbol">cond:</span> <span class="constant">A</span> =&gt; <span class="constant">Boolean</span>, <span class="symbol">msg:</span> =&gt; <span class="constant">Any</span>)</span>: <span class="title">A</span> = { <span class="title">assert</span><span class="params">(cond(<span class="keyword">self</span>)</span>, <span class="title">msg</span>);</span> <span class="keyword">self</span> }
}
</code></pre><p>ensuring内部调用assert方法。</p>
<pre><code><span class="function"><span class="keyword">def</span> <span class="title">doublePositive</span><span class="params">(n: Int)</span>:</span> Int = {
    n * <span class="number">2</span>
} ensuring(n =&gt; n &gt;= <span class="number">0</span> &amp;&amp; n % <span class="number">2</span> == <span class="number">0</span>)

doublelPositive(<span class="number">1</span>) // <span class="number">2</span>
doublelPositive(-<span class="number">1</span>) // java.lang.AssertionError: assertion failed
</code></pre><p>formatted 方法：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> StringFormat[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    @inline def <span class="keyword">formatted</span>(fmtstr: String): String = fmtstr <span class="keyword">format</span> self
}
</code></pre><p>格式化字符串，使用java.lang.String.format方法。</p>
<pre><code><span class="string">"Format"</span> formatted <span class="string">"%s, let's go"</span> <span class="comment">// Format, let's go</span>
</code></pre><p>+ 方法：</p>
<pre><code><span class="type">implicit</span> <span class="keyword">final</span> <span class="keyword">class</span> any2stringadd[A](<span class="keyword">private</span> val self: A) <span class="keyword">extends</span> AnyVal {
    def +(other: String): String = String.valueOf(self) + other
}
</code></pre><p>case class使用+方法：</p>
<pre><code>case class <span class="function"><span class="title">Student</span><span class="params">(name: String, age: Int)</span></span>
<span class="function"><span class="title">Student</span><span class="params">(<span class="string">"format"</span>, <span class="number">11</span>)</span></span> + <span class="string">" 22"</span> <span class="comment">// Student(format,11) 22</span>
</code></pre><p>StringOps类：</p>
<pre><code>@inline <span class="type">implicit</span> def augmentString(x: String): StringOps = new StringOps(x)
@inline <span class="type">implicit</span> def unaugmentString(x: StringOps): String = x.repr
</code></pre><p>StringOps提供了丰富的原生String没提供的方法：</p>
<pre><code><span class="string">"format"</span>.length <span class="comment">// 6 原生String是没有提供length方法的</span>
<span class="string">"format"</span>.foreach(println(_))
<span class="string">"format"</span>.stripPrefix(<span class="string">"for"</span>) <span class="comment">// mat</span>
<span class="string">"format"</span>.slice(<span class="number">1</span>, <span class="number">3</span>) <span class="comment">// or</span>
<span class="string">"format"</span> * <span class="number">5</span> <span class="comment">// formatformatformatformatformat</span>
<span class="string">"true"</span>.toBoolean <span class="comment">// true</span>
</code></pre><p>ArrayOps类：</p>
<pre><code><span class="type">implicit</span> def genericArrayOps[T](xs: Array[T]): ArrayOps[T] = (xs match {
    <span class="keyword">case</span> x: Array[AnyRef]  =&gt; refArrayOps[AnyRef](x)
    <span class="keyword">case</span> x: Array[Boolean] =&gt; booleanArrayOps(x)
    <span class="keyword">case</span> x: Array[Byte]    =&gt; byteArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="built_in">Char</span>]    =&gt; charArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="type">Double</span>]  =&gt; doubleArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="built_in">Float</span>]   =&gt; floatArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="built_in">Int</span>]     =&gt; intArrayOps(x)
    <span class="keyword">case</span> x: Array[Long]    =&gt; longArrayOps(x)
    <span class="keyword">case</span> x: Array[Short]   =&gt; shortArrayOps(x)
    <span class="keyword">case</span> x: Array[<span class="keyword">Unit</span>]    =&gt; unitArrayOps(x)
    <span class="keyword">case</span> null              =&gt; null
}).asInstanceOf[ArrayOps[T]]


  <span class="type">implicit</span> def booleanArrayOps(xs: Array[Boolean]): ArrayOps[Boolean] = new ArrayOps.ofBoolean(xs)
  <span class="type">implicit</span> def byteArrayOps(xs: Array[Byte]): ArrayOps[Byte]          = new ArrayOps.ofByte(xs)
  <span class="type">implicit</span> def charArrayOps(xs: Array[<span class="built_in">Char</span>]): ArrayOps[<span class="built_in">Char</span>]          = new ArrayOps.ofChar(xs)
  <span class="type">implicit</span> def doubleArrayOps(xs: Array[<span class="type">Double</span>]): ArrayOps[<span class="type">Double</span>]    = new ArrayOps.ofDouble(xs)
  <span class="type">implicit</span> def floatArrayOps(xs: Array[<span class="built_in">Float</span>]): ArrayOps[<span class="built_in">Float</span>]       = new ArrayOps.ofFloat(xs)
  <span class="type">implicit</span> def intArrayOps(xs: Array[<span class="built_in">Int</span>]): ArrayOps[<span class="built_in">Int</span>]             = new ArrayOps.ofInt(xs)
  <span class="type">implicit</span> def longArrayOps(xs: Array[Long]): ArrayOps[Long]          = new ArrayOps.ofLong(xs)
  <span class="type">implicit</span> def refArrayOps[T &lt;: AnyRef](xs: Array[T]): ArrayOps[T]    = new ArrayOps.ofRef[T](xs)
  <span class="type">implicit</span> def shortArrayOps(xs: Array[Short]): ArrayOps[Short]       = new ArrayOps.ofShort(xs)
  <span class="type">implicit</span> def unitArrayOps(xs: Array[<span class="keyword">Unit</span>]): ArrayOps[<span class="keyword">Unit</span>]          = new ArrayOps.ofUnit(xs)
</code></pre><p>ArrayOps提供了丰富的原生数组没提供的方法：</p>
<pre><code>Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>) :+ <span class="number">4</span> <span class="comment">// Array(1, 2, 3, 4)</span>
</code></pre><p>Scala程序员可以较少关心装箱和拆箱操作，这也是由于Predef对象里定义了Scala值类型与java基本类型直接的隐式转换：</p>
<pre><code><span class="type">implicit</span> def byte2Byte(x: Byte)           = java.lang.Byte.valueOf(x)
<span class="type">implicit</span> def short2Short(x: Short)        = java.lang.Short.valueOf(x)
<span class="type">implicit</span> def char2Character(x: <span class="built_in">Char</span>)      = java.lang.<span class="type">Character</span>.valueOf(x)
<span class="type">implicit</span> def int2Integer(x: <span class="built_in">Int</span>)          = java.lang.<span class="type">Integer</span>.valueOf(x)
<span class="type">implicit</span> def long2Long(x: Long)           = java.lang.Long.valueOf(x)
<span class="type">implicit</span> def float2Float(x: <span class="built_in">Float</span>)        = java.lang.<span class="built_in">Float</span>.valueOf(x)
<span class="type">implicit</span> def double2Double(x: <span class="type">Double</span>)     = java.lang.<span class="type">Double</span>.valueOf(x)
<span class="type">implicit</span> def boolean2Boolean(x: Boolean)  = java.lang.Boolean.valueOf(x)

<span class="type">implicit</span> def Byte2byte(x: java.lang.Byte): Byte             = x.byteValue
<span class="type">implicit</span> def Short2short(x: java.lang.Short): Short         = x.shortValue
<span class="type">implicit</span> def Character2char(x: java.lang.<span class="type">Character</span>): <span class="built_in">Char</span>   = x.charValue
<span class="type">implicit</span> def Integer2int(x: java.lang.<span class="type">Integer</span>): <span class="built_in">Int</span>         = x.intValue
<span class="type">implicit</span> def Long2long(x: java.lang.Long): Long             = x.longValue
<span class="type">implicit</span> def Float2float(x: java.lang.<span class="built_in">Float</span>): <span class="built_in">Float</span>         = x.floatValue
<span class="type">implicit</span> def Double2double(x: java.lang.<span class="type">Double</span>): <span class="type">Double</span>     = x.doubleValue
<span class="type">implicit</span> def Boolean2boolean(x: java.lang.Boolean): Boolean = x.booleanValue
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Scala每个程序都会自动import以下3个包，分别是 java.lang._  ,  scala._ 和 Predef._ 。跟Java程序都会自动import java.lang一样 ...]]>
    
    </summary>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/tags/jvm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spring Boot 部分特性记录]]></title>
    <link href="http://fangjian0423.github.io/2015/09/21/springboot-intro/"/>
    <id>http://fangjian0423.github.io/2015/09/21/springboot-intro/</id>
    <published>2015-09-20T16:21:49.000Z</published>
    <updated>2015-12-20T17:07:24.000Z</updated>
    <content type="html"><![CDATA[<p>SpringBoot是Java的一个micro-service框架。它设计的目的是简化Spring应用的初始搭建以及开发过程。使用SpringBoot可以避免大量的xml配置文件，它内部使用很多约定的方式。</p>
<p>以一个最简单的MVC例子来说，使用SpringBoot进行开发的话定义好对应的Controller，Repository和Entity之后，加上各自的Annotation即可。</p>
<p>Repository框架可以选择Spring Data或者Hibernate，可通过自由配置。</p>
<p>视图框架也可通过配置选择freemarker或者velocity等视图框架。</p>
<p>下面，介绍一下SpringBoot的一些功能。</p>
<h2 id="SpringBoot框架的启动">SpringBoot框架的启动</h2><p>SpringBoot使用SpringApplication这个类进行项目的启动。一般都会这么写：</p>
<pre><code><span class="annotation">@SpringBootApplication</span>
<span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> {</span>
    <span class="keyword">public</span> <span class="keyword">static</span> <span class="typename">void</span> main(String[] args) {
        SpringApplication.run(Application.<span class="keyword">class</span>, args);
    }
}
</code></pre><p>使用SpringBootApplication注解相当于使用了3个注解，分别是@ComponentScan，@Configuration，@EnableAutoConfiguration。</p>
<p>这里需要注意的是Application这个类所有的package位置。</p>
<p>比如你的项目的包路径是 me.format.project1，对应的controller和repository包是 me.format.project1.controller和me.format.project1.repository。 那么这个Application需要在的包路径为 me.format.project1。 因为SpringBootApplication注解内部是使用ComponentScan注解，这个注解会扫描Application包所在的路径下的各个bean。</p>
<h2 id="Profile的使用">Profile的使用</h2><p>可以在springboot项目中加入配置文件application.yml。</p>
<p>yaml中可以定义多个profile，也可以指定激活的profile：</p>
<pre><code><span class="attribute">spring</span>:
  profiles.<span class="attribute">active</span>: dev

---
<span class="attribute">spring</span>:
  <span class="attribute">profiles</span>: dev
<span class="attribute">myconfig</span>:
  <span class="attribute">config1</span>: dev-enviroment

---
<span class="attribute">spring</span>:
  <span class="attribute">profiles</span>: test
<span class="attribute">myconfig</span>:
  <span class="attribute">config1</span>: test-enviroment

---
<span class="attribute">spring</span>:
  <span class="attribute">profiles</span>: prod
<span class="attribute">myconfig</span>:
  <span class="attribute">config1</span>: prod-envioment
</code></pre><p>也可以在运行的执行指定profile：</p>
<pre><code>java -Dspring<span class="class">.profiles</span><span class="class">.active</span>=<span class="string">"prod"</span> -jar yourjar.jar
</code></pre><p>还可以使用Profile注解，MyConfig只会在prod这个profile下才会生效，其他profile不会生效：</p>
<pre><code><span class="variable">@Profile</span>(<span class="string">"prod"</span>)
<span class="variable">@Component</span>
public class MyConfig {
    ....
}
</code></pre><h2 id="自定义的一些Conveter，Interceptor">自定义的一些Conveter，Interceptor</h2><p>如果想配置springmvc的HandlerInterceptorAdapter或者HttpMessageConverter。 只需要定义自己的interceptor或者converter，然后加上Component注解。这样SpringBoot会自动处理这些类，不用自己在配置文件里指定对应的内容。 这个也是相当方便的。</p>
<pre><code><span class="annotation">@Component</span>
public <span class="class"><span class="keyword">class</span> <span class="title">AuthInterceptor</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">HandlerInterceptorAdapter</span> {</span>
    ...
}

<span class="annotation">@Component</span>
public <span class="class"><span class="keyword">class</span> <span class="title">MyConverter</span> <span class="title">implements</span> <span class="title">HttpMessageConverter&lt;MyObj&gt;</span> {</span> 
    ...
}
</code></pre><h2 id="模板的使用">模板的使用</h2><p>比如使用freemarker的时候，加入以下依赖：</p>
<pre><code><span class="tag">&lt;<span class="title">dependency</span>&gt;</span>
    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span>
    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spring-boot-starter-freemarker<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span>
<span class="tag">&lt;/<span class="title">dependency</span>&gt;</span>
</code></pre><p>然后在resources目录下建立一个templates目录即可，视图将会从这个templates位置开始找。</p>
<h2 id="其他">其他</h2><p>关于其他的特性可以参考官方文档：</p>
<p><a href="http://docs.spring.io/spring-boot/docs/current/reference/html/" target="_blank" rel="external">http://docs.spring.io/spring-boot/docs/current/reference/html/</a></p>
<p>springboot还提供了一系列sample供参考：</p>
<p><a href="https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples" target="_blank" rel="external">https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples</a></p>
]]></content>
    <summary type="html">
    <![CDATA[SpringBoot是Java的一个micro-service框架。它设计的目的是简化Spring应用的初始搭建以及开发过程。使用SpringBoot可以避免大量的xml配置文件，它内部使用很多约定的方式 ...]]>
    
    </summary>
    
      <category term="microservice" scheme="http://fangjian0423.github.io/tags/microservice/"/>
    
      <category term="springboot" scheme="http://fangjian0423.github.io/tags/springboot/"/>
    
      <category term="springboot" scheme="http://fangjian0423.github.io/categories/springboot/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala持久层框架Slick介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/08/18/slick-intro/"/>
    <id>http://fangjian0423.github.io/2015/08/18/slick-intro/</id>
    <published>2015-08-18T15:22:33.000Z</published>
    <updated>2015-12-20T17:05:53.000Z</updated>
    <content type="html"><![CDATA[<h2 id="FRM介绍">FRM介绍</h2><p>最近看到了一个FRM的框架Slick。 FRM的意思是Functional Relational Mapping， 一种基于函数式的ORM。</p>
<p>举一个最简单的例子：</p>
<pre><code>val queryResult = db.query(queryStr)

queryResult.onSuccess { <span class="literal">result</span> =&gt;
    <span class="literal">result</span>.doSomething ...
}
</code></pre><p>数据库db查询一条sql语句。查询成功的时候使用闭包完成处理。 看到这段代码的第一反应就是js的ajax处理，代码几乎是一样的，也发现之前在学校里写nodejs的时候查询db也是这样的语法。</p>
<pre><code><span class="keyword">var</span> jqxhr = $.ajax( {
    url: 'url',
    <span class="keyword">method</span>: '<span class="type">GET</span>',
    data: [user: 'format']
});

jqxhr.success(function() {
    ...
});
</code></pre><p>FRM相比ORM最明显的优势就是FRM基于多线程的Future的数据查询，而ORM是单线程的线性执行。</p>
<p>FRM构造sql查询也是相当简单的：</p>
<pre><code><span class="comment">// 构造查询</span>
val newQuery = students.<span class="function"><span class="title">filter</span><span class="params">(_.age &gt; <span class="number">24</span>)</span></span>.<span class="function"><span class="title">sortBy</span><span class="params">(_.name)</span></span>
<span class="comment">// 执行查询</span>
db.<span class="function"><span class="title">run</span><span class="params">(newQuery)</span></span>
</code></pre><p>FRM其他的优势可以参考<a href="http://slick.typesafe.com/doc/3.0.1/introduction.html#functional-relational-mapping" target="_blank" rel="external">官方文档</a>。</p>
<h2 id="Slick实例">Slick实例</h2><p>下面以一个Students和Classrooms的实例来说明一下Slick的使用。</p>
<p>首先是创建对应的domain，学生与教室的关系是1对多。</p>
<p>Students domain(使用Option类型说明该列是可为空的)：</p>
<pre><code>class Student(tag: Tag) extends Table[<span class="link_label">(Int, String, Int, Int, Option[Date</span>])](tag, "Students") {

  def id: Rep[<span class="link_label">Int</span>] = column[<span class="link_label">Int</span>](<span class="link_url">"id", O.PrimaryKey, O.AutoInc</span>)
  def name: Rep[<span class="link_label">String</span>] = column[<span class="link_label">String</span>](<span class="link_url">"name"</span>)
  def age: Rep[<span class="link_label">Int</span>] = column[<span class="link_label">Int</span>](<span class="link_url">"age"</span>)
  def birthDate: Rep[<span class="link_label">Option[Date</span>]] = column[<span class="link_label">Option[Date</span>]]("birth_date")
  def classroomId = column[<span class="link_label">Int</span>](<span class="link_url">"classroom_id"</span>)

   def * : ProvenShape[(Int, String, Int, Int, Option[Date])] = (id, name, age, classroomId, birthDate)

  def classroom: ForeignKeyQuery[Classroom, (Int, String)] = foreignKey("FK<span class="emphasis">_CLASSROOM", classroomId, TableQuery[Classroom])(_</span>.id)
}
</code></pre><p>Classrooms domain：</p>
<pre><code>class Classroom(tag: Tag) extends Table[<span class="link_label">(Int, String)</span>](<span class="link_url">tag, "Classrooms"</span>) {
  def id = column[<span class="link_label">Int</span>](<span class="link_url">"id", O.PrimaryKey, O.AutoInc</span>)
  def name = column[<span class="link_label">String</span>](<span class="link_url">"name"</span>)

  def * = (id, name)
}
</code></pre><p>各个db操作，schema创建，sql插入，sql查询等操作如下，加了几句备注，具体的代码就不分析了：</p>
<pre><code><span class="keyword">object</span> <span class="type">SampleSlickDemo</span> extends <span class="type">App</span> {

  val db = <span class="type">Database</span>.forConfig(<span class="string">"h2mem1"</span>)

  <span class="keyword">try</span> {

    val classrooms = <span class="type">TableQuery</span>[<span class="type">Classroom</span>]
    val students = <span class="type">TableQuery</span>[<span class="type">Student</span>]


    val setupAction: <span class="type">DBIO</span>[<span class="type">Unit</span>] = <span class="type">DBIO</span>.<span class="type">seq</span>(
      // create student <span class="keyword">and</span> classroom table <span class="keyword">in</span> database
      (classrooms.schema ++ students.schema).create,
      // insert some rows <span class="keyword">in</span> classroom
      classrooms += (<span class="number">1</span>, <span class="string">"classroom1"</span>),
      classrooms += (<span class="number">2</span>, <span class="string">"classroom2"</span>),
      classrooms += (<span class="number">3</span>, <span class="string">"classroom2"</span>)
    )

    val setupFuture = db.run(setupAction)

    val f = setupFuture.flatMap { _ =&gt;

      val insertAction: <span class="type">DBIO</span>[<span class="type">Option</span>[<span class="type">Int</span>]] = students ++= <span class="type">Seq</span> (
        (<span class="number">1</span>, <span class="string">"format1"</span>, <span class="number">11</span>, <span class="number">1</span>, new <span class="type">Date</span>(<span class="type">System</span>.currentTimeMillis())),
        (<span class="number">2</span>, <span class="string">"format2"</span>, <span class="number">22</span>, <span class="number">2</span>, new <span class="type">Date</span>((<span class="type">System</span>.currentTimeMillis()))),
        (<span class="number">3</span>, <span class="string">"format3"</span>, <span class="number">33</span>, <span class="number">3</span>, new <span class="type">Date</span>((<span class="type">System</span>.currentTimeMillis())))
      )

      val insertAndPrintAction = insertAction.map { studentResult =&gt;
        studentResult.foreach { numRows =&gt;
          println(<span class="string">s"inserted $numRows students"</span>)
        }
      }

      db.run(insertAndPrintAction)
    }.flatMap { _ =&gt;

      // print <span class="type">All</span> <span class="type">Classrooms</span>
      db.run(classrooms.<span class="literal">result</span>).map { classroom =&gt;
        classroom.foreach(println);
      }

      // print <span class="type">All</span> <span class="type">Students</span>
      db.run(students.<span class="literal">result</span>).map { studnet =&gt;
        studnet.foreach(println);
      }

      // condition search
      val studentQuery = students.filter(_.age &gt; <span class="number">20</span>).sortBy(_.name)
      db.run(studentQuery.<span class="literal">result</span>).map { student =&gt;
        student.foreach(println)
      }
    }
    <span class="type">Await</span>.<span class="literal">result</span>(f, <span class="type">Duration</span>.<span class="type">Inf</span>)
  } <span class="keyword">finally</span> db.close()
}
</code></pre><h2 id="数据库配置">数据库配置</h2><p>在配置文件application.conf里配置数据库配置信息：</p>
<pre><code><span class="title">h2mem1</span> = {
  <span class="title">url</span> = <span class="string">"jdbc:h2:mem:test1"</span>
  driver = org.h2.Driver
  connectionPool = disabled
  keepAliveConnection = <span class="built_in">true</span>
}
</code></pre><p>然后就可使用Database初始化数据库，参数就是配置文件里对应的数据库name：</p>
<pre><code>val db = Database.<span class="function"><span class="title">forConfig</span><span class="params">(<span class="string">"h2mem1"</span>)</span></span>
</code></pre><h2 id="DBIOAction介绍">DBIOAction介绍</h2><p>DBIOAction就是数据库的一个操作，比如Insert，Update，Delete，Query等操作。</p>
<p>可以使用上面分析的数据库配置变量db进行操作。</p>
<p>db有个run方法使用DBIOAction作为参数，返回Future类型的返回值。</p>
<p>DBIO是一个单例对象，它的seq方法可以传入多个DBIOAction，然后返回一个新的DBIOAction。 += 方法返回的也是DBIOAction。</p>
<pre><code>val setupAction: DBIO[Unit] = DBIO.se<span class="string">q(
  (classrooms.schema ++ students.schema)</span>.create,
  classrooms += (<span class="number">1</span>, <span class="string">"classroom1"</span>),
  classrooms += (<span class="number">2</span>, <span class="string">"classroom2"</span>),
  classrooms += (<span class="number">3</span>, <span class="string">"classroom2"</span>)
)
</code></pre><p>++=方法跟+=方法一样会返回DBIOAction，只不过它的参数是个Iterable：</p>
<pre><code>val insertAction: DBIO[Option[Int]] = students ++= Seq (
    (<span class="number">1</span>, <span class="string">"format1"</span>, <span class="number">11</span>, <span class="number">1</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),
    (<span class="number">2</span>, <span class="string">"format2"</span>, <span class="number">22</span>, <span class="number">2</span>, <span class="keyword">new</span> Date((System.currentTimeMillis()))),
    (<span class="number">3</span>, <span class="string">"format3"</span>, <span class="number">33</span>, <span class="number">3</span>, <span class="keyword">new</span> Date((System.currentTimeMillis())))
  )
</code></pre><p>DBIOAction提供许多好用的方法：</p>
<p>map方法：参数是个函数，这个函数可以返回任意类型的值，返回是个DBIOAction。 所以可以使用map关联起来多个DBIOAction。</p>
<p>flatMap方法：参数是个函数，这个函数的返回值必须是个DBIOAction，返回值是个DBIOAction。作用跟map类似，只不过函数参数的返回值不一样。</p>
<p>filter方法：参数是个函数，这个函数的返回值必须是个Boolean，返回值是个DBIOAction。过滤作用。</p>
<p>andThen方法：参数是个DBIOAction，返回值是个DBIOAction。在Action完成后执行另外一个Action。</p>
<h2 id="增删改查操作">增删改查操作</h2><h3 id="查询">查询</h3><p>Slick的查询可以直接通过TableQuery操作，使用TableQuery提供的filter可以实现过滤操作，使用drop和take完成分页操作，使用sortBy完成排序操作。</p>
<pre><code>students.<span class="function"><span class="title">filter</span><span class="params">(_.classroomId === <span class="number">1</span>)</span></span>
students.<span class="function"><span class="title">drop</span><span class="params">(<span class="number">1</span>)</span></span>.<span class="function"><span class="title">take</span><span class="params">(<span class="number">2</span>)</span></span>
students.<span class="function"><span class="title">sortBy</span><span class="params">(_.age.desc)</span></span>
</code></pre><p>可以使用map方法找出需要的列。 </p>
<p>多列：</p>
<pre><code><span class="component">students.map { student =&gt;
  (student<span class="string">.name</span>, student<span class="string">.age)</span>
}</span>
</code></pre><p>一列：</p>
<pre><code>students.<span class="function"><span class="title">map</span><span class="params">(_.name)</span></span>
</code></pre><p>Join方法：</p>
<p>cross join操作：</p>
<pre><code>val crossJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students <span class="built_in">join</span> classrooms
} yield (s.<span class="built_in">name</span>, c.<span class="built_in">name</span>)
</code></pre><p>inner Join操作：</p>
<pre><code>val innerJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students join classrooms <span class="function_start"><span class="keyword">on</span></span> (_.classroomId === _.<span class="property">id</span>)
} yield (s.<span class="property">name</span>, c.<span class="property">name</span>)
</code></pre><p>另外一个inner join：</p>
<pre><code>val innerJoin = <span class="keyword">for</span> {
  s &lt;- students
  c &lt;- classrooms <span class="keyword">if</span> c.<span class="property">id</span> === s.classroomId
} yield (s.<span class="property">name</span>, c.<span class="property">name</span>)
</code></pre><p>left join操作：</p>
<pre><code>val leftJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students joinLeft classrooms <span class="function_start"><span class="keyword">on</span></span> (_.classroomId === _.<span class="property">id</span>)
} yield (s.<span class="property">name</span>, c.map(_.<span class="property">name</span>))
</code></pre><p>right join操作：</p>
<pre><code>val rightJoin = <span class="keyword">for</span> {
  (s, c) &lt;- students joinRight classrooms <span class="function_start"><span class="keyword">on</span></span> (_.classroomId === _.<span class="property">id</span>)
} yield (s.map(_.<span class="property">name</span>), c.<span class="property">name</span>)
</code></pre><h3 id="新增">新增</h3><p>所有列都有值：</p>
<pre><code>val insertAction = DBIO.seq(
  students += (<span class="number">4</span>, <span class="string">"format4"</span>, <span class="number">44</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),
  students += (<span class="number">5</span>, <span class="string">"format5"</span>, <span class="number">55</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),

  students ++= Seq (
    (<span class="number">6</span>, <span class="string">"format6"</span>, <span class="number">66</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis())),
    (<span class="number">7</span>, <span class="string">"format7"</span>, <span class="number">77</span>, <span class="number">3</span>, <span class="keyword">new</span> Date(System.currentTimeMillis()))
  )
)
</code></pre><p>部分列有值：</p>
<pre><code>students.<span class="built_in">map</span>(s =&gt; (s.name, s.age, s.classroomId)) += (<span class="string">"format8"</span>, <span class="number">88</span>, <span class="number">3</span>)
</code></pre><h3 id="删除">删除</h3><p>删除classroomId为3的所有数据：</p>
<pre><code><span class="variable"><span class="keyword">val</span> q</span> = students.filter(_.classroomId === <span class="number">3</span>)
<span class="variable"><span class="keyword">val</span> affectedRowsCountFuture</span> = db.run(q.delete)
affectedRowsCountFuture.map { rows =&gt;
  println(rows)
}
</code></pre><h3 id="修改">修改</h3><p>修改单列：</p>
<pre><code>val <span class="tag">q</span> = students.<span class="function"><span class="title">filter</span><span class="params">(_.id === <span class="number">2</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(_.name)</span></span>
val updateSql = <span class="tag">q</span>.<span class="function"><span class="title">update</span><span class="params">(<span class="string">"format2222"</span>)</span></span>
db.<span class="function"><span class="title">run</span><span class="params">(updateSql)</span></span>
</code></pre><p>修改多列：</p>
<pre><code>val <span class="tag">q</span> = students.<span class="function"><span class="title">filter</span><span class="params">(_.id === <span class="number">2</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(s =&gt; (s.name, s.age)</span></span>)
val updateSql = <span class="tag">q</span>.<span class="function"><span class="title">update</span><span class="params">((<span class="string">"format2222"</span>, <span class="number">222</span>)</span></span>)
db.<span class="function"><span class="title">run</span><span class="params">(updateSql)</span></span>
</code></pre><h2 id="CaseClass的使用">CaseClass的使用</h2><p>之前的例子都是使用Tuple构造domain。 还有一种更方便的方式，那就是使用CaseClass。</p>
<pre><code><span class="keyword">case</span> <span class="keyword">class</span> People(id: Long, <span class="keyword">name</span>: String, age: <span class="built_in">Int</span>)
</code></pre><p>例子：</p>
<pre><code>private class PeopleTable(tag: Tag) extends Table[<span class="link_label">People</span>](<span class="link_url">tag, "people"</span>) {
  val id = column[<span class="link_label">Long</span>](<span class="link_url">"id", O.PrimaryKey, O.AutoInc</span>)
  val name = column[<span class="link_label">String</span>](<span class="link_url">"name"</span>)
  val age = column[<span class="link_label">Int</span>](<span class="link_url">"age"</span>)

  def * = (id, name, age) <span class="xml"><span class="tag">&lt;&gt;</span></span> ((People.apply _).tupled, People.unapply)
}

val db = Database.forConfig("h2mem1")

try {

  val people = TableQuery[PeopleTable]

  val setupAction = DBIO.seq(
<span class="code">      people.schema.create,</span>
<span class="code">      people += People(1, "format1", 11)</span>
  )

  val setupFuture = db.run(setupAction);

  val f = setupFuture.flatMap { _ =&gt;
<span class="code">    db.run(people.result).map { p =&gt;</span>
<span class="code">      p.foreach(println)</span>
<span class="code">    }</span>
  }

  Await.result(f, Duration.Inf)

} finally db.close()
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[近看到了一个FRM的框架Slick。 FRM的意思是Functional Relational Mapping， 一种基于函数式的ORM ...]]>
    
    </summary>
    
      <category term="frm" scheme="http://fangjian0423.github.io/tags/frm/"/>
    
      <category term="orm" scheme="http://fangjian0423.github.io/tags/orm/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala并发的Future]]></title>
    <link href="http://fangjian0423.github.io/2015/08/14/scala-future/"/>
    <id>http://fangjian0423.github.io/2015/08/14/scala-future/</id>
    <published>2015-08-14T12:22:33.000Z</published>
    <updated>2015-12-20T16:55:17.000Z</updated>
    <content type="html"><![CDATA[<p>Future这个概念其实java里也已经有了， 表示一个未来的意思。 某线程执行一项操作，这个操作有延迟的话，Future会提供一系列方法来处理这个线程过程，可取消，可操作完成后执行其他操作等等。</p>
<p>使用Future是非阻塞的，在Future中可以使用回调函数可以避免阻塞操作。 Scala在Future中提供了flatMap，foreach，filter等方法。</p>
<h2 id="Future概念">Future概念</h2><p>Future的状态：</p>
<p>1.未完成：线程操作还未结束<br>2.已完成：操作操作完成，并且有返回值或者有异常。 当一个Future完成的时候，它就变成了一个不可变对象，永远不会被重写</p>
<p>构造Future最简单的方法是使用Future这个object提供的apply方法：</p>
<pre><code><span class="tag">Future</span> {
    ...
}
</code></pre><p>来看一个最简单的Future操作， 计算和：</p>
<pre><code>import <span class="keyword">scala</span>.concurrent.ExecutionContext.Implicits.<span class="keyword">global</span>
import <span class="keyword">scala</span>.concurrent.duration.Duration
import <span class="keyword">scala</span>.concurrent.{Await, Future}

val sumFuture = Future[Int] {
  <span class="keyword">var</span> <span class="keyword">sum</span> = 0
  <span class="keyword">for</span>(i &lt;- <span class="keyword">Range</span>(1,100000)) <span class="keyword">sum</span> = <span class="keyword">sum</span> + <span class="literal">i</span>
  <span class="literal">sum</span>
}

sumFuture.onSuccess {
  case <span class="keyword">sum</span> =&gt; println(<span class="keyword">sum</span>)
}

Await.result(sumFuture, Duration.<span class="keyword">Inf</span>)
</code></pre><p>Await的作用是阻断Future等待Future的执行结果。 import scala.concurrent.ExecutionContext.Implicits.global 这个global表示一个ExecutionContext，类似线程池，所有线程的提交都得交给ExecutionContext。如果没有import这个global对象，那么执行的时候会报错。</p>
<p>文本文件中找关键字的例子，读io文件的时候如果文件很大，肯定会阻塞。使用Future完成，可以更有效率，等关键字索引找到的时候再去拿数据。这段时间完成可以去做其他事情：</p>
<pre><code><span class="variable"><span class="keyword">val</span> keywordIndex</span> = Future[<span class="typename">Int</span>] {
  <span class="variable"><span class="keyword">val</span> source</span> = scala.io.Source.fromFile(<span class="string">"intro.txx"</span>)
  source.toSeq.indexOfSlice(<span class="string">"format"</span>)
}
</code></pre><h2 id="回调">回调</h2><p>Future提供了3种Callback，分别是 onComplete，onFailure，onSuccess。</p>
<p>onComplete回调表示Future执行完毕了。需要1个Try[T] =&gt; U类型的参数，如果执行成功且没发生一次，那么匹配Success类型，否则匹配Failure类型。</p>
<p>onComplete例子：</p>
<pre><code>val calFuture = <span class="type">Future</span>[<span class="type">Int</span>] {
  val a = <span class="number">1</span> / <span class="number">1</span>
  a
}

calFuture.onComplete {
  <span class="keyword">case</span> <span class="type">Success</span>(<span class="literal">result</span>) =&gt; println(<span class="literal">result</span>)
  <span class="keyword">case</span> <span class="type">Failure</span>(e) =&gt; println(<span class="string">"error: "</span> + e.getMessage)
}
</code></pre><p>onFailure回调表示Future已经执行完成，但是出错了。</p>
<pre><code>val errorFuture = Future[Unit] {
  <span class="number">1</span> / <span class="number">0</span>
}

errorFuture.onFailure {
  <span class="keyword">case</span> e =&gt; println(e.getMessage)
}
</code></pre><p>onSuccess回调表示Future已经执行完成，而且执行成功了。</p>
<pre><code>val successFuture = Future[Int] {
  <span class="number">1000</span>
}

successFuture.onSuccess {
  <span class="keyword">case</span> <span class="built_in">num</span> =&gt; println(<span class="built_in">num</span>)
}
</code></pre><h2 id="Future的组合">Future的组合</h2><p>使用map方法可以组合Future：</p>
<pre><code>val firstValFuture = <span class="type">Future</span>[<span class="type">Int</span>] {
  <span class="number">1</span>
}

val secondFuture = firstValFuture.map { num =&gt;
  println(<span class="string">"firstFuture: "</span> + num)
  num + <span class="string">"111"</span>
}

secondFuture.onSuccess {
  <span class="keyword">case</span> <span class="literal">result</span> =&gt; println(<span class="literal">result</span>)
}
</code></pre><p>flapMap方法也可以组合Future，map方法和flatMap方法唯一的区别是flatMap内部需要返回Future，而map不是。</p>
<pre><code>val firstValFuture = <span class="type">Future</span>[<span class="type">Int</span>] {
  <span class="number">1</span>
}

val secondFuture = firstValFuture.flatMap { num =&gt;
  println(<span class="string">"firstFuture: "</span> + num)
  <span class="type">Future</span> {
    num + <span class="string">"111"</span>
  }
}

secondFuture.onSuccess {
  <span class="keyword">case</span> <span class="literal">result</span> =&gt; println(<span class="literal">result</span>)
}
</code></pre><p>其他资料参考<a href="http://docs.scala-lang.org/overviews/core/futures.html" target="_blank" rel="external">Scala的官方文档</a>即可。</p>
]]></content>
    <summary type="html">
    <![CDATA[Future这个概念其实java里也已经有了， 表示一个未来的意思。 某线程执行一项操作，这个操作有延迟的话，Future会提供一系列方法来处理这个线程过程，可取消，可操作完成...]]>
    
    </summary>
    
      <category term="concurrent" scheme="http://fangjian0423.github.io/tags/concurrent/"/>
    
      <category term="scala" scheme="http://fangjian0423.github.io/tags/scala/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HBase介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/08/07/hbase-intro/"/>
    <id>http://fangjian0423.github.io/2015/08/07/hbase-intro/</id>
    <published>2015-08-06T16:22:33.000Z</published>
    <updated>2015-12-20T17:02:23.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>HBase在公司已经用过一段时间，在Flume中添加一个HBase sink将一些数据存储到HBase里。</p>
<p>当时HBase也没学，看了看几个例子，了解了它是基于列的表设计之后，就马上上手了，而且也把东西做出来了。 现在记录一下HBase的一些学习笔记。</p>
<h2 id="HBase简介">HBase简介</h2><p>HBase是什么？</p>
<p>HBase是运行在hadoop上的数据库，是一个分布式的，扩展性高的，存储大数据的数据库。</p>
<p>HBase也是开源的，非关系型数据库。基于Google的Bigtable设计。</p>
<p>什么时候需要使用HBase？</p>
<p>需要实时地读写大数据。HBase的目的就是管理亿级的数据。</p>
<h2 id="HBase基本概念">HBase基本概念</h2><p>HBase是基于列设计的，那什么是基于列呢？</p>
<p>首先看下关系型数据库的表结构，第一行是table的所有列，第二行开始就是各个列对应的值：</p>
<table>
<thead>
<tr>
<th style="text-align:center">id</th>
<th style="text-align:center">name</th>
<th style="text-align:center">age</th>
<th style="text-align:center">birth_date</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">format1</td>
<td style="text-align:center">11</td>
<td style="text-align:center">1980-01-01</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">format2</td>
<td style="text-align:center">22</td>
<td style="text-align:center">1985-01-01</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">format3</td>
<td style="text-align:center">33</td>
<td style="text-align:center">1990-01-01</td>
</tr>
</tbody>
</table>
<p>HBase的表结构是这样的：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Row Key</th>
<th style="text-align:center">Time Stamp</th>
<th style="text-align:center">ColumnFamily contents</th>
<th style="text-align:center">ColumnFamily names</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">‘me.format.hbase’</td>
<td style="text-align:center">t1</td>
<td style="text-align:center">contents:format = “format1”</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">‘me.format.hbase’</td>
<td style="text-align:center">t2</td>
<td style="text-align:center">contents:title = “title1”</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">‘me.format.hbase’</td>
<td style="text-align:center">t3</td>
<td style="text-align:center"></td>
<td style="text-align:center">names:gogogo = “data1”</td>
</tr>
</tbody>
</table>
<p>从上面这个HBase表的例子来说明HBase的存储结构。</p>
<p>Row Key：行的键值，其实就相当于这一行的标识符。上面的数据其实只有1行，因为他们的标识符是一样的。</p>
<p>TimeStamp：时间戳，创建数据的时间戳，hbase默认会自动生成</p>
<p>ColumnFamily：列的前缀，一列可以存储多条数据，具体存储什么类型的数据还需要另外一个标示符qualify，上面那个例子中，contents和names就是两个Column Family</p>
<p>ColumnFamily qualify：列前缀后的标识符，一个ColumnFamily可以有多个qualify。上面那个例子中format和title就是contents这个ColumnFamily的qualify。gogogo是names这个ColumnFamily的qualify</p>
<h2 id="HBase的启动">HBase的启动</h2><p>HBase下载完之后解压，解压后使用以下命令启动hbase：</p>
<pre><code>$ ./bin/<span class="literal">start</span>-hbase.sh
</code></pre><p>启动之前注意，机器要装好jdk，并且启动hadoop。因为hbase底层数据是存储在hdfs上的。</p>
<h2 id="HBase的基本操作">HBase的基本操作</h2><h3 id="表的创建">表的创建</h3><p>创建一个表名位tableName，ColumnFamily有contents和names的表，qualify不需要声明，每次添加数据随意指定qualify即可：</p>
<pre><code><span class="built_in">create</span> <span class="string">'tableName'</span>,[<span class="string">'contents'</span>, <span class="string">'names'</span>]
</code></pre><h3 id="表的删除">表的删除</h3><p>删除表的所有数据：</p>
<pre><code>truncate <span class="built_in">table</span> <span class="built_in">table</span>Name
</code></pre><p>删除表，删除之前需要先disable表，然后才可删除：</p>
<pre><code><span class="built_in">disable</span> <span class="string">'tableName'</span>
drop <span class="string">'tableName'</span>
</code></pre><h3 id="数据查询">数据查询</h3><p>查询tableName表数据：</p>
<pre><code><span class="built_in">scan</span> tableName
</code></pre><p>返回：</p>
<pre><code>ROW                                         COLUMN+CELL
 me.<span class="keyword">format</span>.hbase                            column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438875060466</span>, <span class="keyword">value</span>=format1
</code></pre><h3 id="数据删除">数据删除</h3><p>比如，表tableName里有如下数据：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875566613</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=names:gogogo, timestamp=<span class="number">1438875597592</span>, value=data1
</code></pre><p>进行删除操作，删除ColumnFamily qulify为contents:format的数据：</p>
<pre><code>delete <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:format'
</code></pre><h3 id="数据修改">数据修改</h3><p>HBase没有直接的update操作，只有put操作，put操作如果对应的地方有值，会覆盖：</p>
<pre><code>put <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:format', <span class="symbol">'format111'</span>
</code></pre><h3 id="添加数据">添加数据</h3><p>在tableName表里插入一个Row Key为me.format.hbase, ColumnFamily为contents，qualify为format，值的format1的数据：</p>
<pre><code>put <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:format', <span class="symbol">'format1'</span>
</code></pre><h3 id="计数器">计数器</h3><p>HBase提供了一种计数器的概念，每次可以对某个值进行incr操作：</p>
<pre><code>incr <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">1</span>
</code></pre><p>查询数据：</p>
<pre><code>me.format.hbase                            column=contents:num, timestamp=1438875837362, value=<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>00<span class="command">\x</span>01
</code></pre><p>可以使用get_counter命令获得计数器的值：</p>
<pre><code>get_counter <span class="symbol">'tableName'</span>,<span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">0</span>
</code></pre><p>返回：</p>
<pre><code>COUNTER VALUE = <span class="number">1</span>
</code></pre><p>再次修改：</p>
<pre><code>incr <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">100</span>

get_counter <span class="symbol">'tableName'</span>,<span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">0</span>

<span class="type">COUNTER</span> <span class="type">VALUE</span> = <span class="number">101</span>

incr <span class="symbol">'tableName'</span>, <span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', -<span class="number">102</span>

get_counter <span class="symbol">'tableName'</span>,<span class="symbol">'me</span>.format.hbase', <span class="symbol">'contents</span>:num', <span class="number">0</span>

<span class="type">COUNTER</span> <span class="type">VALUE</span> = -<span class="number">1</span>
</code></pre><h3 id="带条件的数据查询">带条件的数据查询</h3><p>scan查询可以带几个参数。</p>
<p>COLUMNS： ColumnFamily和qualify的值<br>LIMIT：展示的个数<br>FILTER：过滤条件</p>
<p>比如有以下数据：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875707700</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:num, timestamp=<span class="number">1438876106259</span>, value=\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=names:gogogo, timestamp=<span class="number">1438875597592</span>, value=data1
 me<span class="class">.format</span><span class="class">.hbase1</span>                           column=contents:format, timestamp=<span class="number">1438877417358</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase2</span>                           column=contents:format, timestamp=<span class="number">1438877422756</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase3</span>                           column=contents:format, timestamp=<span class="number">1438877427312</span>, value=format1
</code></pre><p>查询ColumnFamily，qualify为contents:format的数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents:format"</span>, LIMIT =&gt; <span class="number">10</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me.<span class="keyword">format</span>.hbase                            column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438875707700</span>, <span class="keyword">value</span>=format1
 me.<span class="keyword">format</span>.hbase1                           column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438877417358</span>, <span class="keyword">value</span>=format1
 me.<span class="keyword">format</span>.hbase2                           column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438877422756</span>, <span class="keyword">value</span>=format1
 me.<span class="keyword">format</span>.hbase3                           column=contents:<span class="keyword">format</span>, timestamp=<span class="number">1438877427312</span>, <span class="keyword">value</span>=format1
</code></pre><p>查询ColumnFamily未contents的数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents"</span>, LIMIT =&gt; <span class="number">10</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875707700</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:num, timestamp=<span class="number">1438876106259</span>, value=\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase1</span>                           column=contents:format, timestamp=<span class="number">1438877417358</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase2</span>                           column=contents:format, timestamp=<span class="number">1438877422756</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase3</span>                           column=contents:format, timestamp=<span class="number">1438877427312</span>, value=format1
</code></pre><p>查询ColumnFamily未contents的数据，并只展示2行数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents"</span>, LIMIT =&gt; <span class="number">2</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:format, timestamp=<span class="number">1438875707700</span>, value=format1
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:num, timestamp=<span class="number">1438876106259</span>, value=\xFF\xFF\xFF\xFF\xFF\xFF\xFF\xFF
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
 me<span class="class">.format</span><span class="class">.hbase1</span>                           column=contents:format, timestamp=<span class="number">1438877417358</span>, value=format1
</code></pre><p>查询ColumnFamily未contents的数据，并只展示2行数据：</p>
<pre><code>scan <span class="string">'tableName'</span>, { COLUMNS =&gt; <span class="string">"contents"</span>, FILTER =&gt; <span class="string">"ValueFilter( =, 'binaryprefix:title' )"</span> }
</code></pre><p>结果：</p>
<pre><code>ROW                                         COLUMN+CELL
 me<span class="class">.format</span><span class="class">.hbase</span>                            column=contents:title, timestamp=<span class="number">1438875577687</span>, value=title1
</code></pre><p>scan命令具体其他的参数就不一一列举了，可查询文档解决。</p>
]]></content>
    <summary type="html">
    <![CDATA[HBase在公司已经用过一段时间，在Flume中添加一个HBase sink将一些数据存储到HBase里 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="hbase" scheme="http://fangjian0423.github.io/tags/hbase/"/>
    
      <category term="hbase" scheme="http://fangjian0423.github.io/categories/hbase/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hive介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/07/31/hive-intro/"/>
    <id>http://fangjian0423.github.io/2015/07/31/hive-intro/</id>
    <published>2015-07-31T05:32:33.000Z</published>
    <updated>2015-12-20T17:04:55.000Z</updated>
    <content type="html"><![CDATA[<p>Hive是基于Hadoop的一个数据仓库工具，使用它可以查询和管理分布式存储系统上的大数据集。</p>
<p>Hive提供了一种叫做HiveQL的类似SQL查询语言用来查询数据，HiveQL也允许熟悉MapReduce开发者开发自定义的mapper和reducer来处理内建的mapper和reducer无法完成的复杂的分析工作。</p>
<p>Hive的工作模式是提交一个任务，等到任务结束时被通知，而不是实时查询。</p>
<h2 id="Hive安装">Hive安装</h2><p>直接去<a href="https://hive.apache.org/" target="_blank" rel="external">Hive官网</a>下载最新的文件，解压。</p>
<p>运行bin目录里的hive文件，运行之前先启动hadoop，运行的时候可能会出现：</p>
<pre><code>Missing Hive <span class="keyword">CLI</span> Jar ....
</code></pre><p>将hive解压出来的lib目录里的jline<em>.jar拷贝到$HADOOP/share/hadoop/yarn/lib里，同时将$HADOOP/share/hadoop/yarn/lib里的jline</em>.jar删除，重启hadoop。</p>
<p>再次运行，可能还会出现</p>
<pre><code>The reported blocks <span class="number">2662</span> has reached <span class="operator">the</span> threshold <span class="number">0.9990</span> <span class="operator">of</span> total blocks <span class="number">2662.</span> The <span class="built_in">number</span> <span class="operator">of</span> live datanodes <span class="number">1</span> has reached <span class="operator">the</span> minimum <span class="built_in">number</span> <span class="number">0.</span> In safe mode extension. Safe mode will be turned off automatically <span class="operator">in</span> <span class="number">0</span> <span class="built_in">seconds</span>. ...
</code></pre><p>类似的问题，关闭hdfs的安全模式即可：</p>
<pre><code><span class="title">hadoop</span> dfsadmin -safemode leave
</code></pre><h2 id="基本命令">基本命令</h2><p>HiveQL就是模仿sql而创建的，以SQL的角度来介绍HiveQL。</p>
<h3 id="DDL操作">DDL操作</h3><p>创建表：</p>
<pre><code>hive&gt; create table users(age <span class="built_in">INT</span>, name <span class="built_in">STRING</span>)<span class="comment">;</span>
</code></pre><p>查看所有的表：</p>
<pre><code>hive&gt; <span class="built_in">show</span> <span class="built_in">tables</span>;
</code></pre><p>查看以CLIENT开头的表：</p>
<pre><code>hive&gt; <span class="built_in">show</span> <span class="built_in">tables</span> 'CLIENT.*';
</code></pre><p>表加列：</p>
<pre><code>hive&gt; alter table users add columns<span class="list">(<span class="keyword">gender</span> BOOLEAN)</span><span class="comment">;</span>
</code></pre><p>改表名字：</p>
<pre><code>hive&gt; alter <span class="built_in">table</span> users rename <span class="keyword">to</span> <span class="keyword">user</span>;
</code></pre><p>删除表：</p>
<pre><code>hive&gt; drop <span class="built_in">table</span> <span class="keyword">user</span>;
</code></pre><p>查看表的具体信息：</p>
<pre><code>hive&gt; descibe user<span class="comment">;</span>
hive&gt; desc user<span class="comment">;</span>
</code></pre><h3 id="DML操作">DML操作</h3><p>hive创建表的时候可以指定分隔符，由于hive操作的是hdfs，数据最终会存储在hdfs上，所以hdfs上的内容肯定是以某种分隔符分开各个列的。 hive默认的列分隔符是 <strong>^A</strong> 。 我们可以自定义自己的分隔符，在创建表的时候指定分隔符即可。</p>
<p>本地导入数据到hive：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table users;
</code></pre><p>users表的结构只有2列，name和age，而且使用默认的分隔符。</p>
<p>比如本地文件的内容是这样的：</p>
<pre><code><span class="xml">format1</span><span class="keyword">^A11</span><span class="xml">
format2</span><span class="keyword">^A22</span><span class="xml"></span>
</code></pre><p>导入之后进行查询：</p>
<pre><code>hive&gt; <span class="keyword">select</span> * <span class="keyword">from</span> users;
</code></pre><p>显示结果：</p>
<pre><code>OK
format1    <span class="number">11</span>
format2    <span class="number">22</span>
Time taken: <span class="number">0.362</span> seconds, Fetched: <span class="number">2</span> row(s)
</code></pre><p>在创建表的时候可以指定列分隔符和数组分隔符：</p>
<pre><code>hive&gt; create table users(name <span class="built_in">string</span>, age int)
 &gt; ROW FORMAT DELIMITED
 &gt; FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">'\t'</span>
 &gt; COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">','</span>;
</code></pre><p>导入数据还有几个参数：</p>
<p>local参数意味着从本地加载文件，如果没有local参数，那表示从hdfs加载文件。</p>
<p>关键字overwrite意味着当前表中已经存在的数据将会被删除掉，没有overwrite关键字，表示数据是追加，追加到原先数据集里面。</p>
<p>插入数据，插入数据后会起一个map reduce job去跑插入的数据：</p>
<pre><code>hive&gt; <span class="function">insert <span class="keyword">into</span> table users <span class="title">values</span>(<span class="params"><span class="string">'formatgogo'</span>, <span class="number">222</span></span>)</span>;
</code></pre><p>带条件的查询数据：</p>
<pre><code>hive&gt; <span class="keyword">select</span> * <span class="keyword">from</span> users <span class="keyword">where</span> age = <span class="number">11</span>;
</code></pre><p>group by查询：</p>
<pre><code>hive&gt; <span class="keyword">select</span> age, <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> users <span class="built_in">group</span> <span class="keyword">by</span> age;
</code></pre><p>partition的使用，以部门表为例，用type进行partition：</p>
<pre><code>hive&gt; create table dept<span class="list">(<span class="keyword">name</span> STRING)</span> partitioned by <span class="list">(<span class="keyword">type</span> INT)</span><span class="comment">;</span>
</code></pre><p>以2个文件为例，dept1.txt：</p>
<pre><code><span class="label">dept1</span>^<span class="literal">A1</span>
<span class="label">dept11</span>^<span class="literal">A1</span>
<span class="label">dept111</span>^<span class="literal">A1</span>
<span class="label">dept1111</span>^<span class="literal">A1</span>
<span class="label">dept11111</span>^<span class="literal">A1</span>
<span class="label">dept111111</span>^<span class="literal">A1</span>
</code></pre><p>dept2.txt</p>
<pre><code><span class="label">dept2</span>^<span class="literal">A2</span>
<span class="label">dept22</span>^<span class="literal">A2</span>
<span class="label">dept222</span>^<span class="literal">A2</span>
<span class="label">dept2222</span>^<span class="literal">A2</span>
<span class="label">dept22222</span>^<span class="literal">A2</span>
<span class="label">dept222222</span>^<span class="literal">A2</span>
</code></pre><p>使用partition之后，导入数据的时候需要指定对应的partition：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'$PATH/dept1.txt'</span> overwrite <span class="keyword">into</span> table dept partition(<span class="keyword">type</span>=<span class="number">1</span>);
hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'$PATH/dept2.txt'</span> overwrite <span class="keyword">into</span> table dept partition(<span class="keyword">type</span>=<span class="number">2</span>);

hive&gt; <span class="keyword">select</span> * from dept;
</code></pre><p>结果：</p>
<pre><code>OK
dept1    <span class="number">1</span>
dept11    <span class="number">1</span>
dept111    <span class="number">1</span>
dept1111    <span class="number">1</span>
dept11111    <span class="number">1</span>
dept111111    <span class="number">1</span>
dept2    <span class="number">2</span>
dept22    <span class="number">2</span>
dept222    <span class="number">2</span>
dept2222    <span class="number">2</span>
dept22222    <span class="number">2</span>
dept222222    <span class="number">2</span>
Time taken: <span class="number">0.114</span> seconds, Fetched: <span class="number">12</span> row(s)
</code></pre><p>insert可以将数据导出到指定目录，将users表导入到本地文件。 去掉local关键字表示导出到hdfs目录：</p>
<pre><code>hive&gt; insert overwrite <span class="keyword">local</span> directory <span class="string">'localFileName'</span> <span class="keyword">select</span> * from users<span class="comment">;</span>
</code></pre><h3 id="hive存储在hdfs的位置">hive存储在hdfs的位置</h3><p>进入hive控制台之后，可以使用：</p>
<pre><code><span class="tag">hive</span>&gt; <span class="tag">set</span> <span class="tag">hive</span><span class="class">.metastore</span><span class="class">.warehouse</span><span class="class">.dir</span>;
</code></pre><p>查看hive存储在hdfs的位置，默认是存在 /user/hive/warehouse 目录。</p>
<p>之前的users表，会存储在/user/hive/warehouse/user目录里。</p>
<p>有partition的表会存在的不同位置，比如之前的dept表的type为1和2的分别存储在 /user/hive/warehouse/dept/type=1 和 /user/hive/warehouse/dept/type=2。</p>
<h2 id="数据类型">数据类型</h2><p>hive中的数据类型分2种，简单类型和复杂类型。</p>
<p>简单类型有以下几种：TINYINT, SMALLINT, INT, BIGINT, BOOLEAN, FLOAT, DOUBLE, STRING。</p>
<p>复杂类型有以下几种：Structs(结构体，学过C都知道)，MAPS(key-value键值对)，Arrays(数组类型，数组内的元素类型都必须一致)</p>
<p>简单类型就不分析了，来看一下复杂类型的使用：</p>
<h3 id="Structs">Structs</h3><pre><code><span class="label">hive</span>&gt; create table employee(id INT, <span class="preprocessor">info</span> <span class="keyword">struct&lt;name:STRING, </span>age:INT&gt;)
      &gt; ROW FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY </span><span class="string">','</span> 
      &gt; COLLECTION <span class="keyword">ITEMS </span>TERMINATED <span class="keyword">BY </span><span class="string">':'</span><span class="comment">; </span>
</code></pre><p>要导入的数据：</p>
<pre><code><span class="number">1</span>,format:<span class="number">11</span>
<span class="number">2</span>,fj:<span class="number">22</span>
<span class="number">3</span>,formatfj:<span class="number">33</span>
</code></pre><p>导入数据：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table employee;
</code></pre><p>查询：</p>
<pre><code>select * from employee;

OK
<span class="number">1</span>    {<span class="string">"name"</span>:<span class="string">"format"</span>,<span class="string">"age"</span>:<span class="number">11</span>}
<span class="number">2</span>    {<span class="string">"name"</span>:<span class="string">"fj"</span>,<span class="string">"age"</span>:<span class="number">22</span>}
<span class="number">3</span>    {<span class="string">"name"</span>:<span class="string">"formatfj"</span>,<span class="string">"age"</span>:<span class="number">33</span>}
Time taken: <span class="number">0.042</span> seconds, Fetched: <span class="number">3</span> row(s)

hive&gt; select info.name from employee;

OK
format
fj
formatfj
Time taken: <span class="number">0.061</span> seconds, Fetched: <span class="number">3</span> row(s)
</code></pre><h3 id="Maps">Maps</h3><pre><code>hive&gt; create table lessons(id <span class="built_in">string</span>, score <span class="built_in">map</span>&lt;<span class="built_in">string</span>, int&gt;)
    &gt; ROW FORMAT DELIMITED
    &gt; FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">'\t'</span>
    &gt; COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">','</span>
    &gt; <span class="built_in">MAP</span> KEYS TERMINATED <span class="keyword">BY</span> <span class="string">':'</span>;
</code></pre><p>要导入的数据：</p>
<pre><code><span class="number">1</span>       chinese:<span class="number">80</span>,english:<span class="number">60</span>,math:<span class="number">70</span>  
<span class="number">2</span>       computer:<span class="number">60</span>,chemistry:<span class="number">80</span>   
</code></pre><p>导入数据：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table lessons;
</code></pre><p>查询：</p>
<pre><code>hive&gt; <span class="keyword">select</span> score[<span class="string">'chinese'</span>] <span class="keyword">from</span> lessions;

OK
<span class="number">80</span>
NULL
Time taken: <span class="number">0.05</span> seconds, Fetched: <span class="number">2</span> row(s)

hive&gt; <span class="keyword">select</span> * <span class="keyword">from</span> lessons;
OK
<span class="number">1</span>    {<span class="string">"chinese"</span>:<span class="number">80</span>,<span class="string">"english"</span>:<span class="number">60</span>,<span class="string">"math"</span>:<span class="number">70</span>}
<span class="number">2</span>    {<span class="string">"computer"</span>:<span class="number">60</span>,<span class="string">"chemistry"</span>:<span class="number">80</span>}
Time taken: <span class="number">0.032</span> seconds, Fetched: <span class="number">2</span> row(s)
</code></pre><h3 id="Arrays">Arrays</h3><pre><code>hive&gt; create table student(name <span class="built_in">string</span>, hobby_list <span class="built_in">array</span>&lt;<span class="built_in">STRING</span>&gt;)
    &gt; ROW FORMAT DELIMITED
    &gt; FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">','</span>
    &gt; COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">':'</span>;
</code></pre><p>要导入的数据：</p>
<pre><code><span class="tag">format</span>,<span class="tag">basketball</span><span class="pseudo">:football</span><span class="pseudo">:swimming</span>
<span class="tag">fj</span>,<span class="tag">coding</span><span class="pseudo">:running</span>
</code></pre><p>导入数据：</p>
<pre><code>hive&gt; load <span class="built_in">data</span> <span class="built_in">local</span> inpath <span class="string">'localFile'</span> overwrite <span class="keyword">into</span> table student;
</code></pre><p>查询(数组下标没对应的值的话返回NULL)：</p>
<pre><code>hive&gt; select * from student;

OK
format    [<span class="string">"basketball"</span>,<span class="string">"football"</span>,<span class="string">"swimming"</span>]
fj    [<span class="string">"coding"</span>,<span class="string">"running"</span>]
Time taken: <span class="number">0.041</span> seconds, Fetched: <span class="number">2</span> row(s)

hive&gt; select hobby_list[<span class="number">2</span>] from student;

OK
swimming
<span class="literal">NULL</span>
Time taken: <span class="number">0.07</span> seconds, Fetched: <span class="number">2</span> row(s)
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Hive是基于Hadoop的一个数据仓库工具，使用它可以查询和管理分布式存储系统上的大数据集 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="hive" scheme="http://fangjian0423.github.io/tags/hive/"/>
    
      <category term="hive" scheme="http://fangjian0423.github.io/categories/hive/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[google guava类库介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/07/26/google-guava-intro/"/>
    <id>http://fangjian0423.github.io/2015/07/26/google-guava-intro/</id>
    <published>2015-07-26T08:32:33.000Z</published>
    <updated>2015-12-20T16:57:40.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Guava简介">Guava简介</h2><p><a href="https://mail.google.com/mail/u/0/" target="_blank" rel="external">Guava</a>是一个Google开发的基于java的扩展项目，提供了很多有用的工具类，可以让java代码更加优雅，更加简洁。</p>
<p>Guava包括诸多工具类，比如Collections，cache，concurrent，hash，reflect，annotations，eventbus等。</p>
<p>刚好在看flume源码的时候看到源码里面使用了很多guava提供的代码，于是记录学习一下这个类库。</p>
<h2 id="各个模块介绍">各个模块介绍</h2><p><a href="https://code.google.com/p/guava-libraries/wiki/GuavaExplained" target="_blank" rel="external">Guava的wiki</a>已经很明细地介绍了各个工具类的作用和说明。</p>
<p>简单翻译一下各个工具类的说明，有用到的需要了解详情的直接去官网看就可以了。</p>
<h3 id="Basic_utilties_基础工具类">Basic utilties 基础工具类</h3><p>基础工具类的作用是写java语言写的更轻松。它包括了5个子模块：</p>
<p>1.<a href="https://code.google.com/p/guava-libraries/wiki/UsingAndAvoidingNullExplained" target="_blank" rel="external">使用和避免null</a>，null值是有歧义的，也会引起错误。有时候它会让人很不舒服，<br>2.<a href="https://code.google.com/p/guava-libraries/wiki/PreconditionsExplained" target="_blank" rel="external">前置条件</a>,让方法中的条件检查更简单<br>3.<a href="https://code.google.com/p/guava-libraries/wiki/CommonObjectUtilitiesExplained" target="_blank" rel="external">公用的object方法</a>，简化object对象的hashCode和toString<br>4.<a href="https://code.google.com/p/guava-libraries/wiki/OrderingExplained" target="_blank" rel="external">排序</a>，Guava提供了强大的fluent Comparator<br>5.<a href="https://code.google.com/p/guava-libraries/wiki/ThrowablesExplained" target="_blank" rel="external">Throwables</a>，简化了异常和错误的传播与检查</p>
<h3 id="Collections_集合">Collections 集合</h3><p>Guava扩展了jdk提供的集合机制</p>
<p>1.<a href="https://code.google.com/p/guava-libraries/wiki/ImmutableCollectionsExplained" target="_blank" rel="external">不可变集合</a>用不变的集合进行防御性编程和性能提升<br>2.<a href="https://code.google.com/p/guava-libraries/wiki/NewCollectionTypesExplained" target="_blank" rel="external">新集合类型</a>multisets，multimaps，tables，bidirectional map等<br>3.<a href="https://code.google.com/p/guava-libraries/wiki/CollectionUtilitiesExplained" target="_blank" rel="external">强大的集合工具类</a>提供了jdk中没有的集合工具类<br>4.<a href="https://code.google.com/p/guava-libraries/wiki/CollectionHelpersExplained" target="_blank" rel="external">扩展工具类</a>让实现和扩展集合类变得更容易，比如创建Collection的装饰器，或实现迭代器</p>
<h3 id="Caches_缓存">Caches 缓存</h3><p>本地缓存实现，支持多种缓存过期策略</p>
<h3 id="函数式风格">函数式风格</h3><p>Guava的函数式支持可以显著简化代码，但请谨慎使用它</p>
<h3 id="并发">并发</h3><p>1.<a href="https://code.google.com/p/guava-libraries/wiki/ListenableFutureExplained" target="_blank" rel="external">ListenableFuture</a>：完成后触发回调的Future<br>2.<a href="https://code.google.com/p/guava-libraries/wiki/ServiceExplained" target="_blank" rel="external">Service框架</a>：抽象可开启和关闭的服务，帮助你维护服务的状态逻辑</p>
<h3 id="字符串处理">字符串处理</h3><p>非常有用的字符串工具，包括分割、连接、填充等操作</p>
<h3 id="原生类型">原生类型</h3><p>扩展 JDK 未提供的原生类型（如int、char）操作， 包括某些类型的无符号形式</p>
<h3 id="区间">区间</h3><p>可比较类型的区间API，包括连续和离散类型</p>
<h3 id="IO">IO</h3><p>简化I/O尤其是I/O流和文件的操作，针对Java5和6版本</p>
<h3 id="散列">散列</h3><p>提供比Object.hashCode()更复杂的散列实现，并提供布鲁姆过滤器的实现</p>
<h3 id="事件总线">事件总线</h3><p>发布-订阅模式的组件通信，但组件不需要显式地注册到其他组件中</p>
<h3 id="数学运算">数学运算</h3><p>优化的、充分测试的数学工具类</p>
<h3 id="反射">反射</h3><p>Guava的Java反射机制工具类</p>
]]></content>
    <summary type="html">
    <![CDATA[Guava是一个Google开发的基于java的扩展项目，提供了很多有用的工具类，可以让java代码更加优雅，更加简洁 ...]]>
    
    </summary>
    
      <category term="guava" scheme="http://fangjian0423.github.io/tags/guava/"/>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[java内置的线程池笔记]]></title>
    <link href="http://fangjian0423.github.io/2015/07/24/java-poolthread/"/>
    <id>http://fangjian0423.github.io/2015/07/24/java-poolthread/</id>
    <published>2015-07-24T12:32:33.000Z</published>
    <updated>2015-12-20T17:10:06.000Z</updated>
    <content type="html"><![CDATA[<p>目前的工作是接触大数据相关的内容，自己也缺少高并发的知识，刚好前几天看了flume的源码，里面也用到了各种线程池内容，刚好学习一下，做个笔记。</p>
<p>写这篇博客的时候又刚好想起了当时自己实习的时候遇到的一个问题。1000个爬虫任务使用了多线程的处理方式，比如开5个线程处理这1000个任务，每个线程分200个任务，然后各个线程处理那200个爬虫任务→_→，太笨了。其实更合理的方法是使用阻塞队列+线程池的方法。</p>
<h2 id="ExecutorService">ExecutorService</h2><p>ExecutorService就是线程池的概念，ExecutorService的初始化可以使用Executors类的静态方法。</p>
<p>Executors提供了很多方法用来初始化ExecutorService，可以初始化指定数目个线程的线城市或者单个线程的线程池。</p>
<p>比如构造一个10个线程的线程池，使用了guava的ThreadFactoryBuilder，guava的ThreadFactoryBuilder可以传入一个namFormat参数用户来表示线程的name，它内部会使用数字增量表示%d，比如一下的nameFormat，10个线程，名字分别是thread-call-runner-1，thread-call-runner-2 … thread-call-runner-10:</p>
<pre><code>Executors.newFixedThreadPool(<span class="number">10</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
</code></pre><p>ExecutorService线程池使用线程执行任务例子：</p>
<p>1.阻塞队列里有10个元素，初始化带有2个线程的线程池，跑2个线程分别去阻塞队列里取数据执行。</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test01</span><span class="params">()</span> throws Exception </span>{
    ExecutorService es = Executors.newFixedThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
    final LinkedBlockingDeque&lt;String&gt; <span class="built_in">deque</span> = <span class="keyword">new</span> LinkedBlockingDeque&lt;String&gt;();
    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10</span>; i ++) {
        <span class="built_in">deque</span>.add(i + <span class="string">""</span>);
    }
    es.submit(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            <span class="keyword">while</span>(!<span class="built_in">deque</span>.isEmpty()) {
                System.out.println(<span class="built_in">deque</span>.poll() + <span class="string">"-"</span> + Thread.currentThread().getName());
            }
        }
    });
    es.submit(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            <span class="keyword">while</span>(!<span class="built_in">deque</span>.isEmpty()) {
                System.out.println(<span class="built_in">deque</span>.poll() + <span class="string">"-"</span> + Thread.currentThread().getName());
            }
        }
    });
    Thread.sleep(<span class="number">10000l</span>);
}
</code></pre><p>打印，2个线程都会执行：</p>
<pre><code><span class="number">1</span>-thread-call-runner-<span class="number">0</span>
<span class="number">2</span>-thread-call-runner-<span class="number">0</span>
<span class="number">3</span>-thread-call-runner-<span class="number">1</span>
<span class="number">4</span>-thread-call-runner-<span class="number">0</span>
<span class="number">5</span>-thread-call-runner-<span class="number">0</span>
<span class="number">6</span>-thread-call-runner-<span class="number">1</span>
<span class="number">7</span>-thread-call-runner-<span class="number">0</span>
<span class="number">8</span>-thread-call-runner-<span class="number">1</span>
<span class="number">9</span>-thread-call-runner-<span class="number">0</span>
<span class="number">10</span>-thread-call-runner-<span class="number">0</span>
</code></pre><p>2.执行Callable线程，Callable线程和Runnable线程的区别就是Callable的线程会有返回值，这个返回值是Future，未来的意思，而且这Future是个接口，提供了几个实用的方法，比如cancel, idDone, isCancelled, get等方法。</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test02() throws <span class="type">Exception</span> {
    <span class="type">ExecutorService</span> es = <span class="type">Executors</span>.newFixedThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
    final <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt; deque = new <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt;();
    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">500</span>; i ++) {
        deque.add(i + <span class="string">""</span>);
    }
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = es.submit(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="keyword">while</span> (!deque.isEmpty()) {
                <span class="type">System</span>.<span class="keyword">out</span>.println(deque.poll() + <span class="string">"-"</span> + <span class="type">Thread</span>.currentThread().getName());
            }
            <span class="keyword">return</span> <span class="string">"done"</span>;
        }
    });
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.isDone());
    // get方法会阻塞
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.get());
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec next"</span>);
}
</code></pre><p>打印：</p>
<pre><code>先打印出几百个 数字-thread-call-runner-<span class="number">0</span>
然后打印出 isDone的结果， 是<span class="literal">false</span>
<span class="type">Future</span>的get是得到<span class="type">Callable</span>线程执行完毕后的结果，该方法会阻塞，直到该<span class="type">Future</span>对应的线程全部执行完才会继续执行下去。这个例子<span class="type">Callable</span>线程执行完返回done，所以get方法也是返回done

get方法还有个重载的方法，带有<span class="number">2</span>个参数，第一个参数是一个long类型的数字，第二个参数是时间单位。<span class="literal">result</span>.get(<span class="number">1</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>) 就表示<span class="number">1</span>毫秒，等待这个<span class="type">Future</span>的时间为<span class="number">1</span>毫秒，如果时间<span class="number">1</span>毫秒，那么这个get方法的调用会抛出java.util.concurrent.<span class="type">TimeoutException</span>异常，并且线程内部的执行也会停止。注意，但是如果我们catch这个<span class="type">TimeoutException</span>的话，那么线程里的代码还是会被执行完毕。

<span class="keyword">try</span> {
    // catch <span class="type">TimeoutException</span>的话线程里的代码还是会执行下去
    <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.get(<span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>));
} catch (<span class="type">TimeoutException</span> e) {
    e.printStackTrace();
}
</code></pre><p>3.Future的cancel方法的使用</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test03() throws <span class="type">Exception</span> {
    <span class="type">ExecutorService</span> es = <span class="type">Executors</span>.newFixedThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-call-runner-%d"</span>).build());
    final <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt; deque = new <span class="type">LinkedBlockingDeque</span>&lt;<span class="type">String</span>&gt;();
    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">5000</span>; i ++) {
        deque.add(i + <span class="string">""</span>);
    }
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = es.submit(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="keyword">while</span> (!deque.isEmpty() &amp;&amp; !<span class="type">Thread</span>.currentThread().isInterrupted()) {
                <span class="type">System</span>.<span class="keyword">out</span>.println(deque.poll() + <span class="string">"-"</span> + <span class="type">Thread</span>.currentThread().getName());
            }
            <span class="keyword">return</span> <span class="string">"done"</span>;
        }
    });

    <span class="keyword">try</span> {
        <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="literal">result</span>.get(<span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>));
    } catch (<span class="type">TimeoutException</span> e) {
        <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"cancel result: "</span> + <span class="literal">result</span>.cancel(<span class="literal">true</span>));
        <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"is cancelled: "</span> + <span class="literal">result</span>.isCancelled());
    }
    <span class="type">Thread</span>.sleep(<span class="number">2000</span>l);
}
</code></pre><p>打印：</p>
<pre><code>先打印出几百个 数字-thread-call-runner-<span class="number">0</span>
然后打印出
cancel <span class="literal">result</span>: <span class="literal">true</span>
<span class="keyword">is</span> cancelled: <span class="literal">true</span>

cancel方法用来取消线程的继续执行，它有个boolean类型的返回值，表示是否cancel成功。这里我们使用了get方法，<span class="number">10</span>毫秒处理<span class="number">5000</span>条数据，报了<span class="type">TimeoutException</span>异常，catch之后对<span class="type">Future</span>进行了cancel调用。注意，我们在<span class="type">Callable</span>里执行的代码里加上了

!<span class="type">Thread</span>.currentThread().isInterrupted()

如果去掉了这个条件，那么还是会打印出<span class="number">5000</span>条处理数据。cancel方法底层会去interrupted对应的线程，所以才需要加上这个条件的判断。
</code></pre><h2 id="ScheduledExecutorService">ScheduledExecutorService</h2><p>ScheduledExecutorService接口是ExecutorService接口的子类。</p>
<p>看名字也知道，ScheduledExecutorService是基于调度的线程池。</p>
<p>1.ScheduledExecutorService的schedule方法例子：</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test04() throws <span class="type">Exception</span> {
    <span class="type">ScheduledExecutorService</span> ses = <span class="type">Executors</span>.newScheduledThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = ses.schedule(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec task"</span>);
            <span class="keyword">return</span> <span class="string">"ok"</span>;
        }
    }, <span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>);
    <span class="type">Thread</span>.sleep(<span class="number">15000</span>);
}
</code></pre><p>打印：</p>
<pre><code>执行<span class="number">10</span>秒后打印出  <span class="built_in">exec</span> task
</code></pre><p>2.cancel在schedule中的使用：</p>
<pre><code>@<span class="type">Test</span>
public <span class="type">void</span> test05() throws <span class="type">Exception</span> {
    <span class="type">ScheduledExecutorService</span> ses = <span class="type">Executors</span>.newScheduledThreadPool(<span class="number">2</span>, new <span class="type">ThreadFactoryBuilder</span>().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    <span class="type">Future</span>&lt;<span class="type">String</span>&gt; <span class="literal">result</span> = ses.schedule(new <span class="type">Callable</span>&lt;<span class="type">String</span>&gt;() {
        @<span class="type">Override</span>
        public <span class="type">String</span> call() throws <span class="type">Exception</span> {
            <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec task"</span>);
            <span class="keyword">try</span> {
                <span class="type">Thread</span>.sleep(<span class="number">5000</span>l);
            } catch (<span class="type">InterruptedException</span> e) {
                e.printStackTrace();
            }
            <span class="type">System</span>.<span class="keyword">out</span>.println(<span class="string">"exec done, take 5 seconds"</span>);
            <span class="keyword">return</span> <span class="string">"ok"</span>;
        }
    }, <span class="number">10</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>);
    <span class="type">Thread</span>.sleep(<span class="number">11000</span>);
    <span class="literal">result</span>.cancel(<span class="literal">true</span>);
    <span class="type">Thread</span>.sleep(<span class="number">10000</span>);
}
</code></pre><p>打印：</p>
<pre><code>先打印出exec task，然后抛出InterruptedException异常，异常被<span class="keyword">catch</span>。接着打印出exec done, take <span class="number">5</span> seconds
因为Callable线程是<span class="number">10</span>秒后执行的，线程会执行<span class="number">5</span>秒，在<span class="number">11</span>秒的时候会调用Future的cancel方法，会取消线程的时候，由于我们<span class="keyword">catch</span>了异常，所以线程会执行完毕。

注意一下，cancel方法有个boolean类型的参数mayInterruptIfRunning。上个例子中我们传入了<span class="literal">true</span>，所以会打断正在执行的线程，因此抛出了异常。如果我们传入<span class="literal">false</span>，线程正在执行，所以不会去打断它，因此会打印出exec task，然后再打印出exec done, take <span class="number">5</span> seconds，并且没有异常抛出。
</code></pre><p>3.scheduleWithFixedDelay方法，定时器，每隔多少时间执行</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test06</span><span class="params">()</span> throws Exception </span>{
    ScheduledExecutorService sec = Executors.newScheduledThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    sec.scheduleWithFixedDelay(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            System.out.println(<span class="string">"exec"</span>);
        }
    }, <span class="number">0</span>, <span class="number">5</span>, TimeUnit.SECONDS);
    Thread.sleep(<span class="number">16000l</span>);
}
</code></pre><p>打印：</p>
<pre><code>打印出<span class="number">4</span>次exec。
scheduleWithFixedDelay有<span class="number">4</span>个参数，第一个参数是个Runnable接口的实现类，第二个参数是首次执行线程的延迟时间，第三个参数是每隔多少时间再次执行线程时间，第四个参数是时间的单位。
如果Runnable中执行的代码发生了异常并且没有被<span class="keyword">catch</span>的话，那么发生异常之后，Runnable里的代码就不会再次执行。
</code></pre><p>4.scheduleAtFixedRate方法，scheduleAtFixedRate方法跟scheduleWithFixedDelay类似。唯一的区别是scheduleWithFixedDelay是在线程全部执行完毕之后开始计算时间的，而scheduleAtFixedRate是在线程开始执行的时候计算时间的，所以scheduleAtFixedRate有时会产生不是定时执行的感觉。</p>
<p>先看scheduleWithFixedDelay：</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test07</span><span class="params">()</span> throws Exception </span>{
    ScheduledExecutorService sec = Executors.newScheduledThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    sec.scheduleWithFixedDelay(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            System.out.println(<span class="string">"exec"</span>);
            <span class="keyword">try</span> {
                Thread.sleep(<span class="number">3000l</span>);
            } <span class="keyword">catch</span> (Exception e) {
                e.printStackTrace();
            }
        }
    }, <span class="number">0</span>, <span class="number">5</span>, TimeUnit.SECONDS);
    Thread.sleep(<span class="number">17000l</span>);
}
</code></pre><p>打印：</p>
<pre><code>执行<span class="number">3</span>次exec。Runnable每次执行<span class="number">3</span>秒。第一次是在<span class="number">0</span>秒执行，执行了<span class="number">3</span>秒，第二次是在<span class="number">8</span>秒，执行了<span class="number">3</span>秒。第三次是在<span class="number">16</span>秒执行
</code></pre><p>然后再看scheduleAtFixedRate：</p>
<pre><code>@<span class="function">Test
<span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test09</span><span class="params">()</span> throws Exception </span>{
    ScheduledExecutorService sec = Executors.newScheduledThreadPool(<span class="number">2</span>, <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"thread-schedule-runner-%d"</span>).build());
    sec.scheduleAtFixedRate(<span class="keyword">new</span> Runnable() {
        @<span class="function">Override
        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{
            System.out.println(<span class="string">"exec"</span>);
            <span class="keyword">try</span> {
                Thread.sleep(<span class="number">3000l</span>);
            } <span class="keyword">catch</span> (Exception e) {
                e.printStackTrace();
            }
        }
    }, <span class="number">0</span>, <span class="number">5</span>, TimeUnit.SECONDS);
    Thread.sleep(<span class="number">16000l</span>);
}
</code></pre><p>打印：</p>
<pre><code>执行了<span class="number">4</span>次exec，第一次在<span class="number">0</span>秒执行，第二次在<span class="number">5</span>秒，第三次是<span class="number">10</span>秒，第四次在<span class="number">15</span>秒执行
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[写这篇博客的时候又刚好想起了当时自己实习的时候遇到的一个问题。1000个爬虫任务使用了多线程的处理方式，比如开5个线程处理这1000个任务 ...]]>
    
    </summary>
    
      <category term="java" scheme="http://fangjian0423.github.io/tags/java/"/>
    
      <category term="thread" scheme="http://fangjian0423.github.io/tags/thread/"/>
    
      <category term="jvm" scheme="http://fangjian0423.github.io/categories/jvm/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[通过源码分析Flume HDFSSink 写hdfs文件的过程]]></title>
    <link href="http://fangjian0423.github.io/2015/07/20/flume-hdfs-sink/"/>
    <id>http://fangjian0423.github.io/2015/07/20/flume-hdfs-sink/</id>
    <published>2015-07-20T15:32:33.000Z</published>
    <updated>2015-12-20T17:09:49.000Z</updated>
    <content type="html"><![CDATA[<p>Flume有HDFS Sink，可以将Source进来的数据写入到hdfs中。</p>
<p>HDFS Sink具体的逻辑代码是在HDFSEventSink这个类中。</p>
<p>HDFS Sink跟写文件相关的配置如下：</p>
<p>hdfs.path -&gt; hdfs目录路径<br>hdfs.filePrefix -&gt; 文件前缀。默认值FlumeData<br>hdfs.fileSuffix -&gt; 文件后缀<br>hdfs.rollInterval -&gt; 多久时间后close hdfs文件。单位是秒，默认30秒。设置为0的话表示不根据时间close hdfs文件<br>hdfs.rollSize -&gt; 文件大小超过一定值后，close文件。默认值1024，单位是字节。设置为0的话表示不基于文件大小<br>hdfs.rollCount -&gt; 写入了多少个事件后close文件。默认值是10个。设置为0的话表示不基于事件个数<br>hdfs.fileType -&gt; 文件格式， 有3种格式可选择：SequenceFile, DataStream or CompressedStream<br>hdfs.batchSize -&gt; 批次数，HDFS Sink每次从Channel中拿的事件个数。默认值100<br>hdfs.minBlockReplicas -&gt; HDFS每个块最小的replicas数字，不设置的话会取hadoop中的配置<br>hdfs.maxOpenFiles -&gt; 允许最多打开的文件数，默认是5000。如果超过了这个值，越早的文件会被关闭<br>serializer -&gt; HDFS Sink写文件的时候会进行序列化操作。会调用对应的Serializer借口，可以自定义符合需求的Serializer<br>hdfs.retryInterval -&gt; 关闭HDFS文件失败后重新尝试关闭的延迟数，单位是秒<br>hdfs.callTimeout -&gt; HDFS操作允许的时间，比如hdfs文件的open，write，flush，close操作。单位是毫秒，默认值是10000</p>
<h2 id="HDFSEventSink分析">HDFSEventSink分析</h2><p>以一个hdfs.path，hdfs.filePrefix和hdfs.fileSuffix分别为/data/%Y/%m/%d/%H，flume, .txt 为例子，分析源码：</p>
<p>直接看HDFSEventSink的process方法：</p>
<pre><code><span class="keyword">public</span> Status process() <span class="keyword">throws</span> EventDeliveryException {
    <span class="comment">// 得到Channel</span>
    Channel channel = getChannel();
    Transaction transaction = channel.getTransaction();
    <span class="comment">// 构造一个BucketWriter集合，BucketWriter就是处理hdfs文件的具体逻辑实现类</span>
    List&lt;BucketWriter&gt; writers = Lists.newArrayList();
    <span class="comment">// Channel的事务启动</span>
    transaction.begin();
    <span class="keyword">try</span> {
      <span class="built_in">int</span> txnEventCount = <span class="number">0</span>;
      <span class="comment">// 每次处理batchSize个事件。这里的batchSize就是之前配置的hdfs.batchSize</span>
      <span class="keyword">for</span> (txnEventCount = <span class="number">0</span>; txnEventCount &lt; batchSize; txnEventCount++) {
        Event event = channel.take();
        <span class="keyword">if</span> (event == <span class="keyword">null</span>) {
          <span class="keyword">break</span>;
        }

        <span class="comment">// 构造hdfs文件所在的路径</span>
        <span class="keyword">String</span> realPath = BucketPath.escapeString(filePath, event.getHeaders(),
            timeZone, needRounding, roundUnit, roundValue, useLocalTime);
        <span class="comment">// 构造hdfs文件名, fileName就是之前配置的hdfs.filePrefix，即flume</span>
        <span class="keyword">String</span> realName = BucketPath.escapeString(fileName, event.getHeaders(),
          timeZone, needRounding, roundUnit, roundValue, useLocalTime);

        <span class="comment">// 构造hdfs文件路径，根据之前的path，filePrefix，fileSuffix</span>
        <span class="comment">// 得到这里的lookupPath为 /data/2015/07/20/15/flume</span>
        <span class="keyword">String</span> lookupPath = realPath + DIRECTORY_DELIMITER + realName;
        BucketWriter bucketWriter;
        HDFSWriter hdfsWriter = <span class="keyword">null</span>;
        <span class="comment">// 构造一个回调函数</span>
        WriterCallback closeCallback = <span class="keyword">new</span> WriterCallback() {
          @Override
          <span class="keyword">public</span> <span class="keyword">void</span> run(<span class="keyword">String</span> bucketPath) {
            LOG.info(<span class="string">"Writer callback called."</span>);
            <span class="keyword">synchronized</span> (sfWritersLock) {
              <span class="comment">// sfWriters是一个HashMap，最多支持maxOpenFiles个键值对。超过maxOpenFiles的话会关闭越早进来的文件</span>
              <span class="comment">// 回调函数的作用就是hdfs文件close的时候移除sfWriters中对应的那个文件。防止打开的文件数超过maxOpenFiles</span>
              <span class="comment">// sfWriters这个Map中的key是要写的hdfs路径，value是BucketWriter</span>
              sfWriters.remove(bucketPath);
            }
          }
        };
        <span class="keyword">synchronized</span> (sfWritersLock) {
          <span class="comment">// 先查看sfWriters是否已经存在key为/data/2015/07/20/15/flume的BucketWriter</span>
          bucketWriter = sfWriters.<span class="built_in">get</span>(lookupPath);
          <span class="keyword">if</span> (bucketWriter == <span class="keyword">null</span>) {
              <span class="comment">// 没有的话构造一个BucketWriter</span>
            <span class="comment">// 先根据fileType得到对应的HDFSWriter，fileType默认有3种类型，分别是SequenceFile, DataStream or CompressedStream</span>
            hdfsWriter = writerFactory.getWriter(fileType);
            <span class="comment">// 构造一个BucketWriter，会将刚刚构造的hdfsWriter当做参数传入，BucketWriter写hdfs文件的时候会使用HDFSWriter</span>
            bucketWriter = initializeBucketWriter(realPath, realName,
              lookupPath, hdfsWriter, closeCallback);
            <span class="comment">// 新构造的BucketWriter放入到sfWriters中</span>
            sfWriters.put(lookupPath, bucketWriter);
          }
        }

        <span class="comment">// 将BucketWriter放入到writers集合中</span>
        <span class="keyword">if</span> (!writers.contains(bucketWriter)) {
          writers.<span class="built_in">add</span>(bucketWriter);
        }

        <span class="comment">// 写hdfs数据</span>
        <span class="keyword">try</span> {
          bucketWriter.<span class="built_in">append</span>(event);
        } <span class="keyword">catch</span> (BucketClosedException ex) {
          LOG.info(<span class="string">"Bucket was closed while trying to append, "</span> +
            <span class="string">"reinitializing bucket and writing event."</span>);
          hdfsWriter = writerFactory.getWriter(fileType);
          bucketWriter = initializeBucketWriter(realPath, realName,
            lookupPath, hdfsWriter, closeCallback);
          <span class="keyword">synchronized</span> (sfWritersLock) {
            sfWriters.put(lookupPath, bucketWriter);
          }
          bucketWriter.<span class="built_in">append</span>(event);
        }
      }

      <span class="keyword">if</span> (txnEventCount == <span class="number">0</span>) {
        sinkCounter.incrementBatchEmptyCount();
      } <span class="keyword">else</span> <span class="keyword">if</span> (txnEventCount == batchSize) {
        sinkCounter.incrementBatchCompleteCount();
      } <span class="keyword">else</span> {
        sinkCounter.incrementBatchUnderflowCount();
      }

      <span class="comment">// 每个批次全部完成后flush所有的hdfs文件</span>
      <span class="keyword">for</span> (BucketWriter bucketWriter : writers) {
        bucketWriter.flush();
      }

      <span class="comment">// 事务提交</span>
      transaction.commit();

      <span class="keyword">if</span> (txnEventCount &lt; <span class="number">1</span>) {
        <span class="keyword">return</span> Status.BACKOFF;
      } <span class="keyword">else</span> {
        sinkCounter.addToEventDrainSuccessCount(txnEventCount);
        <span class="keyword">return</span> Status.READY;
      }
    } <span class="keyword">catch</span> (IOException eIO) {
      <span class="comment">// 发生异常事务回滚</span>
      transaction.rollback();
      LOG.warn(<span class="string">"HDFS IO error"</span>, eIO);
      <span class="keyword">return</span> Status.BACKOFF;
    } <span class="keyword">catch</span> (Throwable th) {
      <span class="comment">// 发生异常事务回滚</span>
      transaction.rollback();
      LOG.error(<span class="string">"process failed"</span>, th);
      <span class="keyword">if</span> (th <span class="keyword">instanceof</span> Error) {
        <span class="keyword">throw</span> (Error) th;
      } <span class="keyword">else</span> {
        <span class="keyword">throw</span> <span class="keyword">new</span> EventDeliveryException(th);
      }
    } <span class="keyword">finally</span> {
      <span class="comment">// 关闭事务</span>
      transaction.close();
    }
}
</code></pre><h2 id="BucketWriter分析">BucketWriter分析</h2><p>接下来我们看下BucketWriter的append和flush方法。</p>
<p>append方法：</p>
<pre><code>  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="keyword">append</span>(<span class="keyword">final</span> Event event)
      <span class="keyword">throws</span> IOException, InterruptedException {
    checkAndThrowInterruptedException();
    <span class="comment">// If idleFuture is not null, cancel it before we move forward to avoid a</span>
    <span class="comment">// close call in the middle of the append.</span>
    <span class="keyword">if</span>(idleFuture != <span class="keyword">null</span>) {
      idleFuture.cancel(<span class="keyword">false</span>);
      <span class="comment">// There is still a small race condition - if the idleFuture is already</span>
      <span class="comment">// running, interrupting it can cause HDFS close operation to throw -</span>
      <span class="comment">// so we cannot interrupt it while running. If the future could not be</span>
      <span class="comment">// cancelled, it is already running - wait for it to finish before</span>
      <span class="comment">// attempting to write.</span>
      <span class="keyword">if</span>(!idleFuture.isDone()) {
        <span class="keyword">try</span> {
          idleFuture.get(callTimeout, TimeUnit.MILLISECONDS);
        } <span class="keyword">catch</span> (TimeoutException ex) {
          LOG.warn(<span class="string">"Timeout while trying to cancel closing of idle file. Idle"</span> +
            <span class="string">" file close may have failed"</span>, ex);
        } <span class="keyword">catch</span> (Exception ex) {
          LOG.warn(<span class="string">"Error while trying to cancel closing of idle file. "</span>, ex);
        }
      }
      idleFuture = <span class="keyword">null</span>;
    }

    <span class="comment">// 如果hdfs文件没有被打开</span>
    <span class="keyword">if</span> (!isOpen) {
      <span class="comment">// hdfs已关闭的话抛出异常</span>
      <span class="keyword">if</span> (closed) {
        <span class="keyword">throw</span> <span class="keyword">new</span> BucketClosedException(<span class="string">"This bucket writer was closed and "</span> +
          <span class="string">"this handle is thus no longer valid"</span>);
      }
      <span class="comment">// 打开hdfs文件</span>
      open();
    }

    <span class="comment">// 查看是否需要创建新文件</span>
    <span class="keyword">if</span> (shouldRotate()) {
      <span class="keyword">boolean</span> doRotate = <span class="keyword">true</span>;

      <span class="keyword">if</span> (isUnderReplicated) {
        <span class="keyword">if</span> (maxConsecUnderReplRotations &gt; <span class="number">0</span> &amp;&amp;
            consecutiveUnderReplRotateCount &gt;= maxConsecUnderReplRotations) {
          doRotate = <span class="keyword">false</span>;
          <span class="keyword">if</span> (consecutiveUnderReplRotateCount == maxConsecUnderReplRotations) {
            LOG.error(<span class="string">"Hit max consecutive under-replication rotations ({}); "</span> +
                <span class="string">"will not continue rolling files under this path due to "</span> +
                <span class="string">"under-replication"</span>, maxConsecUnderReplRotations);
          }
        } <span class="keyword">else</span> {
          LOG.warn(<span class="string">"Block Under-replication detected. Rotating file."</span>);
        }
        consecutiveUnderReplRotateCount++;
      } <span class="keyword">else</span> {
        consecutiveUnderReplRotateCount = <span class="number">0</span>;
      }

      <span class="keyword">if</span> (doRotate) {
          <span class="comment">// 如果需要创建新文件的时候会关闭文件，然后再打开新的文件。这里的close方法没有参数，表示可以再次打开新的文件</span>
        close();
        open();
      }
    }

    <span class="comment">// 写event数据</span>
    <span class="keyword">try</span> {
      sinkCounter.incrementEventDrainAttemptCount();
      callWithTimeout(<span class="keyword">new</span> CallRunner&lt;<span class="keyword">Void</span>&gt;() {
        @Override
        <span class="keyword">public</span> <span class="keyword">Void</span> <span class="keyword">call</span>() <span class="keyword">throws</span> Exception {
          <span class="comment">// 真正的写数据使用HDFSWriter的append方法</span>
          writer.<span class="keyword">append</span>(event); <span class="comment">// could block</span>
          <span class="keyword">return</span> <span class="keyword">null</span>;
        }
      });
    } <span class="keyword">catch</span> (IOException e) {
      LOG.warn(<span class="string">"Caught IOException writing to HDFSWriter ({}). Closing file ("</span> +
          bucketPath + <span class="string">") and rethrowing exception."</span>,
          e.getMessage());
      <span class="keyword">try</span> {
        close(<span class="keyword">true</span>);
      } <span class="keyword">catch</span> (IOException e2) {
        LOG.warn(<span class="string">"Caught IOException while closing file ("</span> +
             bucketPath + <span class="string">"). Exception follows."</span>, e2);
      }
      <span class="keyword">throw</span> e;
    }

    <span class="comment">// 文件大小+起来</span>
    processSize += event.getBody().length;
    <span class="comment">// 事件个数+1</span>
    eventCounter++;
    <span class="comment">// 批次数+1</span>
    batchCounter++;

    <span class="comment">// 批次数达到配置的hdfs.batchSize的话调用flush方法</span>
    <span class="keyword">if</span> (batchCounter == batchSize) {
      flush();
    }
}
</code></pre><p>先看下open方法，打开hdfs文件的方法：</p>
<pre><code><span class="keyword">private</span> <span class="keyword">void</span> open() <span class="keyword">throws</span> IOException, InterruptedException {
    <span class="comment">// hdfs文件路径或HDFSWriter没构造的话抛出异常</span>
    <span class="keyword">if</span> ((filePath == <span class="keyword">null</span>) || (writer == <span class="keyword">null</span>)) {
      <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Invalid file settings"</span>);
    }

    <span class="keyword">final</span> Configuration config = <span class="keyword">new</span> Configuration();
    <span class="comment">// disable FileSystem JVM shutdown hook</span>
    config.setBoolean(<span class="string">"fs.automatic.close"</span>, <span class="keyword">false</span>);

    <span class="comment">// Hadoop is not thread safe when doing certain RPC operations,</span>
    <span class="comment">// including getFileSystem(), when running under Kerberos.</span>
    <span class="comment">// open() must be called by one thread at a time in the JVM.</span>
    <span class="comment">// <span class="doctag">NOTE:</span> tried synchronizing on the underlying Kerberos principal previously</span>
    <span class="comment">// which caused deadlocks. See FLUME-1231.</span>
    <span class="keyword">synchronized</span> (staticLock) {
      checkAndThrowInterruptedException();

      <span class="keyword">try</span> {
          <span class="comment">// fileExtensionCounter是一个AtomicLong类型的实例，初始化为当前时间戳的数值</span>
        <span class="comment">// 由于之前分析的，可能存在先关闭文件，然后再次open新文件的情况。所以在同一个BucketWriter类中open方法得到的文件名时间戳仅仅相差1</span>
        <span class="comment">// 得到时间戳counter</span>
        <span class="keyword">long</span> counter = fileExtensionCounter.incrementAndGet();

        <span class="comment">// 最终的文件名加上时间戳，这就是为什么flume生成的文件名会带有时间戳的原因</span>
        <span class="comment">// 这里的fullFileName就是 flume.1437375933234</span>
        String fullFileName = fileName + <span class="string">"."</span> + counter;

        <span class="comment">// 加上后缀名， fullFileName就成了flume.1437375933234.txt</span>
        <span class="keyword">if</span> (fileSuffix != <span class="keyword">null</span> &amp;&amp; fileSuffix.length() &gt; <span class="number">0</span>) {
          fullFileName += fileSuffix;
        } <span class="keyword">else</span> <span class="keyword">if</span> (codeC != <span class="keyword">null</span>) {
          fullFileName += codeC.getDefaultExtension();
        }

        <span class="comment">// 由于没配置inUsePrefix和inUseSuffix。 故这两个属性的值分别为""和".tmp"</span>
        <span class="comment">// buckerPath为 /data/2015/07/20/15/flume.1437375933234.txt.tmp</span>
        bucketPath = filePath + <span class="string">"/"</span> + inUsePrefix
          + fullFileName + inUseSuffix;
        <span class="comment">// targetPath为 /data/2015/07/20/15/flume.1437375933234.txt</span>
        targetPath = filePath + <span class="string">"/"</span> + fullFileName;

        LOG.info(<span class="string">"Creating "</span> + bucketPath);
        callWithTimeout(<span class="keyword">new</span> CallRunner&lt;<span class="keyword">Void</span>&gt;() {
          @Override
          <span class="keyword">public</span> <span class="keyword">Void</span> <span class="keyword">call</span>() <span class="keyword">throws</span> Exception {
            <span class="keyword">if</span> (codeC == <span class="keyword">null</span>) {
              <span class="comment">// Need to get reference to FS using above config before underlying</span>
              <span class="comment">// writer does in order to avoid shutdown hook &amp;</span>
              <span class="comment">// IllegalStateExceptions</span>
              <span class="keyword">if</span>(!mockFsInjected) {
                fileSystem = <span class="keyword">new</span> Path(bucketPath).getFileSystem(
                  config);
              }
              <span class="comment">// 使用HDFSWriter打开文件</span>
              writer.open(bucketPath);
            } <span class="keyword">else</span> {
              <span class="comment">// need to get reference to FS before writer does to</span>
              <span class="comment">// avoid shutdown hook</span>
              <span class="keyword">if</span>(!mockFsInjected) {
                fileSystem = <span class="keyword">new</span> Path(bucketPath).getFileSystem(
                  config);
              }
              <span class="comment">// 使用HDFSWriter打开文件</span>
              writer.open(bucketPath, codeC, compType);
            }
            <span class="keyword">return</span> <span class="keyword">null</span>;
          }
        });
      } <span class="keyword">catch</span> (Exception ex) {
        sinkCounter.incrementConnectionFailedCount();
        <span class="keyword">if</span> (ex <span class="keyword">instanceof</span> IOException) {
          <span class="keyword">throw</span> (IOException) ex;
        } <span class="keyword">else</span> {
          <span class="keyword">throw</span> Throwables.propagate(ex);
        }
      }
    }
    isClosedMethod = getRefIsClosed();
    sinkCounter.incrementConnectionCreatedCount();
    <span class="comment">// 重置各个计数器</span>
    resetCounters();

    <span class="comment">// 开线程处理hdfs.rollInterval配置的参数，多长时间后调用close方法</span>
    <span class="keyword">if</span> (rollInterval &gt; <span class="number">0</span>) {
      Callable&lt;<span class="keyword">Void</span>&gt; action = <span class="keyword">new</span> Callable&lt;<span class="keyword">Void</span>&gt;() {
        <span class="keyword">public</span> <span class="keyword">Void</span> <span class="keyword">call</span>() <span class="keyword">throws</span> Exception {
          LOG.debug(<span class="string">"Rolling file ({}): Roll scheduled after {} sec elapsed."</span>,
              bucketPath, rollInterval);
          <span class="keyword">try</span> {
            <span class="comment">// Roll the file and remove reference from sfWriters map.</span>
            close(<span class="keyword">true</span>);
          } <span class="keyword">catch</span>(Throwable t) {
            LOG.error(<span class="string">"Unexpected error"</span>, t);
          }
          <span class="keyword">return</span> <span class="keyword">null</span>;
        }
      };
      <span class="comment">// 以秒为单位在这里指定。将这个线程执行的结果赋值给timedRollFuture这个属性</span>
      timedRollFuture = timedRollerPool.schedule(action, rollInterval,
          TimeUnit.SECONDS);
    }

    isOpen = <span class="keyword">true</span>;
}
</code></pre><p>flush方法，只会在close和append方法(处理的事件数等于批次数)中被调用：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> flush() <span class="keyword">throws</span> IOException, InterruptedException {
    checkAndThrowInterruptedException();
    <span class="keyword">if</span> (!isBatchComplete()) { <span class="comment">//isBatchComplete判断batchCount是否等于0。 所以这里只要batchCount不为0，那么执行下去</span>
      doFlush(); <span class="comment">// doFlush方法会调用HDFSWriter的sync方法，并且将batchCount设置为0</span>

      <span class="comment">// idleTimeout没有配置，以下代码不会执行</span>
      <span class="keyword">if</span>(idleTimeout &gt; <span class="number">0</span>) {
        <span class="comment">// if the future exists and couldn't be cancelled, that would mean it has already run</span>
        <span class="comment">// or been cancelled</span>
        <span class="keyword">if</span>(idleFuture == <span class="keyword">null</span> || idleFuture.cancel(<span class="keyword">false</span>)) {
          Callable&lt;<span class="keyword">Void</span>&gt; idleAction = <span class="keyword">new</span> Callable&lt;<span class="keyword">Void</span>&gt;() {
            <span class="keyword">public</span> <span class="keyword">Void</span> <span class="keyword">call</span>() <span class="keyword">throws</span> Exception {
              LOG.info(<span class="string">"Closing idle bucketWriter {} at {}"</span>, bucketPath,
                System.currentTimeMillis());
              <span class="keyword">if</span> (isOpen) {
                close(<span class="keyword">true</span>);
              }
              <span class="keyword">return</span> <span class="keyword">null</span>;
            }
          };
          idleFuture = timedRollerPool.schedule(idleAction, idleTimeout,
              TimeUnit.SECONDS);
        }
      }
    }
}
</code></pre><p>close方法：</p>
<pre><code>  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> close(<span class="keyword">boolean</span> callCloseCallback)
        <span class="keyword">throws</span> IOException, InterruptedException {
    checkAndThrowInterruptedException();
    <span class="keyword">try</span> {
      <span class="comment">// close的时候先执行flush方法，清空batchCount，并调用HDFSWriter的sync方法</span>
      flush();
    } <span class="keyword">catch</span> (IOException e) {
      LOG.warn(<span class="string">"pre-close flush failed"</span>, e);
    }
    <span class="keyword">boolean</span> failedToClose = <span class="keyword">false</span>;
    LOG.info(<span class="string">"Closing {}"</span>, bucketPath);
    <span class="comment">// 创建一个关闭线程，这个线程会调用HDFSWriter的close方法</span>
    CallRunner&lt;<span class="keyword">Void</span>&gt; closeCallRunner = createCloseCallRunner();
    <span class="keyword">if</span> (isOpen) { <span class="comment">// 如果文件还开着</span>
      <span class="keyword">try</span> {
          <span class="comment">// 执行HDFSWriter的close方法</span>
        callWithTimeout(closeCallRunner);
        sinkCounter.incrementConnectionClosedCount();
      } <span class="keyword">catch</span> (IOException e) {
        LOG.warn(
          <span class="string">"failed to close() HDFSWriter for file ("</span> + bucketPath +
            <span class="string">"). Exception follows."</span>, e);
        sinkCounter.incrementConnectionFailedCount();
        failedToClose = <span class="keyword">true</span>;
        <span class="comment">// 关闭文件失败的话起个线程，retryInterval秒后继续执行</span>
        <span class="keyword">final</span> Callable&lt;<span class="keyword">Void</span>&gt; scheduledClose =
          createScheduledCloseCallable(closeCallRunner);
        timedRollerPool.schedule(scheduledClose, retryInterval,
          TimeUnit.SECONDS);
      }
      isOpen = <span class="keyword">false</span>;
    } <span class="keyword">else</span> {
      LOG.info(<span class="string">"HDFSWriter is already closed: {}"</span>, bucketPath);
    }

    <span class="comment">// timedRollFuture就是根据hdfs.rollInterval配置生成的一个属性。如果hdfs.rollInterval配置为0，那么不会执行以下代码</span>
    <span class="comment">// 因为要close文件，所以如果开启了hdfs.rollInterval等待时间到了flush文件，由于文件已经关闭，再次关闭会有问题</span>
    <span class="comment">// 所以这里取消timedRollFuture线程的执行</span>
    <span class="keyword">if</span> (timedRollFuture != <span class="keyword">null</span> &amp;&amp; !timedRollFuture.isDone()) {
      timedRollFuture.cancel(<span class="keyword">false</span>); <span class="comment">// do not cancel myself if running!</span>
      timedRollFuture = <span class="keyword">null</span>;
    }

    <span class="comment">// 没有配置hdfs.idleTimeout， 不会执行</span>
    <span class="keyword">if</span> (idleFuture != <span class="keyword">null</span> &amp;&amp; !idleFuture.isDone()) {
      idleFuture.cancel(<span class="keyword">false</span>); <span class="comment">// do not cancel myself if running!</span>
      idleFuture = <span class="keyword">null</span>;
    }

    <span class="comment">// 重命名文件，如果报错了，不会重命名文件</span>
    <span class="keyword">if</span> (bucketPath != <span class="keyword">null</span> &amp;&amp; fileSystem != <span class="keyword">null</span> &amp;&amp; !failedToClose) {
      <span class="comment">// 将 /data/2015/07/20/15/flume.1437375933234.txt.tmp 重命名为 /data/2015/07/20/15/flume.1437375933234.txt</span>
      renameBucket(bucketPath, targetPath, fileSystem);
    }
    <span class="keyword">if</span> (callCloseCallback) { <span class="comment">// callCloseCallback是close方法的参数</span>

      <span class="comment">// 调用关闭文件的回调函数，也就是BucketWriter的onCloseCallback属性</span>
      <span class="comment">// 这个onCloseCallback属性就是在HDFSEventSink里的回调函数closeCallback。 用来处理sfWriters.remove(bucketPath);</span>
      <span class="comment">// 如果onCloseCallback属性为true，那么说明这个BucketWriter已经不会再次open新的文件了。生命周期已经到了。</span>
      <span class="comment">// onCloseCallback只有在append方法中调用shouldRotate方法的时候需要close文件的时候才会传入false，其他情况都是true</span>
      runCloseAction(); 

      closed = <span class="keyword">true</span>;
    }
}
</code></pre><p>再回过头来看下append方法里的shouldRotate方法，shouldRotate方法执行下去的话会关闭文件然后再次打开新的文件：</p>
<pre><code><span class="keyword">private</span> <span class="function"><span class="keyword">boolean</span> <span class="title">shouldRotate</span><span class="params">()</span> </span>{
    <span class="keyword">boolean</span> doRotate = <span class="keyword">false</span>;

    <span class="comment">// 调用HDFSWriter的isUnderReplicated方法，用来判断当前hdfs文件是否正在复制。</span>
    <span class="keyword">if</span> (writer.isUnderReplicated()) {
      <span class="keyword">this</span>.isUnderReplicated = <span class="keyword">true</span>;
      doRotate = <span class="keyword">true</span>;
    } <span class="keyword">else</span> {
      <span class="keyword">this</span>.isUnderReplicated = <span class="keyword">false</span>;
    }

    <span class="comment">// rollCount就是配置的hdfs.rollCount。 eventCounter事件数达到rollCount之后，会close文件，然后创建新的文件</span>
    <span class="keyword">if</span> ((rollCount &gt; <span class="number">0</span>) &amp;&amp; (rollCount &lt;= eventCounter)) {
      LOG.debug(<span class="string">"rolling: rollCount: {}, events: {}"</span>, rollCount, eventCounter);
      doRotate = <span class="keyword">true</span>;
    }

    <span class="comment">// rollSize就是配置的hdfs.rollSize。processSize是每个事件加起来的文件大小。当processSize超过rollSize的时候，会close文件，然后创建新的文件</span>
    <span class="keyword">if</span> ((rollSize &gt; <span class="number">0</span>) &amp;&amp; (rollSize &lt;= processSize)) {
      LOG.debug(<span class="string">"rolling: rollSize: {}, bytes: {}"</span>, rollSize, processSize);
      doRotate = <span class="keyword">true</span>;
    }

    <span class="keyword">return</span> doRotate;
}
</code></pre><h2 id="HDFSWriter分析">HDFSWriter分析</h2><p>每个BucketWriter中对应只有一个HDFSWriter。</p>
<p>HDFSWriter是一个接口，有3个具体的实现类，分别是：HDFSDataStream，HDFSSequenceFile和HDFSCompressedDataStream。分别对应fileType为DataStream，SequenceFile和CompressedStream。</p>
<p>我们以HDFSDataStream为例，分析一下在BucketWriter中用到的HDFSWriter的一些方法：</p>
<p>append方法，写hdfs文件：</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">append</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IOException </span>{
    <span class="comment">// 非常简单，直接使用serializer的write方法</span>
    <span class="comment">// serializer是org.apache.flume.serialization.EventSerializer接口的实现类</span>
    <span class="comment">// 默认的Serializer是BodyTextEventSerializer</span>
    serializer.write(e);
}
</code></pre><p>open方法：</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">open</span><span class="params">(String filePath)</span> <span class="keyword">throws</span> IOException </span>{
    Configuration conf = <span class="keyword">new</span> Configuration();
    <span class="comment">// 构造hdfs路径</span>
    Path dstPath = <span class="keyword">new</span> Path(filePath);
    FileSystem hdfs = getDfs(conf, dstPath);
    <span class="comment">// 调用doOpen方法</span>
    doOpen(conf, dstPath, hdfs);
}

<span class="keyword">protected</span> <span class="function"><span class="keyword">void</span> <span class="title">doOpen</span><span class="params">(Configuration conf,
    Path dstPath, FileSystem hdfs)</span> <span class="keyword">throws</span>
        IOException </span>{
    <span class="keyword">if</span>(useRawLocalFileSystem) {
      <span class="keyword">if</span>(hdfs <span class="keyword">instanceof</span> LocalFileSystem) {
        hdfs = ((LocalFileSystem)hdfs).getRaw();
      } <span class="keyword">else</span> {
        logger.warn(<span class="string">"useRawLocalFileSystem is set to true but file system "</span> +
            <span class="string">"is not of type LocalFileSystem: "</span> + hdfs.getClass().getName());
      }
    }

    <span class="keyword">boolean</span> appending = <span class="keyword">false</span>;
    <span class="comment">// 构造FSDataOutputStream，作为属性outStream</span>
    <span class="keyword">if</span> (conf.getBoolean(<span class="string">"hdfs.append.support"</span>, <span class="keyword">false</span>) == <span class="keyword">true</span> &amp;&amp; hdfs.isFile
            (dstPath)) {
      outStream = hdfs.append(dstPath);
      appending = <span class="keyword">true</span>;
    } <span class="keyword">else</span> {
      outStream = hdfs.create(dstPath);
    }

    <span class="comment">// 初始化Serializer</span>
    serializer = EventSerializerFactory.getInstance(
        serializerType, serializerContext, outStream);
    <span class="keyword">if</span> (appending &amp;&amp; !serializer.supportsReopen()) {
      outStream.close();
      serializer = <span class="keyword">null</span>;
      <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"serializer ("</span> + serializerType +
          <span class="string">") does not support append"</span>);
    }

    <span class="comment">// must call superclass to check for replication issues</span>
    registerCurrentStream(outStream, hdfs, dstPath);

    <span class="keyword">if</span> (appending) {
      serializer.afterReopen();
    } <span class="keyword">else</span> {
      serializer.afterCreate();
    }
}
</code></pre><p>close方法：</p>
<pre><code><span class="at_rule">@<span class="keyword">Override</span>
public void <span class="function">close</span>() throws IOException </span>{
    <span class="tag">serializer</span><span class="class">.flush</span>();
    <span class="tag">serializer</span><span class="class">.beforeClose</span>();
    <span class="tag">outStream</span><span class="class">.flush</span>();
    <span class="tag">outStream</span><span class="class">.sync</span>();
    <span class="tag">outStream</span><span class="class">.close</span>();

    <span class="tag">unregisterCurrentStream</span>();
}
</code></pre><p>sync方法：</p>
<pre><code><span class="at_rule">@<span class="keyword">Override</span>
public void <span class="function">sync</span>() throws IOException </span>{
    <span class="tag">serializer</span><span class="class">.flush</span>();
    <span class="tag">outStream</span><span class="class">.flush</span>();
    <span class="tag">outStream</span><span class="class">.sync</span>();
}
</code></pre><p>isUnderReplicated方法，在AbstractHDFSWriter中定义：</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isUnderReplicated</span><span class="params">()</span> </span>{
    <span class="keyword">try</span> {
      <span class="comment">// 得到目前文件replication后的块数</span>
      <span class="keyword">int</span> numBlocks = getNumCurrentReplicas();
      <span class="keyword">if</span> (numBlocks == -<span class="number">1</span>) {
        <span class="keyword">return</span> <span class="keyword">false</span>;
      }
      <span class="keyword">int</span> desiredBlocks;
      <span class="keyword">if</span> (configuredMinReplicas != <span class="keyword">null</span>) {
        <span class="comment">// 如果配置了hdfs.minBlockReplicas</span>
        desiredBlocks = configuredMinReplicas;
      } <span class="keyword">else</span> {
        <span class="comment">// 没配置hdfs.minBlockReplicas的话直接从hdfs配置中拿</span>
        desiredBlocks = getFsDesiredReplication();
      }
      <span class="comment">// 如果当前复制的块比期望要复制的块数字要小的话，返回true</span>
      <span class="keyword">return</span> numBlocks &lt; desiredBlocks;
    } <span class="keyword">catch</span> (IllegalAccessException e) {
      logger.<span class="keyword">error</span>(<span class="string">"Unexpected error while checking replication factor"</span>, e);
    } <span class="keyword">catch</span> (InvocationTargetException e) {
      logger.<span class="keyword">error</span>(<span class="string">"Unexpected error while checking replication factor"</span>, e);
    } <span class="keyword">catch</span> (IllegalArgumentException e) {
      logger.<span class="keyword">error</span>(<span class="string">"Unexpected error while checking replication factor"</span>, e);
    }
    <span class="keyword">return</span> <span class="keyword">false</span>;
}
</code></pre><h2 id="总结">总结</h2><p>hdfs.rollInterval，hdfs.rollSize，hdfs.rollCount，hdfs.minBlockReplicas，hdfs.batchSize这5个配置影响着hdfs文件的关闭。</p>
<p><strong>注意，这5个配置影响的是一个hdfs文件，是一个hdfs文件。当hdfs文件关闭的时候，这些配置指标会重新开始计算。因为BucketWriter中的open方法里会调用resetCounters方法，这个方法会重置计数器。而基于hdfs.rollInterval的timedRollFuture线程返回值是在close方法中被销毁的。因此，只要close文件，并且open新文件的时候，这5个属性都会重新开始计算。</strong></p>
<p>hdfs.rollInterval与时间有关，当时间达到hdfs.rollInterval配置的秒数，那么会close文件。</p>
<p>hdfs.rollSize与每个event的字节大小有关，当一个一个event的字节相加起来大于等于hdfs.rollSize的时候，那么会close文件。</p>
<p>hdfs.rollCount与事件的个数有关，当事件个数大于等于hdfs.rollCount的时候，那么会close文件。</p>
<p>hdfs.batchSize表示当事件添加到hdfs.batchSize个的时候，也就是说HDFS Sink每次会拿hdfs.batchSize个事件，而且这些所有的事件都写进了同一个hdfs文件，这才会触发本次条件，并且其他4个配置都未达成条件。然后会close文件。</p>
<p>hdfs.minBlockReplicas表示期望hdfs对文件最小的复制块数。所以有时候我们配置了hdfs.rollInterval，hdfs.rollSize，hdfs.rollCount这3个参数，并且这3个参数都没有符合条件，但是还是生成了多个文件，这就是因为这个参数导致的，而且这个参数的优先级比hdfs.rollSize，hdfs.rollCount要高。</p>
]]></content>
    <summary type="html">
    <![CDATA[分析Flume HDFSSink写hdfs文件过程]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Flume几个比较有用的功能(用到新功能后会更新文章)]]></title>
    <link href="http://fangjian0423.github.io/2015/07/14/flume-notes/"/>
    <id>http://fangjian0423.github.io/2015/07/14/flume-notes/</id>
    <published>2015-07-14T14:23:23.000Z</published>
    <updated>2015-12-20T16:52:53.000Z</updated>
    <content type="html"><![CDATA[<p>根据项目的经验，介绍几个flume比较有用的功能。</p>
<h2 id="ChannelSelector功能">ChannelSelector功能</h2><p>flume内置的ChannelSelector有两种，分别是Replicating和Multiplexing。</p>
<p>Replicating类型的ChannelSelector会针对每一个Event，拷贝到所有的Channel中，这是默认的ChannelSelector。</p>
<p>replicating类型的ChannelSelector例子如下：</p>
<pre><code>a1.sources = r1
a1.channels = c1 c2 <span class="preprocessor"># 如果有<span class="number">100</span>个Event，那么c1和c2中都会有这<span class="number">100</span>个事件</span>

a1.channels.c1.type = memory
a1.channels.c1.capacity = <span class="number">1000</span>
a1.channels.c1.transactionCapacity = <span class="number">100</span>


a1.channels.c2.type = memory
a1.channels.c2.capacity = <span class="number">1000</span>
a1.channels.c2.transactionCapacity = <span class="number">100</span>
</code></pre><p>Multiplexing类型的ChannelSelector会根据Event中Header中的某个属性决定分发到哪个Channel。</p>
<p>multiplexing类型的ChannelSelector例子如下：</p>
<pre><code><span class="label">a1.sources</span> = <span class="literal">r1</span>

<span class="label">a1.sources.source1.selector.type</span> = <span class="keyword">multiplexing
</span><span class="label">a1.sources.source1.selector.header</span> = validation # 以header中的validation对应的值作为条件
<span class="label">a1.sources.source1.selector.mapping.SUCCESS</span> = <span class="literal">c2</span> # 如果header中validation的值为SUCCESS，使用<span class="literal">c2</span>这个channel
<span class="label">a1.sources.source1.selector.mapping.FAIL</span> = <span class="literal">c1</span> # 如果header中validation的值为FAIL，使用<span class="literal">c1</span>这个channel
<span class="label">a1.sources.source1.selector.default</span> = <span class="literal">c1</span> # 默认使用<span class="literal">c1</span>这个channel
</code></pre><h2 id="Sink的Serializer">Sink的Serializer</h2><p>HDFS Sink， HBase Sink，ElasticSearch Sink都支持Serializer功能。</p>
<p>Serializer的作用是sink写入的时候，做一些处理。</p>
<h3 id="HDFS_Sink的Serializer">HDFS Sink的Serializer</h3><p>在<a href="http://fangjian0423.github.io/2015/06/23/flume-sink/">Flume Sink组件分析中</a>一文中，分析过了HDFS写文件的时候使用BucketWriter写数据，BucketWriter内部使用HDFSWriter属性写数据。HDFSWriter是一个处理hdfs文件的接口。</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">HDFSWriter</span> <span class="keyword">extends</span> <span class="title">Configurable</span> </span>{

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(String filePath)</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(String filePath, CompressionCodec codec,
      CompressionType cType)</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sync</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isUnderReplicated</span><span class="params">()</span></span>;

}
</code></pre><p>HDFSWriter的结构如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/flume-note1.png" alt=""></p>
<p>hdfs sink的fileType配置如下：</p>
<p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/flume-note2.png" alt=""></p>
<p>HDFSDataStream对应DataStream类型，HDFSCompressedDataStream对应CompressedStream，HDFSSequenceFile对应SequenceFile。</p>
<p>以DataStream为例，HDFSDataStream的append方法如下：</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">append</span><span class="params">(Event e)</span> <span class="keyword">throws</span> IOException </span>{
    serializer.write(e);
}
</code></pre><p>这个serializer是HDFSDataStream的属性。是EventSerializer接口类型的属性。HDFSDataStream的append很简单，直接调用serializer的writer方法。</p>
<p>HDFS Sink的Serializer都需要实现EventSerializer接口：</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">EventSerializer</span> </span>{

  <span class="keyword">public</span> <span class="keyword">static</span> String CTX_PREFIX = <span class="string">"serializer."</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterCreate</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterReopen</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Event event)</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flush</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">beforeClose</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">supportsReopen</span><span class="params">()</span></span>;

  <span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Builder</span> </span>{
    <span class="function"><span class="keyword">public</span> EventSerializer <span class="title">build</span><span class="params">(Context context, OutputStream out)</span></span>;
  }

}
</code></pre><p>HDFS Sink默认的serializer是BodyTextEventSerializer类，不配置的话也是使用这个Serializer。</p>
<p>BodyTextEventSerializer的writer方法：</p>
<pre><code>@Override
<span class="keyword">public</span> <span class="keyword">void</span> <span class="keyword">write</span>(Event e) <span class="keyword">throws</span> IOException {
  out.<span class="keyword">write</span>(e.getBody());
  <span class="keyword">if</span> (appendNewline) {
    out.<span class="keyword">write</span>(<span class="string">'\n'</span>);
  }
}
</code></pre><p>这就是为什么hdfs sink写数据的时候写完会自动换行的原因。</p>
<p>当然，我们可以定义自定义的Serializer来满足自身的要求。</p>
<h3 id="HBase_Sink的Serializer">HBase Sink的Serializer</h3><p>HBase Sink的Serializer都需要实现HbaseEventSerializer接口。</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">HbaseEventSerializer</span> <span class="keyword">extends</span> <span class="title">Configurable</span>,
            <span class="title">ConfigurableComponent</span> </span>{

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(Event event, <span class="keyword">byte</span>[] columnFamily)</span></span>;

  <span class="function"><span class="keyword">public</span> List&lt;Row&gt; <span class="title">getActions</span><span class="params">()</span></span>;

  <span class="function"><span class="keyword">public</span> List&lt;Increment&gt; <span class="title">getIncrements</span><span class="params">()</span></span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;

}
</code></pre><p>HBaseSink的process方法关键代码：</p>
<pre><code>for <span class="list">(<span class="comment">; i &lt; batchSize; i++) {</span>
    Event event = channel.take<span class="list">()</span><span class="comment">;</span>
    if <span class="list">(<span class="keyword">event</span> == null)</span> {
      if <span class="list">(<span class="keyword">i</span> == <span class="number">0</span>)</span> {
        status = Status.BACKOFF<span class="comment">;</span>
        sinkCounter.incrementBatchEmptyCount<span class="list">()</span><span class="comment">;</span>
      } else {
        sinkCounter.incrementBatchUnderflowCount<span class="list">()</span><span class="comment">;</span>
      }
      break<span class="comment">;</span>
    } else {
      serializer.initialize<span class="list">(<span class="keyword">event</span>, columnFamily)</span><span class="comment">;</span>
      actions.addAll<span class="list">(<span class="keyword">serializer</span>.getActions<span class="list">()</span>)</span><span class="comment">;</span>
      incs.addAll<span class="list">(<span class="keyword">serializer</span>.getIncrements<span class="list">()</span>)</span><span class="comment">;</span>
    }
  }</span>
</code></pre><p>actions和incs都加入了serializer里的actions和increments。之后会commit这里的actions和increments数据。</p>
<p>HBase默认的Serializer是org.apache.flume.sink.hbase.SimpleHbaseEventSerializer。</p>
<p>我们也可以根据需求定义自定义的HbaseEventSerializer，需要注意的是getActions和getIncrements方法。</p>
<p>HBase Sink会加入这2个方法的返回值，并写入到HBase。</p>
<h3 id="Elasticsearch_Sink的Serializer">Elasticsearch Sink的Serializer</h3><p>Elasticsearch Sink的Serializer都需要实现ElasticSearchEventSerializer接口。</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ElasticSearchEventSerializer</span> <span class="keyword">extends</span> <span class="title">Configurable</span>,
                <span class="title">ConfigurableComponent</span> </span>{

  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Charset charset = Charset.defaultCharset();

  <span class="function"><span class="keyword">abstract</span> BytesStream <span class="title">getContentBuilder</span><span class="params">(Event event)</span> <span class="keyword">throws</span> IOException</span>;
}
</code></pre><p>默认的Serializer是org.apache.flume.sink.elasticsearch.ElasticSearchLogStashEventSerializer。</p>
<p>同样，我们也可以根据需求定义自定义的ElasticSearchEventSerialize，就不分析了。</p>
<h2 id="SinkGroup">SinkGroup</h2><p>这个功能暂时还没用到，不过以后可能会用到。</p>
<p>Sink Group的作用是把多个Sink合并成一个。这样的话Sink处理器会根据配置的类型来决定如何使用Sink。比如可以使用load balance，failover策略，或者可以使用自定义的策略来处理。</p>
<p><a href="https://flume.apache.org/FlumeUserGuide.html#default-sink-processor" target="_blank" rel="external">官方文档Sink Group</a>已经写的很清楚了。</p>
<h2 id="其它">其它</h2><p>目前还正在用Flume开发一些功能，后续可能会使用一些新的功能，到时候回头更新这篇文章。</p>
]]></content>
    <summary type="html">
    <![CDATA[根据项目的经验，介绍几个flume比较有用的功能...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Elasticsearch入门]]></title>
    <link href="http://fangjian0423.github.io/2015/07/12/elasticsearch-tutorials/"/>
    <id>http://fangjian0423.github.io/2015/07/12/elasticsearch-tutorials/</id>
    <published>2015-07-11T19:06:59.000Z</published>
    <updated>2015-12-20T17:07:52.000Z</updated>
    <content type="html"><![CDATA[<p>之前搭建logstash的时候使用过elasticsearch。 刚好最近在公司也用到了es，写篇水文记录一下也当做笔记吧。</p>
<p><a href="https://www.elastic.co/products/elasticsearch" target="_blank" rel="external">Elasticsearch</a>是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，使用RESTful web暴露接口。</p>
<p>它有许多特性，比如以下几个属性：</p>
<p>1.实时数据<br>2.实时分析<br>3.分布式设计<br>4.高可用性<br>5.全文搜索<br>6.面向文档</p>
<h2 id="索引">索引</h2><p>索引Index是es中的一个存储数据的地方。相当于关系型数据库中的数据库。</p>
<p>创建一个员工索引的例子如下，创建索引还有很多选项，就不一一说明了：</p>
<pre><code>POST <span class="variable">$HOST</span>/employee

{
  <span class="string">"mappings"</span>: {
    <span class="string">"employee"</span>: {
      <span class="string">"_ttl"</span>: {
        <span class="string">"enabled"</span>: true,
        <span class="string">"default"</span>: <span class="string">"5d"</span>
      },
      <span class="string">"_timestamp"</span>: {
        <span class="string">"enabled"</span>: true,
        <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
      },
      <span class="string">"properties"</span>: {
        <span class="string">"name"</span>: {
          <span class="string">"type"</span>: <span class="string">"string"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>,
          <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>,
          <span class="string">"index_options"</span>: <span class="string">"docs"</span>
        },
        <span class="string">"birth_date"</span>: {
          <span class="string">"type"</span>: <span class="string">"date"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>,
          <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>,
          <span class="string">"index_options"</span>: <span class="string">"docs"</span>,
          <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
        },
        <span class="string">"age"</span>: {
          <span class="string">"type"</span>: <span class="string">"date"</span>,
          <span class="string">"store"</span>: <span class="string">"no"</span>,
          <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>,
          <span class="string">"index_options"</span>: <span class="string">"docs"</span>,
          <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
        }
      }
    }
  }
}
</code></pre><p>索引创建完之后还可以修改(添加一个hobby属性)，需要注意的是，修改mapping不允许修改属性的类型：</p>
<pre><code>PUT <span class="variable">$HOST</span>/employee/employee/_mapping

{
<span class="string">"employee"</span>: {
    <span class="string">"properties"</span>: {
        <span class="string">"name"</span>: {
            <span class="string">"type"</span>: <span class="string">"string"</span>,
            <span class="string">"store"</span>: <span class="string">"no"</span>,
            <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>,
            <span class="string">"index_options"</span>: <span class="string">"docs"</span>
        },
        <span class="string">"birth_date"</span>: {
            <span class="string">"type"</span>: <span class="string">"date"</span>,
            <span class="string">"store"</span>: <span class="string">"no"</span>,
            <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>,
            <span class="string">"index_options"</span>: <span class="string">"docs"</span>,
            <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
        },
        <span class="string">"age"</span>: {
            <span class="string">"type"</span>: <span class="string">"date"</span>,
            <span class="string">"store"</span>: <span class="string">"no"</span>,
            <span class="string">"index"</span>: <span class="string">"not_analyzed"</span>,
            <span class="string">"index_options"</span>: <span class="string">"docs"</span>,
            <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss"</span>
        },
        <span class="string">"hobby"</span> : {
            <span class="string">"type"</span> : <span class="string">"string"</span>,
            <span class="string">"index_options"</span>: <span class="string">"docs"</span>
        }
    }
}
}
</code></pre><h2 id="文档">文档</h2><p>es存储的数据叫做文档，文档存储在索引中。 每个文档都有4个元数据，分别是_id, _type，_index和_version。</p>
<p>_id代表文档的唯一标识符。</p>
<p>_type表示文档代表的对象种类。</p>
<p>_index表示文档存储在哪个索引。</p>
<p>_version表示文档的版本，文档被修改过一次，_version就会+1。</p>
<p>在员工索引中创建文档：</p>
<pre><code>POST <span class="variable">$HOST</span>/employee/employee

{
    <span class="string">"name"</span>: <span class="string">"format"</span>,
    <span class="string">"age"</span>: <span class="number">100</span>,
    <span class="string">"birth_date"</span>: <span class="string">"1900-01-01 00:00:00"</span>
}
</code></pre><p>返回：</p>
<pre><code>{
    "<span class="attribute">_index</span>": <span class="value"><span class="string">"employee"</span></span>,
    "<span class="attribute">_type</span>": <span class="value"><span class="string">"employee"</span></span>,
    "<span class="attribute">_id</span>": <span class="value"><span class="string">"AU5-epuwslU6QVfs_UoX"</span></span>,
    "<span class="attribute">_version</span>": <span class="value"><span class="number">1</span></span>,
    "<span class="attribute">created</span>": <span class="value"><span class="literal">true</span>
</span>}
</code></pre><p>修改文档：</p>
<pre><code>POST <span class="variable">$HOST</span>/employee/employee/AU5-epuwslU6QVfs_UoX

{
    <span class="string">"name"</span>: <span class="string">"format"</span>,
    <span class="string">"age"</span>: <span class="number">200</span>,
    <span class="string">"birth_date"</span>: <span class="string">"1900-01-01 00:00:00"</span>
}
</code></pre><p>返回：</p>
<pre><code>{
    "<span class="attribute">_index</span>": <span class="value"><span class="string">"employee"</span></span>,
    "<span class="attribute">_type</span>": <span class="value"><span class="string">"employee"</span></span>,
    "<span class="attribute">_id</span>": <span class="value"><span class="string">"AU5-epuwslU6QVfs_UoX"</span></span>,
    "<span class="attribute">_version</span>": <span class="value"><span class="number">2</span></span>,
    "<span class="attribute">created</span>": <span class="value"><span class="literal">false</span>
</span>}
</code></pre><p>删除文档：</p>
<pre><code>DELETE <span class="variable">$HOST</span>/employee/employee/AU5-epuwslU6QVfs_UoX
</code></pre><p>返回：</p>
<pre><code>{
    "<span class="attribute">found</span>": <span class="value"><span class="literal">true</span></span>,
    "<span class="attribute">_index</span>": <span class="value"><span class="string">"employee"</span></span>,
    "<span class="attribute">_type</span>": <span class="value"><span class="string">"employee"</span></span>,
    "<span class="attribute">_id</span>": <span class="value"><span class="string">"AU5-epuwslU6QVfs_UoX"</span></span>,
    "<span class="attribute">_version</span>": <span class="value"><span class="number">3</span>
</span>}
</code></pre><h2 id="总结">总结</h2><p>写了篇水文记录一下es，es还有很多很强大的功能，比如一些query，filter，aggregations等。官方文档上已经写的非常清楚了。这里就不讲了。  - -||</p>
]]></content>
    <summary type="html">
    <![CDATA[Elasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，使用RESTful web暴露接口 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="elasticsearch" scheme="http://fangjian0423.github.io/tags/elasticsearch/"/>
    
      <category term="elasticsearch" scheme="http://fangjian0423.github.io/categories/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Flume运行过程源码分析]]></title>
    <link href="http://fangjian0423.github.io/2015/07/07/flume-run/"/>
    <id>http://fangjian0423.github.io/2015/07/07/flume-run/</id>
    <published>2015-07-07T15:23:23.000Z</published>
    <updated>2015-12-20T16:54:32.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Flume运行过程分析">Flume运行过程分析</h2><h3 id="入口Application">入口Application</h3><p>Flume运行的主类是org.apache.flume.node.Application的main方法。</p>
<p>命令行args参数如下：</p>
<pre><code><span class="literal">-</span><span class="literal">-</span><span class="comment">conf</span><span class="literal">-</span><span class="comment">file</span> <span class="comment">YourConfigFile</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">name</span> <span class="comment">AgentName</span>
</code></pre><p>main方法里会先判断参数中是否有z或者zkConnString，如果配置了这两个参数中其中一个的话，那么会使用PollingZooKeeperConfigurationProvider或StaticZooKeeperConfigurationProvider配置。否则使用PollingPropertiesFileConfigurationProvider或PropertiesFileConfigurationProvider配置。</p>
<p>一般我们只传–conf-file和–name，那么就会使用PollingPropertiesFileConfigurationProvider进行配置。</p>
<pre><code><span class="comment">// EventBus，监听者设计模式</span>
EventBus eventBus = <span class="keyword">new</span> EventBus(agentName + <span class="string">"-event-bus"</span>);
<span class="comment">// 构造PollingPropertiesFileConfigurationProvider，会处理配置文件相关的信息</span>
PollingPropertiesFileConfigurationProvider configurationProvider =
        <span class="keyword">new</span> PollingPropertiesFileConfigurationProvider(
          agentName, configurationFile, eventBus, <span class="number">30</span>);
components.add(configurationProvider);
application = <span class="keyword">new</span> Application(components);
<span class="comment">// EventBus注册application</span>
eventBus.<span class="keyword">register</span>(application);

...

application.start();
</code></pre><p>Application内部有个handleConfigurationEvent方法，使用Subscribe注解，EventBus中被使用到：</p>
<pre><code><span class="annotation">@Subscribe</span>
<span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function"><span class="keyword">void</span> <span class="title">handleConfigurationEvent</span><span class="params">(MaterializedConfiguration conf)</span> </span>{
  stopAllComponents();
  startAllComponents(conf);
}
</code></pre><p>接下来看Application的start方法：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function"><span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>{
    <span class="comment">// 使用生命周期组件管理器进行管理，这里其实只有1个组件，那就是PollingPropertiesFileConfigurationProvider</span>
    <span class="keyword">for</span>(LifecycleAware component : components) {
      <span class="comment">// 管理器管理各个组件的时候会传入2个参数，分别是管理策略(自动重启策略)和所需状态(START状态)</span>
      supervisor.supervise(component,
          <span class="keyword">new</span> SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);
    }
}
</code></pre><p>LifecycleSupervisor生命周期管理者的supervise方法的主要代码：</p>
<pre><code>// Supervisoree是一个带有status和policy这2个属性的封装类。这2个属性分别表示状态和管理策略。其中管理策略只有2种策略，分别是AlwaysRestartPolicy(自动重启策略)和OnceOnlyPolicy(只启动一次策略)；状态表示这个组件状态，状态Status中包括首次发生，最后发生，失败次数，目标状态等属性。
Supervisoree process = new Supervisoree()<span class="comment">;</span>
process.status = new Status()<span class="comment">;</span>

process.policy = policy<span class="comment">;</span>
// 所需状态初始化成START
process.status.desiredState = desiredState<span class="comment">;</span>
process.status.error = false<span class="comment">;</span>

 // 起一个监控线程。 将Supervisoree和组件参数传入
MonitorRunnable monitorRunnable = new MonitorRunnable()<span class="comment">;</span>
monitorRunnable.lifecycleAware = lifecycleAware<span class="comment">;</span>
monitorRunnable.supervisoree = process<span class="comment">;</span>
monitorRunnable.monitorService = monitorService<span class="comment">;</span>

supervisedProcesses.put(lifecycleAware, process)<span class="comment">;</span>

ScheduledFuture&lt;?&gt; future = monitorService.scheduleWithFixedDelay(
    monitorRunnable, 0, 3, TimeUnit.SECONDS)<span class="comment">;</span>
monitorFutures.put(lifecycleAware, future)<span class="comment">;</span>
</code></pre><p>监控线程MonitorRunnable的run方法主要代码：</p>
<pre><code><span class="keyword">switch</span> (supervisoree.status.desiredState) {
      <span class="comment">// 所需状态之前已经初始化成START状态，那么会执行组件的start方法。这里的组件是之前分析的PollingPropertiesFileConfigurationProvider</span>
      <span class="keyword">case</span> START:
        <span class="keyword">try</span> {
          lifecycleAware.start();
        } <span class="keyword">catch</span> (Throwable e) {
          logger.<span class="keyword">error</span>(<span class="string">"Unable to start "</span> + lifecycleAware
              + <span class="string">" - Exception follows."</span>, e);
          <span class="keyword">if</span> (e <span class="keyword">instanceof</span> Error) {
            <span class="comment">// This component can never recover, shut it down.</span>
            supervisoree.status.desiredState = LifecycleState.STOP;
            <span class="keyword">try</span> {
              lifecycleAware.stop();
              logger.warn(<span class="string">"Component {} stopped, since it could not be"</span>
                  + <span class="string">"successfully started due to missing dependencies"</span>,
                  lifecycleAware);
            } <span class="keyword">catch</span> (Throwable e1) {
              logger.<span class="keyword">error</span>(<span class="string">"Unsuccessful attempt to "</span>
                  + <span class="string">"shutdown component: {} due to missing dependencies."</span>
                  + <span class="string">" Please shutdown the agent"</span>
                  + <span class="string">"or disable this component, or the agent will be"</span>
                  + <span class="string">"in an undefined state."</span>, e1);
              supervisoree.status.<span class="keyword">error</span> = <span class="keyword">true</span>;
              <span class="keyword">if</span> (e1 <span class="keyword">instanceof</span> Error) {
                <span class="keyword">throw</span> (Error) e1;
              }
              <span class="comment">// Set the state to stop, so that the conf poller can</span>
              <span class="comment">// proceed.</span>
            }
          }
          supervisoree.status.failures++;
        }
        <span class="keyword">break</span>;
      <span class="keyword">case</span> STOP:
        <span class="keyword">try</span> {
          lifecycleAware.stop();
        } <span class="keyword">catch</span> (Throwable e) {
          logger.<span class="keyword">error</span>(<span class="string">"Unable to stop "</span> + lifecycleAware
              + <span class="string">" - Exception follows."</span>, e);
          <span class="keyword">if</span> (e <span class="keyword">instanceof</span> Error) {
            <span class="keyword">throw</span> (Error) e;
          }
          supervisoree.status.failures++;
        }
        <span class="keyword">break</span>;
      <span class="keyword">default</span>:
        logger.warn(<span class="string">"I refuse to acknowledge {} as a desired state"</span>,
            supervisoree.status.desiredState);
    }
</code></pre><p>PollingPropertiesFileConfigurationProvider的start方法：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">void</span> start() {
    LOGGER.info(<span class="string">"Configuration provider starting"</span>);

    Preconditions.checkState(<span class="keyword">file</span> != <span class="keyword">null</span>,
        <span class="string">"The parameter file must not be null"</span>);

    <span class="comment">// 初始化线程池</span>
    executorService = Executors.newSingleThreadScheduledExecutor(
            <span class="keyword">new</span> ThreadFactoryBuilder().setNameFormat(<span class="string">"conf-file-poller-%d"</span>)
                .build());

    <span class="comment">// 起一个文件观察线程</span>
    FileWatcherRunnable fileWatcherRunnable =
        <span class="keyword">new</span> FileWatcherRunnable(<span class="keyword">file</span>, counterGroup);

    executorService.scheduleWithFixedDelay(fileWatcherRunnable, <span class="number">0</span>, interval,
        TimeUnit.SECONDS);
    <span class="comment">// 初始化组件的状态为START</span>
    lifecycleState = LifecycleState.START;

    LOGGER.debug(<span class="string">"Configuration provider started"</span>);
}
</code></pre><p>文件观察线程FileWatcherRunnable的run方法：</p>
<pre><code>public void <span class="keyword">run</span>() {
  LOGGER.debug(<span class="string">"Checking file:{} for changes"</span>, <span class="keyword">file</span>);

  counterGroup.incrementAndGet(<span class="string">"file.checks"</span>);
  <span class="comment">// 得到文件的上次修改时间</span>
  long lastModified = <span class="keyword">file</span>.lastModified();
  <span class="comment">// 如果修改了文件，那么会执行以下代码。首次发生的时候lastChange为0，所以肯定会执行一次。以后只要配置改了才会再次执行</span>
  <span class="keyword">if</span> (lastModified &gt; lastChange) {
    LOGGER.info(<span class="string">"Reloading configuration file:{}"</span>, <span class="keyword">file</span>);

    counterGroup.incrementAndGet(<span class="string">"file.loads"</span>);

    lastChange = lastModified;

    try {
      <span class="comment">// eventBus属性之前在Application中分析过，而且它注册了application实例</span>
      <span class="comment">// Application中有个handleConfigurationEvent方法，eventBus是个观察者设计模式，所以会钓鱼handleConfigurationEvent这个方法</span>
      <span class="comment">// getConfiguration方法会parse配置文件中的配置信息</span>
      eventBus.<span class="keyword">post</span>(getConfiguration());
    } catch (Exception <span class="keyword">e</span>) {
      LOGGER.<span class="keyword">error</span>(<span class="string">"Failed to load configuration data. Exception follows."</span>,
          <span class="keyword">e</span>);
    } catch (NoClassDefFoundError <span class="keyword">e</span>) {
      LOGGER.<span class="keyword">error</span>(<span class="string">"Failed to start agent because dependencies were not "</span> +
          <span class="string">"found in classpath. Error follows."</span>, <span class="keyword">e</span>);
    } catch (Throwable t) {
      <span class="comment">// caught because the caller does not handle or log Throwables</span>
      LOGGER.<span class="keyword">error</span>(<span class="string">"Unhandled error"</span>, t);
    }
  }
}
</code></pre><p>getConfiguration方法是在PollingPropertiesFileConfigurationProvider的父类AbstractConfigurationProvider中定义的：</p>
<pre><code>public MaterializedConfiguration getConfiguration() {
    MaterializedConfiguration conf = <span class="keyword">new</span> SimpleMaterializedConfiguration();
    FlumeConfiguration fconfig = getFlumeConfiguration();
    AgentConfiguration agentConf = fconfig.getConfigurationFor(getAgentName());
    <span class="keyword">if</span> (agentConf != <span class="keyword">null</span>) {
      <span class="comment">// 构造Channel</span>
      <span class="built_in">Map</span>&lt;<span class="built_in">String</span>, ChannelComponent&gt; channelComponentMap = Maps.newHashMap();
      <span class="comment">// 构造Source</span>
      <span class="built_in">Map</span>&lt;<span class="built_in">String</span>, SourceRunner&gt; sourceRunnerMap = Maps.newHashMap();
      <span class="comment">// 构造Sink</span>
      <span class="built_in">Map</span>&lt;<span class="built_in">String</span>, SinkRunner&gt; sinkRunnerMap = Maps.newHashMap();
      <span class="keyword">try</span> {
        loadChannels(agentConf, channelComponentMap);
        loadSources(agentConf, channelComponentMap, sourceRunnerMap);
        loadSinks(agentConf, channelComponentMap, sinkRunnerMap);
        <span class="built_in">Set</span>&lt;<span class="built_in">String</span>&gt; channelNames =
            <span class="keyword">new</span> HashSet&lt;<span class="built_in">String</span>&gt;(channelComponentMap.keySet());
        <span class="keyword">for</span>(<span class="built_in">String</span> channelName : channelNames) {
          ChannelComponent channelComponent = channelComponentMap.
              <span class="literal">get</span>(channelName);
          <span class="keyword">if</span>(channelComponent.components.isEmpty()) {
            LOGGER.warn(<span class="built_in">String</span>.format(<span class="string">"Channel %s has no components connected"</span> +
                <span class="string">" and has been removed."</span>, channelName));
            channelComponentMap.remove(channelName);
            <span class="built_in">Map</span>&lt;<span class="built_in">String</span>, Channel&gt; nameChannelMap = channelCache.
                <span class="literal">get</span>(channelComponent.channel.getClass());
            <span class="keyword">if</span>(nameChannelMap != <span class="keyword">null</span>) {
              nameChannelMap.remove(channelName);
            }
          } <span class="keyword">else</span> {
            LOGGER.info(<span class="built_in">String</span>.format(<span class="string">"Channel %s connected to %s"</span>,
                channelName, channelComponent.components.toString()));
            conf.addChannel(channelName, channelComponent.channel);
          }
        }
        <span class="keyword">for</span>(<span class="built_in">Map</span>.Entry&lt;<span class="built_in">String</span>, SourceRunner&gt; entry : sourceRunnerMap.entrySet()) {
          conf.addSourceRunner(entry.getKey(), entry.getValue());
        }
        <span class="keyword">for</span>(<span class="built_in">Map</span>.Entry&lt;<span class="built_in">String</span>, SinkRunner&gt; entry : sinkRunnerMap.entrySet()) {
          conf.addSinkRunner(entry.getKey(), entry.getValue());
        }
      } <span class="keyword">catch</span> (InstantiationException ex) {
        LOGGER.error(<span class="string">"Failed to instantiate component"</span>, ex);
      } <span class="keyword">finally</span> {
        channelComponentMap.clear();
        sourceRunnerMap.clear();
        sinkRunnerMap.clear();
      }
    } <span class="keyword">else</span> {
      LOGGER.warn(<span class="string">"No configuration found for this host:{}"</span>, getAgentName());
    }
    <span class="keyword">return</span> conf;
}
</code></pre><p>当所有的组件，Source，Channel，Sink加载完之后。会调用handleConfigurationEvent方法：</p>
<pre><code><span class="annotation">@Subscribe</span>
<span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function"><span class="keyword">void</span> <span class="title">handleConfigurationEvent</span><span class="params">(MaterializedConfiguration conf)</span> </span>{
  <span class="comment">// 先关闭之前启动的所有Source，Channel，Sink组件(首次运行的时候并没有任何开启的组件)</span>
  stopAllComponents();
  <span class="comment">// 再重新开启这些组件</span>
  startAllComponents(conf);
}
</code></pre><p>看下startAllComponents方法：</p>
<pre><code><span class="comment">// 启动各个组件的时候同样适用supervisor的supervise方法。然后启动MonitorRunnable线程调用各个组件的start方法。</span>
<span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">startAllComponents</span><span class="params">(MaterializedConfiguration materializedConfiguration)</span> </span>{
    logger.info(<span class="string">"Starting new configuration:{}"</span>, materializedConfiguration);

    <span class="keyword">this</span>.materializedConfiguration = materializedConfiguration;

    <span class="keyword">for</span> (Entry&lt;String, Channel&gt; entry :
      materializedConfiguration.getChannels().entrySet()) {
      <span class="keyword">try</span>{
        logger.info(<span class="string">"Starting Channel "</span> + entry.getKey());
        supervisor.supervise(entry.getValue(),
            <span class="keyword">new</span> SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);
      } <span class="keyword">catch</span> (Exception e){
        logger.<span class="keyword">error</span>(<span class="string">"Error while starting {}"</span>, entry.getValue(), e);
      }
    }

    <span class="comment">/*
     * Wait for all channels to start.
     */</span>
    <span class="keyword">for</span>(Channel ch: materializedConfiguration.getChannels().values()){
      <span class="keyword">while</span>(ch.getLifecycleState() != LifecycleState.START
          &amp;&amp; !supervisor.isComponentInErrorState(ch)){
        <span class="keyword">try</span> {
          logger.info(<span class="string">"Waiting for channel: "</span> + ch.getName() +
              <span class="string">" to start. Sleeping for 500 ms"</span>);
          Thread.sleep(<span class="number">500</span>);
        } <span class="keyword">catch</span> (InterruptedException e) {
          logger.<span class="keyword">error</span>(<span class="string">"Interrupted while waiting for channel to start."</span>, e);
          Throwables.propagate(e);
        }
      }
    }

    <span class="keyword">for</span> (Entry&lt;String, SinkRunner&gt; entry : materializedConfiguration.getSinkRunners()
        .entrySet()) {
      <span class="keyword">try</span>{
        logger.info(<span class="string">"Starting Sink "</span> + entry.getKey());
        supervisor.supervise(entry.getValue(),
          <span class="keyword">new</span> SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);
      } <span class="keyword">catch</span> (Exception e) {
        logger.<span class="keyword">error</span>(<span class="string">"Error while starting {}"</span>, entry.getValue(), e);
      }
    }

    <span class="keyword">for</span> (Entry&lt;String, SourceRunner&gt; entry : materializedConfiguration
        .getSourceRunners().entrySet()) {
      <span class="keyword">try</span>{
        logger.info(<span class="string">"Starting Source "</span> + entry.getKey());
        supervisor.supervise(entry.getValue(),
          <span class="keyword">new</span> SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START);
      } <span class="keyword">catch</span> (Exception e) {
        logger.<span class="keyword">error</span>(<span class="string">"Error while starting {}"</span>, entry.getValue(), e);
      }
    }

    <span class="keyword">this</span>.loadMonitoring();
}
</code></pre><h3 id="Flume启动过程总结">Flume启动过程总结</h3><p>首先通过Application类加载配置文件，加载后调用Application的start方法。</p>
<p>Application的start方法内部会针对PollingPropertiesFileConfigurationProvider组件适用组件管理者管理这个组件。也就是使用LifecycleSupervisor的supervise方法。</p>
<p>supervise方法内部使用MonitorRunnable监控线程。  监控线程内部会调用组件的start方法。 即PollingPropertiesFileConfigurationProvider的start方法。</p>
<p>PollingPropertiesFileConfigurationProvider的start方法会使用FileWatcherRunnable文件查看进程判断配置文件是否已经修改，修改的话重新加载配置文件信息，然后通过EventBus调用Application的handleConfigurationEvent方法关闭目前正在启动的组件，关闭之后重新开启组件。</p>
<p>各个新开启的组件会做类似的工作，使用LifecycleSupervisor的supervise方法，也就是起各个MonitorRunnable监控线程启动各个组件的start方法。</p>
<h2 id="Source组件的构造过程">Source组件的构造过程</h2><p>直接看AbstractConfigurationProvider的loadSources方法：</p>
<pre><code>private <span class="keyword">void</span> loadSources(AgentConfiguration agentConf,
  <span class="built_in">Map</span>&lt;<span class="built_in">String</span>, ChannelComponent&gt; channelComponentMap,
  <span class="built_in">Map</span>&lt;<span class="built_in">String</span>, SourceRunner&gt; sourceRunnerMap)
  throws InstantiationException {

<span class="built_in">Map</span>&lt;<span class="built_in">String</span>, Context&gt; sourceContexts = agentConf.getSourceContext();
    <span class="keyword">for</span> (<span class="built_in">String</span> sourceName : sourceNames) {
      Context context = sourceContexts.<span class="literal">get</span>(sourceName);
      <span class="keyword">if</span>(context != <span class="keyword">null</span>){
        <span class="comment">// 使用sourceFactory构造Source</span>
        Source source =
            sourceFactory.create(sourceName,
                context.getString(BasicConfigurationConstants.CONFIG_TYPE));
        <span class="keyword">try</span> {
          Configurables.configure(source, context);
          <span class="built_in">List</span>&lt;Channel&gt; sourceChannels = <span class="keyword">new</span> ArrayList&lt;Channel&gt;();
          <span class="built_in">String</span>[] channelNames = context.getString(
              BasicConfigurationConstants.CONFIG_CHANNELS).split(<span class="string">"\\s+"</span>);
          <span class="keyword">for</span> (<span class="built_in">String</span> chName : channelNames) {
            ChannelComponent channelComponent = channelComponentMap.<span class="literal">get</span>(chName);
            <span class="keyword">if</span>(channelComponent != <span class="keyword">null</span>) {
              sourceChannels.add(channelComponent.channel);
            }
          }
          <span class="keyword">if</span>(sourceChannels.isEmpty()) {
            <span class="built_in">String</span> msg = <span class="built_in">String</span>.format(<span class="string">"Source %s is not connected to a "</span> +
                <span class="string">"channel"</span>,  sourceName);
            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(msg);
          }
          <span class="built_in">Map</span>&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; selectorConfig = context.getSubProperties(
              BasicConfigurationConstants.CONFIG_SOURCE_CHANNELSELECTOR_PREFIX);

          ChannelSelector selector = ChannelSelectorFactory.create(
              sourceChannels, selectorConfig);

          ChannelProcessor channelProcessor = <span class="keyword">new</span> ChannelProcessor(selector);
          Configurables.configure(channelProcessor, context);
          source.setChannelProcessor(channelProcessor);
          sourceRunnerMap.put(sourceName,
              SourceRunner.forSource(source));
          <span class="comment">// source关联Channel    </span>
          <span class="keyword">for</span>(Channel channel : sourceChannels) {
            ChannelComponent channelComponent = Preconditions.
                checkNotNull(channelComponentMap.<span class="literal">get</span>(channel.getName()),
                    <span class="built_in">String</span>.format(<span class="string">"Channel %s"</span>, channel.getName()));
            channelComponent.components.add(sourceName);
          }
        } <span class="keyword">catch</span> (Exception e) {
          <span class="built_in">String</span> msg = <span class="built_in">String</span>.format(<span class="string">"Source %s has been removed due to an "</span> +
              <span class="string">"error during configuration"</span>, sourceName);
          LOGGER.error(msg, e);
        }
      }
    }
  }
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[Flume运行的主类是org.apache.flume.node.Application的main方法 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Flume Sink组件分析]]></title>
    <link href="http://fangjian0423.github.io/2015/06/23/flume-sink/"/>
    <id>http://fangjian0423.github.io/2015/06/23/flume-sink/</id>
    <published>2015-06-22T17:23:23.000Z</published>
    <updated>2015-12-20T16:55:37.000Z</updated>
    <content type="html"><![CDATA[<p>Source和Channel组件已经分析过，接下来看Sink组件。</p>
<p>Flume内置了很多Sink，比如HDFS Sink, Hive Sink, File Roll Sink, HBase Sink, ElasticSearch Sink等。</p>
<h2 id="Sink接口">Sink接口</h2><pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Sink</span> <span class="keyword">extends</span> <span class="title">LifecycleAware</span>, <span class="title">NamedComponent</span> </span>{

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setChannel</span><span class="params">(Channel channel)</span></span>;

  <span class="function"><span class="keyword">public</span> Channel <span class="title">getChannel</span><span class="params">()</span></span>;

  <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException</span>;

  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">enum</span> Status {
    READY, BACKOFF
  }
}
</code></pre><p>沒啥好讲的，跟之前的都类似。</p>
<p>Sink实现类AbstractSink：</p>
<pre><code><span class="keyword">abstract</span> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractSink</span> <span class="keyword">implements</span> <span class="title">Sink</span>, <span class="title">LifecycleAware</span></span>
</code></pre><p>跟AbstractSource类似。</p>
<h2 id="Flume内置的Sink例子">Flume内置的Sink例子</h2><h3 id="HDFSEventSink">HDFSEventSink</h3><p>1个简单的hdfs sink配置：</p>
<pre><code>a1.channels = c1
a1.sinks = k1
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = /flume/events/<span class="decorator">%y</span>-<span class="decorator">%m</span>-<span class="decorator">%d</span>/<span class="decorator">%H</span><span class="decorator">%M</span>/<span class="decorator">%S</span>
a1.sinks.k1.hdfs.filePrefix = events-
a1.sinks.k1.hdfs.round = <span class="literal">true</span>
a1.sinks.k1.hdfs.roundValue = <span class="number">10</span>
a1.sinks.k1.hdfs.roundUnit = minute
</code></pre><p>HDFSEventSink中的process(Sink接口提供的)方法记录着如何写入hdfs文件：</p>
<pre><code><span class="keyword">public</span> Status process() <span class="keyword">throws</span> EventDeliveryException {
    <span class="comment">// 得到Channel</span>
    Channel channel = getChannel();
    <span class="comment">// 得到Channel里的Transaction，接下来事件的提交，回滚都基于这个Transaction</span>
    Transaction transaction = channel.getTransaction();
    List&lt;BucketWriter&gt; writers = Lists.newArrayList();
    <span class="comment">// 事务开始，一般的Transaction实现类不会覆盖这个方法，除非有特殊要求，begin方法默认的实现不做任何事</span>
    transaction.begin();
    <span class="keyword">try</span> {
      <span class="built_in">int</span> txnEventCount = <span class="number">0</span>;
      <span class="comment">// 每次操作都处理batchSize个事件，batchSize可配置</span>
      <span class="keyword">for</span> (txnEventCount = <span class="number">0</span>; txnEventCount &lt; batchSize; txnEventCount++) {
          <span class="comment">// channel的take方法内部会使用Transaction的take方法，Transaction回滚后这些take出来的事件全部都会回滚到Channel里</span>
        Event event = channel.take();
        <span class="keyword">if</span> (event == <span class="keyword">null</span>) {
          <span class="keyword">break</span>;
        }

        <span class="comment">// reconstruct the path name by substituting place holders</span>
        <span class="keyword">String</span> realPath = BucketPath.escapeString(filePath, event.getHeaders(),
            timeZone, needRounding, roundUnit, roundValue, useLocalTime);
        <span class="keyword">String</span> realName = BucketPath.escapeString(fileName, event.getHeaders(),
          timeZone, needRounding, roundUnit, roundValue, useLocalTime);

        <span class="keyword">String</span> lookupPath = realPath + DIRECTORY_DELIMITER + realName;
        BucketWriter bucketWriter;
        HDFSWriter hdfsWriter = <span class="keyword">null</span>;
        <span class="comment">// Callback to remove the reference to the bucket writer from the</span>
        <span class="comment">// sfWriters map so that all buffers used by the HDFS file</span>
        <span class="comment">// handles are garbage collected.</span>
        WriterCallback closeCallback = <span class="keyword">new</span> WriterCallback() {
          @Override
          <span class="keyword">public</span> <span class="keyword">void</span> run(<span class="keyword">String</span> bucketPath) {
            LOG.info(<span class="string">"Writer callback called."</span>);
            <span class="keyword">synchronized</span> (sfWritersLock) {
              sfWriters.remove(bucketPath);
            }
          }
        };
        <span class="keyword">synchronized</span> (sfWritersLock) {
          bucketWriter = sfWriters.<span class="built_in">get</span>(lookupPath);
          <span class="comment">// we haven't seen this file yet, so open it and cache the handle</span>
          <span class="keyword">if</span> (bucketWriter == <span class="keyword">null</span>) {
            hdfsWriter = writerFactory.getWriter(fileType);
            bucketWriter = initializeBucketWriter(realPath, realName,
              lookupPath, hdfsWriter, closeCallback);
            sfWriters.put(lookupPath, bucketWriter);
          }
        }

        <span class="comment">// track the buckets getting written in this transaction</span>
        <span class="keyword">if</span> (!writers.contains(bucketWriter)) {
          writers.<span class="built_in">add</span>(bucketWriter);
        }

        <span class="comment">// Write the data to HDFS</span>
        <span class="keyword">try</span> {
          bucketWriter.<span class="built_in">append</span>(event);
        } <span class="keyword">catch</span> (BucketClosedException ex) {
          LOG.info(<span class="string">"Bucket was closed while trying to append, "</span> +
            <span class="string">"reinitializing bucket and writing event."</span>);
          hdfsWriter = writerFactory.getWriter(fileType);
          bucketWriter = initializeBucketWriter(realPath, realName,
            lookupPath, hdfsWriter, closeCallback);
          <span class="keyword">synchronized</span> (sfWritersLock) {
            sfWriters.put(lookupPath, bucketWriter);
          }
          bucketWriter.<span class="built_in">append</span>(event);
        }
      }

      <span class="keyword">if</span> (txnEventCount == <span class="number">0</span>) {
        sinkCounter.incrementBatchEmptyCount();
      } <span class="keyword">else</span> <span class="keyword">if</span> (txnEventCount == batchSize) {
        sinkCounter.incrementBatchCompleteCount();
      } <span class="keyword">else</span> {
        sinkCounter.incrementBatchUnderflowCount();
      }

      <span class="comment">// flush all pending buckets before committing the transaction</span>
      <span class="keyword">for</span> (BucketWriter bucketWriter : writers) {
        bucketWriter.flush();
      }

      <span class="comment">// 写完数据无误后，commit，清空Transaction里的数据</span>
      transaction.commit();

      <span class="keyword">if</span> (txnEventCount &lt; <span class="number">1</span>) {
        <span class="keyword">return</span> Status.BACKOFF;
      } <span class="keyword">else</span> {
        sinkCounter.addToEventDrainSuccessCount(txnEventCount);
        <span class="keyword">return</span> Status.READY;
      }
    } <span class="keyword">catch</span> (IOException eIO) {
      <span class="comment">// 发送异常回滚</span>
      transaction.rollback();
      LOG.warn(<span class="string">"HDFS IO error"</span>, eIO);
      <span class="keyword">return</span> Status.BACKOFF;
    } <span class="keyword">catch</span> (Throwable th) {
      <span class="comment">// 发送异常回滚</span>
      transaction.rollback();
      LOG.error(<span class="string">"process failed"</span>, th);
      <span class="keyword">if</span> (th <span class="keyword">instanceof</span> Error) {
        <span class="keyword">throw</span> (Error) th;
      } <span class="keyword">else</span> {
        <span class="keyword">throw</span> <span class="keyword">new</span> EventDeliveryException(th);
      }
    } <span class="keyword">finally</span> {
      <span class="comment">// 事务结束，一般的Transaction实现类不会覆盖这个方法，除非有特殊要求，close方法默认的实现不做任何事</span>
      transaction.close();
    }
  }    
</code></pre><p>下面我们重点分析一下HDFS Sink如何写入数据到hdfs文件。</p>
<p>分析之前，有几个配置跟是否创建hdfs新文件有关，当达到这些配置的条件后，sink会关闭文件，然后重新创建1个新文件重复执行</p>
<pre><code>hdfs<span class="class">.rollInterval</span> -&gt; 每隔多少秒会关闭文件。默认<span class="number">30</span>秒
hdfs<span class="class">.rollSize</span> -&gt; 文件大小到达一定量后，会关闭文件。单位bytes。默认<span class="number">1024</span>，如果是<span class="number">0</span>的话表示跟文件大小无关
hdfs<span class="class">.batchSize</span> -&gt; 批次数，Sink每次处理都会处理batchSize个事件, 不会创建新文件。默认<span class="number">100</span>个
hdfs<span class="class">.rollCount</span> -&gt; 事件处理了rollCount个后，会关闭文件。默认<span class="number">10</span>个，如果是<span class="number">0</span>的话表示跟事件个数无关
hdfs<span class="class">.minBlockReplicas</span> -&gt; hadoop中的dfs.replication配置属性，表示复制块的个数，默认会根据hadoop的配置
</code></pre><p>HDFS Sink每个根据batchSize，遍历channel中的事件，针对每个Event的生成地址构造BucketWriter。然后在sfWriters这个Map中以文件的路径为key，BucketWriter为value进行存储。</p>
<p>接下来使用BucketWriter的append方法，当batchSize个事件遍历完成，调用BucketWriter的flush方法。</p>
<pre><code><span class="keyword">bucketWriter.append(event);
</span>
<span class="label">for</span> (<span class="keyword">BucketWriter </span><span class="keyword">bucketWriter </span>: writers) {
    <span class="keyword">bucketWriter.flush();
</span>}
</code></pre><p>BucketWriter的append方法部分代码如下：</p>
<pre><code><span class="comment">// 打开文件</span>
<span class="keyword">if</span> (!isOpen) {
  <span class="keyword">if</span> (closed) {
    <span class="keyword">throw</span> <span class="keyword">new</span> BucketClosedException(<span class="string">"This bucket writer was closed and "</span> +
      <span class="string">"this handle is thus no longer valid"</span>);
  }
  open();
}

<span class="comment">// 是否需要创建新文件</span>
<span class="keyword">if</span> (shouldRotate()) {
  <span class="keyword">boolean</span> doRotate = <span class="keyword">true</span>;

  <span class="keyword">if</span> (isUnderReplicated) {
    <span class="keyword">if</span> (maxConsecUnderReplRotations &gt; <span class="number">0</span> &amp;&amp;
        consecutiveUnderReplRotateCount &gt;= maxConsecUnderReplRotations) {
      doRotate = <span class="keyword">false</span>;
      <span class="keyword">if</span> (consecutiveUnderReplRotateCount == maxConsecUnderReplRotations) {
        LOG.error(<span class="string">"Hit max consecutive under-replication rotations ({}); "</span> +
            <span class="string">"will not continue rolling files under this path due to "</span> +
            <span class="string">"under-replication"</span>, maxConsecUnderReplRotations);
      }
    } <span class="keyword">else</span> {
      LOG.warn(<span class="string">"Block Under-replication detected. Rotating file."</span>);
    }
    consecutiveUnderReplRotateCount++;
  } <span class="keyword">else</span> {
    consecutiveUnderReplRotateCount = <span class="number">0</span>;
  }

  <span class="keyword">if</span> (doRotate) {
    close();
    open();
  }
}

<span class="keyword">try</span> {
  sinkCounter.incrementEventDrainAttemptCount();
  <span class="comment">// 写hdfs文件</span>
  callWithTimeout(<span class="keyword">new</span> CallRunner&lt;<span class="keyword">Void</span>&gt;() {
    @Override
    <span class="keyword">public</span> <span class="keyword">Void</span> <span class="keyword">call</span>() <span class="keyword">throws</span> Exception {
      writer.<span class="keyword">append</span>(event); <span class="comment">// could block</span>
      <span class="keyword">return</span> <span class="keyword">null</span>;
    }
  });
} <span class="keyword">catch</span> (IOException e) {
  LOG.warn(<span class="string">"Caught IOException writing to HDFSWriter ({}). Closing file ("</span> +
      bucketPath + <span class="string">") and rethrowing exception."</span>,
      e.getMessage());
  <span class="keyword">try</span> {
    close(<span class="keyword">true</span>);
  } <span class="keyword">catch</span> (IOException e2) {
    LOG.warn(<span class="string">"Caught IOException while closing file ("</span> +
         bucketPath + <span class="string">"). Exception follows."</span>, e2);
  }
  <span class="keyword">throw</span> e;
}

<span class="comment">// 更新一些属性</span>
processSize += event.getBody().length;
eventCounter++;
batchCounter++;

<span class="comment">// 写入的事件达到批次数，flush</span>
<span class="keyword">if</span> (batchCounter == batchSize) {
  flush();
}
</code></pre><p>shouldRotate方法表示是否需要创建新文件：</p>
<pre><code><span class="keyword">private</span> <span class="function"><span class="keyword">boolean</span> <span class="title">shouldRotate</span><span class="params">()</span> </span>{
    <span class="keyword">boolean</span> doRotate = <span class="keyword">false</span>;

    <span class="comment">// 先判断HDFS是否正在复制块，优先级最高，也就是配置文件的minBlockReplicas</span>
    <span class="keyword">if</span> (writer.isUnderReplicated()) {
      <span class="keyword">this</span>.isUnderReplicated = <span class="keyword">true</span>;
      doRotate = <span class="keyword">true</span>;
    } <span class="keyword">else</span> {
      <span class="keyword">this</span>.isUnderReplicated = <span class="keyword">false</span>;
    }

    <span class="comment">// 配置文件中的rollCount，也就是事件个数</span>
    <span class="keyword">if</span> ((rollCount &gt; <span class="number">0</span>) &amp;&amp; (rollCount &lt;= eventCounter)) {
      LOG.debug(<span class="string">"rolling: rollCount: {}, events: {}"</span>, rollCount, eventCounter);
      doRotate = <span class="keyword">true</span>;
    }
    <span class="comment">// 配置文件中的rollSize，也就文件大小</span>
    <span class="keyword">if</span> ((rollSize &gt; <span class="number">0</span>) &amp;&amp; (rollSize &lt;= processSize)) {
      LOG.debug(<span class="string">"rolling: rollSize: {}, bytes: {}"</span>, rollSize, processSize);
      doRotate = <span class="keyword">true</span>;
    }

    <span class="keyword">return</span> doRotate;
}
</code></pre><p>有的同学可能因为没有配置minBlockReplicas，而配置了其他属性，所以hdfs还是会生成很多文件，这是因为minBlockReplicas的优先级最高，如果当前正在复制块，其他所有的条件都会被无视。</p>
<p>flush方法：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> flush() <span class="keyword">throws</span> IOException, InterruptedException {
    checkAndThrowInterruptedException();
    <span class="keyword">if</span> (!isBatchComplete()) {
      doFlush();

      <span class="keyword">if</span>(idleTimeout &gt; <span class="number">0</span>) {
        <span class="comment">// if the future exists and couldn't be cancelled, that would mean it has already run</span>
        <span class="comment">// or been cancelled</span>
        <span class="keyword">if</span>(idleFuture == <span class="keyword">null</span> || idleFuture.cancel(<span class="keyword">false</span>)) {
          Callable&lt;<span class="keyword">Void</span>&gt; idleAction = <span class="keyword">new</span> Callable&lt;<span class="keyword">Void</span>&gt;() {
            <span class="keyword">public</span> <span class="keyword">Void</span> <span class="keyword">call</span>() <span class="keyword">throws</span> Exception {
              LOG.info(<span class="string">"Closing idle bucketWriter {} at {}"</span>, bucketPath,
                System.currentTimeMillis());
              <span class="keyword">if</span> (isOpen) {
                close(<span class="keyword">true</span>);
              }
              <span class="keyword">return</span> <span class="keyword">null</span>;
            }
          };
          idleFuture = timedRollerPool.schedule(idleAction, idleTimeout,
              TimeUnit.SECONDS);
        }
      }
    }
}
</code></pre><p>总结:</p>
<p>HDFSSink里的hdfs.rollSize，hdfs.rollCount，hdfs.minBlockReplicas，hdfs.rollInterval,hdfs.batchSize这些配置都是在BucketWriter中才会使用。</p>
<p>hdfs.rollSize, hdfs.rollCount和hdfs.minBlockReplicas, hdfs.rollInterval这4个属性决定是否创建新文件。</p>
<p>hdfs.batchSize决定是否flush文件数据。 由于HDFS Sink中每次最多只有batchSize个事件，因此BucketWriter会事件个数达到了batchSize后直接flush数据即可。</p>
]]></content>
    <summary type="html">
    <![CDATA[Flume内置了很多Sink，比如HDFS Sink, Hive Sink, File Roll Sink, HBase Sink, ElasticSearch Sink等 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Flume Channel组件分析]]></title>
    <link href="http://fangjian0423.github.io/2015/06/22/flume-channel/"/>
    <id>http://fangjian0423.github.io/2015/06/22/flume-channel/</id>
    <published>2015-06-22T15:23:23.000Z</published>
    <updated>2015-12-20T17:08:47.000Z</updated>
    <content type="html"><![CDATA[<p>Source组件已经分析过，接下来看Channel组件。</p>
<p>Flume内置了很多Channel，比如Memory Channel, JDBC Channel, Kafka Channel, File Channel等。</p>
<h2 id="Channel接口">Channel接口</h2><p>Channel接口定义：</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Channel</span> <span class="keyword">extends</span> <span class="title">LifecycleAware</span>, <span class="title">NamedComponent</span> </span>{

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(Event event)</span> <span class="keyword">throws</span> ChannelException</span>;

  <span class="function"><span class="keyword">public</span> Event <span class="title">take</span><span class="params">()</span> <span class="keyword">throws</span> ChannelException</span>;

  <span class="function"><span class="keyword">public</span> Transaction <span class="title">getTransaction</span><span class="params">()</span></span>;
}
</code></pre><p>LifecycleAware接口和NamedComponent接口之前在分析Source的时候已经说明过。</p>
<p>Channel接口有3个方法，分别是put，take和getTransaction。</p>
<p>Channel是存储Source收集过来的数据的，所以提供put(存储Source的Event)和take(交付给Sink)方法，它也支持事务。</p>
<p>AbstractChannel抽象类是Channel接口的实现类，作用跟AbstractSource是Source的实现类类似。</p>
<p>BasicChannelSemantics是AbstractChannel的子类，是Channel的基础实现类，内部使用ThreadLocal完成事件的处理和事务功能，实现了getTransaction方法，在这个方法内部调用createTransaction得到事务对象，createTransaction也被抽象成了一个抽象方法。</p>
<p>BasicChannelSemantics实现了Channel接口的put和take方法，使用Transaction接口的put和take方法。</p>
<pre><code><span class="annotation">@Override</span>
  <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">put</span><span class="params">(Event event)</span> <span class="keyword">throws</span> ChannelException </span>{
    BasicTransactionSemantics transaction = currentTransaction.get();
    Preconditions.checkState(transaction != <span class="keyword">null</span>,
        <span class="string">"No transaction exists for this thread"</span>);
    transaction.put(event);
  }

<span class="annotation">@Override</span>
  <span class="keyword">public</span> <span class="function">Event <span class="title">take</span><span class="params">()</span> <span class="keyword">throws</span> ChannelException </span>{
    BasicTransactionSemantics transaction = currentTransaction.get();
    Preconditions.checkState(transaction != <span class="keyword">null</span>,
        <span class="string">"No transaction exists for this thread"</span>);
    <span class="function"><span class="keyword">return</span> transaction.<span class="title">take</span><span class="params">()</span></span>;
}
</code></pre><p>currentTransaction是个ThreadLocal：</p>
<pre><code><span class="keyword">private</span> ThreadLocal&lt;BasicTransactionSemantics&gt; currentTransaction
      = <span class="keyword">new</span> ThreadLocal&lt;BasicTransactionSemantics&gt;();
</code></pre><p>currentTransaction的初始化：</p>
<pre><code><span class="annotation">@Override</span>
  <span class="keyword">public</span> <span class="function">Transaction <span class="title">getTransaction</span><span class="params">()</span> </span>{

    <span class="keyword">if</span> (!initialized) {
      <span class="keyword">synchronized</span> (<span class="keyword">this</span>) {
        <span class="keyword">if</span> (!initialized) {
          initialize();
          initialized = <span class="keyword">true</span>;
        }
      }
    }

    BasicTransactionSemantics transaction = currentTransaction.get();
    <span class="keyword">if</span> (transaction == <span class="keyword">null</span> || transaction.getState().equals(
            BasicTransactionSemantics.State.CLOSED)) {
      transaction = createTransaction(); <span class="comment">// 抽象方法，子类实现</span>
      currentTransaction.set(transaction);
    }
    <span class="keyword">return</span> transaction;
  }
</code></pre><p>这样，Channel的put和take操作就由抽象类createTransaction中得到的Transaction实现。<strong>因此，Channel对事件的各个操作由具体的Transaction实现。</strong></p>
<p>一般普通的Channel继承这个BasicChannelSemantics类即可。</p>
<h2 id="Transaction接口">Transaction接口</h2><p>由于Channel内部使用了事务特性，因此有必要介绍一个Flume事务相关的结构。</p>
<p>Transaction接口定义：</p>
<pre><code><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title">Transaction</span> {

<span class="keyword">public</span> <span class="keyword">enum</span> TransactionState {Started, Committed, RolledBack, Closed };

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">begin</span>(<span class="params"></span>)</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commit</span>(<span class="params"></span>)</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">rollback</span>(<span class="params"></span>)</span>;

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span>(<span class="params"></span>)</span>;
}
</code></pre><p>基本的事务封装类BasicTransactionSemantics，BasicTransactionSemantics实现了Transaction接口的4个方法，并引入了状态的概念，内部也有个state变量表示状态，Transaction接口的4个方法的实现会基于状态：</p>
<p>BasicTransactionSemantics构造函数会初始化状态state为State.NEW, 实现Transaction接口的begin方法如下：</p>
<pre><code>public void begin() {
    Preconditions.checkState(Thread.currentThread().getId() == initialThreadId,
        <span class="string">"begin() called from different thread than getTransaction()!"</span>);
    Preconditions.checkState(<span class="keyword">state</span>.equals(State.NEW),
        <span class="string">"begin() called when transaction is "</span> + <span class="keyword">state</span> + <span class="string">"!"</span>);

    try {
      doBegin();
    } catch (InterruptedException e) {
      Thread.currentThread().interrupt();
      throw new ChannelException(e.<span class="keyword">to</span>String(), e);
    }
    <span class="keyword">state</span> = State.OPEN;
  }
</code></pre><p>begin方法需要状态为NEW才可执行。同理commit方法需要状态为OPEN，rollback需要状态为OPEN，close方法需要状态为NEW或COMPLETED。BasicTransactionSemantics又提供了put和take方法用来放事件和取事件，所以又抽象出了4个抽象方法和2个默认实现方法：</p>
<pre><code><span class="keyword">protected</span> <span class="function"><span class="keyword">void</span> <span class="title">doBegin</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>{}
<span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="function"><span class="keyword">void</span> <span class="title">doPut</span><span class="params">(Event event)</span> <span class="keyword">throws</span> InterruptedException</span>;
<span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="function">Event <span class="title">doTake</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException</span>;
<span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="function"><span class="keyword">void</span> <span class="title">doCommit</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException</span>;
<span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="function"><span class="keyword">void</span> <span class="title">doRollback</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException</span>;
<span class="keyword">protected</span> <span class="function"><span class="keyword">void</span> <span class="title">doClose</span><span class="params">()</span> </span>{}
</code></pre><h2 id="Flume内置的Channel">Flume内置的Channel</h2><h3 id="MemoryChannel">MemoryChannel</h3><p>Memory Channel的配置如下：</p>
<pre><code>a1<span class="class">.channels</span> = c1
a1<span class="class">.channels</span><span class="class">.c1</span><span class="class">.type</span> = memory
a1<span class="class">.channels</span><span class="class">.c1</span><span class="class">.capacity</span> = <span class="number">10000</span>
a1<span class="class">.channels</span><span class="class">.c1</span><span class="class">.transactionCapacity</span> = <span class="number">10000</span>
</code></pre><p>MemoryChannel继承BasicChannelSemantics。</p>
<p>MemoryChannel的configure读取配置信息，它的内部有个重要的属性如下：</p>
<pre><code><span class="keyword">private</span> LinkedBlockingDeque&lt;Event&gt; <span class="built_in">queue</span>; <span class="comment">// 阻塞事件队列</span>
</code></pre><p>MemoryChannel内部有个MemoryTransaction类继承自BasicTransactionSemantics，这个事务对象就是MemoryChannel使用的事务对象。</p>
<pre><code><span class="annotation">@Override</span>
<span class="keyword">protected</span> <span class="function">BasicTransactionSemantics <span class="title">createTransaction</span><span class="params">()</span> </span>{
  <span class="keyword">return</span> <span class="keyword">new</span> MemoryTransaction(transCapacity, channelCounter);
}
</code></pre><p>MemoryTransaction的属性如下：</p>
<pre><code><span class="comment">// 取走事件列表</span>
<span class="keyword">private</span> LinkedBlockingDeque&lt;Event&gt; takeList;
<span class="comment">// 放入事件列表</span>
<span class="keyword">private</span> LinkedBlockingDeque&lt;Event&gt; putList;
<span class="keyword">private</span> final ChannelCounter channelCounter;
<span class="keyword">private</span> <span class="keyword">int</span> putByteCounter = <span class="number">0</span>;
<span class="keyword">private</span> <span class="keyword">int</span> takeByteCounter = <span class="number">0</span>;
</code></pre><p>之前分析过，Channel对事件的各个操作由具体的Transaction实现，也就是由MemoryTransaction实现。</p>
<p>doPut方法，加入事件，doPut方法只针对Source提供过来的Event，跟Sink无关：</p>
<pre><code>@<span class="function">Override
<span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doPut</span>(<span class="params">Event <span class="keyword">event</span></span>) throws InterruptedException </span>{
  channelCounter.incrementEventPutAttemptCount();
  <span class="keyword">int</span> eventByteSize = (<span class="keyword">int</span>)Math.ceil(estimateEventSize(<span class="keyword">event</span>)/byteCapacitySlotSize);

  <span class="keyword">if</span> (!putList.offer(<span class="keyword">event</span>)) { <span class="comment">// 事件存储到MemoryTransaction的加入事件列表中</span>
    <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(
      <span class="string">"Put queue for MemoryTransaction of capacity "</span> +
        putList.size() + <span class="string">" full, consider committing more frequently, "</span> +
        <span class="string">"increasing capacity or increasing thread count"</span>);
  }
  putByteCounter += eventByteSize;
}
</code></pre><p>doTake方法，取走事件，doTake方法只针对Sink，负责给Sink传递Event数据：</p>
<pre><code>@<span class="function">Override
<span class="keyword">protected</span> Event <span class="title">doTake</span>(<span class="params"></span>) throws InterruptedException </span>{
  channelCounter.incrementEventTakeAttemptCount();
  <span class="keyword">if</span>(takeList.remainingCapacity() == <span class="number">0</span>) {
    <span class="keyword">throw</span> <span class="keyword">new</span> ChannelException(<span class="string">"Take list for MemoryTransaction, capacity "</span> +
        takeList.size() + <span class="string">" full, consider committing more frequently, "</span> +
        <span class="string">"increasing capacity, or increasing thread count"</span>);
  }
  <span class="keyword">if</span>(!queueStored.tryAcquire(keepAlive, TimeUnit.SECONDS)) {
    <span class="keyword">return</span> <span class="keyword">null</span>;
  }
  Event <span class="keyword">event</span>;
  synchronized(queueLock) {
    <span class="keyword">event</span> = queue.poll(); <span class="comment">// 取出MemoryChannel中阻塞事件队列中的事件</span>
  }
  Preconditions.checkNotNull(<span class="keyword">event</span>, <span class="string">"Queue.poll returned NULL despite semaphore "</span> +
      <span class="string">"signalling existence of entry"</span>);
  takeList.put(<span class="keyword">event</span>); <span class="comment">// 放到MemoryTransaction的取出事件列表中</span>

  <span class="keyword">int</span> eventByteSize = (<span class="keyword">int</span>)Math.ceil(estimateEventSize(<span class="keyword">event</span>)/byteCapacitySlotSize);
  takeByteCounter += eventByteSize;

  <span class="keyword">return</span> <span class="keyword">event</span>;
}
</code></pre><p>doCommit的部分代码，提交处理的事件，doCommit只处理putList，所以提交的是Source过来的数据，并交给MemoryChannel内部的阻塞队列：</p>
<pre><code>@Override
<span class="keyword">protected</span> <span class="keyword">void</span> doCommit() <span class="keyword">throws</span> InterruptedException {
  <span class="built_in">int</span> puts = putList.<span class="built_in">size</span>();
  <span class="built_in">int</span> takes = takeList.<span class="built_in">size</span>();
  <span class="keyword">synchronized</span>(queueLock) {
    <span class="keyword">if</span>(puts &gt; <span class="number">0</span> ) {
      <span class="keyword">while</span>(!putList.isEmpty()) { <span class="comment">// 加入事件列表中不空的话</span>
        <span class="keyword">if</span>(!queue.offer(putList.removeFirst())) { <span class="comment">// 在MemoryChannel里的阻塞事件队列中加入MemoryTransaction的加入事件队列，也就是说在MemoryTransaction中新加入的所有事件全部移植到MemoryChannel中的阻塞队列中</span>
          <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Queue add failed, this shouldn't be able to happen"</span>);
        }
      }
    }
    putList.<span class="built_in">clear</span>();
    takeList.<span class="built_in">clear</span>();
  }
}
</code></pre><p>doRollback方法，回滚处理的事件, 回滚操作只针对Sink，回滚要交给Sink的事件到MemoryChannel的阻塞队列里：</p>
<pre><code>@<span class="function">Override
<span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doRollback</span><span class="params">()</span> </span>{
  <span class="keyword">int</span> takes = takeList.size();
  synchronized(queueLock) {
    Preconditions.checkState(<span class="built_in">queue</span>.remainingCapacity() &gt;= takeList.size(), <span class="string">"Not enough space in memory channel "</span> +
        <span class="string">"queue to rollback takes. This should never happen, please report"</span>);
    <span class="keyword">while</span>(!takeList.isEmpty()) {
      <span class="comment">// MemoryTransaction处理的事件全部回滚，回到MemoryChannel的阻塞队列里。因为MemoryTransaction处理的事件数据是从MemoryChannel的阻塞队列里取走的</span>
      <span class="built_in">queue</span>.addFirst(takeList.removeLast());
    }
    putList.clear();
  }
  bytesRemaining.release(putByteCounter);
  putByteCounter = <span class="number">0</span>;
  takeByteCounter = <span class="number">0</span>;

  queueStored.release(takes);
  channelCounter.setChannelSize(<span class="built_in">queue</span>.size());
}
</code></pre><p>MemoryChannel总结：</p>
<p>MemoryChannel是个Channel，Channel的作用是收集Source发来的事件数据(put操作)和发送给Sink由Source发来的事件数据(take操作)。Channel内部对事件的处理使用Transaction完成。</p>
<p>put操作：先放到MemoryTransaction的putList列表中，MemoryTransaction做commit操作的时候写入到MemoryChannel的阻塞队列里。</p>
<p>take操作：先取出MemoryChannel中阻塞队列里的事件数据，然后放入MemoryTransaction里的takeList列表中，当需要回滚的时候会将takeList列表中的事件数据回滚到MemoryChannel中的阻塞队列里。commit操作不需要任何处理，因为事件已经从MemoryChannel中的阻塞队列里取出。</p>
<p>MemoryChannel有2个队列容量配置，分别是capacity和transactionCapacity。capacity是MemoryChannel里的阻塞队列的容量，transactionCapacity是MemoryTransaction里的putList和takeList列表容量。</p>
<h2 id="Channel总结">Channel总结</h2><p>Channel是Flume中的桥梁，连接着Source和Sink,重要性不言而喻。Flume内置也有很多的Channel。同样地，并不是所有的Channel都能符合需求。</p>
<p>比如MemoryChannel在内存中处理，速度很快，但是却不能持久化，当服务器挂了，Channel中的数据会丢失，这时就需要用FileChannel或JDBCChannel，但这2种Channel速度慢。因此可以实现一个自定义的Channel，美团的<a href="http://tech.meituan.com/mt-log-system-optimization.html地址上就说明了一些Flume可以改进和优化的地方。" target="_blank" rel="external">http://tech.meituan.com/mt-log-system-optimization.html地址上就说明了一些Flume可以改进和优化的地方。</a></p>
]]></content>
    <summary type="html">
    <![CDATA[Flume内置了很多Channel，比如Memory Channel, JDBC Channel, Kafka Channel, File Channel等 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Flume Source组件分析]]></title>
    <link href="http://fangjian0423.github.io/2015/06/21/flume-source/"/>
    <id>http://fangjian0423.github.io/2015/06/21/flume-source/</id>
    <published>2015-06-21T15:23:23.000Z</published>
    <updated>2015-12-20T16:56:32.000Z</updated>
    <content type="html"><![CDATA[<p>Flume的介绍以及它的架构之前已经分析过。本文分析flume的Source组件。</p>
<p>Flume内置了很多Source，比如Avro Source，Spooling Directory Source，NetCat Source，Kafka Source等。</p>
<p>以源码的角度分析Source组件。</p>
<h2 id="Source接口">Source接口</h2><p>Source接口的定义：</p>
<pre><code><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Source</span> <span class="keyword">extends</span> <span class="title">LifecycleAware</span>, <span class="title">NamedComponent</span> </span>{

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setChannelProcessor</span><span class="params">(ChannelProcessor channelProcessor)</span></span>;

  <span class="function"><span class="keyword">public</span> ChannelProcessor <span class="title">getChannelProcessor</span><span class="params">()</span></span>;

}
</code></pre><p>Source接口只有2个方法，分别是setChannelProcessor和getChannelProcessor。ChannelProcessor是一个Channel处理器，会暴露一些操作用来将事件存储到channel中。</p>
<p>Source接口继承了LifecycleAware和NamedComponent接口。</p>
<p>LifecycleAware接口表示实现了该接口的类是具有状态的，有生命周期的特性。</p>
<pre><code><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title">LifecycleAware</span> {

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span>(<span class="params"></span>)</span>; <span class="comment">// 启动一个服务或组件</span>

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">stop</span>(<span class="params"></span>)</span>; <span class="comment">// 关闭一个服务或组件</span>

  <span class="function"><span class="keyword">public</span> LifecycleState <span class="title">getLifecycleState</span>(<span class="params"></span>)</span>; <span class="comment">// 得到当前组件或服务的状态</span>

}
</code></pre><p>NamedComponent接口是一个可以让组件拥有名字的接口，这样组件可以在配置中被引用。</p>
<pre><code><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title">NamedComponent</span> {

  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span>(<span class="params">String name</span>)</span>;

  <span class="function"><span class="keyword">public</span> String <span class="title">getName</span>(<span class="params"></span>)</span>;

}
</code></pre><p>Source接口的实现类AbstractSource，基本上所有的Source都会继承这个AbstractSource：</p>
<p>AbstractSource属性：</p>
<pre><code><span class="keyword">private</span> ChannelProcessor channelProcessor;
<span class="keyword">private</span> <span class="keyword">String</span> name;
<span class="keyword">private</span> LifecycleState lifecycleState;
</code></pre><p>这3个属性是都是都会在Source接口的方法中被使用到。</p>
<pre><code><span class="annotation">@Override</span>
  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function"><span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>{
    Preconditions.checkState(channelProcessor != <span class="keyword">null</span>,
        <span class="string">"No channel processor configured"</span>);

    lifecycleState = LifecycleState.START;
  }

  <span class="annotation">@Override</span>
  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function"><span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span> </span>{
    lifecycleState = LifecycleState.STOP;
  }

  <span class="annotation">@Override</span>
  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function"><span class="keyword">void</span> <span class="title">setChannelProcessor</span><span class="params">(ChannelProcessor cp)</span> </span>{
    channelProcessor = cp;
  }

  <span class="annotation">@Override</span>
  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function">ChannelProcessor <span class="title">getChannelProcessor</span><span class="params">()</span> </span>{
    <span class="keyword">return</span> channelProcessor;
  }

  <span class="annotation">@Override</span>
  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function">LifecycleState <span class="title">getLifecycleState</span><span class="params">()</span> </span>{
    <span class="keyword">return</span> lifecycleState;
  }

  <span class="annotation">@Override</span>
  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function"><span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>{
    <span class="keyword">this</span>.name = name;
  }

  <span class="annotation">@Override</span>
  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function">String <span class="title">getName</span><span class="params">()</span> </span>{
    <span class="keyword">return</span> name;
  }
</code></pre><h2 id="Flume内置的Source例子">Flume内置的Source例子</h2><h3 id="NetCat_Source">NetCat Source</h3><p>首先看最简单的NetCat Source。</p>
<p>NetCat Source配置如下：</p>
<pre><code>a1<span class="class">.sources</span><span class="class">.r1</span><span class="class">.type</span> = netcat
a1<span class="class">.sources</span><span class="class">.r1</span><span class="class">.bind</span> = localhost
a1<span class="class">.sources</span><span class="class">.r1</span><span class="class">.port</span> = <span class="number">44444</span>
</code></pre><p>NetCat Source对应的Source类是NetcatSource，定义如下：</p>
<pre><code>public <span class="class"><span class="keyword">class</span> <span class="title">NetcatSource</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">AbstractSource</span> <span class="title">implements</span> <span class="title">Configurable</span>,
</span>        <span class="type">EventDrivenSource</span>
</code></pre><p>EventDrivenSource接口表示这个Source不需要外部驱动来收集event，而是自身会实现。这个接口只是一个标识，没有任何方法。</p>
<p>Configurable接口提供了一个public void configure(Context context);方法，会使用以下Context进行一些数据的配置。</p>
<p>NetcatSource覆盖了AbstractSource的start和stop方法，也实现了Configurable中的configure方法。</p>
<p>实现configure方法：</p>
<p>读取配置文件中的信息host和port信息(还有其他一些属性的配置，比如max-line-length和ack-every-event)。</p>
<p>覆盖start方法：</p>
<p>内部会使用ServerSocketChannel监听对应地址和端口发来的数据。</p>
<p>覆盖stop方法：</p>
<p>关闭ServerSocketChannel。</p>
<p>start方法内部会用线程池起一个新的进程来监听数据，其中有个processEvents方法，processEvents方法内部会读取Socket发来的数据，有段代码：</p>
<pre><code>Event <span class="keyword">event</span> = EventBuilder.withBody(body);
<span class="comment">// process event</span>
ChannelException ex = <span class="keyword">null</span>;
<span class="keyword">try</span> {
  source.getChannelProcessor().processEvent(<span class="keyword">event</span>);
} <span class="keyword">catch</span> (ChannelException chEx) {
  ex = chEx;
}
</code></pre><p>这段代码就会处理socket读取的数据并构造成一个Event，然后放到ChannelProcessor里，这个ChannelProcessor是在AbstractSource中定义的。</p>
<h3 id="Kafka_Source">Kafka Source</h3><p>Kafka Source也是Flume内置的一个Source之一，配置如下：</p>
<pre><code>tier1<span class="class">.sources</span><span class="class">.source1</span><span class="class">.type</span> = org<span class="class">.apache</span><span class="class">.flume</span><span class="class">.source</span><span class="class">.kafka</span><span class="class">.KafkaSource</span>
tier1<span class="class">.sources</span><span class="class">.source1</span><span class="class">.channels</span> = channel1
tier1<span class="class">.sources</span><span class="class">.source1</span><span class="class">.zookeeperConnect</span> = localhost:<span class="number">2181</span>
tier1<span class="class">.sources</span><span class="class">.source1</span><span class="class">.topic</span> = test1
tier1<span class="class">.sources</span><span class="class">.source1</span><span class="class">.groupId</span> = flume
tier1<span class="class">.sources</span><span class="class">.source1</span><span class="class">.kafka</span><span class="class">.consumer</span><span class="class">.timeout</span><span class="class">.ms</span> = <span class="number">100</span>
</code></pre><p>对应的Source类是KafkaSource：</p>
<pre><code>public <span class="class"><span class="keyword">class</span> <span class="title">KafkaSource</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">AbstractSource</span>
</span>    implements <span class="type">Configurable</span>, <span class="type">PollableSource</span>
</code></pre><p>PollableSource接口表示Source需要自己去查询数据(poll)。</p>
<pre><code>public interface <span class="constant">PollableSource</span> extends <span class="constant">Source</span> {

  <span class="regexp">//</span> process方法的返回值是个<span class="constant">Status</span>，有<span class="number">2</span>个状态，分别是<span class="constant">READY</span>和<span class="constant">BACKOFF</span>
  public <span class="constant">Status</span> process() throws <span class="constant">EventDeliveryException</span>;

  public static <span class="class"><span class="keyword">enum</span> <span class="title">Status</span> {</span>
    <span class="constant">READY</span>, <span class="constant">BACKOFF</span>
  }
}
</code></pre><p>KafkaSource的configure方法同样是读取配置文件信息。KafkaSource有几个特殊的配置，zookeeperConnect，groupId，topic都是kafka本身需要的配置。batchSize和batchDurationMillis是批处理的配置。batchSize表示批次数，每batchSize个event需要写入到Channel中，默认值是1000。batchDurationMillis是处理Kafka对i读取数据的时间，默认是1000毫秒，比如批次个数没有到1000，但是batchDurationMillis到了的话还是会丢到channel里。</p>
<p>start方法构造Kafka的ConsumerConnector，start方法还会调用父类AbstractSource的start方法，也就是初始化lifecycleState属性，说明这个Source是有生命周期的。</p>
<p>stop方法关闭Kafka的ConsumerConnector，同理stop也会调用父类的stop方法。</p>
<p>process方法是PollableSource接口的方法，KafkaSource需要我们自己去查询数据。</p>
<p>process部分源码如下：</p>
<pre><code><span class="keyword">long</span> batchStartTime = System.currentTimeMillis();
<span class="keyword">long</span> batchEndTime = System.currentTimeMillis() + timeUpperLimit;
<span class="keyword">try</span> {
  <span class="built_in">boolean</span> iterStatus = <span class="keyword">false</span>;
  <span class="comment">// 批次个数和处理时间全部符合才会进行处理</span>
  <span class="keyword">while</span> (eventList.<span class="built_in">size</span>() &lt; batchUpperLimit &amp;&amp;
          System.currentTimeMillis() &lt; batchEndTime) {
    iterStatus = hasNext();
    <span class="keyword">if</span> (iterStatus) {
      <span class="comment">// kafka队列存在数据的话，提取数据，构造Event，放到eventList里，eventList是KafkaSource里的一个属性</span>
      MessageAndMetadata&lt;<span class="built_in">byte</span>[], <span class="built_in">byte</span>[]&gt; messageAndMetadata = it.next();
      kafkaMessage = messageAndMetadata.message();
      kafkaKey = messageAndMetadata.<span class="variable">key</span>();

      headers = <span class="keyword">new</span> <span class="keyword">HashMap</span>&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;();
      headers.put(KafkaSourceConstants.TIMESTAMP,
              <span class="keyword">String</span>.valueOf(System.currentTimeMillis()));
      headers.put(KafkaSourceConstants.TOPIC, topic);
      headers.put(KafkaSourceConstants.KEY, <span class="keyword">new</span> <span class="keyword">String</span>(kafkaKey));

      event = EventBuilder.withBody(kafkaMessage, headers);
      eventList.<span class="built_in">add</span>(event);
    }

  }
  <span class="keyword">if</span> (eventList.<span class="built_in">size</span>() &gt; <span class="number">0</span>) {
    <span class="comment">// eventList列表里有数据的话，将数据丢到channel里，清空eventList。代码执行到这里说明要么读取了1000条数据，要么处理时间到了</span>
    getChannelProcessor().processEventBatch(eventList);
    eventList.<span class="built_in">clear</span>();

    <span class="keyword">if</span> (!kafkaAutoCommitEnabled) {
      consumer.commitOffsets();
    }
  }
  <span class="comment">// kafka队列没有数据的话返回BACKOFF状态</span>
  <span class="keyword">if</span> (!iterStatus) {
    <span class="keyword">return</span> Status.BACKOFF;
  }
  <span class="comment">// kafka队列还有数据的话返回READY状态</span>
  <span class="keyword">return</span> Status.READY;
} <span class="keyword">catch</span> (Exception e) {
  <span class="built_in">log</span>.error(<span class="string">"KafkaSource EXCEPTION, {}"</span>, e);
  <span class="keyword">return</span> Status.BACKOFF;
}
</code></pre><h3 id="编写自定义的Source">编写自定义的Source</h3><p>Flume尽管已经提供了不少Source，但始终无法满足所有的需求。 比如数据库中的数据想使用Flume写入了其他渠道。</p>
<p>编写SQLSource只需要继承AbstractSource，且实现Configurable和PollableSource接口，SQLSource跟KafkaSource一样都需要自身去查询数据，所以都得实现PollableSource接口。</p>
<p>SQLSource在github上已经有人实现过了: <a href="https://github.com/keedio/flume-ng-sql-source" target="_blank" rel="external">https://github.com/keedio/flume-ng-sql-source</a></p>
]]></content>
    <summary type="html">
    <![CDATA[Flume内置了很多Source，比如Avro Source，Spooling Directory Source，NetCat Source，Kafka Source等 ...]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Flume介绍]]></title>
    <link href="http://fangjian0423.github.io/2015/06/21/flume-introduction/"/>
    <id>http://fangjian0423.github.io/2015/06/21/flume-introduction/</id>
    <published>2015-06-21T13:23:34.000Z</published>
    <updated>2015-12-20T16:51:17.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://flume.apache.org/" target="_blank" rel="external">Flume</a>是一个分布式的，效率高的用来收集日志数据的开源框架。它的架构是基于流式数据，有3个重要的组件，分别是Source，Channel和Sink。</p>
<h2 id="Flume架构和特点">Flume架构和特点</h2><p><img src="http://7x2wh6.com1.z0.glb.clouddn.com/flume1.png" alt=""></p>
<p>Flume架构图如上，非常简单。</p>
<p>一个Flume的事件(event)表示数据流中的一个单位，它会带有字节数据和可选的字符串属性。一个Flume的agent是一个JVM进程，agent持有3个组件，这3个组件分别是Source，Channel和Sink。</p>
<p>Source组件会消费来自外部的一些事件源数据，这个外部事件源比如是一个web服务器。外部事件源会将事件以某种格式发送给Flume的Source，当Source接收到事件之后，会存储到一个或多个Channel中。</p>
<p>Channel是一个被动型的存储容器，它会一直保留事件直到事件被Sink消费。</p>
<p>Sink会消费Channel里的事件，然后会将事件放到外部仓库里，比如hdfs；或者Sink会转发到下一步Flume agent里做重复的事情。</p>
<p>Source和Sink在agent里异步执行处理channel里的事件。</p>
<p>Flume内部提供了一些常用的Source，Channel和Sink。</p>
<p>举个例子：</p>
<p>Source使用Spooling Directory Source，这个Source会读取文件系统中的文件数据(文件系统中的外部数据相当于之前说的事件源)，读取数据之后会放到Channel中，比如使用Memory Channel,将Source接收到的事件存储到内存中，最后使用HDFS这个Sink将Memory Channel中的事件数据写入到hdfs中。</p>
<p>Spooling Directory Source， Memory Channel, HDFS Sink都是Flume内部提供的组件。</p>
<p>Flume非常可靠。每个agent的事件从Source进入到Channel之后，会存储在Channel中。这些事件只有在<strong>存储到下一步agent的Channel中或者外部存储仓库(比如hdfs)</strong>后，才会在Channel中被移除。</p>
<p>Flume还会使用事务来保证事件处理过程。</p>
<p>Flume还具有很高的可恢复性。事件是存储在channel中的，当使用File Channel的时候，当服务器挂了之后这些文件都还在，但是如果使用的是Memory Channel，就不具备容灾性。</p>
<p>一个简单的Flume配置如下：</p>
<pre><code><span class="preprocessor"># agent名字是a1，有<span class="number">1</span>个source:r1, <span class="number">1</span>个sink:k1, <span class="number">1</span>个channel:c1</span>
a1.sources = r1
a1.sinks = k1
a1.channels = c1

<span class="preprocessor"># 使用netcat source</span>
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = <span class="number">44444</span>

<span class="preprocessor"># 使用 logger sink</span>
a1.sinks.k1.type = logger

<span class="preprocessor"># 使用memory channel，容器为<span class="number">1000</span>，事务容量为<span class="number">100</span></span>
a1.channels.c1.type = memory
a1.channels.c1.capacity = <span class="number">1000</span>
a1.channels.c1.transactionCapacity = <span class="number">100</span>

<span class="preprocessor"># 将channel讲source和sink关联</span>
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
</code></pre><p>之后至少还会有3篇Flume相关的文章，分别是讲解Source，Channel和Sink源码相关的文章。</p>
]]></content>
    <summary type="html">
    <![CDATA[Flume是一个分布式的，效率高的用来收集日志数据的开源框架。它的架构是基于流式数据，有3个重要的组件，分别是Source，Channel和Sink]]>
    
    </summary>
    
      <category term="big data" scheme="http://fangjian0423.github.io/tags/big-data/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/tags/flume/"/>
    
      <category term="flume" scheme="http://fangjian0423.github.io/categories/flume/"/>
    
  </entry>
  
</feed>
